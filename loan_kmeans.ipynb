{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "153c4fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import learning_curve\n",
    "import time\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "from bench_k_means_loan import bench_k_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27bbdd2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_all = pd.read_csv('data_all.csv')\n",
    "X = data_all.drop(columns='Loan_Status')\n",
    "Y = data_all['Loan_Status']\n",
    "col_names = X.columns\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "X = pd.DataFrame(X_scaled, columns=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "096692a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# digits: 2; # samples: 373; # features 11\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "all_data = X.to_numpy()\n",
    "all_labels = Y.to_numpy()\n",
    "data = X_train.to_numpy()\n",
    "test_data = X_test.to_numpy()\n",
    "labels = Y_train.to_numpy()\n",
    "test_labels = Y_test.to_numpy()\n",
    "\n",
    "\n",
    "(n_samples, n_features), n_digits = all_data.shape, np.unique(all_labels).size\n",
    "print(f\"# digits: {n_digits}; # samples: {n_samples}; # features {n_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7935309",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ZhangshiLiu\\anaconda3\\envs\\a3\\lib\\site-packages\\sklearn\\decomposition\\_fastica.py:542: FutureWarning: Starting in v1.3, whiten='unit-variance' will be used by default.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "n_components = min(7, n_features)\n",
    "\n",
    "\n",
    "# Dimensionality Reduction\n",
    "pca = PCA(n_components=n_components).fit(all_data)\n",
    "pca_data = pca.transform(data)\n",
    "pca_test_data = pca.transform(test_data)\n",
    "\n",
    "ica = FastICA(n_components=n_components).fit(all_data)\n",
    "ica_data = ica.transform(data)\n",
    "ica_test_data = ica.transform(test_data)\n",
    "\n",
    "rp = GaussianRandomProjection(n_components=n_components).fit(all_data)\n",
    "rp_data = rp.transform(data)\n",
    "rp_test_data = rp.transform(test_data)\n",
    "\n",
    "lda = LinearDiscriminantAnalysis(n_components=min(n_digits, n_features)-1).fit(all_data, all_labels)\n",
    "lda_data = lda.transform(data)\n",
    "lda_test_data = lda.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3746659f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pca, ica, rp, lda\n",
    "data = rp_data\n",
    "test_data =rp_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ed95077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: accuracy 0.7987\n",
      "Testing: accuracy: 0.7067\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.7666666666666667"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nn_model = MLPClassifier(alpha=4, hidden_layer_sizes=(16, 32, 8, 4),\n",
    "              learning_rate_init=0.0001, max_iter=100000, n_iter_no_change=2000,\n",
    "              random_state=0, early_stopping=True)\n",
    "\n",
    "nn_model.fit(data, labels)\n",
    "\n",
    "score = nn_model.score(data, labels)\n",
    "print(\"Training: accuracy %0.4f\" % (score))\n",
    "score = nn_model.score(test_data, test_labels)\n",
    "print(\"Testing: accuracy: %0.4f\\n\" % (score))\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\n",
    "ax[0].plot(nn_model.loss_curve_)\n",
    "ax[0].set_xlabel('Number of iterations')\n",
    "ax[0].set_ylabel('Loss')\n",
    "plt.ylim(0,1)\n",
    "\n",
    "ax[1].plot(nn_model.validation_scores_)\n",
    "ax[1].set_xlabel('Number of iterations')\n",
    "ax[1].set_ylabel('Accuracy')\n",
    "\n",
    "plt.ylim(0,1)\n",
    "plt.show()\n",
    "nn_model.validation_scores_[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d6140da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time:  6.5953638553619385\n"
     ]
    }
   ],
   "source": [
    "nn_model = MLPClassifier(solver='adam', hidden_layer_sizes=(16,32,8,4), alpha=4, n_iter_no_change=2000, early_stopping=True, learning_rate_init=0.0001,random_state=0, max_iter=200000)\n",
    "start = time.time()\n",
    "nn_model.fit(data, labels)\n",
    "stop = time.time()\n",
    "print(\"Training Time: \", (stop-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bf09c9d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\n",
    "ax[0].plot(nn_model.loss_curve_)\n",
    "ax[0].set_xlabel('Number of iterations')\n",
    "ax[0].set_ylabel('Loss')\n",
    "plt.ylim(0,1)\n",
    "\n",
    "\n",
    "ax[1].plot(nn_model.validation_scores_)\n",
    "ax[1].set_xlabel('Number of iterations')\n",
    "ax[1].set_ylabel('Accuracy')\n",
    "\n",
    "plt.ylim(0,1)\n",
    "plt.show()\n",
    "nn_model.validation_scores_[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "634ba059",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ZhangshiLiu\\anaconda3\\envs\\a3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ZhangshiLiu\\anaconda3\\envs\\a3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ZhangshiLiu\\anaconda3\\envs\\a3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ZhangshiLiu\\anaconda3\\envs\\a3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ZhangshiLiu\\anaconda3\\envs\\a3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "n_init = 5\n",
    "n_digits = 2\n",
    "\n",
    "vanilla_KMeans = KMeans(init=\"k-means++\", n_clusters=n_digits, n_init=n_init, random_state=0).fit(data)\n",
    "new_data = vanilla_KMeans.transform(data)\n",
    "new_test_data = vanilla_KMeans.transform(test_data)\n",
    "\n",
    "\n",
    "# kmeans + PCA\n",
    "pca_KMeans = KMeans(init=\"k-means++\", n_clusters=n_digits, n_init=n_init, random_state=0).fit(pca_data)\n",
    "new_pca_data = pca_KMeans.transform(pca_data)\n",
    "new_pca_test_data = pca_KMeans.transform(pca_test_data)\n",
    "\n",
    "\n",
    "# kmeans + ICA\n",
    "ica_KMeans = KMeans(init=\"k-means++\", n_clusters=n_digits, n_init=n_init, random_state=0).fit(ica_data)\n",
    "new_ica_data = ica_KMeans.transform(ica_data)\n",
    "new_ica_test_data = ica_KMeans.transform(ica_test_data)\n",
    "\n",
    "\n",
    "# kmeans + random projection\n",
    "rp_KMeans = KMeans(init=\"k-means++\", n_clusters=n_digits, n_init=n_init, random_state=0).fit(rp_data)\n",
    "new_rp_data = rp_KMeans.transform(rp_data)\n",
    "new_rp_test_data = rp_KMeans.transform(rp_test_data)\n",
    "\n",
    "\n",
    "# kmeans + LDA\n",
    "lda_KMeans = KMeans(init=\"k-means++\", n_clusters=n_digits, n_init=n_init, random_state=0).fit(lda_data)\n",
    "new_lda_data = lda_KMeans.transform(lda_data)\n",
    "new_lda_test_data = lda_KMeans.transform(lda_test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4fbbf53e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x1200 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(12, 12))\n",
    "\n",
    "axs[0, 0].scatter(new_pca_data[:, 0], new_pca_data[:, 1], c=labels)\n",
    "axs[0, 0].set_title(\"K_Means + PCA\")\n",
    "\n",
    "axs[0, 1].scatter(new_ica_data[:, 0], new_ica_data[:, 1], c=labels)\n",
    "axs[0, 1].set_title(\"K_Means + ICA\")\n",
    "\n",
    "axs[1, 0].scatter(new_rp_data[:, 0], new_rp_data[:, 1], c=labels)\n",
    "axs[1, 0].set_title(\"K_Means + Random Projection\")\n",
    "\n",
    "#axs[1, 1].scatter(lda_data[:, 0], lda_data[:, 1], c=labels)\n",
    "#axs[1, 1].set_title(\"LDA\")\n",
    "\n",
    "plt.suptitle(\"K_Means + Dimension Reduction\").set_y(0.95)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0d542b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = new_rp_data\n",
    "new_test_data = new_rp_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea756a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: accuracy 0.7987\n",
      "Testing: accuracy: 0.7067\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.7666666666666667"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model = MLPClassifier(alpha=4, hidden_layer_sizes=(16, 32, 8, 4),\n",
    "              learning_rate_init=0.0001, max_iter=100000, n_iter_no_change=2000,\n",
    "              random_state=0, early_stopping=True)\n",
    "\n",
    "nn_model.fit(data, labels)\n",
    "\n",
    "score = nn_model.score(data, labels)\n",
    "print(\"Training: accuracy %0.4f\" % (score))\n",
    "score = nn_model.score(test_data, test_labels)\n",
    "print(\"Testing: accuracy: %0.4f\\n\" % (score))\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\n",
    "ax[0].plot(nn_model.loss_curve_)\n",
    "ax[0].set_xlabel('Number of iterations')\n",
    "ax[0].set_ylabel('Loss')\n",
    "plt.ylim(0,1)\n",
    "\n",
    "ax[1].plot(nn_model.validation_scores_)\n",
    "ax[1].set_xlabel('Number of iterations')\n",
    "ax[1].set_ylabel('Accuracy')\n",
    "\n",
    "plt.ylim(0,1)\n",
    "plt.show()\n",
    "nn_model.validation_scores_[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6853fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.63529663\n",
      "Iteration 2, loss = 0.63493072\n",
      "Iteration 3, loss = 0.63462219\n",
      "Iteration 4, loss = 0.63427591\n",
      "Iteration 5, loss = 0.63404237\n",
      "Iteration 6, loss = 0.63378190\n",
      "Iteration 7, loss = 0.63349533\n",
      "Iteration 8, loss = 0.63327058\n",
      "Iteration 9, loss = 0.63305298\n",
      "Iteration 10, loss = 0.63283319\n",
      "Iteration 11, loss = 0.63261140\n",
      "Iteration 12, loss = 0.63236389\n",
      "Iteration 13, loss = 0.63214884\n",
      "Iteration 14, loss = 0.63196884\n",
      "Iteration 15, loss = 0.63176360\n",
      "Iteration 16, loss = 0.63154558\n",
      "Iteration 17, loss = 0.63136352\n",
      "Iteration 18, loss = 0.63116788\n",
      "Iteration 19, loss = 0.63096204\n",
      "Iteration 20, loss = 0.63081422\n",
      "Iteration 21, loss = 0.63063207\n",
      "Iteration 22, loss = 0.63041871\n",
      "Iteration 23, loss = 0.63025431\n",
      "Iteration 24, loss = 0.63007697\n",
      "Iteration 25, loss = 0.62990206\n",
      "Iteration 26, loss = 0.62973018\n",
      "Iteration 27, loss = 0.62957104\n",
      "Iteration 28, loss = 0.62940921\n",
      "Iteration 29, loss = 0.62923754\n",
      "Iteration 30, loss = 0.62903049\n",
      "Iteration 31, loss = 0.62891208\n",
      "Iteration 32, loss = 0.62872853\n",
      "Iteration 33, loss = 0.62859624\n",
      "Iteration 34, loss = 0.62841802\n",
      "Iteration 35, loss = 0.62828736\n",
      "Iteration 36, loss = 0.62812915\n",
      "Iteration 37, loss = 0.62799417\n",
      "Iteration 38, loss = 0.62786574\n",
      "Iteration 39, loss = 0.62773061\n",
      "Iteration 40, loss = 0.62759159\n",
      "Iteration 41, loss = 0.62747745\n",
      "Iteration 42, loss = 0.62732763\n",
      "Iteration 43, loss = 0.62719754\n",
      "Iteration 44, loss = 0.62706435\n",
      "Iteration 45, loss = 0.62697629\n",
      "Iteration 46, loss = 0.62681586\n",
      "Iteration 47, loss = 0.62668810\n",
      "Iteration 48, loss = 0.62658827\n",
      "Iteration 49, loss = 0.62648597\n",
      "Iteration 50, loss = 0.62637808\n",
      "Iteration 51, loss = 0.62630621\n",
      "Iteration 52, loss = 0.62625739\n",
      "Iteration 53, loss = 0.62615979\n",
      "Iteration 54, loss = 0.62606636\n",
      "Iteration 55, loss = 0.62597173\n",
      "Iteration 56, loss = 0.62588205\n",
      "Iteration 57, loss = 0.62582936\n",
      "Iteration 58, loss = 0.62572563\n",
      "Iteration 59, loss = 0.62562406\n",
      "Iteration 60, loss = 0.62552769\n",
      "Iteration 61, loss = 0.62539787\n",
      "Iteration 62, loss = 0.62527129\n",
      "Iteration 63, loss = 0.62514723\n",
      "Iteration 64, loss = 0.62499987\n",
      "Iteration 65, loss = 0.62489617\n",
      "Iteration 66, loss = 0.62476590\n",
      "Iteration 67, loss = 0.62466599\n",
      "Iteration 68, loss = 0.62455925\n",
      "Iteration 69, loss = 0.62446787\n",
      "Iteration 70, loss = 0.62437092\n",
      "Iteration 71, loss = 0.62427794\n",
      "Iteration 72, loss = 0.62415699\n",
      "Iteration 73, loss = 0.62406291\n",
      "Iteration 74, loss = 0.62396129\n",
      "Iteration 75, loss = 0.62386657\n",
      "Iteration 76, loss = 0.62378724\n",
      "Iteration 77, loss = 0.62369561\n",
      "Iteration 78, loss = 0.62359089\n",
      "Iteration 79, loss = 0.62349830\n",
      "Iteration 80, loss = 0.62340337\n",
      "Iteration 81, loss = 0.62332144\n",
      "Iteration 82, loss = 0.62327061\n",
      "Iteration 83, loss = 0.62315347\n",
      "Iteration 84, loss = 0.62311150\n",
      "Iteration 85, loss = 0.62302265\n",
      "Iteration 86, loss = 0.62294534\n",
      "Iteration 87, loss = 0.62287354\n",
      "Iteration 88, loss = 0.62280109\n",
      "Iteration 89, loss = 0.62273610\n",
      "Iteration 90, loss = 0.62266695\n",
      "Iteration 91, loss = 0.62260323\n",
      "Iteration 92, loss = 0.62254076\n",
      "Iteration 93, loss = 0.62247814\n",
      "Iteration 94, loss = 0.62241444\n",
      "Iteration 95, loss = 0.62233998\n",
      "Iteration 96, loss = 0.62225931\n",
      "Iteration 97, loss = 0.62217261\n",
      "Iteration 98, loss = 0.62206555\n",
      "Iteration 99, loss = 0.62203414\n",
      "Iteration 100, loss = 0.62193663\n",
      "Iteration 101, loss = 0.62195013\n",
      "Iteration 102, loss = 0.62179979\n",
      "Iteration 103, loss = 0.62174413\n",
      "Iteration 104, loss = 0.62171956\n",
      "Iteration 105, loss = 0.62165419\n",
      "Iteration 106, loss = 0.62159973\n",
      "Iteration 107, loss = 0.62154381\n",
      "Iteration 108, loss = 0.62149755\n",
      "Iteration 109, loss = 0.62144064\n",
      "Iteration 110, loss = 0.62138216\n",
      "Iteration 111, loss = 0.62133494\n",
      "Iteration 112, loss = 0.62130979\n",
      "Iteration 113, loss = 0.62124222\n",
      "Iteration 114, loss = 0.62118028\n",
      "Iteration 115, loss = 0.62111834\n",
      "Iteration 116, loss = 0.62111022\n",
      "Iteration 117, loss = 0.62100637\n",
      "Iteration 118, loss = 0.62095327\n",
      "Iteration 119, loss = 0.62090209\n",
      "Iteration 120, loss = 0.62085145\n",
      "Iteration 121, loss = 0.62080123\n",
      "Iteration 122, loss = 0.62075927\n",
      "Iteration 123, loss = 0.62070161\n",
      "Iteration 124, loss = 0.62066302\n",
      "Iteration 125, loss = 0.62061036\n",
      "Iteration 126, loss = 0.62055773\n",
      "Iteration 127, loss = 0.62051195\n",
      "Iteration 128, loss = 0.62047294\n",
      "Iteration 129, loss = 0.62040927\n",
      "Iteration 130, loss = 0.62035969\n",
      "Iteration 131, loss = 0.62034550\n",
      "Iteration 132, loss = 0.62029591\n",
      "Iteration 133, loss = 0.62025480\n",
      "Iteration 134, loss = 0.62021366\n",
      "Iteration 135, loss = 0.62018085\n",
      "Iteration 136, loss = 0.62014981\n",
      "Iteration 137, loss = 0.62011230\n",
      "Iteration 138, loss = 0.62011030\n",
      "Iteration 139, loss = 0.62007095\n",
      "Iteration 140, loss = 0.62003137\n",
      "Iteration 141, loss = 0.61999206\n",
      "Iteration 142, loss = 0.61996586\n",
      "Iteration 143, loss = 0.61990943\n",
      "Iteration 144, loss = 0.61989723\n",
      "Iteration 145, loss = 0.61984903\n",
      "Iteration 146, loss = 0.61980361\n",
      "Iteration 147, loss = 0.61976303\n",
      "Iteration 148, loss = 0.61972907\n",
      "Iteration 149, loss = 0.61968043\n",
      "Iteration 150, loss = 0.61964805\n",
      "Iteration 151, loss = 0.61959766\n",
      "Iteration 152, loss = 0.61956281\n",
      "Iteration 153, loss = 0.61952332\n",
      "Iteration 154, loss = 0.61948597\n",
      "Iteration 155, loss = 0.61943782\n",
      "Iteration 156, loss = 0.61940706\n",
      "Iteration 157, loss = 0.61934484\n",
      "Iteration 158, loss = 0.61933317\n",
      "Iteration 159, loss = 0.61927281\n",
      "Iteration 160, loss = 0.61924164\n",
      "Iteration 161, loss = 0.61928316\n",
      "Iteration 162, loss = 0.61917998\n",
      "Iteration 163, loss = 0.61919163\n",
      "Iteration 164, loss = 0.61913684\n",
      "Iteration 165, loss = 0.61910682\n",
      "Iteration 166, loss = 0.61907628\n",
      "Iteration 167, loss = 0.61904869\n",
      "Iteration 168, loss = 0.61901378\n",
      "Iteration 169, loss = 0.61900583\n",
      "Iteration 170, loss = 0.61895962\n",
      "Iteration 171, loss = 0.61895252\n",
      "Iteration 172, loss = 0.61890606\n",
      "Iteration 173, loss = 0.61888155\n",
      "Iteration 174, loss = 0.61887849\n",
      "Iteration 175, loss = 0.61884745\n",
      "Iteration 176, loss = 0.61881670\n",
      "Iteration 177, loss = 0.61881232\n",
      "Iteration 178, loss = 0.61876834\n",
      "Iteration 179, loss = 0.61875593\n",
      "Iteration 180, loss = 0.61871176\n",
      "Iteration 181, loss = 0.61866547\n",
      "Iteration 182, loss = 0.61871732\n",
      "Iteration 183, loss = 0.61884779\n",
      "Iteration 184, loss = 0.61869208\n",
      "Iteration 185, loss = 0.61860931\n",
      "Iteration 186, loss = 0.61859008\n",
      "Iteration 187, loss = 0.61856966\n",
      "Iteration 188, loss = 0.61855107\n",
      "Iteration 189, loss = 0.61852755\n",
      "Iteration 190, loss = 0.61852376\n",
      "Iteration 191, loss = 0.61851199\n",
      "Iteration 192, loss = 0.61848210\n",
      "Iteration 193, loss = 0.61846631\n",
      "Iteration 194, loss = 0.61843951\n",
      "Iteration 195, loss = 0.61842025\n",
      "Iteration 196, loss = 0.61840201\n",
      "Iteration 197, loss = 0.61838199\n",
      "Iteration 198, loss = 0.61836958\n",
      "Iteration 199, loss = 0.61835257\n",
      "Iteration 200, loss = 0.61833012\n",
      "Iteration 201, loss = 0.61831273\n",
      "Iteration 202, loss = 0.61829015\n",
      "Iteration 203, loss = 0.61827448\n",
      "Iteration 204, loss = 0.61825748\n",
      "Iteration 205, loss = 0.61823889\n",
      "Iteration 206, loss = 0.61822456\n",
      "Iteration 207, loss = 0.61819919\n",
      "Iteration 208, loss = 0.61819154\n",
      "Iteration 209, loss = 0.61825721\n",
      "Iteration 210, loss = 0.61818226\n",
      "Iteration 211, loss = 0.61813891\n",
      "Iteration 212, loss = 0.61809855\n",
      "Iteration 213, loss = 0.61807650\n",
      "Iteration 214, loss = 0.61809367\n",
      "Iteration 215, loss = 0.61809153\n",
      "Iteration 216, loss = 0.61807851\n",
      "Iteration 217, loss = 0.61807071\n",
      "Iteration 218, loss = 0.61805038\n",
      "Iteration 219, loss = 0.61803667\n",
      "Iteration 220, loss = 0.61803139\n",
      "Iteration 221, loss = 0.61801168\n",
      "Iteration 222, loss = 0.61801061\n",
      "Iteration 223, loss = 0.61798655\n",
      "Iteration 224, loss = 0.61796705\n",
      "Iteration 225, loss = 0.61795853\n",
      "Iteration 226, loss = 0.61793671\n",
      "Iteration 227, loss = 0.61792496\n",
      "Iteration 228, loss = 0.61790893\n",
      "Iteration 229, loss = 0.61789158\n",
      "Iteration 230, loss = 0.61788592\n",
      "Iteration 231, loss = 0.61787397\n",
      "Iteration 232, loss = 0.61785221\n",
      "Iteration 233, loss = 0.61785411\n",
      "Iteration 234, loss = 0.61782987\n",
      "Iteration 235, loss = 0.61782021\n",
      "Iteration 236, loss = 0.61780686\n",
      "Iteration 237, loss = 0.61779356\n",
      "Iteration 238, loss = 0.61779160\n",
      "Iteration 239, loss = 0.61777177\n",
      "Iteration 240, loss = 0.61776751\n",
      "Iteration 241, loss = 0.61775841\n",
      "Iteration 242, loss = 0.61776047\n",
      "Iteration 243, loss = 0.61773043\n",
      "Iteration 244, loss = 0.61772339\n",
      "Iteration 245, loss = 0.61771435\n",
      "Iteration 246, loss = 0.61769775\n",
      "Iteration 247, loss = 0.61775144\n",
      "Iteration 248, loss = 0.61768873\n",
      "Iteration 249, loss = 0.61767778\n",
      "Iteration 250, loss = 0.61766828\n",
      "Iteration 251, loss = 0.61765723\n",
      "Iteration 252, loss = 0.61765085\n",
      "Iteration 253, loss = 0.61766420\n",
      "Iteration 254, loss = 0.61763955\n",
      "Iteration 255, loss = 0.61762100\n",
      "Iteration 256, loss = 0.61760508\n",
      "Iteration 257, loss = 0.61760612\n",
      "Iteration 258, loss = 0.61758393\n",
      "Iteration 259, loss = 0.61759044\n",
      "Iteration 260, loss = 0.61758828\n",
      "Iteration 261, loss = 0.61756065\n",
      "Iteration 262, loss = 0.61754957\n",
      "Iteration 263, loss = 0.61753579\n",
      "Iteration 264, loss = 0.61752521\n",
      "Iteration 265, loss = 0.61754758\n",
      "Iteration 266, loss = 0.61751804\n",
      "Iteration 267, loss = 0.61750945\n",
      "Iteration 268, loss = 0.61752638\n",
      "Iteration 269, loss = 0.61752679\n",
      "Iteration 270, loss = 0.61751573\n",
      "Iteration 271, loss = 0.61751305\n",
      "Iteration 272, loss = 0.61754582\n",
      "Iteration 273, loss = 0.61753227\n",
      "Iteration 274, loss = 0.61753107\n",
      "Iteration 275, loss = 0.61756204\n",
      "Iteration 276, loss = 0.61757697\n",
      "Iteration 277, loss = 0.61759662\n",
      "Iteration 278, loss = 0.61763575\n",
      "Iteration 279, loss = 0.61764374\n",
      "Iteration 280, loss = 0.61764109\n",
      "Iteration 281, loss = 0.61763778\n",
      "Iteration 282, loss = 0.61762902\n",
      "Iteration 283, loss = 0.61761182\n",
      "Iteration 284, loss = 0.61759992\n",
      "Iteration 285, loss = 0.61758632\n",
      "Iteration 286, loss = 0.61756995\n",
      "Iteration 287, loss = 0.61754976\n",
      "Iteration 288, loss = 0.61754831\n",
      "Iteration 289, loss = 0.61752030\n",
      "Iteration 290, loss = 0.61752297\n",
      "Iteration 291, loss = 0.61753054\n",
      "Iteration 292, loss = 0.61753515\n",
      "Iteration 293, loss = 0.61754168\n",
      "Iteration 294, loss = 0.61755397\n",
      "Iteration 295, loss = 0.61753788\n",
      "Iteration 296, loss = 0.61750897\n",
      "Iteration 297, loss = 0.61747257\n",
      "Iteration 298, loss = 0.61748076\n",
      "Iteration 299, loss = 0.61743208\n",
      "Iteration 300, loss = 0.61740916\n",
      "Iteration 301, loss = 0.61740190\n",
      "Iteration 302, loss = 0.61739525\n",
      "Iteration 303, loss = 0.61738848\n",
      "Iteration 304, loss = 0.61738410\n",
      "Iteration 305, loss = 0.61737450\n",
      "Iteration 306, loss = 0.61737042\n",
      "Iteration 307, loss = 0.61737095\n",
      "Iteration 308, loss = 0.61736207\n",
      "Iteration 309, loss = 0.61735357\n",
      "Iteration 310, loss = 0.61734431\n",
      "Iteration 311, loss = 0.61731978\n",
      "Iteration 312, loss = 0.61731635\n",
      "Iteration 313, loss = 0.61728426\n",
      "Iteration 314, loss = 0.61728611\n",
      "Iteration 315, loss = 0.61726421\n",
      "Iteration 316, loss = 0.61725428\n",
      "Iteration 317, loss = 0.61724443\n",
      "Iteration 318, loss = 0.61723788\n",
      "Iteration 319, loss = 0.61722943\n",
      "Iteration 320, loss = 0.61721533\n",
      "Iteration 321, loss = 0.61720810\n",
      "Iteration 322, loss = 0.61723018\n",
      "Iteration 323, loss = 0.61721093\n",
      "Iteration 324, loss = 0.61720349\n",
      "Iteration 325, loss = 0.61719711\n",
      "Iteration 326, loss = 0.61719474\n",
      "Iteration 327, loss = 0.61720409\n",
      "Iteration 328, loss = 0.61719119\n",
      "Iteration 329, loss = 0.61718270\n",
      "Iteration 330, loss = 0.61718843\n",
      "Iteration 331, loss = 0.61718550\n",
      "Iteration 332, loss = 0.61717350\n",
      "Iteration 333, loss = 0.61716955\n",
      "Iteration 334, loss = 0.61716696\n",
      "Iteration 335, loss = 0.61715589\n",
      "Iteration 336, loss = 0.61716328\n",
      "Iteration 337, loss = 0.61715795\n",
      "Iteration 338, loss = 0.61716619\n",
      "Iteration 339, loss = 0.61719367\n",
      "Iteration 340, loss = 0.61717915\n",
      "Iteration 341, loss = 0.61715753\n",
      "Iteration 342, loss = 0.61715970\n",
      "Iteration 343, loss = 0.61715729\n",
      "Iteration 344, loss = 0.61717346\n",
      "Iteration 345, loss = 0.61716900\n",
      "Iteration 346, loss = 0.61717021\n",
      "Iteration 347, loss = 0.61717765\n",
      "Iteration 348, loss = 0.61716812\n",
      "Iteration 349, loss = 0.61715416\n",
      "Iteration 350, loss = 0.61714775\n",
      "Iteration 351, loss = 0.61713127\n",
      "Iteration 352, loss = 0.61712549\n",
      "Iteration 353, loss = 0.61709787\n",
      "Iteration 354, loss = 0.61708218\n",
      "Iteration 355, loss = 0.61712167\n",
      "Iteration 356, loss = 0.61710667\n",
      "Iteration 357, loss = 0.61712324\n",
      "Iteration 358, loss = 0.61710152\n",
      "Iteration 359, loss = 0.61709997\n",
      "Iteration 360, loss = 0.61709772\n",
      "Iteration 361, loss = 0.61710441\n",
      "Iteration 362, loss = 0.61709839\n",
      "Iteration 363, loss = 0.61709670\n",
      "Iteration 364, loss = 0.61709529\n",
      "Iteration 365, loss = 0.61710108\n",
      "Iteration 366, loss = 0.61710683\n",
      "Iteration 367, loss = 0.61710175\n",
      "Iteration 368, loss = 0.61709797\n",
      "Iteration 369, loss = 0.61712109\n",
      "Iteration 370, loss = 0.61710590\n",
      "Iteration 371, loss = 0.61710423\n",
      "Iteration 372, loss = 0.61707992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 373, loss = 0.61708454\n",
      "Iteration 374, loss = 0.61709287\n",
      "Iteration 375, loss = 0.61707010\n",
      "Iteration 376, loss = 0.61706104\n",
      "Iteration 377, loss = 0.61706438\n",
      "Iteration 378, loss = 0.61705541\n",
      "Iteration 379, loss = 0.61706943\n",
      "Iteration 380, loss = 0.61705560\n",
      "Iteration 381, loss = 0.61704634\n",
      "Iteration 382, loss = 0.61704557\n",
      "Iteration 383, loss = 0.61704774\n",
      "Iteration 384, loss = 0.61704054\n",
      "Iteration 385, loss = 0.61702789\n",
      "Iteration 386, loss = 0.61702124\n",
      "Iteration 387, loss = 0.61702620\n",
      "Iteration 388, loss = 0.61705814\n",
      "Iteration 389, loss = 0.61705128\n",
      "Iteration 390, loss = 0.61708314\n",
      "Iteration 391, loss = 0.61709359\n",
      "Iteration 392, loss = 0.61708338\n",
      "Iteration 393, loss = 0.61707057\n",
      "Iteration 394, loss = 0.61706300\n",
      "Iteration 395, loss = 0.61705228\n",
      "Iteration 396, loss = 0.61705607\n",
      "Iteration 397, loss = 0.61703869\n",
      "Iteration 398, loss = 0.61703427\n",
      "Iteration 399, loss = 0.61702941\n",
      "Iteration 400, loss = 0.61702869\n",
      "Iteration 401, loss = 0.61702661\n",
      "Iteration 402, loss = 0.61701666\n",
      "Iteration 403, loss = 0.61703689\n",
      "Iteration 404, loss = 0.61704250\n",
      "Iteration 405, loss = 0.61703604\n",
      "Iteration 406, loss = 0.61703136\n",
      "Iteration 407, loss = 0.61702379\n",
      "Iteration 408, loss = 0.61702611\n",
      "Iteration 409, loss = 0.61701577\n",
      "Iteration 410, loss = 0.61701002\n",
      "Iteration 411, loss = 0.61699900\n",
      "Iteration 412, loss = 0.61698666\n",
      "Iteration 413, loss = 0.61701687\n",
      "Iteration 414, loss = 0.61698021\n",
      "Iteration 415, loss = 0.61697198\n",
      "Iteration 416, loss = 0.61696055\n",
      "Iteration 417, loss = 0.61700601\n",
      "Iteration 418, loss = 0.61698298\n",
      "Iteration 419, loss = 0.61698007\n",
      "Iteration 420, loss = 0.61697645\n",
      "Iteration 421, loss = 0.61697587\n",
      "Iteration 422, loss = 0.61697335\n",
      "Iteration 423, loss = 0.61698318\n",
      "Iteration 424, loss = 0.61697544\n",
      "Iteration 425, loss = 0.61696367\n",
      "Iteration 426, loss = 0.61697651\n",
      "Iteration 427, loss = 0.61696475\n",
      "Iteration 428, loss = 0.61696248\n",
      "Iteration 429, loss = 0.61696223\n",
      "Iteration 430, loss = 0.61695986\n",
      "Iteration 431, loss = 0.61696584\n",
      "Iteration 432, loss = 0.61695900\n",
      "Iteration 433, loss = 0.61696182\n",
      "Iteration 434, loss = 0.61696184\n",
      "Iteration 435, loss = 0.61696053\n",
      "Iteration 436, loss = 0.61695935\n",
      "Iteration 437, loss = 0.61695927\n",
      "Iteration 438, loss = 0.61697711\n",
      "Iteration 439, loss = 0.61695490\n",
      "Iteration 440, loss = 0.61695196\n",
      "Iteration 441, loss = 0.61695922\n",
      "Iteration 442, loss = 0.61694722\n",
      "Iteration 443, loss = 0.61694966\n",
      "Iteration 444, loss = 0.61694021\n",
      "Iteration 445, loss = 0.61694016\n",
      "Iteration 446, loss = 0.61693948\n",
      "Iteration 447, loss = 0.61693377\n",
      "Iteration 448, loss = 0.61693514\n",
      "Iteration 449, loss = 0.61693082\n",
      "Iteration 450, loss = 0.61692943\n",
      "Iteration 451, loss = 0.61692745\n",
      "Iteration 452, loss = 0.61692723\n",
      "Iteration 453, loss = 0.61692441\n",
      "Iteration 454, loss = 0.61692458\n",
      "Iteration 455, loss = 0.61692528\n",
      "Iteration 456, loss = 0.61693074\n",
      "Iteration 457, loss = 0.61692337\n",
      "Iteration 458, loss = 0.61692499\n",
      "Iteration 459, loss = 0.61692053\n",
      "Iteration 460, loss = 0.61691983\n",
      "Iteration 461, loss = 0.61691834\n",
      "Iteration 462, loss = 0.61691926\n",
      "Iteration 463, loss = 0.61691576\n",
      "Iteration 464, loss = 0.61691086\n",
      "Iteration 465, loss = 0.61693244\n",
      "Iteration 466, loss = 0.61691085\n",
      "Iteration 467, loss = 0.61690417\n",
      "Iteration 468, loss = 0.61691287\n",
      "Iteration 469, loss = 0.61692291\n",
      "Iteration 470, loss = 0.61691475\n",
      "Iteration 471, loss = 0.61691274\n",
      "Iteration 472, loss = 0.61691355\n",
      "Iteration 473, loss = 0.61692351\n",
      "Iteration 474, loss = 0.61691162\n",
      "Iteration 475, loss = 0.61691635\n",
      "Iteration 476, loss = 0.61691573\n",
      "Iteration 477, loss = 0.61691644\n",
      "Iteration 478, loss = 0.61691697\n",
      "Iteration 479, loss = 0.61691930\n",
      "Iteration 480, loss = 0.61692097\n",
      "Iteration 481, loss = 0.61691683\n",
      "Iteration 482, loss = 0.61691595\n",
      "Iteration 483, loss = 0.61691482\n",
      "Iteration 484, loss = 0.61691319\n",
      "Iteration 485, loss = 0.61692018\n",
      "Iteration 486, loss = 0.61692747\n",
      "Iteration 487, loss = 0.61695088\n",
      "Iteration 488, loss = 0.61694499\n",
      "Iteration 489, loss = 0.61694484\n",
      "Iteration 490, loss = 0.61694066\n",
      "Iteration 491, loss = 0.61695289\n",
      "Iteration 492, loss = 0.61692446\n",
      "Iteration 493, loss = 0.61693451\n",
      "Iteration 494, loss = 0.61696289\n",
      "Iteration 495, loss = 0.61696072\n",
      "Iteration 496, loss = 0.61697671\n",
      "Iteration 497, loss = 0.61696315\n",
      "Iteration 498, loss = 0.61696118\n",
      "Iteration 499, loss = 0.61695643\n",
      "Iteration 500, loss = 0.61695140\n",
      "Iteration 501, loss = 0.61695239\n",
      "Iteration 502, loss = 0.61695515\n",
      "Iteration 503, loss = 0.61694671\n",
      "Iteration 504, loss = 0.61694912\n",
      "Iteration 505, loss = 0.61694427\n",
      "Iteration 506, loss = 0.61695094\n",
      "Iteration 507, loss = 0.61693374\n",
      "Iteration 508, loss = 0.61693103\n",
      "Iteration 509, loss = 0.61692858\n",
      "Iteration 510, loss = 0.61692559\n",
      "Iteration 511, loss = 0.61692437\n",
      "Iteration 512, loss = 0.61691929\n",
      "Iteration 513, loss = 0.61692420\n",
      "Iteration 514, loss = 0.61692342\n",
      "Iteration 515, loss = 0.61692849\n",
      "Iteration 516, loss = 0.61692683\n",
      "Iteration 517, loss = 0.61693413\n",
      "Iteration 518, loss = 0.61692140\n",
      "Iteration 519, loss = 0.61691517\n",
      "Iteration 520, loss = 0.61690409\n",
      "Iteration 521, loss = 0.61689907\n",
      "Iteration 522, loss = 0.61688764\n",
      "Iteration 523, loss = 0.61688508\n",
      "Iteration 524, loss = 0.61687514\n",
      "Iteration 525, loss = 0.61686441\n",
      "Iteration 526, loss = 0.61684748\n",
      "Iteration 527, loss = 0.61683232\n",
      "Iteration 528, loss = 0.61687437\n",
      "Iteration 529, loss = 0.61684570\n",
      "Iteration 530, loss = 0.61683485\n",
      "Iteration 531, loss = 0.61683276\n",
      "Iteration 532, loss = 0.61683979\n",
      "Iteration 533, loss = 0.61684427\n",
      "Iteration 534, loss = 0.61683857\n",
      "Iteration 535, loss = 0.61682942\n",
      "Iteration 536, loss = 0.61683750\n",
      "Iteration 537, loss = 0.61682988\n",
      "Iteration 538, loss = 0.61682951\n",
      "Iteration 539, loss = 0.61682868\n",
      "Iteration 540, loss = 0.61682589\n",
      "Iteration 541, loss = 0.61682484\n",
      "Iteration 542, loss = 0.61682439\n",
      "Iteration 543, loss = 0.61680068\n",
      "Iteration 544, loss = 0.61681516\n",
      "Iteration 545, loss = 0.61682462\n",
      "Iteration 546, loss = 0.61682777\n",
      "Iteration 547, loss = 0.61682076\n",
      "Iteration 548, loss = 0.61683999\n",
      "Iteration 549, loss = 0.61683593\n",
      "Iteration 550, loss = 0.61683562\n",
      "Iteration 551, loss = 0.61684591\n",
      "Iteration 552, loss = 0.61684774\n",
      "Iteration 553, loss = 0.61684849\n",
      "Iteration 554, loss = 0.61689721\n",
      "Iteration 555, loss = 0.61684767\n",
      "Iteration 556, loss = 0.61683868\n",
      "Iteration 557, loss = 0.61685571\n",
      "Iteration 558, loss = 0.61683107\n",
      "Iteration 559, loss = 0.61682733\n",
      "Iteration 560, loss = 0.61682408\n",
      "Iteration 561, loss = 0.61682459\n",
      "Iteration 562, loss = 0.61682573\n",
      "Iteration 563, loss = 0.61682298\n",
      "Iteration 564, loss = 0.61682110\n",
      "Iteration 565, loss = 0.61682204\n",
      "Iteration 566, loss = 0.61682324\n",
      "Iteration 567, loss = 0.61682082\n",
      "Iteration 568, loss = 0.61682123\n",
      "Iteration 569, loss = 0.61682051\n",
      "Iteration 570, loss = 0.61682068\n",
      "Iteration 571, loss = 0.61683866\n",
      "Iteration 572, loss = 0.61684944\n",
      "Iteration 573, loss = 0.61684759\n",
      "Iteration 574, loss = 0.61686310\n",
      "Iteration 575, loss = 0.61684997\n",
      "Iteration 576, loss = 0.61684846\n",
      "Iteration 577, loss = 0.61684017\n",
      "Iteration 578, loss = 0.61683626\n",
      "Iteration 579, loss = 0.61683300\n",
      "Iteration 580, loss = 0.61683458\n",
      "Iteration 581, loss = 0.61682761\n",
      "Iteration 582, loss = 0.61683361\n",
      "Iteration 583, loss = 0.61682878\n",
      "Iteration 584, loss = 0.61683009\n",
      "Iteration 585, loss = 0.61682627\n",
      "Iteration 586, loss = 0.61682348\n",
      "Iteration 587, loss = 0.61682917\n",
      "Iteration 588, loss = 0.61681864\n",
      "Iteration 589, loss = 0.61681867\n",
      "Iteration 590, loss = 0.61682041\n",
      "Iteration 591, loss = 0.61681430\n",
      "Iteration 592, loss = 0.61681918\n",
      "Iteration 593, loss = 0.61681769\n",
      "Iteration 594, loss = 0.61681789\n",
      "Iteration 595, loss = 0.61681737\n",
      "Iteration 596, loss = 0.61680832\n",
      "Iteration 597, loss = 0.61680853\n",
      "Iteration 598, loss = 0.61680727\n",
      "Iteration 599, loss = 0.61679102\n",
      "Iteration 600, loss = 0.61679913\n",
      "Iteration 601, loss = 0.61678030\n",
      "Iteration 602, loss = 0.61677220\n",
      "Iteration 603, loss = 0.61678563\n",
      "Iteration 604, loss = 0.61676545\n",
      "Iteration 605, loss = 0.61677126\n",
      "Iteration 606, loss = 0.61675921\n",
      "Iteration 607, loss = 0.61675892\n",
      "Iteration 608, loss = 0.61674888\n",
      "Iteration 609, loss = 0.61674164\n",
      "Iteration 610, loss = 0.61675160\n",
      "Iteration 611, loss = 0.61674731\n",
      "Iteration 612, loss = 0.61676242\n",
      "Iteration 613, loss = 0.61675766\n",
      "Iteration 614, loss = 0.61675989\n",
      "Iteration 615, loss = 0.61675646\n",
      "Iteration 616, loss = 0.61675601\n",
      "Iteration 617, loss = 0.61675692\n",
      "Iteration 618, loss = 0.61675674\n",
      "Iteration 619, loss = 0.61675426\n",
      "Iteration 620, loss = 0.61675082\n",
      "Iteration 621, loss = 0.61674300\n",
      "Iteration 622, loss = 0.61674423\n",
      "Iteration 623, loss = 0.61674918\n",
      "Iteration 624, loss = 0.61674247\n",
      "Iteration 625, loss = 0.61674282\n",
      "Iteration 626, loss = 0.61673848\n",
      "Iteration 627, loss = 0.61673182\n",
      "Iteration 628, loss = 0.61674353\n",
      "Iteration 629, loss = 0.61673316\n",
      "Iteration 630, loss = 0.61674189\n",
      "Iteration 631, loss = 0.61674242\n",
      "Iteration 632, loss = 0.61673733\n",
      "Iteration 633, loss = 0.61673974\n",
      "Iteration 634, loss = 0.61673439\n",
      "Iteration 635, loss = 0.61673710\n",
      "Iteration 636, loss = 0.61673557\n",
      "Iteration 637, loss = 0.61673325\n",
      "Iteration 638, loss = 0.61673429\n",
      "Iteration 639, loss = 0.61673540\n",
      "Iteration 640, loss = 0.61673237\n",
      "Iteration 641, loss = 0.61674631\n",
      "Iteration 642, loss = 0.61674034\n",
      "Iteration 643, loss = 0.61674777\n",
      "Iteration 644, loss = 0.61675466\n",
      "Iteration 645, loss = 0.61675022\n",
      "Iteration 646, loss = 0.61675105\n",
      "Iteration 647, loss = 0.61674344\n",
      "Iteration 648, loss = 0.61674760\n",
      "Iteration 649, loss = 0.61677379\n",
      "Iteration 650, loss = 0.61677968\n",
      "Iteration 651, loss = 0.61678859\n",
      "Iteration 652, loss = 0.61677740\n",
      "Iteration 653, loss = 0.61676574\n",
      "Iteration 654, loss = 0.61676161\n",
      "Iteration 655, loss = 0.61674031\n",
      "Iteration 656, loss = 0.61674639\n",
      "Iteration 657, loss = 0.61675160\n",
      "Iteration 658, loss = 0.61672819\n",
      "Iteration 659, loss = 0.61672694\n",
      "Iteration 660, loss = 0.61672414\n",
      "Iteration 661, loss = 0.61672137\n",
      "Iteration 662, loss = 0.61672054\n",
      "Iteration 663, loss = 0.61671852\n",
      "Iteration 664, loss = 0.61671313\n",
      "Iteration 665, loss = 0.61670937\n",
      "Iteration 666, loss = 0.61670324\n",
      "Iteration 667, loss = 0.61670950\n",
      "Iteration 668, loss = 0.61669674\n",
      "Iteration 669, loss = 0.61670163\n",
      "Iteration 670, loss = 0.61670451\n",
      "Iteration 671, loss = 0.61670944\n",
      "Iteration 672, loss = 0.61670189\n",
      "Iteration 673, loss = 0.61670561\n",
      "Iteration 674, loss = 0.61671065\n",
      "Iteration 675, loss = 0.61670193\n",
      "Iteration 676, loss = 0.61670133\n",
      "Iteration 677, loss = 0.61670096\n",
      "Iteration 678, loss = 0.61669968\n",
      "Iteration 679, loss = 0.61670145\n",
      "Iteration 680, loss = 0.61669857\n",
      "Iteration 681, loss = 0.61670779\n",
      "Iteration 682, loss = 0.61669724\n",
      "Iteration 683, loss = 0.61670016\n",
      "Iteration 684, loss = 0.61669233\n",
      "Iteration 685, loss = 0.61669200\n",
      "Iteration 686, loss = 0.61669092\n",
      "Iteration 687, loss = 0.61669048\n",
      "Iteration 688, loss = 0.61668876\n",
      "Iteration 689, loss = 0.61669427\n",
      "Iteration 690, loss = 0.61668731\n",
      "Iteration 691, loss = 0.61668525\n",
      "Iteration 692, loss = 0.61668655\n",
      "Iteration 693, loss = 0.61668882\n",
      "Iteration 694, loss = 0.61668512\n",
      "Iteration 695, loss = 0.61668284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 696, loss = 0.61668248\n",
      "Iteration 697, loss = 0.61668091\n",
      "Iteration 698, loss = 0.61668866\n",
      "Iteration 699, loss = 0.61668747\n",
      "Iteration 700, loss = 0.61667471\n",
      "Iteration 701, loss = 0.61667396\n",
      "Iteration 702, loss = 0.61668834\n",
      "Iteration 703, loss = 0.61667404\n",
      "Iteration 704, loss = 0.61667889\n",
      "Iteration 705, loss = 0.61667339\n",
      "Iteration 706, loss = 0.61668288\n",
      "Iteration 707, loss = 0.61668173\n",
      "Iteration 708, loss = 0.61667878\n",
      "Iteration 709, loss = 0.61668438\n",
      "Iteration 710, loss = 0.61667986\n",
      "Iteration 711, loss = 0.61667749\n",
      "Iteration 712, loss = 0.61667082\n",
      "Iteration 713, loss = 0.61667669\n",
      "Iteration 714, loss = 0.61667364\n",
      "Iteration 715, loss = 0.61666479\n",
      "Iteration 716, loss = 0.61666775\n",
      "Iteration 717, loss = 0.61667681\n",
      "Iteration 718, loss = 0.61666729\n",
      "Iteration 719, loss = 0.61666784\n",
      "Iteration 720, loss = 0.61666703\n",
      "Iteration 721, loss = 0.61667092\n",
      "Iteration 722, loss = 0.61667051\n",
      "Iteration 723, loss = 0.61666672\n",
      "Iteration 724, loss = 0.61666520\n",
      "Iteration 725, loss = 0.61666584\n",
      "Iteration 726, loss = 0.61666437\n",
      "Iteration 727, loss = 0.61666346\n",
      "Iteration 728, loss = 0.61666297\n",
      "Iteration 729, loss = 0.61666250\n",
      "Iteration 730, loss = 0.61666212\n",
      "Iteration 731, loss = 0.61665969\n",
      "Iteration 732, loss = 0.61666504\n",
      "Iteration 733, loss = 0.61665839\n",
      "Iteration 734, loss = 0.61665702\n",
      "Iteration 735, loss = 0.61665748\n",
      "Iteration 736, loss = 0.61665648\n",
      "Iteration 737, loss = 0.61665504\n",
      "Iteration 738, loss = 0.61665736\n",
      "Iteration 739, loss = 0.61665632\n",
      "Iteration 740, loss = 0.61665185\n",
      "Iteration 741, loss = 0.61665379\n",
      "Iteration 742, loss = 0.61665758\n",
      "Iteration 743, loss = 0.61665210\n",
      "Iteration 744, loss = 0.61665828\n",
      "Iteration 745, loss = 0.61665098\n",
      "Iteration 746, loss = 0.61665082\n",
      "Iteration 747, loss = 0.61664964\n",
      "Iteration 748, loss = 0.61664997\n",
      "Iteration 749, loss = 0.61664798\n",
      "Iteration 750, loss = 0.61664995\n",
      "Iteration 751, loss = 0.61664899\n",
      "Iteration 752, loss = 0.61664526\n",
      "Iteration 753, loss = 0.61665226\n",
      "Iteration 754, loss = 0.61664817\n",
      "Iteration 755, loss = 0.61664260\n",
      "Iteration 756, loss = 0.61664158\n",
      "Iteration 757, loss = 0.61663757\n",
      "Iteration 758, loss = 0.61665959\n",
      "Iteration 759, loss = 0.61664693\n",
      "Iteration 760, loss = 0.61664550\n",
      "Iteration 761, loss = 0.61665071\n",
      "Iteration 762, loss = 0.61664994\n",
      "Iteration 763, loss = 0.61664857\n",
      "Iteration 764, loss = 0.61664878\n",
      "Iteration 765, loss = 0.61664767\n",
      "Iteration 766, loss = 0.61664511\n",
      "Iteration 767, loss = 0.61664351\n",
      "Iteration 768, loss = 0.61663945\n",
      "Iteration 769, loss = 0.61663606\n",
      "Iteration 770, loss = 0.61664005\n",
      "Iteration 771, loss = 0.61663110\n",
      "Iteration 772, loss = 0.61664412\n",
      "Iteration 773, loss = 0.61663837\n",
      "Iteration 774, loss = 0.61663369\n",
      "Iteration 775, loss = 0.61663545\n",
      "Iteration 776, loss = 0.61664821\n",
      "Iteration 777, loss = 0.61663063\n",
      "Iteration 778, loss = 0.61663199\n",
      "Iteration 779, loss = 0.61663423\n",
      "Iteration 780, loss = 0.61663031\n",
      "Iteration 781, loss = 0.61663285\n",
      "Iteration 782, loss = 0.61663552\n",
      "Iteration 783, loss = 0.61662843\n",
      "Iteration 784, loss = 0.61662605\n",
      "Iteration 785, loss = 0.61662923\n",
      "Iteration 786, loss = 0.61662286\n",
      "Iteration 787, loss = 0.61662326\n",
      "Iteration 788, loss = 0.61662357\n",
      "Iteration 789, loss = 0.61663026\n",
      "Iteration 790, loss = 0.61662859\n",
      "Iteration 791, loss = 0.61662468\n",
      "Iteration 792, loss = 0.61663409\n",
      "Iteration 793, loss = 0.61662934\n",
      "Iteration 794, loss = 0.61663056\n",
      "Iteration 795, loss = 0.61663306\n",
      "Iteration 796, loss = 0.61663684\n",
      "Iteration 797, loss = 0.61665674\n",
      "Iteration 798, loss = 0.61666173\n",
      "Iteration 799, loss = 0.61665711\n",
      "Iteration 800, loss = 0.61665726\n",
      "Iteration 801, loss = 0.61665780\n",
      "Iteration 802, loss = 0.61666105\n",
      "Iteration 803, loss = 0.61665683\n",
      "Iteration 804, loss = 0.61665558\n",
      "Iteration 805, loss = 0.61664781\n",
      "Iteration 806, loss = 0.61664768\n",
      "Iteration 807, loss = 0.61663733\n",
      "Iteration 808, loss = 0.61663858\n",
      "Iteration 809, loss = 0.61662893\n",
      "Iteration 810, loss = 0.61662420\n",
      "Iteration 811, loss = 0.61662052\n",
      "Iteration 812, loss = 0.61663203\n",
      "Iteration 813, loss = 0.61662180\n",
      "Iteration 814, loss = 0.61661871\n",
      "Iteration 815, loss = 0.61661805\n",
      "Iteration 816, loss = 0.61662145\n",
      "Iteration 817, loss = 0.61662016\n",
      "Iteration 818, loss = 0.61661948\n",
      "Iteration 819, loss = 0.61660966\n",
      "Iteration 820, loss = 0.61663303\n",
      "Iteration 821, loss = 0.61663618\n",
      "Iteration 822, loss = 0.61662749\n",
      "Iteration 823, loss = 0.61662922\n",
      "Iteration 824, loss = 0.61662247\n",
      "Iteration 825, loss = 0.61661821\n",
      "Iteration 826, loss = 0.61663015\n",
      "Iteration 827, loss = 0.61661865\n",
      "Iteration 828, loss = 0.61661570\n",
      "Iteration 829, loss = 0.61661334\n",
      "Iteration 830, loss = 0.61660937\n",
      "Iteration 831, loss = 0.61660691\n",
      "Iteration 832, loss = 0.61659992\n",
      "Iteration 833, loss = 0.61659972\n",
      "Iteration 834, loss = 0.61660153\n",
      "Iteration 835, loss = 0.61660787\n",
      "Iteration 836, loss = 0.61660121\n",
      "Iteration 837, loss = 0.61659457\n",
      "Iteration 838, loss = 0.61660877\n",
      "Iteration 839, loss = 0.61659571\n",
      "Iteration 840, loss = 0.61659649\n",
      "Iteration 841, loss = 0.61662396\n",
      "Iteration 842, loss = 0.61661586\n",
      "Iteration 843, loss = 0.61661315\n",
      "Iteration 844, loss = 0.61660939\n",
      "Iteration 845, loss = 0.61660960\n",
      "Iteration 846, loss = 0.61661971\n",
      "Iteration 847, loss = 0.61661893\n",
      "Iteration 848, loss = 0.61663814\n",
      "Iteration 849, loss = 0.61662956\n",
      "Iteration 850, loss = 0.61663302\n",
      "Iteration 851, loss = 0.61662890\n",
      "Iteration 852, loss = 0.61662714\n",
      "Iteration 853, loss = 0.61663018\n",
      "Iteration 854, loss = 0.61662906\n",
      "Iteration 855, loss = 0.61662745\n",
      "Iteration 856, loss = 0.61661090\n",
      "Iteration 857, loss = 0.61666262\n",
      "Iteration 858, loss = 0.61666322\n",
      "Iteration 859, loss = 0.61665592\n",
      "Iteration 860, loss = 0.61665698\n",
      "Iteration 861, loss = 0.61666135\n",
      "Iteration 862, loss = 0.61665305\n",
      "Iteration 863, loss = 0.61666076\n",
      "Iteration 864, loss = 0.61666616\n",
      "Iteration 865, loss = 0.61666572\n",
      "Iteration 866, loss = 0.61666666\n",
      "Iteration 867, loss = 0.61666129\n",
      "Iteration 868, loss = 0.61665313\n",
      "Iteration 869, loss = 0.61665245\n",
      "Iteration 870, loss = 0.61663520\n",
      "Iteration 871, loss = 0.61662973\n",
      "Iteration 872, loss = 0.61662867\n",
      "Iteration 873, loss = 0.61661507\n",
      "Iteration 874, loss = 0.61662633\n",
      "Iteration 875, loss = 0.61661912\n",
      "Iteration 876, loss = 0.61660818\n",
      "Iteration 877, loss = 0.61661874\n",
      "Iteration 878, loss = 0.61660856\n",
      "Iteration 879, loss = 0.61660916\n",
      "Iteration 880, loss = 0.61660957\n",
      "Iteration 881, loss = 0.61660185\n",
      "Iteration 882, loss = 0.61660065\n",
      "Iteration 883, loss = 0.61660113\n",
      "Iteration 884, loss = 0.61659869\n",
      "Iteration 885, loss = 0.61659826\n",
      "Iteration 886, loss = 0.61659876\n",
      "Iteration 887, loss = 0.61660232\n",
      "Iteration 888, loss = 0.61659399\n",
      "Iteration 889, loss = 0.61659641\n",
      "Iteration 890, loss = 0.61660250\n",
      "Iteration 891, loss = 0.61660893\n",
      "Iteration 892, loss = 0.61661180\n",
      "Iteration 893, loss = 0.61661360\n",
      "Iteration 894, loss = 0.61661381\n",
      "Iteration 895, loss = 0.61661619\n",
      "Iteration 896, loss = 0.61661622\n",
      "Iteration 897, loss = 0.61661860\n",
      "Iteration 898, loss = 0.61663079\n",
      "Iteration 899, loss = 0.61662451\n",
      "Iteration 900, loss = 0.61662333\n",
      "Iteration 901, loss = 0.61661859\n",
      "Iteration 902, loss = 0.61662601\n",
      "Iteration 903, loss = 0.61662843\n",
      "Iteration 904, loss = 0.61663309\n",
      "Iteration 905, loss = 0.61663395\n",
      "Iteration 906, loss = 0.61663714\n",
      "Iteration 907, loss = 0.61663816\n",
      "Iteration 908, loss = 0.61664014\n",
      "Iteration 909, loss = 0.61664005\n",
      "Iteration 910, loss = 0.61663807\n",
      "Iteration 911, loss = 0.61663993\n",
      "Iteration 912, loss = 0.61664130\n",
      "Iteration 913, loss = 0.61664290\n",
      "Iteration 914, loss = 0.61664705\n",
      "Iteration 915, loss = 0.61664976\n",
      "Iteration 916, loss = 0.61665298\n",
      "Iteration 917, loss = 0.61665589\n",
      "Iteration 918, loss = 0.61666871\n",
      "Iteration 919, loss = 0.61667308\n",
      "Iteration 920, loss = 0.61667016\n",
      "Iteration 921, loss = 0.61666546\n",
      "Iteration 922, loss = 0.61666050\n",
      "Iteration 923, loss = 0.61664992\n",
      "Iteration 924, loss = 0.61663419\n",
      "Iteration 925, loss = 0.61663730\n",
      "Iteration 926, loss = 0.61662235\n",
      "Iteration 927, loss = 0.61662098\n",
      "Iteration 928, loss = 0.61660643\n",
      "Iteration 929, loss = 0.61660848\n",
      "Iteration 930, loss = 0.61659950\n",
      "Iteration 931, loss = 0.61659799\n",
      "Iteration 932, loss = 0.61659803\n",
      "Iteration 933, loss = 0.61659331\n",
      "Iteration 934, loss = 0.61659303\n",
      "Iteration 935, loss = 0.61659268\n",
      "Iteration 936, loss = 0.61659178\n",
      "Iteration 937, loss = 0.61659007\n",
      "Iteration 938, loss = 0.61660367\n",
      "Iteration 939, loss = 0.61660030\n",
      "Iteration 940, loss = 0.61660056\n",
      "Iteration 941, loss = 0.61660035\n",
      "Iteration 942, loss = 0.61660168\n",
      "Iteration 943, loss = 0.61660641\n",
      "Iteration 944, loss = 0.61661353\n",
      "Iteration 945, loss = 0.61661268\n",
      "Iteration 946, loss = 0.61661320\n",
      "Iteration 947, loss = 0.61661745\n",
      "Iteration 948, loss = 0.61662238\n",
      "Iteration 949, loss = 0.61662337\n",
      "Iteration 950, loss = 0.61662676\n",
      "Iteration 951, loss = 0.61662836\n",
      "Iteration 952, loss = 0.61662859\n",
      "Iteration 953, loss = 0.61662899\n",
      "Iteration 954, loss = 0.61662812\n",
      "Iteration 955, loss = 0.61663146\n",
      "Iteration 956, loss = 0.61663566\n",
      "Iteration 957, loss = 0.61663672\n",
      "Iteration 958, loss = 0.61663733\n",
      "Iteration 959, loss = 0.61663665\n",
      "Iteration 960, loss = 0.61664748\n",
      "Iteration 961, loss = 0.61665533\n",
      "Iteration 962, loss = 0.61666736\n",
      "Iteration 963, loss = 0.61667255\n",
      "Iteration 964, loss = 0.61668748\n",
      "Iteration 965, loss = 0.61669348\n",
      "Iteration 966, loss = 0.61670387\n",
      "Iteration 967, loss = 0.61670352\n",
      "Iteration 968, loss = 0.61670168\n",
      "Iteration 969, loss = 0.61670181\n",
      "Iteration 970, loss = 0.61672062\n",
      "Iteration 971, loss = 0.61672190\n",
      "Iteration 972, loss = 0.61673757\n",
      "Iteration 973, loss = 0.61674197\n",
      "Iteration 974, loss = 0.61673622\n",
      "Iteration 975, loss = 0.61672490\n",
      "Iteration 976, loss = 0.61671413\n",
      "Iteration 977, loss = 0.61670414\n",
      "Iteration 978, loss = 0.61668817\n",
      "Iteration 979, loss = 0.61667256\n",
      "Iteration 980, loss = 0.61665853\n",
      "Iteration 981, loss = 0.61664172\n",
      "Iteration 982, loss = 0.61663125\n",
      "Iteration 983, loss = 0.61662722\n",
      "Iteration 984, loss = 0.61661531\n",
      "Iteration 985, loss = 0.61660317\n",
      "Iteration 986, loss = 0.61659400\n",
      "Iteration 987, loss = 0.61659879\n",
      "Iteration 988, loss = 0.61658709\n",
      "Iteration 989, loss = 0.61657973\n",
      "Iteration 990, loss = 0.61658388\n",
      "Iteration 991, loss = 0.61659066\n",
      "Iteration 992, loss = 0.61657653\n",
      "Iteration 993, loss = 0.61657549\n",
      "Iteration 994, loss = 0.61658271\n",
      "Iteration 995, loss = 0.61657098\n",
      "Iteration 996, loss = 0.61657742\n",
      "Iteration 997, loss = 0.61656753\n",
      "Iteration 998, loss = 0.61656373\n",
      "Iteration 999, loss = 0.61657422\n",
      "Iteration 1000, loss = 0.61655860\n",
      "Iteration 1001, loss = 0.61656256\n",
      "Iteration 1002, loss = 0.61655712\n",
      "Iteration 1003, loss = 0.61655776\n",
      "Iteration 1004, loss = 0.61655390\n",
      "Iteration 1005, loss = 0.61656144\n",
      "Iteration 1006, loss = 0.61655308\n",
      "Iteration 1007, loss = 0.61655556\n",
      "Iteration 1008, loss = 0.61655288\n",
      "Iteration 1009, loss = 0.61655777\n",
      "Iteration 1010, loss = 0.61655048\n",
      "Iteration 1011, loss = 0.61655886\n",
      "Iteration 1012, loss = 0.61655306\n",
      "Iteration 1013, loss = 0.61655388\n",
      "Iteration 1014, loss = 0.61655210\n",
      "Iteration 1015, loss = 0.61655596\n",
      "Iteration 1016, loss = 0.61655203\n",
      "Iteration 1017, loss = 0.61655615\n",
      "Iteration 1018, loss = 0.61655118\n",
      "Iteration 1019, loss = 0.61655041\n",
      "Iteration 1020, loss = 0.61655142\n",
      "Iteration 1021, loss = 0.61655140\n",
      "Iteration 1022, loss = 0.61655124\n",
      "Iteration 1023, loss = 0.61654995\n",
      "Iteration 1024, loss = 0.61655083\n",
      "Iteration 1025, loss = 0.61654967\n",
      "Iteration 1026, loss = 0.61655513\n",
      "Iteration 1027, loss = 0.61655167\n",
      "Iteration 1028, loss = 0.61654942\n",
      "Iteration 1029, loss = 0.61654836\n",
      "Iteration 1030, loss = 0.61654841\n",
      "Iteration 1031, loss = 0.61655112\n",
      "Iteration 1032, loss = 0.61654839\n",
      "Iteration 1033, loss = 0.61654722\n",
      "Iteration 1034, loss = 0.61654692\n",
      "Iteration 1035, loss = 0.61654579\n",
      "Iteration 1036, loss = 0.61654337\n",
      "Iteration 1037, loss = 0.61654880\n",
      "Iteration 1038, loss = 0.61654889\n",
      "Iteration 1039, loss = 0.61655556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1040, loss = 0.61655334\n",
      "Iteration 1041, loss = 0.61655023\n",
      "Iteration 1042, loss = 0.61654931\n",
      "Iteration 1043, loss = 0.61655066\n",
      "Iteration 1044, loss = 0.61654894\n",
      "Iteration 1045, loss = 0.61654598\n",
      "Iteration 1046, loss = 0.61654607\n",
      "Iteration 1047, loss = 0.61654476\n",
      "Iteration 1048, loss = 0.61654394\n",
      "Iteration 1049, loss = 0.61654270\n",
      "Iteration 1050, loss = 0.61655094\n",
      "Iteration 1051, loss = 0.61655333\n",
      "Iteration 1052, loss = 0.61653617\n",
      "Iteration 1053, loss = 0.61654136\n",
      "Iteration 1054, loss = 0.61653867\n",
      "Iteration 1055, loss = 0.61653344\n",
      "Iteration 1056, loss = 0.61653812\n",
      "Iteration 1057, loss = 0.61653882\n",
      "Iteration 1058, loss = 0.61653525\n",
      "Iteration 1059, loss = 0.61653636\n",
      "Iteration 1060, loss = 0.61653194\n",
      "Iteration 1061, loss = 0.61653531\n",
      "Iteration 1062, loss = 0.61653318\n",
      "Iteration 1063, loss = 0.61653243\n",
      "Iteration 1064, loss = 0.61653655\n",
      "Iteration 1065, loss = 0.61653262\n",
      "Iteration 1066, loss = 0.61653750\n",
      "Iteration 1067, loss = 0.61653598\n",
      "Iteration 1068, loss = 0.61653547\n",
      "Iteration 1069, loss = 0.61654325\n",
      "Iteration 1070, loss = 0.61654092\n",
      "Iteration 1071, loss = 0.61653422\n",
      "Iteration 1072, loss = 0.61653858\n",
      "Iteration 1073, loss = 0.61655096\n",
      "Iteration 1074, loss = 0.61654544\n",
      "Iteration 1075, loss = 0.61654588\n",
      "Iteration 1076, loss = 0.61654283\n",
      "Iteration 1077, loss = 0.61655289\n",
      "Iteration 1078, loss = 0.61654234\n",
      "Iteration 1079, loss = 0.61654575\n",
      "Iteration 1080, loss = 0.61653887\n",
      "Iteration 1081, loss = 0.61653740\n",
      "Iteration 1082, loss = 0.61654542\n",
      "Iteration 1083, loss = 0.61653679\n",
      "Iteration 1084, loss = 0.61653487\n",
      "Iteration 1085, loss = 0.61653404\n",
      "Iteration 1086, loss = 0.61653495\n",
      "Iteration 1087, loss = 0.61653323\n",
      "Iteration 1088, loss = 0.61653413\n",
      "Iteration 1089, loss = 0.61653179\n",
      "Iteration 1090, loss = 0.61652781\n",
      "Iteration 1091, loss = 0.61653554\n",
      "Iteration 1092, loss = 0.61652889\n",
      "Iteration 1093, loss = 0.61652769\n",
      "Iteration 1094, loss = 0.61652941\n",
      "Iteration 1095, loss = 0.61652738\n",
      "Iteration 1096, loss = 0.61652719\n",
      "Iteration 1097, loss = 0.61653584\n",
      "Iteration 1098, loss = 0.61652913\n",
      "Iteration 1099, loss = 0.61652893\n",
      "Iteration 1100, loss = 0.61652693\n",
      "Iteration 1101, loss = 0.61652575\n",
      "Iteration 1102, loss = 0.61653053\n",
      "Iteration 1103, loss = 0.61652667\n",
      "Iteration 1104, loss = 0.61652699\n",
      "Iteration 1105, loss = 0.61652522\n",
      "Iteration 1106, loss = 0.61654564\n",
      "Iteration 1107, loss = 0.61653543\n",
      "Iteration 1108, loss = 0.61653389\n",
      "Iteration 1109, loss = 0.61653425\n",
      "Iteration 1110, loss = 0.61653174\n",
      "Iteration 1111, loss = 0.61653877\n",
      "Iteration 1112, loss = 0.61653606\n",
      "Iteration 1113, loss = 0.61653575\n",
      "Iteration 1114, loss = 0.61653850\n",
      "Iteration 1115, loss = 0.61654266\n",
      "Iteration 1116, loss = 0.61653448\n",
      "Iteration 1117, loss = 0.61654221\n",
      "Iteration 1118, loss = 0.61653817\n",
      "Iteration 1119, loss = 0.61654402\n",
      "Iteration 1120, loss = 0.61653609\n",
      "Iteration 1121, loss = 0.61654250\n",
      "Iteration 1122, loss = 0.61654014\n",
      "Iteration 1123, loss = 0.61653426\n",
      "Iteration 1124, loss = 0.61653541\n",
      "Iteration 1125, loss = 0.61653571\n",
      "Iteration 1126, loss = 0.61653383\n",
      "Iteration 1127, loss = 0.61653872\n",
      "Iteration 1128, loss = 0.61653626\n",
      "Iteration 1129, loss = 0.61653532\n",
      "Iteration 1130, loss = 0.61653704\n",
      "Iteration 1131, loss = 0.61653474\n",
      "Iteration 1132, loss = 0.61653765\n",
      "Iteration 1133, loss = 0.61653620\n",
      "Iteration 1134, loss = 0.61653723\n",
      "Iteration 1135, loss = 0.61653901\n",
      "Iteration 1136, loss = 0.61653902\n",
      "Iteration 1137, loss = 0.61654544\n",
      "Iteration 1138, loss = 0.61654178\n",
      "Iteration 1139, loss = 0.61653891\n",
      "Iteration 1140, loss = 0.61653915\n",
      "Iteration 1141, loss = 0.61654052\n",
      "Iteration 1142, loss = 0.61653805\n",
      "Iteration 1143, loss = 0.61653789\n",
      "Iteration 1144, loss = 0.61653593\n",
      "Iteration 1145, loss = 0.61653533\n",
      "Iteration 1146, loss = 0.61653359\n",
      "Iteration 1147, loss = 0.61653344\n",
      "Iteration 1148, loss = 0.61653639\n",
      "Iteration 1149, loss = 0.61652972\n",
      "Iteration 1150, loss = 0.61653382\n",
      "Iteration 1151, loss = 0.61652878\n",
      "Iteration 1152, loss = 0.61652520\n",
      "Iteration 1153, loss = 0.61651934\n",
      "Iteration 1154, loss = 0.61652812\n",
      "Iteration 1155, loss = 0.61651578\n",
      "Iteration 1156, loss = 0.61652870\n",
      "Iteration 1157, loss = 0.61651676\n",
      "Iteration 1158, loss = 0.61652219\n",
      "Iteration 1159, loss = 0.61651504\n",
      "Iteration 1160, loss = 0.61652004\n",
      "Iteration 1161, loss = 0.61651754\n",
      "Iteration 1162, loss = 0.61651771\n",
      "Iteration 1163, loss = 0.61651660\n",
      "Iteration 1164, loss = 0.61651581\n",
      "Iteration 1165, loss = 0.61652229\n",
      "Iteration 1166, loss = 0.61652665\n",
      "Iteration 1167, loss = 0.61651977\n",
      "Iteration 1168, loss = 0.61651736\n",
      "Iteration 1169, loss = 0.61651440\n",
      "Iteration 1170, loss = 0.61651795\n",
      "Iteration 1171, loss = 0.61651141\n",
      "Iteration 1172, loss = 0.61652013\n",
      "Iteration 1173, loss = 0.61651342\n",
      "Iteration 1174, loss = 0.61651347\n",
      "Iteration 1175, loss = 0.61651331\n",
      "Iteration 1176, loss = 0.61651246\n",
      "Iteration 1177, loss = 0.61651370\n",
      "Iteration 1178, loss = 0.61651078\n",
      "Iteration 1179, loss = 0.61651001\n",
      "Iteration 1180, loss = 0.61651167\n",
      "Iteration 1181, loss = 0.61650961\n",
      "Iteration 1182, loss = 0.61652031\n",
      "Iteration 1183, loss = 0.61651160\n",
      "Iteration 1184, loss = 0.61651217\n",
      "Iteration 1185, loss = 0.61651186\n",
      "Iteration 1186, loss = 0.61651511\n",
      "Iteration 1187, loss = 0.61651321\n",
      "Iteration 1188, loss = 0.61651895\n",
      "Iteration 1189, loss = 0.61651555\n",
      "Iteration 1190, loss = 0.61651756\n",
      "Iteration 1191, loss = 0.61651800\n",
      "Iteration 1192, loss = 0.61652308\n",
      "Iteration 1193, loss = 0.61652614\n",
      "Iteration 1194, loss = 0.61652607\n",
      "Iteration 1195, loss = 0.61652546\n",
      "Iteration 1196, loss = 0.61652504\n",
      "Iteration 1197, loss = 0.61652534\n",
      "Iteration 1198, loss = 0.61652438\n",
      "Iteration 1199, loss = 0.61652474\n",
      "Iteration 1200, loss = 0.61653078\n",
      "Iteration 1201, loss = 0.61652634\n",
      "Iteration 1202, loss = 0.61652348\n",
      "Iteration 1203, loss = 0.61652072\n",
      "Iteration 1204, loss = 0.61652411\n",
      "Iteration 1205, loss = 0.61651673\n",
      "Iteration 1206, loss = 0.61652576\n",
      "Iteration 1207, loss = 0.61651530\n",
      "Iteration 1208, loss = 0.61652100\n",
      "Iteration 1209, loss = 0.61651305\n",
      "Iteration 1210, loss = 0.61651239\n",
      "Iteration 1211, loss = 0.61652528\n",
      "Iteration 1212, loss = 0.61651803\n",
      "Iteration 1213, loss = 0.61652598\n",
      "Iteration 1214, loss = 0.61652351\n",
      "Iteration 1215, loss = 0.61653244\n",
      "Iteration 1216, loss = 0.61652864\n",
      "Iteration 1217, loss = 0.61653001\n",
      "Iteration 1218, loss = 0.61652780\n",
      "Iteration 1219, loss = 0.61652447\n",
      "Iteration 1220, loss = 0.61652499\n",
      "Iteration 1221, loss = 0.61652559\n",
      "Iteration 1222, loss = 0.61651998\n",
      "Iteration 1223, loss = 0.61652088\n",
      "Iteration 1224, loss = 0.61651878\n",
      "Iteration 1225, loss = 0.61652200\n",
      "Iteration 1226, loss = 0.61653549\n",
      "Iteration 1227, loss = 0.61651532\n",
      "Iteration 1228, loss = 0.61651740\n",
      "Iteration 1229, loss = 0.61651784\n",
      "Iteration 1230, loss = 0.61651543\n",
      "Iteration 1231, loss = 0.61651305\n",
      "Iteration 1232, loss = 0.61651546\n",
      "Iteration 1233, loss = 0.61651092\n",
      "Iteration 1234, loss = 0.61650746\n",
      "Iteration 1235, loss = 0.61650793\n",
      "Iteration 1236, loss = 0.61650829\n",
      "Iteration 1237, loss = 0.61650306\n",
      "Iteration 1238, loss = 0.61649642\n",
      "Iteration 1239, loss = 0.61650492\n",
      "Iteration 1240, loss = 0.61650335\n",
      "Iteration 1241, loss = 0.61650653\n",
      "Iteration 1242, loss = 0.61650215\n",
      "Iteration 1243, loss = 0.61650126\n",
      "Iteration 1244, loss = 0.61650201\n",
      "Iteration 1245, loss = 0.61650068\n",
      "Iteration 1246, loss = 0.61650171\n",
      "Iteration 1247, loss = 0.61649906\n",
      "Iteration 1248, loss = 0.61650577\n",
      "Iteration 1249, loss = 0.61650030\n",
      "Iteration 1250, loss = 0.61649971\n",
      "Iteration 1251, loss = 0.61649934\n",
      "Iteration 1252, loss = 0.61650208\n",
      "Iteration 1253, loss = 0.61649925\n",
      "Iteration 1254, loss = 0.61649958\n",
      "Iteration 1255, loss = 0.61650285\n",
      "Iteration 1256, loss = 0.61649712\n",
      "Iteration 1257, loss = 0.61649984\n",
      "Iteration 1258, loss = 0.61649953\n",
      "Iteration 1259, loss = 0.61649871\n",
      "Iteration 1260, loss = 0.61649803\n",
      "Iteration 1261, loss = 0.61649961\n",
      "Iteration 1262, loss = 0.61649748\n",
      "Iteration 1263, loss = 0.61650230\n",
      "Iteration 1264, loss = 0.61650260\n",
      "Iteration 1265, loss = 0.61649978\n",
      "Iteration 1266, loss = 0.61649939\n",
      "Iteration 1267, loss = 0.61649947\n",
      "Iteration 1268, loss = 0.61649858\n",
      "Iteration 1269, loss = 0.61649942\n",
      "Iteration 1270, loss = 0.61649824\n",
      "Iteration 1271, loss = 0.61649820\n",
      "Iteration 1272, loss = 0.61649723\n",
      "Iteration 1273, loss = 0.61650872\n",
      "Iteration 1274, loss = 0.61649685\n",
      "Iteration 1275, loss = 0.61649693\n",
      "Iteration 1276, loss = 0.61649701\n",
      "Iteration 1277, loss = 0.61649923\n",
      "Iteration 1278, loss = 0.61649687\n",
      "Iteration 1279, loss = 0.61649537\n",
      "Iteration 1280, loss = 0.61650150\n",
      "Iteration 1281, loss = 0.61649675\n",
      "Iteration 1282, loss = 0.61649636\n",
      "Iteration 1283, loss = 0.61649577\n",
      "Iteration 1284, loss = 0.61649707\n",
      "Iteration 1285, loss = 0.61649638\n",
      "Iteration 1286, loss = 0.61649484\n",
      "Iteration 1287, loss = 0.61649569\n",
      "Iteration 1288, loss = 0.61649821\n",
      "Iteration 1289, loss = 0.61649625\n",
      "Iteration 1290, loss = 0.61649983\n",
      "Iteration 1291, loss = 0.61649675\n",
      "Iteration 1292, loss = 0.61649636\n",
      "Iteration 1293, loss = 0.61649578\n",
      "Iteration 1294, loss = 0.61649525\n",
      "Iteration 1295, loss = 0.61649421\n",
      "Iteration 1296, loss = 0.61649640\n",
      "Iteration 1297, loss = 0.61649462\n",
      "Iteration 1298, loss = 0.61649383\n",
      "Iteration 1299, loss = 0.61649289\n",
      "Iteration 1300, loss = 0.61649202\n",
      "Iteration 1301, loss = 0.61649378\n",
      "Iteration 1302, loss = 0.61649455\n",
      "Iteration 1303, loss = 0.61649363\n",
      "Iteration 1304, loss = 0.61649302\n",
      "Iteration 1305, loss = 0.61649412\n",
      "Iteration 1306, loss = 0.61649227\n",
      "Iteration 1307, loss = 0.61649368\n",
      "Iteration 1308, loss = 0.61649304\n",
      "Iteration 1309, loss = 0.61649680\n",
      "Iteration 1310, loss = 0.61649266\n",
      "Iteration 1311, loss = 0.61649202\n",
      "Iteration 1312, loss = 0.61649257\n",
      "Iteration 1313, loss = 0.61649198\n",
      "Iteration 1314, loss = 0.61649179\n",
      "Iteration 1315, loss = 0.61649179\n",
      "Iteration 1316, loss = 0.61649304\n",
      "Iteration 1317, loss = 0.61649128\n",
      "Iteration 1318, loss = 0.61649832\n",
      "Iteration 1319, loss = 0.61649104\n",
      "Iteration 1320, loss = 0.61649128\n",
      "Iteration 1321, loss = 0.61649077\n",
      "Iteration 1322, loss = 0.61649240\n",
      "Iteration 1323, loss = 0.61648624\n",
      "Iteration 1324, loss = 0.61649177\n",
      "Iteration 1325, loss = 0.61649198\n",
      "Iteration 1326, loss = 0.61649588\n",
      "Iteration 1327, loss = 0.61649301\n",
      "Iteration 1328, loss = 0.61649211\n",
      "Iteration 1329, loss = 0.61649441\n",
      "Iteration 1330, loss = 0.61649519\n",
      "Iteration 1331, loss = 0.61649136\n",
      "Iteration 1332, loss = 0.61649115\n",
      "Iteration 1333, loss = 0.61649066\n",
      "Iteration 1334, loss = 0.61649035\n",
      "Iteration 1335, loss = 0.61648760\n",
      "Iteration 1336, loss = 0.61648898\n",
      "Iteration 1337, loss = 0.61649058\n",
      "Iteration 1338, loss = 0.61648859\n",
      "Iteration 1339, loss = 0.61649620\n",
      "Iteration 1340, loss = 0.61648851\n",
      "Iteration 1341, loss = 0.61648812\n",
      "Iteration 1342, loss = 0.61648901\n",
      "Iteration 1343, loss = 0.61648969\n",
      "Iteration 1344, loss = 0.61648851\n",
      "Iteration 1345, loss = 0.61648613\n",
      "Iteration 1346, loss = 0.61648921\n",
      "Iteration 1347, loss = 0.61648801\n",
      "Iteration 1348, loss = 0.61648493\n",
      "Iteration 1349, loss = 0.61649825\n",
      "Iteration 1350, loss = 0.61648851\n",
      "Iteration 1351, loss = 0.61648755\n",
      "Iteration 1352, loss = 0.61649450\n",
      "Iteration 1353, loss = 0.61649577\n",
      "Iteration 1354, loss = 0.61649352\n",
      "Iteration 1355, loss = 0.61649275\n",
      "Iteration 1356, loss = 0.61649595\n",
      "Iteration 1357, loss = 0.61649255\n",
      "Iteration 1358, loss = 0.61649352\n",
      "Iteration 1359, loss = 0.61649341\n",
      "Iteration 1360, loss = 0.61649221\n",
      "Iteration 1361, loss = 0.61649123\n",
      "Iteration 1362, loss = 0.61649192\n",
      "Iteration 1363, loss = 0.61648969\n",
      "Iteration 1364, loss = 0.61648868\n",
      "Iteration 1365, loss = 0.61648668\n",
      "Iteration 1366, loss = 0.61648346\n",
      "Iteration 1367, loss = 0.61647853\n",
      "Iteration 1368, loss = 0.61647675\n",
      "Iteration 1369, loss = 0.61648475\n",
      "Iteration 1370, loss = 0.61648216\n",
      "Iteration 1371, loss = 0.61648548\n",
      "Iteration 1372, loss = 0.61648869\n",
      "Iteration 1373, loss = 0.61649284\n",
      "Iteration 1374, loss = 0.61649045\n",
      "Iteration 1375, loss = 0.61648956\n",
      "Iteration 1376, loss = 0.61649065\n",
      "Iteration 1377, loss = 0.61648635\n",
      "Iteration 1378, loss = 0.61648232\n",
      "Iteration 1379, loss = 0.61648729\n",
      "Iteration 1380, loss = 0.61648446\n",
      "Iteration 1381, loss = 0.61648405\n",
      "Iteration 1382, loss = 0.61649302\n",
      "Iteration 1383, loss = 0.61648515\n",
      "Iteration 1384, loss = 0.61648779\n",
      "Iteration 1385, loss = 0.61648532\n",
      "Iteration 1386, loss = 0.61648435\n",
      "Iteration 1387, loss = 0.61648663\n",
      "Iteration 1388, loss = 0.61648292\n",
      "Iteration 1389, loss = 0.61648961\n",
      "Iteration 1390, loss = 0.61648457\n",
      "Iteration 1391, loss = 0.61648357\n",
      "Iteration 1392, loss = 0.61649041\n",
      "Iteration 1393, loss = 0.61648533\n",
      "Iteration 1394, loss = 0.61648426\n",
      "Iteration 1395, loss = 0.61648347\n",
      "Iteration 1396, loss = 0.61648297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1397, loss = 0.61648366\n",
      "Iteration 1398, loss = 0.61648289\n",
      "Iteration 1399, loss = 0.61648166\n",
      "Iteration 1400, loss = 0.61647608\n",
      "Iteration 1401, loss = 0.61648720\n",
      "Iteration 1402, loss = 0.61648086\n",
      "Iteration 1403, loss = 0.61647444\n",
      "Iteration 1404, loss = 0.61648676\n",
      "Iteration 1405, loss = 0.61647650\n",
      "Iteration 1406, loss = 0.61648415\n",
      "Iteration 1407, loss = 0.61649063\n",
      "Iteration 1408, loss = 0.61650592\n",
      "Iteration 1409, loss = 0.61649867\n",
      "Iteration 1410, loss = 0.61650171\n",
      "Iteration 1411, loss = 0.61649994\n",
      "Iteration 1412, loss = 0.61649968\n",
      "Iteration 1413, loss = 0.61649997\n",
      "Iteration 1414, loss = 0.61649997\n",
      "Iteration 1415, loss = 0.61650003\n",
      "Iteration 1416, loss = 0.61649919\n",
      "Iteration 1417, loss = 0.61649985\n",
      "Iteration 1418, loss = 0.61650283\n",
      "Iteration 1419, loss = 0.61650118\n",
      "Iteration 1420, loss = 0.61650378\n",
      "Iteration 1421, loss = 0.61650481\n",
      "Iteration 1422, loss = 0.61650507\n",
      "Iteration 1423, loss = 0.61650630\n",
      "Iteration 1424, loss = 0.61650627\n",
      "Iteration 1425, loss = 0.61650559\n",
      "Iteration 1426, loss = 0.61650781\n",
      "Iteration 1427, loss = 0.61650717\n",
      "Iteration 1428, loss = 0.61650716\n",
      "Iteration 1429, loss = 0.61650782\n",
      "Iteration 1430, loss = 0.61650829\n",
      "Iteration 1431, loss = 0.61651899\n",
      "Iteration 1432, loss = 0.61651480\n",
      "Iteration 1433, loss = 0.61651446\n",
      "Iteration 1434, loss = 0.61651345\n",
      "Iteration 1435, loss = 0.61651305\n",
      "Iteration 1436, loss = 0.61651055\n",
      "Iteration 1437, loss = 0.61650995\n",
      "Iteration 1438, loss = 0.61650761\n",
      "Iteration 1439, loss = 0.61651457\n",
      "Iteration 1440, loss = 0.61650906\n",
      "Iteration 1441, loss = 0.61650720\n",
      "Iteration 1442, loss = 0.61650751\n",
      "Iteration 1443, loss = 0.61650656\n",
      "Iteration 1444, loss = 0.61650678\n",
      "Iteration 1445, loss = 0.61650871\n",
      "Iteration 1446, loss = 0.61650853\n",
      "Iteration 1447, loss = 0.61650949\n",
      "Iteration 1448, loss = 0.61650722\n",
      "Iteration 1449, loss = 0.61650497\n",
      "Iteration 1450, loss = 0.61650867\n",
      "Iteration 1451, loss = 0.61650539\n",
      "Iteration 1452, loss = 0.61650173\n",
      "Iteration 1453, loss = 0.61650385\n",
      "Iteration 1454, loss = 0.61650618\n",
      "Iteration 1455, loss = 0.61650617\n",
      "Iteration 1456, loss = 0.61650520\n",
      "Iteration 1457, loss = 0.61650480\n",
      "Iteration 1458, loss = 0.61650626\n",
      "Iteration 1459, loss = 0.61650709\n",
      "Iteration 1460, loss = 0.61650597\n",
      "Iteration 1461, loss = 0.61651505\n",
      "Iteration 1462, loss = 0.61651075\n",
      "Iteration 1463, loss = 0.61650992\n",
      "Iteration 1464, loss = 0.61650963\n",
      "Iteration 1465, loss = 0.61651239\n",
      "Iteration 1466, loss = 0.61650568\n",
      "Iteration 1467, loss = 0.61650670\n",
      "Iteration 1468, loss = 0.61651081\n",
      "Iteration 1469, loss = 0.61651516\n",
      "Iteration 1470, loss = 0.61651634\n",
      "Iteration 1471, loss = 0.61651747\n",
      "Iteration 1472, loss = 0.61652086\n",
      "Iteration 1473, loss = 0.61652509\n",
      "Iteration 1474, loss = 0.61652257\n",
      "Iteration 1475, loss = 0.61652262\n",
      "Iteration 1476, loss = 0.61652081\n",
      "Iteration 1477, loss = 0.61652147\n",
      "Iteration 1478, loss = 0.61652758\n",
      "Iteration 1479, loss = 0.61652195\n",
      "Iteration 1480, loss = 0.61652058\n",
      "Iteration 1481, loss = 0.61651751\n",
      "Iteration 1482, loss = 0.61652066\n",
      "Iteration 1483, loss = 0.61651948\n",
      "Iteration 1484, loss = 0.61651772\n",
      "Iteration 1485, loss = 0.61651715\n",
      "Iteration 1486, loss = 0.61652026\n",
      "Iteration 1487, loss = 0.61651703\n",
      "Iteration 1488, loss = 0.61651747\n",
      "Iteration 1489, loss = 0.61651827\n",
      "Iteration 1490, loss = 0.61651906\n",
      "Iteration 1491, loss = 0.61652593\n",
      "Iteration 1492, loss = 0.61652718\n",
      "Iteration 1493, loss = 0.61652686\n",
      "Iteration 1494, loss = 0.61652713\n",
      "Iteration 1495, loss = 0.61652674\n",
      "Iteration 1496, loss = 0.61652592\n",
      "Iteration 1497, loss = 0.61652428\n",
      "Iteration 1498, loss = 0.61652277\n",
      "Iteration 1499, loss = 0.61652154\n",
      "Iteration 1500, loss = 0.61651857\n",
      "Iteration 1501, loss = 0.61651679\n",
      "Iteration 1502, loss = 0.61651411\n",
      "Iteration 1503, loss = 0.61651861\n",
      "Iteration 1504, loss = 0.61651235\n",
      "Iteration 1505, loss = 0.61651254\n",
      "Iteration 1506, loss = 0.61650780\n",
      "Iteration 1507, loss = 0.61650358\n",
      "Iteration 1508, loss = 0.61650717\n",
      "Iteration 1509, loss = 0.61650460\n",
      "Iteration 1510, loss = 0.61649903\n",
      "Iteration 1511, loss = 0.61649652\n",
      "Iteration 1512, loss = 0.61649311\n",
      "Iteration 1513, loss = 0.61648400\n",
      "Iteration 1514, loss = 0.61650181\n",
      "Iteration 1515, loss = 0.61648277\n",
      "Iteration 1516, loss = 0.61649008\n",
      "Iteration 1517, loss = 0.61648563\n",
      "Iteration 1518, loss = 0.61648210\n",
      "Iteration 1519, loss = 0.61648360\n",
      "Iteration 1520, loss = 0.61648348\n",
      "Iteration 1521, loss = 0.61648274\n",
      "Iteration 1522, loss = 0.61648285\n",
      "Iteration 1523, loss = 0.61648235\n",
      "Iteration 1524, loss = 0.61648299\n",
      "Iteration 1525, loss = 0.61648451\n",
      "Iteration 1526, loss = 0.61648456\n",
      "Iteration 1527, loss = 0.61648502\n",
      "Iteration 1528, loss = 0.61649231\n",
      "Iteration 1529, loss = 0.61648116\n",
      "Iteration 1530, loss = 0.61649240\n",
      "Iteration 1531, loss = 0.61648436\n",
      "Iteration 1532, loss = 0.61648131\n",
      "Iteration 1533, loss = 0.61648225\n",
      "Iteration 1534, loss = 0.61648021\n",
      "Iteration 1535, loss = 0.61647713\n",
      "Iteration 1536, loss = 0.61647641\n",
      "Iteration 1537, loss = 0.61647671\n",
      "Iteration 1538, loss = 0.61647387\n",
      "Iteration 1539, loss = 0.61648533\n",
      "Iteration 1540, loss = 0.61647472\n",
      "Iteration 1541, loss = 0.61648109\n",
      "Iteration 1542, loss = 0.61647357\n",
      "Iteration 1543, loss = 0.61647127\n",
      "Iteration 1544, loss = 0.61647382\n",
      "Iteration 1545, loss = 0.61648007\n",
      "Iteration 1546, loss = 0.61647745\n",
      "Iteration 1547, loss = 0.61647723\n",
      "Iteration 1548, loss = 0.61647687\n",
      "Iteration 1549, loss = 0.61647839\n",
      "Iteration 1550, loss = 0.61648050\n",
      "Iteration 1551, loss = 0.61647650\n",
      "Iteration 1552, loss = 0.61647546\n",
      "Iteration 1553, loss = 0.61647561\n",
      "Iteration 1554, loss = 0.61647482\n",
      "Iteration 1555, loss = 0.61647514\n",
      "Iteration 1556, loss = 0.61647516\n",
      "Iteration 1557, loss = 0.61647464\n",
      "Iteration 1558, loss = 0.61647388\n",
      "Iteration 1559, loss = 0.61647360\n",
      "Iteration 1560, loss = 0.61647509\n",
      "Iteration 1561, loss = 0.61647856\n",
      "Iteration 1562, loss = 0.61647913\n",
      "Iteration 1563, loss = 0.61647768\n",
      "Iteration 1564, loss = 0.61647751\n",
      "Iteration 1565, loss = 0.61647509\n",
      "Iteration 1566, loss = 0.61647706\n",
      "Iteration 1567, loss = 0.61647260\n",
      "Iteration 1568, loss = 0.61647702\n",
      "Iteration 1569, loss = 0.61647396\n",
      "Iteration 1570, loss = 0.61647275\n",
      "Iteration 1571, loss = 0.61647244\n",
      "Iteration 1572, loss = 0.61647116\n",
      "Iteration 1573, loss = 0.61647631\n",
      "Iteration 1574, loss = 0.61647248\n",
      "Iteration 1575, loss = 0.61647172\n",
      "Iteration 1576, loss = 0.61647101\n",
      "Iteration 1577, loss = 0.61647292\n",
      "Iteration 1578, loss = 0.61647118\n",
      "Iteration 1579, loss = 0.61647084\n",
      "Iteration 1580, loss = 0.61647079\n",
      "Iteration 1581, loss = 0.61647121\n",
      "Iteration 1582, loss = 0.61646974\n",
      "Iteration 1583, loss = 0.61647108\n",
      "Iteration 1584, loss = 0.61647645\n",
      "Iteration 1585, loss = 0.61647174\n",
      "Iteration 1586, loss = 0.61646962\n",
      "Iteration 1587, loss = 0.61646786\n",
      "Iteration 1588, loss = 0.61648909\n",
      "Iteration 1589, loss = 0.61647036\n",
      "Iteration 1590, loss = 0.61646946\n",
      "Iteration 1591, loss = 0.61647105\n",
      "Iteration 1592, loss = 0.61646959\n",
      "Iteration 1593, loss = 0.61646875\n",
      "Iteration 1594, loss = 0.61646836\n",
      "Iteration 1595, loss = 0.61647051\n",
      "Iteration 1596, loss = 0.61646731\n",
      "Iteration 1597, loss = 0.61646176\n",
      "Iteration 1598, loss = 0.61646656\n",
      "Iteration 1599, loss = 0.61646788\n",
      "Iteration 1600, loss = 0.61648316\n",
      "Iteration 1601, loss = 0.61647266\n",
      "Iteration 1602, loss = 0.61647987\n",
      "Iteration 1603, loss = 0.61647567\n",
      "Iteration 1604, loss = 0.61647701\n",
      "Iteration 1605, loss = 0.61647502\n",
      "Iteration 1606, loss = 0.61647333\n",
      "Iteration 1607, loss = 0.61647375\n",
      "Iteration 1608, loss = 0.61646896\n",
      "Iteration 1609, loss = 0.61647423\n",
      "Iteration 1610, loss = 0.61646772\n",
      "Iteration 1611, loss = 0.61647911\n",
      "Iteration 1612, loss = 0.61647057\n",
      "Iteration 1613, loss = 0.61647212\n",
      "Iteration 1614, loss = 0.61646973\n",
      "Iteration 1615, loss = 0.61647078\n",
      "Iteration 1616, loss = 0.61646984\n",
      "Iteration 1617, loss = 0.61646885\n",
      "Iteration 1618, loss = 0.61646942\n",
      "Iteration 1619, loss = 0.61646888\n",
      "Iteration 1620, loss = 0.61647193\n",
      "Iteration 1621, loss = 0.61647325\n",
      "Iteration 1622, loss = 0.61647173\n",
      "Iteration 1623, loss = 0.61647742\n",
      "Iteration 1624, loss = 0.61647239\n",
      "Iteration 1625, loss = 0.61647114\n",
      "Iteration 1626, loss = 0.61647161\n",
      "Iteration 1627, loss = 0.61647043\n",
      "Iteration 1628, loss = 0.61647368\n",
      "Iteration 1629, loss = 0.61646998\n",
      "Iteration 1630, loss = 0.61647087\n",
      "Iteration 1631, loss = 0.61647077\n",
      "Iteration 1632, loss = 0.61647357\n",
      "Iteration 1633, loss = 0.61647089\n",
      "Iteration 1634, loss = 0.61647369\n",
      "Iteration 1635, loss = 0.61647225\n",
      "Iteration 1636, loss = 0.61647296\n",
      "Iteration 1637, loss = 0.61647268\n",
      "Iteration 1638, loss = 0.61647224\n",
      "Iteration 1639, loss = 0.61647322\n",
      "Iteration 1640, loss = 0.61648677\n",
      "Iteration 1641, loss = 0.61647634\n",
      "Iteration 1642, loss = 0.61647413\n",
      "Iteration 1643, loss = 0.61647326\n",
      "Iteration 1644, loss = 0.61647067\n",
      "Iteration 1645, loss = 0.61647434\n",
      "Iteration 1646, loss = 0.61647142\n",
      "Iteration 1647, loss = 0.61647004\n",
      "Iteration 1648, loss = 0.61647641\n",
      "Iteration 1649, loss = 0.61647673\n",
      "Iteration 1650, loss = 0.61646735\n",
      "Iteration 1651, loss = 0.61646978\n",
      "Iteration 1652, loss = 0.61646891\n",
      "Iteration 1653, loss = 0.61647385\n",
      "Iteration 1654, loss = 0.61646997\n",
      "Iteration 1655, loss = 0.61646663\n",
      "Iteration 1656, loss = 0.61647181\n",
      "Iteration 1657, loss = 0.61646748\n",
      "Iteration 1658, loss = 0.61646647\n",
      "Iteration 1659, loss = 0.61646386\n",
      "Iteration 1660, loss = 0.61646943\n",
      "Iteration 1661, loss = 0.61646482\n",
      "Iteration 1662, loss = 0.61646707\n",
      "Iteration 1663, loss = 0.61646701\n",
      "Iteration 1664, loss = 0.61646495\n",
      "Iteration 1665, loss = 0.61646443\n",
      "Iteration 1666, loss = 0.61646984\n",
      "Iteration 1667, loss = 0.61646814\n",
      "Iteration 1668, loss = 0.61647398\n",
      "Iteration 1669, loss = 0.61646684\n",
      "Iteration 1670, loss = 0.61646968\n",
      "Iteration 1671, loss = 0.61646905\n",
      "Iteration 1672, loss = 0.61646695\n",
      "Iteration 1673, loss = 0.61646697\n",
      "Iteration 1674, loss = 0.61646649\n",
      "Iteration 1675, loss = 0.61646593\n",
      "Iteration 1676, loss = 0.61646862\n",
      "Iteration 1677, loss = 0.61646904\n",
      "Iteration 1678, loss = 0.61647014\n",
      "Iteration 1679, loss = 0.61646935\n",
      "Iteration 1680, loss = 0.61646954\n",
      "Iteration 1681, loss = 0.61646913\n",
      "Iteration 1682, loss = 0.61647040\n",
      "Iteration 1683, loss = 0.61647031\n",
      "Iteration 1684, loss = 0.61647098\n",
      "Iteration 1685, loss = 0.61647146\n",
      "Iteration 1686, loss = 0.61646774\n",
      "Iteration 1687, loss = 0.61646844\n",
      "Iteration 1688, loss = 0.61646494\n",
      "Iteration 1689, loss = 0.61646314\n",
      "Iteration 1690, loss = 0.61647559\n",
      "Iteration 1691, loss = 0.61646450\n",
      "Iteration 1692, loss = 0.61646301\n",
      "Iteration 1693, loss = 0.61646660\n",
      "Iteration 1694, loss = 0.61646906\n",
      "Iteration 1695, loss = 0.61646641\n",
      "Iteration 1696, loss = 0.61646450\n",
      "Iteration 1697, loss = 0.61646463\n",
      "Iteration 1698, loss = 0.61646328\n",
      "Iteration 1699, loss = 0.61646187\n",
      "Iteration 1700, loss = 0.61646437\n",
      "Iteration 1701, loss = 0.61647279\n",
      "Iteration 1702, loss = 0.61646593\n",
      "Iteration 1703, loss = 0.61646505\n",
      "Iteration 1704, loss = 0.61647666\n",
      "Iteration 1705, loss = 0.61646720\n",
      "Iteration 1706, loss = 0.61646886\n",
      "Iteration 1707, loss = 0.61646715\n",
      "Iteration 1708, loss = 0.61646717\n",
      "Iteration 1709, loss = 0.61646731\n",
      "Iteration 1710, loss = 0.61646791\n",
      "Iteration 1711, loss = 0.61646842\n",
      "Iteration 1712, loss = 0.61646488\n",
      "Iteration 1713, loss = 0.61646395\n",
      "Iteration 1714, loss = 0.61647026\n",
      "Iteration 1715, loss = 0.61647938\n",
      "Iteration 1716, loss = 0.61647158\n",
      "Iteration 1717, loss = 0.61647144\n",
      "Iteration 1718, loss = 0.61647018\n",
      "Iteration 1719, loss = 0.61646985\n",
      "Iteration 1720, loss = 0.61647328\n",
      "Iteration 1721, loss = 0.61646835\n",
      "Iteration 1722, loss = 0.61646397\n",
      "Iteration 1723, loss = 0.61645843\n",
      "Iteration 1724, loss = 0.61646783\n",
      "Iteration 1725, loss = 0.61646402\n",
      "Iteration 1726, loss = 0.61646367\n",
      "Iteration 1727, loss = 0.61645915\n",
      "Iteration 1728, loss = 0.61647364\n",
      "Iteration 1729, loss = 0.61646397\n",
      "Iteration 1730, loss = 0.61646343\n",
      "Iteration 1731, loss = 0.61646417\n",
      "Iteration 1732, loss = 0.61646301\n",
      "Iteration 1733, loss = 0.61647098\n",
      "Iteration 1734, loss = 0.61646584\n",
      "Iteration 1735, loss = 0.61646850\n",
      "Iteration 1736, loss = 0.61646617\n",
      "Iteration 1737, loss = 0.61646349\n",
      "Iteration 1738, loss = 0.61646346\n",
      "Iteration 1739, loss = 0.61646117\n",
      "Iteration 1740, loss = 0.61646253\n",
      "Iteration 1741, loss = 0.61646434\n",
      "Iteration 1742, loss = 0.61646924\n",
      "Iteration 1743, loss = 0.61646359\n",
      "Iteration 1744, loss = 0.61646471\n",
      "Iteration 1745, loss = 0.61646739\n",
      "Iteration 1746, loss = 0.61646004\n",
      "Iteration 1747, loss = 0.61645911\n",
      "Iteration 1748, loss = 0.61646509\n",
      "Iteration 1749, loss = 0.61646326\n",
      "Iteration 1750, loss = 0.61647037\n",
      "Iteration 1751, loss = 0.61646447\n",
      "Iteration 1752, loss = 0.61647094\n",
      "Iteration 1753, loss = 0.61646714\n",
      "Iteration 1754, loss = 0.61646414\n",
      "Iteration 1755, loss = 0.61646392\n",
      "Iteration 1756, loss = 0.61646339\n",
      "Iteration 1757, loss = 0.61646880\n",
      "Iteration 1758, loss = 0.61646332\n",
      "Iteration 1759, loss = 0.61645937\n",
      "Iteration 1760, loss = 0.61646890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1761, loss = 0.61646386\n",
      "Iteration 1762, loss = 0.61646746\n",
      "Iteration 1763, loss = 0.61646524\n",
      "Iteration 1764, loss = 0.61646897\n",
      "Iteration 1765, loss = 0.61647471\n",
      "Iteration 1766, loss = 0.61646988\n",
      "Iteration 1767, loss = 0.61646900\n",
      "Iteration 1768, loss = 0.61646979\n",
      "Iteration 1769, loss = 0.61646777\n",
      "Iteration 1770, loss = 0.61647196\n",
      "Iteration 1771, loss = 0.61646759\n",
      "Iteration 1772, loss = 0.61646728\n",
      "Iteration 1773, loss = 0.61647284\n",
      "Iteration 1774, loss = 0.61646901\n",
      "Iteration 1775, loss = 0.61646891\n",
      "Iteration 1776, loss = 0.61646692\n",
      "Iteration 1777, loss = 0.61648148\n",
      "Iteration 1778, loss = 0.61647259\n",
      "Iteration 1779, loss = 0.61647302\n",
      "Iteration 1780, loss = 0.61647485\n",
      "Iteration 1781, loss = 0.61647074\n",
      "Iteration 1782, loss = 0.61646762\n",
      "Iteration 1783, loss = 0.61646197\n",
      "Iteration 1784, loss = 0.61646288\n",
      "Iteration 1785, loss = 0.61646650\n",
      "Iteration 1786, loss = 0.61647233\n",
      "Iteration 1787, loss = 0.61646106\n",
      "Iteration 1788, loss = 0.61646382\n",
      "Iteration 1789, loss = 0.61646320\n",
      "Iteration 1790, loss = 0.61645983\n",
      "Iteration 1791, loss = 0.61646658\n",
      "Iteration 1792, loss = 0.61646122\n",
      "Iteration 1793, loss = 0.61645937\n",
      "Iteration 1794, loss = 0.61646113\n",
      "Iteration 1795, loss = 0.61645865\n",
      "Iteration 1796, loss = 0.61645804\n",
      "Iteration 1797, loss = 0.61646007\n",
      "Iteration 1798, loss = 0.61646125\n",
      "Iteration 1799, loss = 0.61646272\n",
      "Iteration 1800, loss = 0.61647681\n",
      "Iteration 1801, loss = 0.61646819\n",
      "Iteration 1802, loss = 0.61646968\n",
      "Iteration 1803, loss = 0.61647078\n",
      "Iteration 1804, loss = 0.61647186\n",
      "Iteration 1805, loss = 0.61647152\n",
      "Iteration 1806, loss = 0.61647260\n",
      "Iteration 1807, loss = 0.61647716\n",
      "Iteration 1808, loss = 0.61648452\n",
      "Iteration 1809, loss = 0.61647829\n",
      "Iteration 1810, loss = 0.61648032\n",
      "Iteration 1811, loss = 0.61648520\n",
      "Iteration 1812, loss = 0.61647618\n",
      "Iteration 1813, loss = 0.61647552\n",
      "Iteration 1814, loss = 0.61647730\n",
      "Iteration 1815, loss = 0.61648270\n",
      "Iteration 1816, loss = 0.61648132\n",
      "Iteration 1817, loss = 0.61648064\n",
      "Iteration 1818, loss = 0.61647937\n",
      "Iteration 1819, loss = 0.61648070\n",
      "Iteration 1820, loss = 0.61648051\n",
      "Iteration 1821, loss = 0.61647691\n",
      "Iteration 1822, loss = 0.61647411\n",
      "Iteration 1823, loss = 0.61647823\n",
      "Iteration 1824, loss = 0.61647190\n",
      "Iteration 1825, loss = 0.61647433\n",
      "Iteration 1826, loss = 0.61646948\n",
      "Iteration 1827, loss = 0.61646660\n",
      "Iteration 1828, loss = 0.61645984\n",
      "Iteration 1829, loss = 0.61646518\n",
      "Iteration 1830, loss = 0.61647828\n",
      "Iteration 1831, loss = 0.61646077\n",
      "Iteration 1832, loss = 0.61646911\n",
      "Iteration 1833, loss = 0.61646410\n",
      "Iteration 1834, loss = 0.61646220\n",
      "Iteration 1835, loss = 0.61646167\n",
      "Iteration 1836, loss = 0.61646206\n",
      "Iteration 1837, loss = 0.61646147\n",
      "Iteration 1838, loss = 0.61645998\n",
      "Iteration 1839, loss = 0.61645683\n",
      "Iteration 1840, loss = 0.61645614\n",
      "Iteration 1841, loss = 0.61645571\n",
      "Iteration 1842, loss = 0.61646061\n",
      "Iteration 1843, loss = 0.61646854\n",
      "Iteration 1844, loss = 0.61646473\n",
      "Iteration 1845, loss = 0.61646459\n",
      "Iteration 1846, loss = 0.61646282\n",
      "Iteration 1847, loss = 0.61646249\n",
      "Iteration 1848, loss = 0.61646129\n",
      "Iteration 1849, loss = 0.61646553\n",
      "Iteration 1850, loss = 0.61646199\n",
      "Iteration 1851, loss = 0.61646334\n",
      "Iteration 1852, loss = 0.61646224\n",
      "Iteration 1853, loss = 0.61646218\n",
      "Iteration 1854, loss = 0.61646194\n",
      "Iteration 1855, loss = 0.61646213\n",
      "Iteration 1856, loss = 0.61646229\n",
      "Iteration 1857, loss = 0.61646018\n",
      "Iteration 1858, loss = 0.61646512\n",
      "Iteration 1859, loss = 0.61646274\n",
      "Iteration 1860, loss = 0.61646098\n",
      "Iteration 1861, loss = 0.61646026\n",
      "Iteration 1862, loss = 0.61646023\n",
      "Iteration 1863, loss = 0.61646396\n",
      "Iteration 1864, loss = 0.61646388\n",
      "Iteration 1865, loss = 0.61646606\n",
      "Iteration 1866, loss = 0.61647300\n",
      "Iteration 1867, loss = 0.61646365\n",
      "Iteration 1868, loss = 0.61646216\n",
      "Iteration 1869, loss = 0.61646232\n",
      "Iteration 1870, loss = 0.61646227\n",
      "Iteration 1871, loss = 0.61646372\n",
      "Iteration 1872, loss = 0.61646301\n",
      "Iteration 1873, loss = 0.61646196\n",
      "Iteration 1874, loss = 0.61646176\n",
      "Iteration 1875, loss = 0.61646132\n",
      "Iteration 1876, loss = 0.61646169\n",
      "Iteration 1877, loss = 0.61646201\n",
      "Iteration 1878, loss = 0.61646115\n",
      "Iteration 1879, loss = 0.61646137\n",
      "Iteration 1880, loss = 0.61646174\n",
      "Iteration 1881, loss = 0.61646146\n",
      "Iteration 1882, loss = 0.61646038\n",
      "Iteration 1883, loss = 0.61646092\n",
      "Iteration 1884, loss = 0.61646015\n",
      "Iteration 1885, loss = 0.61646059\n",
      "Iteration 1886, loss = 0.61646251\n",
      "Iteration 1887, loss = 0.61645913\n",
      "Iteration 1888, loss = 0.61645753\n",
      "Iteration 1889, loss = 0.61645847\n",
      "Iteration 1890, loss = 0.61646867\n",
      "Iteration 1891, loss = 0.61646322\n",
      "Iteration 1892, loss = 0.61646177\n",
      "Iteration 1893, loss = 0.61646143\n",
      "Iteration 1894, loss = 0.61645837\n",
      "Iteration 1895, loss = 0.61646736\n",
      "Iteration 1896, loss = 0.61645974\n",
      "Iteration 1897, loss = 0.61645897\n",
      "Iteration 1898, loss = 0.61645743\n",
      "Iteration 1899, loss = 0.61645591\n",
      "Iteration 1900, loss = 0.61646777\n",
      "Iteration 1901, loss = 0.61646220\n",
      "Iteration 1902, loss = 0.61646070\n",
      "Iteration 1903, loss = 0.61646174\n",
      "Iteration 1904, loss = 0.61646124\n",
      "Iteration 1905, loss = 0.61645911\n",
      "Iteration 1906, loss = 0.61646031\n",
      "Iteration 1907, loss = 0.61646634\n",
      "Iteration 1908, loss = 0.61645840\n",
      "Iteration 1909, loss = 0.61646228\n",
      "Iteration 1910, loss = 0.61645952\n",
      "Iteration 1911, loss = 0.61645899\n",
      "Iteration 1912, loss = 0.61645861\n",
      "Iteration 1913, loss = 0.61646166\n",
      "Iteration 1914, loss = 0.61646060\n",
      "Iteration 1915, loss = 0.61645977\n",
      "Iteration 1916, loss = 0.61646016\n",
      "Iteration 1917, loss = 0.61645992\n",
      "Iteration 1918, loss = 0.61645872\n",
      "Iteration 1919, loss = 0.61646423\n",
      "Iteration 1920, loss = 0.61645955\n",
      "Iteration 1921, loss = 0.61645962\n",
      "Iteration 1922, loss = 0.61646057\n",
      "Iteration 1923, loss = 0.61645400\n",
      "Iteration 1924, loss = 0.61645551\n",
      "Iteration 1925, loss = 0.61645867\n",
      "Iteration 1926, loss = 0.61645764\n",
      "Iteration 1927, loss = 0.61645675\n",
      "Iteration 1928, loss = 0.61646743\n",
      "Iteration 1929, loss = 0.61645822\n",
      "Iteration 1930, loss = 0.61646007\n",
      "Iteration 1931, loss = 0.61646257\n",
      "Iteration 1932, loss = 0.61647810\n",
      "Iteration 1933, loss = 0.61647572\n",
      "Iteration 1934, loss = 0.61648916\n",
      "Iteration 1935, loss = 0.61648330\n",
      "Iteration 1936, loss = 0.61648008\n",
      "Iteration 1937, loss = 0.61647956\n",
      "Iteration 1938, loss = 0.61647708\n",
      "Iteration 1939, loss = 0.61648030\n",
      "Iteration 1940, loss = 0.61647577\n",
      "Iteration 1941, loss = 0.61647413\n",
      "Iteration 1942, loss = 0.61648512\n",
      "Iteration 1943, loss = 0.61648001\n",
      "Iteration 1944, loss = 0.61648078\n",
      "Iteration 1945, loss = 0.61648159\n",
      "Iteration 1946, loss = 0.61647898\n",
      "Iteration 1947, loss = 0.61648125\n",
      "Iteration 1948, loss = 0.61648419\n",
      "Iteration 1949, loss = 0.61649296\n",
      "Iteration 1950, loss = 0.61649231\n",
      "Iteration 1951, loss = 0.61649423\n",
      "Iteration 1952, loss = 0.61650008\n",
      "Iteration 1953, loss = 0.61649779\n",
      "Iteration 1954, loss = 0.61649668\n",
      "Iteration 1955, loss = 0.61649707\n",
      "Iteration 1956, loss = 0.61649581\n",
      "Iteration 1957, loss = 0.61649548\n",
      "Iteration 1958, loss = 0.61649753\n",
      "Iteration 1959, loss = 0.61650140\n",
      "Iteration 1960, loss = 0.61649841\n",
      "Iteration 1961, loss = 0.61649545\n",
      "Iteration 1962, loss = 0.61648886\n",
      "Iteration 1963, loss = 0.61647818\n",
      "Iteration 1964, loss = 0.61647634\n",
      "Iteration 1965, loss = 0.61645773\n",
      "Iteration 1966, loss = 0.61648678\n",
      "Iteration 1967, loss = 0.61647425\n",
      "Iteration 1968, loss = 0.61647249\n",
      "Iteration 1969, loss = 0.61646237\n",
      "Iteration 1970, loss = 0.61646157\n",
      "Iteration 1971, loss = 0.61646032\n",
      "Iteration 1972, loss = 0.61646269\n",
      "Iteration 1973, loss = 0.61646465\n",
      "Iteration 1974, loss = 0.61646111\n",
      "Iteration 1975, loss = 0.61646023\n",
      "Iteration 1976, loss = 0.61646428\n",
      "Iteration 1977, loss = 0.61646186\n",
      "Iteration 1978, loss = 0.61646118\n",
      "Iteration 1979, loss = 0.61646197\n",
      "Iteration 1980, loss = 0.61646049\n",
      "Iteration 1981, loss = 0.61646026\n",
      "Iteration 1982, loss = 0.61646164\n",
      "Iteration 1983, loss = 0.61646068\n",
      "Iteration 1984, loss = 0.61645988\n",
      "Iteration 1985, loss = 0.61646135\n",
      "Iteration 1986, loss = 0.61646026\n",
      "Iteration 1987, loss = 0.61647044\n",
      "Iteration 1988, loss = 0.61646744\n",
      "Iteration 1989, loss = 0.61646157\n",
      "Iteration 1990, loss = 0.61646576\n",
      "Iteration 1991, loss = 0.61646326\n",
      "Iteration 1992, loss = 0.61646332\n",
      "Iteration 1993, loss = 0.61646347\n",
      "Iteration 1994, loss = 0.61646333\n",
      "Iteration 1995, loss = 0.61646378\n",
      "Iteration 1996, loss = 0.61646892\n",
      "Iteration 1997, loss = 0.61646605\n",
      "Iteration 1998, loss = 0.61646553\n",
      "Iteration 1999, loss = 0.61647083\n",
      "Iteration 2000, loss = 0.61646807\n",
      "Iteration 2001, loss = 0.61646899\n",
      "Iteration 2002, loss = 0.61646894\n",
      "Iteration 2003, loss = 0.61646648\n",
      "Iteration 2004, loss = 0.61646426\n",
      "Iteration 2005, loss = 0.61647043\n",
      "Iteration 2006, loss = 0.61647210\n",
      "Iteration 2007, loss = 0.61647592\n",
      "Iteration 2008, loss = 0.61648897\n",
      "Iteration 2009, loss = 0.61648725\n",
      "Iteration 2010, loss = 0.61649563\n",
      "Iteration 2011, loss = 0.61650482\n",
      "Iteration 2012, loss = 0.61650367\n",
      "Iteration 2013, loss = 0.61652175\n",
      "Iteration 2014, loss = 0.61650589\n",
      "Iteration 2015, loss = 0.61650439\n",
      "Iteration 2016, loss = 0.61649657\n",
      "Iteration 2017, loss = 0.61648475\n",
      "Iteration 2018, loss = 0.61648251\n",
      "Iteration 2019, loss = 0.61648699\n",
      "Iteration 2020, loss = 0.61648761\n",
      "Iteration 2021, loss = 0.61647881\n",
      "Iteration 2022, loss = 0.61648203\n",
      "Iteration 2023, loss = 0.61647378\n",
      "Iteration 2024, loss = 0.61647433\n",
      "Iteration 2025, loss = 0.61647482\n",
      "Iteration 2026, loss = 0.61647397\n",
      "Iteration 2027, loss = 0.61647097\n",
      "Iteration 2028, loss = 0.61647711\n",
      "Iteration 2029, loss = 0.61647003\n",
      "Iteration 2030, loss = 0.61647203\n",
      "Iteration 2031, loss = 0.61646901\n",
      "Iteration 2032, loss = 0.61646985\n",
      "Iteration 2033, loss = 0.61646770\n",
      "Iteration 2034, loss = 0.61646732\n",
      "Iteration 2035, loss = 0.61646601\n",
      "Iteration 2036, loss = 0.61646283\n",
      "Iteration 2037, loss = 0.61646524\n",
      "Iteration 2038, loss = 0.61646051\n",
      "Iteration 2039, loss = 0.61645606\n",
      "Iteration 2040, loss = 0.61646523\n",
      "Iteration 2041, loss = 0.61646675\n",
      "Iteration 2042, loss = 0.61645922\n",
      "Iteration 2043, loss = 0.61646006\n",
      "Iteration 2044, loss = 0.61646026\n",
      "Iteration 2045, loss = 0.61646447\n",
      "Iteration 2046, loss = 0.61645915\n",
      "Iteration 2047, loss = 0.61645897\n",
      "Iteration 2048, loss = 0.61646161\n",
      "Iteration 2049, loss = 0.61645868\n",
      "Iteration 2050, loss = 0.61645965\n",
      "Iteration 2051, loss = 0.61646247\n",
      "Iteration 2052, loss = 0.61646281\n",
      "Iteration 2053, loss = 0.61646137\n",
      "Iteration 2054, loss = 0.61646720\n",
      "Iteration 2055, loss = 0.61646024\n",
      "Iteration 2056, loss = 0.61646222\n",
      "Iteration 2057, loss = 0.61646056\n",
      "Iteration 2058, loss = 0.61646027\n",
      "Iteration 2059, loss = 0.61646057\n",
      "Iteration 2060, loss = 0.61646127\n",
      "Iteration 2061, loss = 0.61646048\n",
      "Iteration 2062, loss = 0.61646127\n",
      "Iteration 2063, loss = 0.61645907\n",
      "Iteration 2064, loss = 0.61646182\n",
      "Iteration 2065, loss = 0.61645972\n",
      "Iteration 2066, loss = 0.61646397\n",
      "Iteration 2067, loss = 0.61645844\n",
      "Iteration 2068, loss = 0.61646065\n",
      "Iteration 2069, loss = 0.61645848\n",
      "Iteration 2070, loss = 0.61645917\n",
      "Iteration 2071, loss = 0.61645631\n",
      "Iteration 2072, loss = 0.61645782\n",
      "Iteration 2073, loss = 0.61647358\n",
      "Iteration 2074, loss = 0.61646783\n",
      "Iteration 2075, loss = 0.61646835\n",
      "Iteration 2076, loss = 0.61647614\n",
      "Iteration 2077, loss = 0.61647425\n",
      "Iteration 2078, loss = 0.61647443\n",
      "Iteration 2079, loss = 0.61647632\n",
      "Iteration 2080, loss = 0.61647325\n",
      "Iteration 2081, loss = 0.61647340\n",
      "Iteration 2082, loss = 0.61647420\n",
      "Iteration 2083, loss = 0.61646858\n",
      "Iteration 2084, loss = 0.61646575\n",
      "Iteration 2085, loss = 0.61646934\n",
      "Iteration 2086, loss = 0.61647098\n",
      "Iteration 2087, loss = 0.61646516\n",
      "Iteration 2088, loss = 0.61646332\n",
      "Iteration 2089, loss = 0.61646747\n",
      "Iteration 2090, loss = 0.61646259\n",
      "Iteration 2091, loss = 0.61646166\n",
      "Iteration 2092, loss = 0.61645906\n",
      "Iteration 2093, loss = 0.61646202\n",
      "Iteration 2094, loss = 0.61645742\n",
      "Iteration 2095, loss = 0.61647116\n",
      "Iteration 2096, loss = 0.61645909\n",
      "Iteration 2097, loss = 0.61645801\n",
      "Iteration 2098, loss = 0.61645797\n",
      "Iteration 2099, loss = 0.61645718\n",
      "Iteration 2100, loss = 0.61646167\n",
      "Iteration 2101, loss = 0.61646391\n",
      "Iteration 2102, loss = 0.61646461\n",
      "Iteration 2103, loss = 0.61646334\n",
      "Iteration 2104, loss = 0.61646276\n",
      "Iteration 2105, loss = 0.61646519\n",
      "Iteration 2106, loss = 0.61646434\n",
      "Iteration 2107, loss = 0.61646271\n",
      "Iteration 2108, loss = 0.61646319\n",
      "Iteration 2109, loss = 0.61646248\n",
      "Iteration 2110, loss = 0.61646395\n",
      "Iteration 2111, loss = 0.61646306\n",
      "Iteration 2112, loss = 0.61646263\n",
      "Iteration 2113, loss = 0.61646166\n",
      "Iteration 2114, loss = 0.61645743\n",
      "Iteration 2115, loss = 0.61646210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2116, loss = 0.61646790\n",
      "Iteration 2117, loss = 0.61645818\n",
      "Iteration 2118, loss = 0.61645795\n",
      "Training loss did not improve more than tol=0.000100 for 2000 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.63532657\n",
      "Iteration 2, loss = 0.63499154\n",
      "Iteration 3, loss = 0.63465991\n",
      "Iteration 4, loss = 0.63436554\n",
      "Iteration 5, loss = 0.63409131\n",
      "Iteration 6, loss = 0.63381254\n",
      "Iteration 7, loss = 0.63355819\n",
      "Iteration 8, loss = 0.63328824\n",
      "Iteration 9, loss = 0.63305272\n",
      "Iteration 10, loss = 0.63281468\n",
      "Iteration 11, loss = 0.63259306\n",
      "Iteration 12, loss = 0.63235721\n",
      "Iteration 13, loss = 0.63215217\n",
      "Iteration 14, loss = 0.63194797\n",
      "Iteration 15, loss = 0.63176322\n",
      "Iteration 16, loss = 0.63155451\n",
      "Iteration 17, loss = 0.63136494\n",
      "Iteration 18, loss = 0.63117744\n",
      "Iteration 19, loss = 0.63099634\n",
      "Iteration 20, loss = 0.63081741\n",
      "Iteration 21, loss = 0.63062468\n",
      "Iteration 22, loss = 0.63044657\n",
      "Iteration 23, loss = 0.63027362\n",
      "Iteration 24, loss = 0.63009948\n",
      "Iteration 25, loss = 0.62991619\n",
      "Iteration 26, loss = 0.62974738\n",
      "Iteration 27, loss = 0.62958262\n",
      "Iteration 28, loss = 0.62942732\n",
      "Iteration 29, loss = 0.62927363\n",
      "Iteration 30, loss = 0.62911233\n",
      "Iteration 31, loss = 0.62895371\n",
      "Iteration 32, loss = 0.62877184\n",
      "Iteration 33, loss = 0.62859742\n",
      "Iteration 34, loss = 0.62842557\n",
      "Iteration 35, loss = 0.62828184\n",
      "Iteration 36, loss = 0.62812528\n",
      "Iteration 37, loss = 0.62798197\n",
      "Iteration 38, loss = 0.62784594\n",
      "Iteration 39, loss = 0.62771023\n",
      "Iteration 40, loss = 0.62757679\n",
      "Iteration 41, loss = 0.62744364\n",
      "Iteration 42, loss = 0.62732176\n",
      "Iteration 43, loss = 0.62719123\n",
      "Iteration 44, loss = 0.62704672\n",
      "Iteration 45, loss = 0.62691436\n",
      "Iteration 46, loss = 0.62677089\n",
      "Iteration 47, loss = 0.62664770\n",
      "Iteration 48, loss = 0.62650748\n",
      "Iteration 49, loss = 0.62638856\n",
      "Iteration 50, loss = 0.62625424\n",
      "Iteration 51, loss = 0.62615401\n",
      "Iteration 52, loss = 0.62605586\n",
      "Iteration 53, loss = 0.62595567\n",
      "Iteration 54, loss = 0.62584058\n",
      "Iteration 55, loss = 0.62571405\n",
      "Iteration 56, loss = 0.62559722\n",
      "Iteration 57, loss = 0.62547502\n",
      "Iteration 58, loss = 0.62537730\n",
      "Iteration 59, loss = 0.62525115\n",
      "Iteration 60, loss = 0.62514629\n",
      "Iteration 61, loss = 0.62504949\n",
      "Iteration 62, loss = 0.62494369\n",
      "Iteration 63, loss = 0.62483223\n",
      "Iteration 64, loss = 0.62473144\n",
      "Iteration 65, loss = 0.62463018\n",
      "Iteration 66, loss = 0.62453025\n",
      "Iteration 67, loss = 0.62444088\n",
      "Iteration 68, loss = 0.62433161\n",
      "Iteration 69, loss = 0.62424857\n",
      "Iteration 70, loss = 0.62417712\n",
      "Iteration 71, loss = 0.62406630\n",
      "Iteration 72, loss = 0.62396102\n",
      "Iteration 73, loss = 0.62389023\n",
      "Iteration 74, loss = 0.62378430\n",
      "Iteration 75, loss = 0.62369636\n",
      "Iteration 76, loss = 0.62361745\n",
      "Iteration 77, loss = 0.62352350\n",
      "Iteration 78, loss = 0.62343100\n",
      "Iteration 79, loss = 0.62336211\n",
      "Iteration 80, loss = 0.62330205\n",
      "Iteration 81, loss = 0.62318167\n",
      "Iteration 82, loss = 0.62309765\n",
      "Iteration 83, loss = 0.62302966\n",
      "Iteration 84, loss = 0.62293869\n",
      "Iteration 85, loss = 0.62286529\n",
      "Iteration 86, loss = 0.62279189\n",
      "Iteration 87, loss = 0.62271719\n",
      "Iteration 88, loss = 0.62263078\n",
      "Iteration 89, loss = 0.62256041\n",
      "Iteration 90, loss = 0.62248046\n",
      "Iteration 91, loss = 0.62241007\n",
      "Iteration 92, loss = 0.62233503\n",
      "Iteration 93, loss = 0.62226134\n",
      "Iteration 94, loss = 0.62219324\n",
      "Iteration 95, loss = 0.62211431\n",
      "Iteration 96, loss = 0.62206818\n",
      "Iteration 97, loss = 0.62199001\n",
      "Iteration 98, loss = 0.62190939\n",
      "Iteration 99, loss = 0.62185559\n",
      "Iteration 100, loss = 0.62180163\n",
      "Iteration 101, loss = 0.62172426\n",
      "Iteration 102, loss = 0.62165241\n",
      "Iteration 103, loss = 0.62158826\n",
      "Iteration 104, loss = 0.62156192\n",
      "Iteration 105, loss = 0.62148560\n",
      "Iteration 106, loss = 0.62142739\n",
      "Iteration 107, loss = 0.62137747\n",
      "Iteration 108, loss = 0.62133204\n",
      "Iteration 109, loss = 0.62124757\n",
      "Iteration 110, loss = 0.62119700\n",
      "Iteration 111, loss = 0.62113428\n",
      "Iteration 112, loss = 0.62108595\n",
      "Iteration 113, loss = 0.62102657\n",
      "Iteration 114, loss = 0.62097089\n",
      "Iteration 115, loss = 0.62092117\n",
      "Iteration 116, loss = 0.62089394\n",
      "Iteration 117, loss = 0.62081447\n",
      "Iteration 118, loss = 0.62076078\n",
      "Iteration 119, loss = 0.62070848\n",
      "Iteration 120, loss = 0.62065711\n",
      "Iteration 121, loss = 0.62060269\n",
      "Iteration 122, loss = 0.62056893\n",
      "Iteration 123, loss = 0.62051188\n",
      "Iteration 124, loss = 0.62046568\n",
      "Iteration 125, loss = 0.62042629\n",
      "Iteration 126, loss = 0.62038703\n",
      "Iteration 127, loss = 0.62033050\n",
      "Iteration 128, loss = 0.62029603\n",
      "Iteration 129, loss = 0.62023891\n",
      "Iteration 130, loss = 0.62018424\n",
      "Iteration 131, loss = 0.62016121\n",
      "Iteration 132, loss = 0.62010350\n",
      "Iteration 133, loss = 0.62006105\n",
      "Iteration 134, loss = 0.62001900\n",
      "Iteration 135, loss = 0.61998101\n",
      "Iteration 136, loss = 0.61993867\n",
      "Iteration 137, loss = 0.61989786\n",
      "Iteration 138, loss = 0.61987450\n",
      "Iteration 139, loss = 0.61982718\n",
      "Iteration 140, loss = 0.61979400\n",
      "Iteration 141, loss = 0.61975484\n",
      "Iteration 142, loss = 0.61971587\n",
      "Iteration 143, loss = 0.61967727\n",
      "Iteration 144, loss = 0.61964142\n",
      "Iteration 145, loss = 0.61961347\n",
      "Iteration 146, loss = 0.61956710\n",
      "Iteration 147, loss = 0.61952982\n",
      "Iteration 148, loss = 0.61950546\n",
      "Iteration 149, loss = 0.61945675\n",
      "Iteration 150, loss = 0.61942325\n",
      "Iteration 151, loss = 0.61938626\n",
      "Iteration 152, loss = 0.61935774\n",
      "Iteration 153, loss = 0.61932687\n",
      "Iteration 154, loss = 0.61931036\n",
      "Iteration 155, loss = 0.61928851\n",
      "Iteration 156, loss = 0.61927098\n",
      "Iteration 157, loss = 0.61921503\n",
      "Iteration 158, loss = 0.61918679\n",
      "Iteration 159, loss = 0.61915014\n",
      "Iteration 160, loss = 0.61914350\n",
      "Iteration 161, loss = 0.61911172\n",
      "Iteration 162, loss = 0.61908420\n",
      "Iteration 163, loss = 0.61910380\n",
      "Iteration 164, loss = 0.61905303\n",
      "Iteration 165, loss = 0.61901842\n",
      "Iteration 166, loss = 0.61899448\n",
      "Iteration 167, loss = 0.61895068\n",
      "Iteration 168, loss = 0.61890954\n",
      "Iteration 169, loss = 0.61891186\n",
      "Iteration 170, loss = 0.61883841\n",
      "Iteration 171, loss = 0.61884298\n",
      "Iteration 172, loss = 0.61878939\n",
      "Iteration 173, loss = 0.61875355\n",
      "Iteration 174, loss = 0.61873271\n",
      "Iteration 175, loss = 0.61869411\n",
      "Iteration 176, loss = 0.61867223\n",
      "Iteration 177, loss = 0.61865057\n",
      "Iteration 178, loss = 0.61862422\n",
      "Iteration 179, loss = 0.61859836\n",
      "Iteration 180, loss = 0.61859108\n",
      "Iteration 181, loss = 0.61858026\n",
      "Iteration 182, loss = 0.61854089\n",
      "Iteration 183, loss = 0.61852933\n",
      "Iteration 184, loss = 0.61852029\n",
      "Iteration 185, loss = 0.61847188\n",
      "Iteration 186, loss = 0.61844755\n",
      "Iteration 187, loss = 0.61842683\n",
      "Iteration 188, loss = 0.61843314\n",
      "Iteration 189, loss = 0.61840341\n",
      "Iteration 190, loss = 0.61837560\n",
      "Iteration 191, loss = 0.61836535\n",
      "Iteration 192, loss = 0.61833816\n",
      "Iteration 193, loss = 0.61831751\n",
      "Iteration 194, loss = 0.61829955\n",
      "Iteration 195, loss = 0.61827659\n",
      "Iteration 196, loss = 0.61826368\n",
      "Iteration 197, loss = 0.61824168\n",
      "Iteration 198, loss = 0.61822120\n",
      "Iteration 199, loss = 0.61820455\n",
      "Iteration 200, loss = 0.61818617\n",
      "Iteration 201, loss = 0.61816719\n",
      "Iteration 202, loss = 0.61814599\n",
      "Iteration 203, loss = 0.61814417\n",
      "Iteration 204, loss = 0.61811480\n",
      "Iteration 205, loss = 0.61810715\n",
      "Iteration 206, loss = 0.61810770\n",
      "Iteration 207, loss = 0.61807398\n",
      "Iteration 208, loss = 0.61805585\n",
      "Iteration 209, loss = 0.61803603\n",
      "Iteration 210, loss = 0.61802554\n",
      "Iteration 211, loss = 0.61800979\n",
      "Iteration 212, loss = 0.61798615\n",
      "Iteration 213, loss = 0.61797876\n",
      "Iteration 214, loss = 0.61797338\n",
      "Iteration 215, loss = 0.61795716\n",
      "Iteration 216, loss = 0.61795342\n",
      "Iteration 217, loss = 0.61794648\n",
      "Iteration 218, loss = 0.61793130\n",
      "Iteration 219, loss = 0.61791987\n",
      "Iteration 220, loss = 0.61791034\n",
      "Iteration 221, loss = 0.61789710\n",
      "Iteration 222, loss = 0.61788502\n",
      "Iteration 223, loss = 0.61787757\n",
      "Iteration 224, loss = 0.61786010\n",
      "Iteration 225, loss = 0.61785455\n",
      "Iteration 226, loss = 0.61784337\n",
      "Iteration 227, loss = 0.61782936\n",
      "Iteration 228, loss = 0.61782500\n",
      "Iteration 229, loss = 0.61779559\n",
      "Iteration 230, loss = 0.61778286\n",
      "Iteration 231, loss = 0.61776879\n",
      "Iteration 232, loss = 0.61775671\n",
      "Iteration 233, loss = 0.61775158\n",
      "Iteration 234, loss = 0.61775237\n",
      "Iteration 235, loss = 0.61776528\n",
      "Iteration 236, loss = 0.61774848\n",
      "Iteration 237, loss = 0.61773865\n",
      "Iteration 238, loss = 0.61773289\n",
      "Iteration 239, loss = 0.61771294\n",
      "Iteration 240, loss = 0.61773772\n",
      "Iteration 241, loss = 0.61771974\n",
      "Iteration 242, loss = 0.61773423\n",
      "Iteration 243, loss = 0.61774153\n",
      "Iteration 244, loss = 0.61771755\n",
      "Iteration 245, loss = 0.61769793\n",
      "Iteration 246, loss = 0.61770942\n",
      "Iteration 247, loss = 0.61766839\n",
      "Iteration 248, loss = 0.61764807\n",
      "Iteration 249, loss = 0.61761726\n",
      "Iteration 250, loss = 0.61762375\n",
      "Iteration 251, loss = 0.61757734\n",
      "Iteration 252, loss = 0.61758393\n",
      "Iteration 253, loss = 0.61755038\n",
      "Iteration 254, loss = 0.61753992\n",
      "Iteration 255, loss = 0.61751953\n",
      "Iteration 256, loss = 0.61751221\n",
      "Iteration 257, loss = 0.61750206\n",
      "Iteration 258, loss = 0.61748585\n",
      "Iteration 259, loss = 0.61746785\n",
      "Iteration 260, loss = 0.61749318\n",
      "Iteration 261, loss = 0.61745804\n",
      "Iteration 262, loss = 0.61744496\n",
      "Iteration 263, loss = 0.61743364\n",
      "Iteration 264, loss = 0.61743380\n",
      "Iteration 265, loss = 0.61744435\n",
      "Iteration 266, loss = 0.61742945\n",
      "Iteration 267, loss = 0.61743140\n",
      "Iteration 268, loss = 0.61743596\n",
      "Iteration 269, loss = 0.61743018\n",
      "Iteration 270, loss = 0.61742229\n",
      "Iteration 271, loss = 0.61743226\n",
      "Iteration 272, loss = 0.61744321\n",
      "Iteration 273, loss = 0.61744152\n",
      "Iteration 274, loss = 0.61744672\n",
      "Iteration 275, loss = 0.61749954\n",
      "Iteration 276, loss = 0.61749978\n",
      "Iteration 277, loss = 0.61753694\n",
      "Iteration 278, loss = 0.61752217\n",
      "Iteration 279, loss = 0.61751540\n",
      "Iteration 280, loss = 0.61751036\n",
      "Iteration 281, loss = 0.61746826\n",
      "Iteration 282, loss = 0.61745062\n",
      "Iteration 283, loss = 0.61743789\n",
      "Iteration 284, loss = 0.61740693\n",
      "Iteration 285, loss = 0.61741407\n",
      "Iteration 286, loss = 0.61737744\n",
      "Iteration 287, loss = 0.61737385\n",
      "Iteration 288, loss = 0.61736129\n",
      "Iteration 289, loss = 0.61735703\n",
      "Iteration 290, loss = 0.61736031\n",
      "Iteration 291, loss = 0.61736231\n",
      "Iteration 292, loss = 0.61736312\n",
      "Iteration 293, loss = 0.61736396\n",
      "Iteration 294, loss = 0.61736050\n",
      "Iteration 295, loss = 0.61735400\n",
      "Iteration 296, loss = 0.61732282\n",
      "Iteration 297, loss = 0.61730953\n",
      "Iteration 298, loss = 0.61729091\n",
      "Iteration 299, loss = 0.61729616\n",
      "Iteration 300, loss = 0.61725355\n",
      "Iteration 301, loss = 0.61724765\n",
      "Iteration 302, loss = 0.61723876\n",
      "Iteration 303, loss = 0.61723105\n",
      "Iteration 304, loss = 0.61722198\n",
      "Iteration 305, loss = 0.61721290\n",
      "Iteration 306, loss = 0.61720983\n",
      "Iteration 307, loss = 0.61719730\n",
      "Iteration 308, loss = 0.61718765\n",
      "Iteration 309, loss = 0.61719192\n",
      "Iteration 310, loss = 0.61717157\n",
      "Iteration 311, loss = 0.61716580\n",
      "Iteration 312, loss = 0.61716661\n",
      "Iteration 313, loss = 0.61715631\n",
      "Iteration 314, loss = 0.61715328\n",
      "Iteration 315, loss = 0.61714961\n",
      "Iteration 316, loss = 0.61714676\n",
      "Iteration 317, loss = 0.61714377\n",
      "Iteration 318, loss = 0.61714592\n",
      "Iteration 319, loss = 0.61714199\n",
      "Iteration 320, loss = 0.61713720\n",
      "Iteration 321, loss = 0.61713049\n",
      "Iteration 322, loss = 0.61713235\n",
      "Iteration 323, loss = 0.61712026\n",
      "Iteration 324, loss = 0.61711527\n",
      "Iteration 325, loss = 0.61711067\n",
      "Iteration 326, loss = 0.61711247\n",
      "Iteration 327, loss = 0.61713392\n",
      "Iteration 328, loss = 0.61711625\n",
      "Iteration 329, loss = 0.61711240\n",
      "Iteration 330, loss = 0.61710608\n",
      "Iteration 331, loss = 0.61709513\n",
      "Iteration 332, loss = 0.61709993\n",
      "Iteration 333, loss = 0.61708692\n",
      "Iteration 334, loss = 0.61708208\n",
      "Iteration 335, loss = 0.61708106\n",
      "Iteration 336, loss = 0.61708783\n",
      "Iteration 337, loss = 0.61708711\n",
      "Iteration 338, loss = 0.61708598\n",
      "Iteration 339, loss = 0.61707680\n",
      "Iteration 340, loss = 0.61708904\n",
      "Iteration 341, loss = 0.61707505\n",
      "Iteration 342, loss = 0.61706223\n",
      "Iteration 343, loss = 0.61706796\n",
      "Iteration 344, loss = 0.61706564\n",
      "Iteration 345, loss = 0.61707400\n",
      "Iteration 346, loss = 0.61706982\n",
      "Iteration 347, loss = 0.61706889\n",
      "Iteration 348, loss = 0.61706698\n",
      "Iteration 349, loss = 0.61706511\n",
      "Iteration 350, loss = 0.61706309\n",
      "Iteration 351, loss = 0.61706078\n",
      "Iteration 352, loss = 0.61705876\n",
      "Iteration 353, loss = 0.61706024\n",
      "Iteration 354, loss = 0.61705670\n",
      "Iteration 355, loss = 0.61706229\n",
      "Iteration 356, loss = 0.61704540\n",
      "Iteration 357, loss = 0.61704371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 358, loss = 0.61704181\n",
      "Iteration 359, loss = 0.61704374\n",
      "Iteration 360, loss = 0.61704452\n",
      "Iteration 361, loss = 0.61704821\n",
      "Iteration 362, loss = 0.61705000\n",
      "Iteration 363, loss = 0.61706953\n",
      "Iteration 364, loss = 0.61705634\n",
      "Iteration 365, loss = 0.61705522\n",
      "Iteration 366, loss = 0.61703859\n",
      "Iteration 367, loss = 0.61702876\n",
      "Iteration 368, loss = 0.61703275\n",
      "Iteration 369, loss = 0.61701757\n",
      "Iteration 370, loss = 0.61702074\n",
      "Iteration 371, loss = 0.61702030\n",
      "Iteration 372, loss = 0.61702008\n",
      "Iteration 373, loss = 0.61704304\n",
      "Iteration 374, loss = 0.61704983\n",
      "Iteration 375, loss = 0.61704402\n",
      "Iteration 376, loss = 0.61704197\n",
      "Iteration 377, loss = 0.61703675\n",
      "Iteration 378, loss = 0.61703770\n",
      "Iteration 379, loss = 0.61705176\n",
      "Iteration 380, loss = 0.61704618\n",
      "Iteration 381, loss = 0.61704678\n",
      "Iteration 382, loss = 0.61704757\n",
      "Iteration 383, loss = 0.61706352\n",
      "Iteration 384, loss = 0.61703570\n",
      "Iteration 385, loss = 0.61699574\n",
      "Iteration 386, loss = 0.61700730\n",
      "Iteration 387, loss = 0.61695639\n",
      "Iteration 388, loss = 0.61695294\n",
      "Iteration 389, loss = 0.61693878\n",
      "Iteration 390, loss = 0.61691750\n",
      "Iteration 391, loss = 0.61693560\n",
      "Iteration 392, loss = 0.61690973\n",
      "Iteration 393, loss = 0.61691942\n",
      "Iteration 394, loss = 0.61690928\n",
      "Iteration 395, loss = 0.61691547\n",
      "Iteration 396, loss = 0.61691132\n",
      "Iteration 397, loss = 0.61690878\n",
      "Iteration 398, loss = 0.61690803\n",
      "Iteration 399, loss = 0.61691364\n",
      "Iteration 400, loss = 0.61690982\n",
      "Iteration 401, loss = 0.61690661\n",
      "Iteration 402, loss = 0.61690579\n",
      "Iteration 403, loss = 0.61692347\n",
      "Iteration 404, loss = 0.61696922\n",
      "Iteration 405, loss = 0.61691951\n",
      "Iteration 406, loss = 0.61691359\n",
      "Iteration 407, loss = 0.61690968\n",
      "Iteration 408, loss = 0.61690482\n",
      "Iteration 409, loss = 0.61689327\n",
      "Iteration 410, loss = 0.61687799\n",
      "Iteration 411, loss = 0.61685951\n",
      "Iteration 412, loss = 0.61685909\n",
      "Iteration 413, loss = 0.61689796\n",
      "Iteration 414, loss = 0.61686320\n",
      "Iteration 415, loss = 0.61686804\n",
      "Iteration 416, loss = 0.61685729\n",
      "Iteration 417, loss = 0.61691505\n",
      "Iteration 418, loss = 0.61689555\n",
      "Iteration 419, loss = 0.61690117\n",
      "Iteration 420, loss = 0.61690162\n",
      "Iteration 421, loss = 0.61689478\n",
      "Iteration 422, loss = 0.61687810\n",
      "Iteration 423, loss = 0.61691035\n",
      "Iteration 424, loss = 0.61688482\n",
      "Iteration 425, loss = 0.61686818\n",
      "Iteration 426, loss = 0.61686764\n",
      "Iteration 427, loss = 0.61687076\n",
      "Iteration 428, loss = 0.61687834\n",
      "Iteration 429, loss = 0.61688079\n",
      "Iteration 430, loss = 0.61688292\n",
      "Iteration 431, loss = 0.61692536\n",
      "Iteration 432, loss = 0.61690018\n",
      "Iteration 433, loss = 0.61689946\n",
      "Iteration 434, loss = 0.61689473\n",
      "Iteration 435, loss = 0.61688983\n",
      "Iteration 436, loss = 0.61689033\n",
      "Iteration 437, loss = 0.61689113\n",
      "Iteration 438, loss = 0.61692199\n",
      "Iteration 439, loss = 0.61690202\n",
      "Iteration 440, loss = 0.61689475\n",
      "Iteration 441, loss = 0.61689704\n",
      "Iteration 442, loss = 0.61688577\n",
      "Iteration 443, loss = 0.61688341\n",
      "Iteration 444, loss = 0.61688263\n",
      "Iteration 445, loss = 0.61687630\n",
      "Iteration 446, loss = 0.61687687\n",
      "Iteration 447, loss = 0.61687224\n",
      "Iteration 448, loss = 0.61687246\n",
      "Iteration 449, loss = 0.61687094\n",
      "Iteration 450, loss = 0.61687206\n",
      "Iteration 451, loss = 0.61687259\n",
      "Iteration 452, loss = 0.61687655\n",
      "Iteration 453, loss = 0.61688856\n",
      "Iteration 454, loss = 0.61688947\n",
      "Iteration 455, loss = 0.61690641\n",
      "Iteration 456, loss = 0.61690386\n",
      "Iteration 457, loss = 0.61689636\n",
      "Iteration 458, loss = 0.61688910\n",
      "Iteration 459, loss = 0.61688553\n",
      "Iteration 460, loss = 0.61687216\n",
      "Iteration 461, loss = 0.61686434\n",
      "Iteration 462, loss = 0.61685823\n",
      "Iteration 463, loss = 0.61686326\n",
      "Iteration 464, loss = 0.61684687\n",
      "Iteration 465, loss = 0.61685308\n",
      "Iteration 466, loss = 0.61684295\n",
      "Iteration 467, loss = 0.61682666\n",
      "Iteration 468, loss = 0.61684287\n",
      "Iteration 469, loss = 0.61681760\n",
      "Iteration 470, loss = 0.61681518\n",
      "Iteration 471, loss = 0.61680915\n",
      "Iteration 472, loss = 0.61680066\n",
      "Iteration 473, loss = 0.61680351\n",
      "Iteration 474, loss = 0.61679209\n",
      "Iteration 475, loss = 0.61679434\n",
      "Iteration 476, loss = 0.61678117\n",
      "Iteration 477, loss = 0.61678053\n",
      "Iteration 478, loss = 0.61677301\n",
      "Iteration 479, loss = 0.61679569\n",
      "Iteration 480, loss = 0.61677416\n",
      "Iteration 481, loss = 0.61677280\n",
      "Iteration 482, loss = 0.61677467\n",
      "Iteration 483, loss = 0.61677161\n",
      "Iteration 484, loss = 0.61676915\n",
      "Iteration 485, loss = 0.61676801\n",
      "Iteration 486, loss = 0.61676985\n",
      "Iteration 487, loss = 0.61676690\n",
      "Iteration 488, loss = 0.61676728\n",
      "Iteration 489, loss = 0.61676224\n",
      "Iteration 490, loss = 0.61676061\n",
      "Iteration 491, loss = 0.61675579\n",
      "Iteration 492, loss = 0.61675271\n",
      "Iteration 493, loss = 0.61674198\n",
      "Iteration 494, loss = 0.61676770\n",
      "Iteration 495, loss = 0.61675173\n",
      "Iteration 496, loss = 0.61676877\n",
      "Iteration 497, loss = 0.61676528\n",
      "Iteration 498, loss = 0.61676421\n",
      "Iteration 499, loss = 0.61676264\n",
      "Iteration 500, loss = 0.61675917\n",
      "Iteration 501, loss = 0.61678570\n",
      "Iteration 502, loss = 0.61677126\n",
      "Iteration 503, loss = 0.61676674\n",
      "Iteration 504, loss = 0.61678274\n",
      "Iteration 505, loss = 0.61678247\n",
      "Iteration 506, loss = 0.61679019\n",
      "Iteration 507, loss = 0.61679961\n",
      "Iteration 508, loss = 0.61681352\n",
      "Iteration 509, loss = 0.61681563\n",
      "Iteration 510, loss = 0.61682205\n",
      "Iteration 511, loss = 0.61682130\n",
      "Iteration 512, loss = 0.61682054\n",
      "Iteration 513, loss = 0.61682000\n",
      "Iteration 514, loss = 0.61681961\n",
      "Iteration 515, loss = 0.61682197\n",
      "Iteration 516, loss = 0.61682136\n",
      "Iteration 517, loss = 0.61682500\n",
      "Iteration 518, loss = 0.61682349\n",
      "Iteration 519, loss = 0.61682255\n",
      "Iteration 520, loss = 0.61682227\n",
      "Iteration 521, loss = 0.61681984\n",
      "Iteration 522, loss = 0.61681815\n",
      "Iteration 523, loss = 0.61681390\n",
      "Iteration 524, loss = 0.61681248\n",
      "Iteration 525, loss = 0.61680695\n",
      "Iteration 526, loss = 0.61680006\n",
      "Iteration 527, loss = 0.61679041\n",
      "Iteration 528, loss = 0.61679349\n",
      "Iteration 529, loss = 0.61678953\n",
      "Iteration 530, loss = 0.61677569\n",
      "Iteration 531, loss = 0.61677192\n",
      "Iteration 532, loss = 0.61677350\n",
      "Iteration 533, loss = 0.61677351\n",
      "Iteration 534, loss = 0.61677133\n",
      "Iteration 535, loss = 0.61676997\n",
      "Iteration 536, loss = 0.61677009\n",
      "Iteration 537, loss = 0.61677495\n",
      "Iteration 538, loss = 0.61677379\n",
      "Iteration 539, loss = 0.61677560\n",
      "Iteration 540, loss = 0.61678253\n",
      "Iteration 541, loss = 0.61678438\n",
      "Iteration 542, loss = 0.61677968\n",
      "Iteration 543, loss = 0.61677473\n",
      "Iteration 544, loss = 0.61676606\n",
      "Iteration 545, loss = 0.61676625\n",
      "Iteration 546, loss = 0.61675869\n",
      "Iteration 547, loss = 0.61674896\n",
      "Iteration 548, loss = 0.61675407\n",
      "Iteration 549, loss = 0.61674703\n",
      "Iteration 550, loss = 0.61674051\n",
      "Iteration 551, loss = 0.61673960\n",
      "Iteration 552, loss = 0.61673905\n",
      "Iteration 553, loss = 0.61673441\n",
      "Iteration 554, loss = 0.61674723\n",
      "Iteration 555, loss = 0.61672985\n",
      "Iteration 556, loss = 0.61673205\n",
      "Iteration 557, loss = 0.61672846\n",
      "Iteration 558, loss = 0.61672432\n",
      "Iteration 559, loss = 0.61672673\n",
      "Iteration 560, loss = 0.61671807\n",
      "Iteration 561, loss = 0.61671561\n",
      "Iteration 562, loss = 0.61671133\n",
      "Iteration 563, loss = 0.61671203\n",
      "Iteration 564, loss = 0.61670705\n",
      "Iteration 565, loss = 0.61670459\n",
      "Iteration 566, loss = 0.61670629\n",
      "Iteration 567, loss = 0.61670173\n",
      "Iteration 568, loss = 0.61670121\n",
      "Iteration 569, loss = 0.61670219\n",
      "Iteration 570, loss = 0.61669386\n",
      "Iteration 571, loss = 0.61669268\n",
      "Iteration 572, loss = 0.61669315\n",
      "Iteration 573, loss = 0.61668910\n",
      "Iteration 574, loss = 0.61669279\n",
      "Iteration 575, loss = 0.61668377\n",
      "Iteration 576, loss = 0.61668413\n",
      "Iteration 577, loss = 0.61668275\n",
      "Iteration 578, loss = 0.61668464\n",
      "Iteration 579, loss = 0.61668841\n",
      "Iteration 580, loss = 0.61668824\n",
      "Iteration 581, loss = 0.61668495\n",
      "Iteration 582, loss = 0.61668183\n",
      "Iteration 583, loss = 0.61668302\n",
      "Iteration 584, loss = 0.61667608\n",
      "Iteration 585, loss = 0.61667013\n",
      "Iteration 586, loss = 0.61667893\n",
      "Iteration 587, loss = 0.61667336\n",
      "Iteration 588, loss = 0.61666938\n",
      "Iteration 589, loss = 0.61666651\n",
      "Iteration 590, loss = 0.61666823\n",
      "Iteration 591, loss = 0.61666269\n",
      "Iteration 592, loss = 0.61666304\n",
      "Iteration 593, loss = 0.61666100\n",
      "Iteration 594, loss = 0.61665772\n",
      "Iteration 595, loss = 0.61666418\n",
      "Iteration 596, loss = 0.61665615\n",
      "Iteration 597, loss = 0.61665845\n",
      "Iteration 598, loss = 0.61665534\n",
      "Iteration 599, loss = 0.61665786\n",
      "Iteration 600, loss = 0.61665467\n",
      "Iteration 601, loss = 0.61665333\n",
      "Iteration 602, loss = 0.61665207\n",
      "Iteration 603, loss = 0.61664962\n",
      "Iteration 604, loss = 0.61664985\n",
      "Iteration 605, loss = 0.61664865\n",
      "Iteration 606, loss = 0.61664762\n",
      "Iteration 607, loss = 0.61664369\n",
      "Iteration 608, loss = 0.61664225\n",
      "Iteration 609, loss = 0.61664247\n",
      "Iteration 610, loss = 0.61666267\n",
      "Iteration 611, loss = 0.61665217\n",
      "Iteration 612, loss = 0.61666123\n",
      "Iteration 613, loss = 0.61665880\n",
      "Iteration 614, loss = 0.61666161\n",
      "Iteration 615, loss = 0.61665995\n",
      "Iteration 616, loss = 0.61666051\n",
      "Iteration 617, loss = 0.61665979\n",
      "Iteration 618, loss = 0.61665914\n",
      "Iteration 619, loss = 0.61666894\n",
      "Iteration 620, loss = 0.61666210\n",
      "Iteration 621, loss = 0.61666470\n",
      "Iteration 622, loss = 0.61666175\n",
      "Iteration 623, loss = 0.61666060\n",
      "Iteration 624, loss = 0.61665913\n",
      "Iteration 625, loss = 0.61665535\n",
      "Iteration 626, loss = 0.61665328\n",
      "Iteration 627, loss = 0.61665069\n",
      "Iteration 628, loss = 0.61664704\n",
      "Iteration 629, loss = 0.61664385\n",
      "Iteration 630, loss = 0.61664580\n",
      "Iteration 631, loss = 0.61663900\n",
      "Iteration 632, loss = 0.61664054\n",
      "Iteration 633, loss = 0.61663466\n",
      "Iteration 634, loss = 0.61663102\n",
      "Iteration 635, loss = 0.61663965\n",
      "Iteration 636, loss = 0.61662837\n",
      "Iteration 637, loss = 0.61663707\n",
      "Iteration 638, loss = 0.61662691\n",
      "Iteration 639, loss = 0.61662737\n",
      "Iteration 640, loss = 0.61662763\n",
      "Iteration 641, loss = 0.61662491\n",
      "Iteration 642, loss = 0.61662310\n",
      "Iteration 643, loss = 0.61662261\n",
      "Iteration 644, loss = 0.61662608\n",
      "Iteration 645, loss = 0.61661951\n",
      "Iteration 646, loss = 0.61662639\n",
      "Iteration 647, loss = 0.61662105\n",
      "Iteration 648, loss = 0.61661762\n",
      "Iteration 649, loss = 0.61661714\n",
      "Iteration 650, loss = 0.61661716\n",
      "Iteration 651, loss = 0.61661436\n",
      "Iteration 652, loss = 0.61661575\n",
      "Iteration 653, loss = 0.61661709\n",
      "Iteration 654, loss = 0.61661207\n",
      "Iteration 655, loss = 0.61661175\n",
      "Iteration 656, loss = 0.61661142\n",
      "Iteration 657, loss = 0.61661242\n",
      "Iteration 658, loss = 0.61661082\n",
      "Iteration 659, loss = 0.61661001\n",
      "Iteration 660, loss = 0.61660914\n",
      "Iteration 661, loss = 0.61661024\n",
      "Iteration 662, loss = 0.61660842\n",
      "Iteration 663, loss = 0.61660843\n",
      "Iteration 664, loss = 0.61660628\n",
      "Iteration 665, loss = 0.61660764\n",
      "Iteration 666, loss = 0.61660657\n",
      "Iteration 667, loss = 0.61660739\n",
      "Iteration 668, loss = 0.61660509\n",
      "Iteration 669, loss = 0.61660481\n",
      "Iteration 670, loss = 0.61660672\n",
      "Iteration 671, loss = 0.61660395\n",
      "Iteration 672, loss = 0.61660679\n",
      "Iteration 673, loss = 0.61660186\n",
      "Iteration 674, loss = 0.61660033\n",
      "Iteration 675, loss = 0.61659504\n",
      "Iteration 676, loss = 0.61659264\n",
      "Iteration 677, loss = 0.61659856\n",
      "Iteration 678, loss = 0.61659752\n",
      "Iteration 679, loss = 0.61659860\n",
      "Iteration 680, loss = 0.61659520\n",
      "Iteration 681, loss = 0.61659291\n",
      "Iteration 682, loss = 0.61659983\n",
      "Iteration 683, loss = 0.61659523\n",
      "Iteration 684, loss = 0.61659621\n",
      "Iteration 685, loss = 0.61659382\n",
      "Iteration 686, loss = 0.61659385\n",
      "Iteration 687, loss = 0.61659228\n",
      "Iteration 688, loss = 0.61659028\n",
      "Iteration 689, loss = 0.61660407\n",
      "Iteration 690, loss = 0.61659261\n",
      "Iteration 691, loss = 0.61658995\n",
      "Iteration 692, loss = 0.61658930\n",
      "Iteration 693, loss = 0.61658922\n",
      "Iteration 694, loss = 0.61658801\n",
      "Iteration 695, loss = 0.61658826\n",
      "Iteration 696, loss = 0.61658616\n",
      "Iteration 697, loss = 0.61658611\n",
      "Iteration 698, loss = 0.61658563\n",
      "Iteration 699, loss = 0.61658850\n",
      "Iteration 700, loss = 0.61658507\n",
      "Iteration 701, loss = 0.61658475\n",
      "Iteration 702, loss = 0.61658578\n",
      "Iteration 703, loss = 0.61658220\n",
      "Iteration 704, loss = 0.61658350\n",
      "Iteration 705, loss = 0.61657934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 706, loss = 0.61658979\n",
      "Iteration 707, loss = 0.61658305\n",
      "Iteration 708, loss = 0.61658174\n",
      "Iteration 709, loss = 0.61658214\n",
      "Iteration 710, loss = 0.61658089\n",
      "Iteration 711, loss = 0.61658133\n",
      "Iteration 712, loss = 0.61657795\n",
      "Iteration 713, loss = 0.61657987\n",
      "Iteration 714, loss = 0.61657887\n",
      "Iteration 715, loss = 0.61657736\n",
      "Iteration 716, loss = 0.61657685\n",
      "Iteration 717, loss = 0.61658111\n",
      "Iteration 718, loss = 0.61657558\n",
      "Iteration 719, loss = 0.61657443\n",
      "Iteration 720, loss = 0.61658007\n",
      "Iteration 721, loss = 0.61657549\n",
      "Iteration 722, loss = 0.61657662\n",
      "Iteration 723, loss = 0.61657484\n",
      "Iteration 724, loss = 0.61657470\n",
      "Iteration 725, loss = 0.61657421\n",
      "Iteration 726, loss = 0.61657208\n",
      "Iteration 727, loss = 0.61657493\n",
      "Iteration 728, loss = 0.61657311\n",
      "Iteration 729, loss = 0.61657490\n",
      "Iteration 730, loss = 0.61657190\n",
      "Iteration 731, loss = 0.61657483\n",
      "Iteration 732, loss = 0.61657112\n",
      "Iteration 733, loss = 0.61657048\n",
      "Iteration 734, loss = 0.61657048\n",
      "Iteration 735, loss = 0.61657041\n",
      "Iteration 736, loss = 0.61657019\n",
      "Iteration 737, loss = 0.61656890\n",
      "Iteration 738, loss = 0.61656990\n",
      "Iteration 739, loss = 0.61656856\n",
      "Iteration 740, loss = 0.61656810\n",
      "Iteration 741, loss = 0.61656955\n",
      "Iteration 742, loss = 0.61656782\n",
      "Iteration 743, loss = 0.61656884\n",
      "Iteration 744, loss = 0.61658092\n",
      "Iteration 745, loss = 0.61657118\n",
      "Iteration 746, loss = 0.61656964\n",
      "Iteration 747, loss = 0.61657047\n",
      "Iteration 748, loss = 0.61656813\n",
      "Iteration 749, loss = 0.61657082\n",
      "Iteration 750, loss = 0.61656729\n",
      "Iteration 751, loss = 0.61656708\n",
      "Iteration 752, loss = 0.61656694\n",
      "Iteration 753, loss = 0.61656856\n",
      "Iteration 754, loss = 0.61656413\n",
      "Iteration 755, loss = 0.61656721\n",
      "Iteration 756, loss = 0.61656441\n",
      "Iteration 757, loss = 0.61656434\n",
      "Iteration 758, loss = 0.61658169\n",
      "Iteration 759, loss = 0.61657182\n",
      "Iteration 760, loss = 0.61657167\n",
      "Iteration 761, loss = 0.61657318\n",
      "Iteration 762, loss = 0.61657203\n",
      "Iteration 763, loss = 0.61657322\n",
      "Iteration 764, loss = 0.61657192\n",
      "Iteration 765, loss = 0.61657085\n",
      "Iteration 766, loss = 0.61657037\n",
      "Iteration 767, loss = 0.61656568\n",
      "Iteration 768, loss = 0.61656788\n",
      "Iteration 769, loss = 0.61656007\n",
      "Iteration 770, loss = 0.61656856\n",
      "Iteration 771, loss = 0.61655732\n",
      "Iteration 772, loss = 0.61655790\n",
      "Iteration 773, loss = 0.61655630\n",
      "Iteration 774, loss = 0.61655358\n",
      "Iteration 775, loss = 0.61654955\n",
      "Iteration 776, loss = 0.61655216\n",
      "Iteration 777, loss = 0.61655115\n",
      "Iteration 778, loss = 0.61655410\n",
      "Iteration 779, loss = 0.61654742\n",
      "Iteration 780, loss = 0.61654208\n",
      "Iteration 781, loss = 0.61656543\n",
      "Iteration 782, loss = 0.61654828\n",
      "Iteration 783, loss = 0.61654801\n",
      "Iteration 784, loss = 0.61655219\n",
      "Iteration 785, loss = 0.61654859\n",
      "Iteration 786, loss = 0.61654556\n",
      "Iteration 787, loss = 0.61654298\n",
      "Iteration 788, loss = 0.61655344\n",
      "Iteration 789, loss = 0.61655671\n",
      "Iteration 790, loss = 0.61655627\n",
      "Iteration 791, loss = 0.61655415\n",
      "Iteration 792, loss = 0.61656765\n",
      "Iteration 793, loss = 0.61655582\n",
      "Iteration 794, loss = 0.61655530\n",
      "Iteration 795, loss = 0.61655498\n",
      "Iteration 796, loss = 0.61655440\n",
      "Iteration 797, loss = 0.61655283\n",
      "Iteration 798, loss = 0.61655066\n",
      "Iteration 799, loss = 0.61654756\n",
      "Iteration 800, loss = 0.61654971\n",
      "Iteration 801, loss = 0.61655130\n",
      "Iteration 802, loss = 0.61654520\n",
      "Iteration 803, loss = 0.61654251\n",
      "Iteration 804, loss = 0.61654669\n",
      "Iteration 805, loss = 0.61653947\n",
      "Iteration 806, loss = 0.61654516\n",
      "Iteration 807, loss = 0.61653890\n",
      "Iteration 808, loss = 0.61654096\n",
      "Iteration 809, loss = 0.61654024\n",
      "Iteration 810, loss = 0.61653968\n",
      "Iteration 811, loss = 0.61653921\n",
      "Iteration 812, loss = 0.61653879\n",
      "Iteration 813, loss = 0.61653839\n",
      "Iteration 814, loss = 0.61654131\n",
      "Iteration 815, loss = 0.61653754\n",
      "Iteration 816, loss = 0.61654075\n",
      "Iteration 817, loss = 0.61653875\n",
      "Iteration 818, loss = 0.61653742\n",
      "Iteration 819, loss = 0.61653134\n",
      "Iteration 820, loss = 0.61654015\n",
      "Iteration 821, loss = 0.61654421\n",
      "Iteration 822, loss = 0.61653574\n",
      "Iteration 823, loss = 0.61653510\n",
      "Iteration 824, loss = 0.61653617\n",
      "Iteration 825, loss = 0.61653507\n",
      "Iteration 826, loss = 0.61653425\n",
      "Iteration 827, loss = 0.61653672\n",
      "Iteration 828, loss = 0.61653531\n",
      "Iteration 829, loss = 0.61653344\n",
      "Iteration 830, loss = 0.61653250\n",
      "Iteration 831, loss = 0.61653190\n",
      "Iteration 832, loss = 0.61652946\n",
      "Iteration 833, loss = 0.61653266\n",
      "Iteration 834, loss = 0.61653102\n",
      "Iteration 835, loss = 0.61653537\n",
      "Iteration 836, loss = 0.61653138\n",
      "Iteration 837, loss = 0.61653016\n",
      "Iteration 838, loss = 0.61653903\n",
      "Iteration 839, loss = 0.61653263\n",
      "Iteration 840, loss = 0.61653108\n",
      "Iteration 841, loss = 0.61653440\n",
      "Iteration 842, loss = 0.61653383\n",
      "Iteration 843, loss = 0.61652951\n",
      "Iteration 844, loss = 0.61652754\n",
      "Iteration 845, loss = 0.61652707\n",
      "Iteration 846, loss = 0.61653885\n",
      "Iteration 847, loss = 0.61653326\n",
      "Iteration 848, loss = 0.61653912\n",
      "Iteration 849, loss = 0.61653566\n",
      "Iteration 850, loss = 0.61653563\n",
      "Iteration 851, loss = 0.61653471\n",
      "Iteration 852, loss = 0.61653447\n",
      "Iteration 853, loss = 0.61653407\n",
      "Iteration 854, loss = 0.61653412\n",
      "Iteration 855, loss = 0.61653403\n",
      "Iteration 856, loss = 0.61652724\n",
      "Iteration 857, loss = 0.61654339\n",
      "Iteration 858, loss = 0.61654453\n",
      "Iteration 859, loss = 0.61654020\n",
      "Iteration 860, loss = 0.61653779\n",
      "Iteration 861, loss = 0.61653969\n",
      "Iteration 862, loss = 0.61653724\n",
      "Iteration 863, loss = 0.61653543\n",
      "Iteration 864, loss = 0.61653462\n",
      "Iteration 865, loss = 0.61653432\n",
      "Iteration 866, loss = 0.61653282\n",
      "Iteration 867, loss = 0.61653069\n",
      "Iteration 868, loss = 0.61653031\n",
      "Iteration 869, loss = 0.61653179\n",
      "Iteration 870, loss = 0.61652845\n",
      "Iteration 871, loss = 0.61652649\n",
      "Iteration 872, loss = 0.61652547\n",
      "Iteration 873, loss = 0.61652277\n",
      "Iteration 874, loss = 0.61653265\n",
      "Iteration 875, loss = 0.61652292\n",
      "Iteration 876, loss = 0.61652202\n",
      "Iteration 877, loss = 0.61652743\n",
      "Iteration 878, loss = 0.61652284\n",
      "Iteration 879, loss = 0.61652172\n",
      "Iteration 880, loss = 0.61652227\n",
      "Iteration 881, loss = 0.61652105\n",
      "Iteration 882, loss = 0.61652061\n",
      "Iteration 883, loss = 0.61652062\n",
      "Iteration 884, loss = 0.61651992\n",
      "Iteration 885, loss = 0.61651863\n",
      "Iteration 886, loss = 0.61652464\n",
      "Iteration 887, loss = 0.61652089\n",
      "Iteration 888, loss = 0.61651919\n",
      "Iteration 889, loss = 0.61652832\n",
      "Iteration 890, loss = 0.61652870\n",
      "Iteration 891, loss = 0.61652637\n",
      "Iteration 892, loss = 0.61652760\n",
      "Iteration 893, loss = 0.61652882\n",
      "Iteration 894, loss = 0.61652998\n",
      "Iteration 895, loss = 0.61653201\n",
      "Iteration 896, loss = 0.61653211\n",
      "Iteration 897, loss = 0.61653588\n",
      "Iteration 898, loss = 0.61653165\n",
      "Iteration 899, loss = 0.61653215\n",
      "Iteration 900, loss = 0.61653107\n",
      "Iteration 901, loss = 0.61653196\n",
      "Iteration 902, loss = 0.61652585\n",
      "Iteration 903, loss = 0.61652622\n",
      "Iteration 904, loss = 0.61652931\n",
      "Iteration 905, loss = 0.61652885\n",
      "Iteration 906, loss = 0.61653146\n",
      "Iteration 907, loss = 0.61653117\n",
      "Iteration 908, loss = 0.61653575\n",
      "Iteration 909, loss = 0.61653357\n",
      "Iteration 910, loss = 0.61653371\n",
      "Iteration 911, loss = 0.61653310\n",
      "Iteration 912, loss = 0.61653427\n",
      "Iteration 913, loss = 0.61653635\n",
      "Iteration 914, loss = 0.61653562\n",
      "Iteration 915, loss = 0.61653580\n",
      "Iteration 916, loss = 0.61653598\n",
      "Iteration 917, loss = 0.61653877\n",
      "Iteration 918, loss = 0.61653468\n",
      "Iteration 919, loss = 0.61653454\n",
      "Iteration 920, loss = 0.61653361\n",
      "Iteration 921, loss = 0.61653496\n",
      "Iteration 922, loss = 0.61653250\n",
      "Iteration 923, loss = 0.61652770\n",
      "Iteration 924, loss = 0.61652354\n",
      "Iteration 925, loss = 0.61652679\n",
      "Iteration 926, loss = 0.61651902\n",
      "Iteration 927, loss = 0.61652280\n",
      "Iteration 928, loss = 0.61651653\n",
      "Iteration 929, loss = 0.61651511\n",
      "Iteration 930, loss = 0.61651194\n",
      "Iteration 931, loss = 0.61652221\n",
      "Iteration 932, loss = 0.61651198\n",
      "Iteration 933, loss = 0.61651555\n",
      "Iteration 934, loss = 0.61651620\n",
      "Iteration 935, loss = 0.61651034\n",
      "Iteration 936, loss = 0.61651182\n",
      "Iteration 937, loss = 0.61650821\n",
      "Iteration 938, loss = 0.61650885\n",
      "Iteration 939, loss = 0.61650711\n",
      "Iteration 940, loss = 0.61650589\n",
      "Iteration 941, loss = 0.61650302\n",
      "Iteration 942, loss = 0.61651006\n",
      "Iteration 943, loss = 0.61651107\n",
      "Iteration 944, loss = 0.61652051\n",
      "Iteration 945, loss = 0.61652006\n",
      "Iteration 946, loss = 0.61652643\n",
      "Iteration 947, loss = 0.61653144\n",
      "Iteration 948, loss = 0.61654427\n",
      "Iteration 949, loss = 0.61654227\n",
      "Iteration 950, loss = 0.61655436\n",
      "Iteration 951, loss = 0.61654967\n",
      "Iteration 952, loss = 0.61654961\n",
      "Iteration 953, loss = 0.61655012\n",
      "Iteration 954, loss = 0.61655122\n",
      "Iteration 955, loss = 0.61656223\n",
      "Iteration 956, loss = 0.61656529\n",
      "Iteration 957, loss = 0.61657394\n",
      "Iteration 958, loss = 0.61657240\n",
      "Iteration 959, loss = 0.61657082\n",
      "Iteration 960, loss = 0.61658079\n",
      "Iteration 961, loss = 0.61658234\n",
      "Iteration 962, loss = 0.61658643\n",
      "Iteration 963, loss = 0.61658232\n",
      "Iteration 964, loss = 0.61657760\n",
      "Iteration 965, loss = 0.61658107\n",
      "Iteration 966, loss = 0.61657035\n",
      "Iteration 967, loss = 0.61657066\n",
      "Iteration 968, loss = 0.61657043\n",
      "Iteration 969, loss = 0.61656383\n",
      "Iteration 970, loss = 0.61656957\n",
      "Iteration 971, loss = 0.61657406\n",
      "Iteration 972, loss = 0.61657806\n",
      "Iteration 973, loss = 0.61657882\n",
      "Iteration 974, loss = 0.61657960\n",
      "Iteration 975, loss = 0.61657673\n",
      "Iteration 976, loss = 0.61657966\n",
      "Iteration 977, loss = 0.61657801\n",
      "Iteration 978, loss = 0.61657802\n",
      "Iteration 979, loss = 0.61657706\n",
      "Iteration 980, loss = 0.61657582\n",
      "Iteration 981, loss = 0.61657423\n",
      "Iteration 982, loss = 0.61657170\n",
      "Iteration 983, loss = 0.61657104\n",
      "Iteration 984, loss = 0.61657122\n",
      "Iteration 985, loss = 0.61657198\n",
      "Iteration 986, loss = 0.61657155\n",
      "Iteration 987, loss = 0.61657105\n",
      "Iteration 988, loss = 0.61656970\n",
      "Iteration 989, loss = 0.61656801\n",
      "Iteration 990, loss = 0.61657081\n",
      "Iteration 991, loss = 0.61656886\n",
      "Iteration 992, loss = 0.61656257\n",
      "Iteration 993, loss = 0.61656249\n",
      "Iteration 994, loss = 0.61656231\n",
      "Iteration 995, loss = 0.61656128\n",
      "Iteration 996, loss = 0.61656198\n",
      "Iteration 997, loss = 0.61655754\n",
      "Iteration 998, loss = 0.61656782\n",
      "Iteration 999, loss = 0.61656403\n",
      "Iteration 1000, loss = 0.61656154\n",
      "Iteration 1001, loss = 0.61656218\n",
      "Iteration 1002, loss = 0.61656916\n",
      "Iteration 1003, loss = 0.61657653\n",
      "Iteration 1004, loss = 0.61657513\n",
      "Iteration 1005, loss = 0.61657820\n",
      "Iteration 1006, loss = 0.61657586\n",
      "Iteration 1007, loss = 0.61658540\n",
      "Iteration 1008, loss = 0.61658206\n",
      "Iteration 1009, loss = 0.61658841\n",
      "Iteration 1010, loss = 0.61658738\n",
      "Iteration 1011, loss = 0.61659298\n",
      "Iteration 1012, loss = 0.61659164\n",
      "Iteration 1013, loss = 0.61659256\n",
      "Iteration 1014, loss = 0.61659295\n",
      "Iteration 1015, loss = 0.61660144\n",
      "Iteration 1016, loss = 0.61659686\n",
      "Iteration 1017, loss = 0.61659497\n",
      "Iteration 1018, loss = 0.61659202\n",
      "Iteration 1019, loss = 0.61658749\n",
      "Iteration 1020, loss = 0.61658726\n",
      "Iteration 1021, loss = 0.61657765\n",
      "Iteration 1022, loss = 0.61658059\n",
      "Iteration 1023, loss = 0.61657060\n",
      "Iteration 1024, loss = 0.61657397\n",
      "Iteration 1025, loss = 0.61656688\n",
      "Iteration 1026, loss = 0.61656943\n",
      "Iteration 1027, loss = 0.61656616\n",
      "Iteration 1028, loss = 0.61656184\n",
      "Iteration 1029, loss = 0.61655897\n",
      "Iteration 1030, loss = 0.61655709\n",
      "Iteration 1031, loss = 0.61655167\n",
      "Iteration 1032, loss = 0.61654627\n",
      "Iteration 1033, loss = 0.61654195\n",
      "Iteration 1034, loss = 0.61654342\n",
      "Iteration 1035, loss = 0.61653410\n",
      "Iteration 1036, loss = 0.61653769\n",
      "Iteration 1037, loss = 0.61652909\n",
      "Iteration 1038, loss = 0.61652863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1039, loss = 0.61652767\n",
      "Iteration 1040, loss = 0.61652721\n",
      "Iteration 1041, loss = 0.61652389\n",
      "Iteration 1042, loss = 0.61652076\n",
      "Iteration 1043, loss = 0.61652673\n",
      "Iteration 1044, loss = 0.61651775\n",
      "Iteration 1045, loss = 0.61651665\n",
      "Iteration 1046, loss = 0.61651589\n",
      "Iteration 1047, loss = 0.61651420\n",
      "Iteration 1048, loss = 0.61651422\n",
      "Iteration 1049, loss = 0.61651625\n",
      "Iteration 1050, loss = 0.61651121\n",
      "Iteration 1051, loss = 0.61651443\n",
      "Iteration 1052, loss = 0.61651086\n",
      "Iteration 1053, loss = 0.61651009\n",
      "Iteration 1054, loss = 0.61650737\n",
      "Iteration 1055, loss = 0.61650525\n",
      "Iteration 1056, loss = 0.61650547\n",
      "Iteration 1057, loss = 0.61650588\n",
      "Iteration 1058, loss = 0.61650550\n",
      "Iteration 1059, loss = 0.61650895\n",
      "Iteration 1060, loss = 0.61649927\n",
      "Iteration 1061, loss = 0.61649733\n",
      "Iteration 1062, loss = 0.61649828\n",
      "Iteration 1063, loss = 0.61649762\n",
      "Iteration 1064, loss = 0.61649329\n",
      "Iteration 1065, loss = 0.61649379\n",
      "Iteration 1066, loss = 0.61649199\n",
      "Iteration 1067, loss = 0.61649027\n",
      "Iteration 1068, loss = 0.61649136\n",
      "Iteration 1069, loss = 0.61649011\n",
      "Iteration 1070, loss = 0.61648995\n",
      "Iteration 1071, loss = 0.61648906\n",
      "Iteration 1072, loss = 0.61648961\n",
      "Iteration 1073, loss = 0.61648944\n",
      "Iteration 1074, loss = 0.61648887\n",
      "Iteration 1075, loss = 0.61648822\n",
      "Iteration 1076, loss = 0.61648808\n",
      "Iteration 1077, loss = 0.61648816\n",
      "Iteration 1078, loss = 0.61648767\n",
      "Iteration 1079, loss = 0.61648784\n",
      "Iteration 1080, loss = 0.61649201\n",
      "Iteration 1081, loss = 0.61648717\n",
      "Iteration 1082, loss = 0.61648726\n",
      "Iteration 1083, loss = 0.61648612\n",
      "Iteration 1084, loss = 0.61648511\n",
      "Iteration 1085, loss = 0.61648721\n",
      "Iteration 1086, loss = 0.61648495\n",
      "Iteration 1087, loss = 0.61648444\n",
      "Iteration 1088, loss = 0.61648740\n",
      "Iteration 1089, loss = 0.61648383\n",
      "Iteration 1090, loss = 0.61647892\n",
      "Iteration 1091, loss = 0.61648131\n",
      "Iteration 1092, loss = 0.61648068\n",
      "Iteration 1093, loss = 0.61648364\n",
      "Iteration 1094, loss = 0.61648110\n",
      "Iteration 1095, loss = 0.61648071\n",
      "Iteration 1096, loss = 0.61648078\n",
      "Iteration 1097, loss = 0.61648604\n",
      "Iteration 1098, loss = 0.61648317\n",
      "Iteration 1099, loss = 0.61648453\n",
      "Iteration 1100, loss = 0.61648018\n",
      "Iteration 1101, loss = 0.61648219\n",
      "Iteration 1102, loss = 0.61647993\n",
      "Iteration 1103, loss = 0.61648292\n",
      "Iteration 1104, loss = 0.61648050\n",
      "Iteration 1105, loss = 0.61647831\n",
      "Iteration 1106, loss = 0.61649548\n",
      "Iteration 1107, loss = 0.61647884\n",
      "Iteration 1108, loss = 0.61647868\n",
      "Iteration 1109, loss = 0.61648193\n",
      "Iteration 1110, loss = 0.61647802\n",
      "Iteration 1111, loss = 0.61647737\n",
      "Iteration 1112, loss = 0.61648128\n",
      "Iteration 1113, loss = 0.61647835\n",
      "Iteration 1114, loss = 0.61648196\n",
      "Iteration 1115, loss = 0.61648012\n",
      "Iteration 1116, loss = 0.61647453\n",
      "Iteration 1117, loss = 0.61647435\n",
      "Iteration 1118, loss = 0.61648806\n",
      "Iteration 1119, loss = 0.61649039\n",
      "Iteration 1120, loss = 0.61648368\n",
      "Iteration 1121, loss = 0.61648454\n",
      "Iteration 1122, loss = 0.61648448\n",
      "Iteration 1123, loss = 0.61648298\n",
      "Iteration 1124, loss = 0.61648243\n",
      "Iteration 1125, loss = 0.61648335\n",
      "Iteration 1126, loss = 0.61648166\n",
      "Iteration 1127, loss = 0.61648459\n",
      "Iteration 1128, loss = 0.61648405\n",
      "Iteration 1129, loss = 0.61648161\n",
      "Iteration 1130, loss = 0.61648165\n",
      "Iteration 1131, loss = 0.61648154\n",
      "Iteration 1132, loss = 0.61648431\n",
      "Iteration 1133, loss = 0.61648301\n",
      "Iteration 1134, loss = 0.61648476\n",
      "Iteration 1135, loss = 0.61648403\n",
      "Iteration 1136, loss = 0.61648419\n",
      "Iteration 1137, loss = 0.61648527\n",
      "Iteration 1138, loss = 0.61648418\n",
      "Iteration 1139, loss = 0.61648367\n",
      "Iteration 1140, loss = 0.61648367\n",
      "Iteration 1141, loss = 0.61648206\n",
      "Iteration 1142, loss = 0.61648084\n",
      "Iteration 1143, loss = 0.61648066\n",
      "Iteration 1144, loss = 0.61647869\n",
      "Iteration 1145, loss = 0.61648159\n",
      "Iteration 1146, loss = 0.61647892\n",
      "Iteration 1147, loss = 0.61647794\n",
      "Iteration 1148, loss = 0.61647720\n",
      "Iteration 1149, loss = 0.61647998\n",
      "Iteration 1150, loss = 0.61647809\n",
      "Iteration 1151, loss = 0.61647677\n",
      "Iteration 1152, loss = 0.61647764\n",
      "Iteration 1153, loss = 0.61647387\n",
      "Iteration 1154, loss = 0.61648074\n",
      "Iteration 1155, loss = 0.61647634\n",
      "Iteration 1156, loss = 0.61647627\n",
      "Iteration 1157, loss = 0.61647412\n",
      "Iteration 1158, loss = 0.61647219\n",
      "Iteration 1159, loss = 0.61646975\n",
      "Iteration 1160, loss = 0.61648852\n",
      "Iteration 1161, loss = 0.61647645\n",
      "Iteration 1162, loss = 0.61647685\n",
      "Iteration 1163, loss = 0.61647899\n",
      "Iteration 1164, loss = 0.61647793\n",
      "Iteration 1165, loss = 0.61647970\n",
      "Iteration 1166, loss = 0.61647926\n",
      "Iteration 1167, loss = 0.61647859\n",
      "Iteration 1168, loss = 0.61647873\n",
      "Iteration 1169, loss = 0.61647771\n",
      "Iteration 1170, loss = 0.61647484\n",
      "Iteration 1171, loss = 0.61647148\n",
      "Iteration 1172, loss = 0.61647671\n",
      "Iteration 1173, loss = 0.61647534\n",
      "Iteration 1174, loss = 0.61647107\n",
      "Iteration 1175, loss = 0.61646831\n",
      "Iteration 1176, loss = 0.61647923\n",
      "Iteration 1177, loss = 0.61647447\n",
      "Iteration 1178, loss = 0.61647165\n",
      "Iteration 1179, loss = 0.61647191\n",
      "Iteration 1180, loss = 0.61647192\n",
      "Iteration 1181, loss = 0.61647495\n",
      "Iteration 1182, loss = 0.61647276\n",
      "Iteration 1183, loss = 0.61647335\n",
      "Iteration 1184, loss = 0.61647237\n",
      "Iteration 1185, loss = 0.61647279\n",
      "Iteration 1186, loss = 0.61647233\n",
      "Iteration 1187, loss = 0.61647408\n",
      "Iteration 1188, loss = 0.61647174\n",
      "Iteration 1189, loss = 0.61647131\n",
      "Iteration 1190, loss = 0.61647110\n",
      "Iteration 1191, loss = 0.61646975\n",
      "Iteration 1192, loss = 0.61646914\n",
      "Iteration 1193, loss = 0.61647347\n",
      "Iteration 1194, loss = 0.61647374\n",
      "Iteration 1195, loss = 0.61647116\n",
      "Iteration 1196, loss = 0.61647101\n",
      "Iteration 1197, loss = 0.61647063\n",
      "Iteration 1198, loss = 0.61646964\n",
      "Iteration 1199, loss = 0.61647052\n",
      "Iteration 1200, loss = 0.61647825\n",
      "Iteration 1201, loss = 0.61647148\n",
      "Iteration 1202, loss = 0.61647714\n",
      "Iteration 1203, loss = 0.61647152\n",
      "Iteration 1204, loss = 0.61647101\n",
      "Iteration 1205, loss = 0.61648071\n",
      "Iteration 1206, loss = 0.61647769\n",
      "Iteration 1207, loss = 0.61647143\n",
      "Iteration 1208, loss = 0.61647162\n",
      "Iteration 1209, loss = 0.61647479\n",
      "Iteration 1210, loss = 0.61647011\n",
      "Iteration 1211, loss = 0.61647279\n",
      "Iteration 1212, loss = 0.61647059\n",
      "Iteration 1213, loss = 0.61647375\n",
      "Iteration 1214, loss = 0.61647055\n",
      "Iteration 1215, loss = 0.61647040\n",
      "Iteration 1216, loss = 0.61647055\n",
      "Iteration 1217, loss = 0.61646922\n",
      "Iteration 1218, loss = 0.61646480\n",
      "Iteration 1219, loss = 0.61647372\n",
      "Iteration 1220, loss = 0.61647210\n",
      "Iteration 1221, loss = 0.61647326\n",
      "Iteration 1222, loss = 0.61646988\n",
      "Iteration 1223, loss = 0.61647045\n",
      "Iteration 1224, loss = 0.61646710\n",
      "Iteration 1225, loss = 0.61647475\n",
      "Iteration 1226, loss = 0.61646876\n",
      "Iteration 1227, loss = 0.61647064\n",
      "Iteration 1228, loss = 0.61646908\n",
      "Iteration 1229, loss = 0.61646900\n",
      "Iteration 1230, loss = 0.61646873\n",
      "Iteration 1231, loss = 0.61647137\n",
      "Iteration 1232, loss = 0.61646892\n",
      "Iteration 1233, loss = 0.61646909\n",
      "Iteration 1234, loss = 0.61647301\n",
      "Iteration 1235, loss = 0.61647077\n",
      "Iteration 1236, loss = 0.61646769\n",
      "Iteration 1237, loss = 0.61646902\n",
      "Iteration 1238, loss = 0.61646655\n",
      "Iteration 1239, loss = 0.61646924\n",
      "Iteration 1240, loss = 0.61647030\n",
      "Iteration 1241, loss = 0.61647212\n",
      "Iteration 1242, loss = 0.61646832\n",
      "Iteration 1243, loss = 0.61646665\n",
      "Iteration 1244, loss = 0.61646991\n",
      "Iteration 1245, loss = 0.61646525\n",
      "Iteration 1246, loss = 0.61647091\n",
      "Iteration 1247, loss = 0.61647553\n",
      "Iteration 1248, loss = 0.61646818\n",
      "Iteration 1249, loss = 0.61646806\n",
      "Iteration 1250, loss = 0.61646974\n",
      "Iteration 1251, loss = 0.61646793\n",
      "Iteration 1252, loss = 0.61646846\n",
      "Iteration 1253, loss = 0.61646871\n",
      "Iteration 1254, loss = 0.61646771\n",
      "Iteration 1255, loss = 0.61646990\n",
      "Iteration 1256, loss = 0.61646651\n",
      "Iteration 1257, loss = 0.61647283\n",
      "Iteration 1258, loss = 0.61646543\n",
      "Iteration 1259, loss = 0.61646781\n",
      "Iteration 1260, loss = 0.61646580\n",
      "Iteration 1261, loss = 0.61646888\n",
      "Iteration 1262, loss = 0.61646640\n",
      "Iteration 1263, loss = 0.61646973\n",
      "Iteration 1264, loss = 0.61647096\n",
      "Iteration 1265, loss = 0.61646786\n",
      "Iteration 1266, loss = 0.61646738\n",
      "Iteration 1267, loss = 0.61646718\n",
      "Iteration 1268, loss = 0.61646664\n",
      "Iteration 1269, loss = 0.61646581\n",
      "Iteration 1270, loss = 0.61646690\n",
      "Iteration 1271, loss = 0.61646961\n",
      "Iteration 1272, loss = 0.61646842\n",
      "Iteration 1273, loss = 0.61646837\n",
      "Iteration 1274, loss = 0.61646675\n",
      "Iteration 1275, loss = 0.61646634\n",
      "Iteration 1276, loss = 0.61646679\n",
      "Iteration 1277, loss = 0.61646613\n",
      "Iteration 1278, loss = 0.61646564\n",
      "Iteration 1279, loss = 0.61646554\n",
      "Iteration 1280, loss = 0.61646683\n",
      "Iteration 1281, loss = 0.61646589\n",
      "Iteration 1282, loss = 0.61646580\n",
      "Iteration 1283, loss = 0.61646703\n",
      "Iteration 1284, loss = 0.61646439\n",
      "Iteration 1285, loss = 0.61647065\n",
      "Iteration 1286, loss = 0.61646670\n",
      "Iteration 1287, loss = 0.61646631\n",
      "Iteration 1288, loss = 0.61646567\n",
      "Iteration 1289, loss = 0.61646655\n",
      "Iteration 1290, loss = 0.61646561\n",
      "Iteration 1291, loss = 0.61646832\n",
      "Iteration 1292, loss = 0.61646603\n",
      "Iteration 1293, loss = 0.61646644\n",
      "Iteration 1294, loss = 0.61646503\n",
      "Iteration 1295, loss = 0.61646609\n",
      "Iteration 1296, loss = 0.61647263\n",
      "Iteration 1297, loss = 0.61647265\n",
      "Iteration 1298, loss = 0.61647137\n",
      "Iteration 1299, loss = 0.61647135\n",
      "Iteration 1300, loss = 0.61647379\n",
      "Iteration 1301, loss = 0.61647278\n",
      "Iteration 1302, loss = 0.61647738\n",
      "Iteration 1303, loss = 0.61647137\n",
      "Iteration 1304, loss = 0.61647318\n",
      "Iteration 1305, loss = 0.61646937\n",
      "Iteration 1306, loss = 0.61647075\n",
      "Iteration 1307, loss = 0.61646961\n",
      "Iteration 1308, loss = 0.61646725\n",
      "Iteration 1309, loss = 0.61647150\n",
      "Iteration 1310, loss = 0.61646706\n",
      "Iteration 1311, loss = 0.61646864\n",
      "Iteration 1312, loss = 0.61646728\n",
      "Iteration 1313, loss = 0.61646651\n",
      "Iteration 1314, loss = 0.61646807\n",
      "Iteration 1315, loss = 0.61646756\n",
      "Iteration 1316, loss = 0.61646703\n",
      "Iteration 1317, loss = 0.61646476\n",
      "Iteration 1318, loss = 0.61647527\n",
      "Iteration 1319, loss = 0.61646575\n",
      "Iteration 1320, loss = 0.61646585\n",
      "Iteration 1321, loss = 0.61646638\n",
      "Iteration 1322, loss = 0.61646427\n",
      "Iteration 1323, loss = 0.61646790\n",
      "Iteration 1324, loss = 0.61646580\n",
      "Iteration 1325, loss = 0.61646617\n",
      "Iteration 1326, loss = 0.61647054\n",
      "Iteration 1327, loss = 0.61646684\n",
      "Iteration 1328, loss = 0.61646561\n",
      "Iteration 1329, loss = 0.61646163\n",
      "Iteration 1330, loss = 0.61647258\n",
      "Iteration 1331, loss = 0.61646412\n",
      "Iteration 1332, loss = 0.61646376\n",
      "Iteration 1333, loss = 0.61646255\n",
      "Iteration 1334, loss = 0.61646321\n",
      "Iteration 1335, loss = 0.61646051\n",
      "Iteration 1336, loss = 0.61646102\n",
      "Iteration 1337, loss = 0.61646670\n",
      "Iteration 1338, loss = 0.61646429\n",
      "Iteration 1339, loss = 0.61646594\n",
      "Iteration 1340, loss = 0.61646423\n",
      "Iteration 1341, loss = 0.61646738\n",
      "Iteration 1342, loss = 0.61646597\n",
      "Iteration 1343, loss = 0.61646275\n",
      "Iteration 1344, loss = 0.61646668\n",
      "Iteration 1345, loss = 0.61646694\n",
      "Iteration 1346, loss = 0.61646391\n",
      "Iteration 1347, loss = 0.61646373\n",
      "Iteration 1348, loss = 0.61646353\n",
      "Iteration 1349, loss = 0.61646678\n",
      "Iteration 1350, loss = 0.61646329\n",
      "Iteration 1351, loss = 0.61646430\n",
      "Iteration 1352, loss = 0.61646334\n",
      "Iteration 1353, loss = 0.61646098\n",
      "Iteration 1354, loss = 0.61646401\n",
      "Iteration 1355, loss = 0.61647127\n",
      "Iteration 1356, loss = 0.61647160\n",
      "Iteration 1357, loss = 0.61646613\n",
      "Iteration 1358, loss = 0.61646945\n",
      "Iteration 1359, loss = 0.61646705\n",
      "Iteration 1360, loss = 0.61646838\n",
      "Iteration 1361, loss = 0.61646555\n",
      "Iteration 1362, loss = 0.61646622\n",
      "Iteration 1363, loss = 0.61646617\n",
      "Iteration 1364, loss = 0.61646599\n",
      "Iteration 1365, loss = 0.61646599\n",
      "Iteration 1366, loss = 0.61646651\n",
      "Iteration 1367, loss = 0.61646525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1368, loss = 0.61646850\n",
      "Iteration 1369, loss = 0.61646705\n",
      "Iteration 1370, loss = 0.61646726\n",
      "Iteration 1371, loss = 0.61646704\n",
      "Iteration 1372, loss = 0.61646932\n",
      "Iteration 1373, loss = 0.61646606\n",
      "Iteration 1374, loss = 0.61646622\n",
      "Iteration 1375, loss = 0.61647033\n",
      "Iteration 1376, loss = 0.61646532\n",
      "Iteration 1377, loss = 0.61646686\n",
      "Iteration 1378, loss = 0.61646772\n",
      "Iteration 1379, loss = 0.61646706\n",
      "Iteration 1380, loss = 0.61646677\n",
      "Iteration 1381, loss = 0.61646693\n",
      "Iteration 1382, loss = 0.61646705\n",
      "Iteration 1383, loss = 0.61646844\n",
      "Iteration 1384, loss = 0.61646691\n",
      "Iteration 1385, loss = 0.61646876\n",
      "Iteration 1386, loss = 0.61647258\n",
      "Iteration 1387, loss = 0.61647157\n",
      "Iteration 1388, loss = 0.61647413\n",
      "Iteration 1389, loss = 0.61648169\n",
      "Iteration 1390, loss = 0.61647712\n",
      "Iteration 1391, loss = 0.61647592\n",
      "Iteration 1392, loss = 0.61647825\n",
      "Iteration 1393, loss = 0.61647501\n",
      "Iteration 1394, loss = 0.61647657\n",
      "Iteration 1395, loss = 0.61647659\n",
      "Iteration 1396, loss = 0.61647973\n",
      "Iteration 1397, loss = 0.61647809\n",
      "Iteration 1398, loss = 0.61647502\n",
      "Iteration 1399, loss = 0.61647413\n",
      "Iteration 1400, loss = 0.61647131\n",
      "Iteration 1401, loss = 0.61647955\n",
      "Iteration 1402, loss = 0.61647093\n",
      "Iteration 1403, loss = 0.61646900\n",
      "Iteration 1404, loss = 0.61647898\n",
      "Iteration 1405, loss = 0.61646670\n",
      "Iteration 1406, loss = 0.61646350\n",
      "Iteration 1407, loss = 0.61646260\n",
      "Iteration 1408, loss = 0.61646954\n",
      "Iteration 1409, loss = 0.61646777\n",
      "Iteration 1410, loss = 0.61646378\n",
      "Iteration 1411, loss = 0.61646327\n",
      "Iteration 1412, loss = 0.61646520\n",
      "Iteration 1413, loss = 0.61646193\n",
      "Iteration 1414, loss = 0.61646099\n",
      "Iteration 1415, loss = 0.61646175\n",
      "Iteration 1416, loss = 0.61646536\n",
      "Iteration 1417, loss = 0.61646697\n",
      "Iteration 1418, loss = 0.61646518\n",
      "Iteration 1419, loss = 0.61646655\n",
      "Iteration 1420, loss = 0.61646560\n",
      "Iteration 1421, loss = 0.61647265\n",
      "Iteration 1422, loss = 0.61646898\n",
      "Iteration 1423, loss = 0.61647010\n",
      "Iteration 1424, loss = 0.61647031\n",
      "Iteration 1425, loss = 0.61646844\n",
      "Iteration 1426, loss = 0.61646843\n",
      "Iteration 1427, loss = 0.61646715\n",
      "Iteration 1428, loss = 0.61646548\n",
      "Iteration 1429, loss = 0.61646524\n",
      "Iteration 1430, loss = 0.61646113\n",
      "Iteration 1431, loss = 0.61647186\n",
      "Iteration 1432, loss = 0.61646194\n",
      "Iteration 1433, loss = 0.61646429\n",
      "Iteration 1434, loss = 0.61646312\n",
      "Iteration 1435, loss = 0.61646198\n",
      "Iteration 1436, loss = 0.61646175\n",
      "Iteration 1437, loss = 0.61646192\n",
      "Iteration 1438, loss = 0.61646144\n",
      "Iteration 1439, loss = 0.61646171\n",
      "Iteration 1440, loss = 0.61645964\n",
      "Iteration 1441, loss = 0.61646559\n",
      "Iteration 1442, loss = 0.61646070\n",
      "Iteration 1443, loss = 0.61645864\n",
      "Iteration 1444, loss = 0.61646090\n",
      "Iteration 1445, loss = 0.61646082\n",
      "Iteration 1446, loss = 0.61646198\n",
      "Iteration 1447, loss = 0.61646075\n",
      "Iteration 1448, loss = 0.61646073\n",
      "Iteration 1449, loss = 0.61646070\n",
      "Iteration 1450, loss = 0.61646172\n",
      "Iteration 1451, loss = 0.61645771\n",
      "Iteration 1452, loss = 0.61645615\n",
      "Iteration 1453, loss = 0.61646289\n",
      "Iteration 1454, loss = 0.61646570\n",
      "Iteration 1455, loss = 0.61646162\n",
      "Iteration 1456, loss = 0.61646538\n",
      "Iteration 1457, loss = 0.61646412\n",
      "Iteration 1458, loss = 0.61646283\n",
      "Iteration 1459, loss = 0.61647878\n",
      "Iteration 1460, loss = 0.61646904\n",
      "Iteration 1461, loss = 0.61647332\n",
      "Iteration 1462, loss = 0.61646976\n",
      "Iteration 1463, loss = 0.61646776\n",
      "Iteration 1464, loss = 0.61646948\n",
      "Iteration 1465, loss = 0.61646789\n",
      "Iteration 1466, loss = 0.61647020\n",
      "Iteration 1467, loss = 0.61646565\n",
      "Iteration 1468, loss = 0.61646662\n",
      "Iteration 1469, loss = 0.61646697\n",
      "Iteration 1470, loss = 0.61646697\n",
      "Iteration 1471, loss = 0.61646736\n",
      "Iteration 1472, loss = 0.61646744\n",
      "Iteration 1473, loss = 0.61646793\n",
      "Iteration 1474, loss = 0.61646760\n",
      "Iteration 1475, loss = 0.61647416\n",
      "Iteration 1476, loss = 0.61646549\n",
      "Iteration 1477, loss = 0.61646405\n",
      "Iteration 1478, loss = 0.61647365\n",
      "Iteration 1479, loss = 0.61647246\n",
      "Iteration 1480, loss = 0.61647116\n",
      "Iteration 1481, loss = 0.61647104\n",
      "Iteration 1482, loss = 0.61647088\n",
      "Iteration 1483, loss = 0.61647148\n",
      "Iteration 1484, loss = 0.61647354\n",
      "Iteration 1485, loss = 0.61647690\n",
      "Iteration 1486, loss = 0.61647715\n",
      "Iteration 1487, loss = 0.61647378\n",
      "Iteration 1488, loss = 0.61647381\n",
      "Iteration 1489, loss = 0.61647888\n",
      "Iteration 1490, loss = 0.61647673\n",
      "Iteration 1491, loss = 0.61647681\n",
      "Iteration 1492, loss = 0.61647620\n",
      "Iteration 1493, loss = 0.61647454\n",
      "Iteration 1494, loss = 0.61647883\n",
      "Iteration 1495, loss = 0.61647290\n",
      "Iteration 1496, loss = 0.61647125\n",
      "Iteration 1497, loss = 0.61647070\n",
      "Iteration 1498, loss = 0.61646586\n",
      "Iteration 1499, loss = 0.61647406\n",
      "Iteration 1500, loss = 0.61646716\n",
      "Iteration 1501, loss = 0.61646805\n",
      "Iteration 1502, loss = 0.61646585\n",
      "Iteration 1503, loss = 0.61646613\n",
      "Iteration 1504, loss = 0.61646575\n",
      "Iteration 1505, loss = 0.61646599\n",
      "Iteration 1506, loss = 0.61646676\n",
      "Iteration 1507, loss = 0.61646402\n",
      "Iteration 1508, loss = 0.61646771\n",
      "Iteration 1509, loss = 0.61646374\n",
      "Iteration 1510, loss = 0.61646337\n",
      "Iteration 1511, loss = 0.61646193\n",
      "Iteration 1512, loss = 0.61646208\n",
      "Iteration 1513, loss = 0.61645897\n",
      "Iteration 1514, loss = 0.61646413\n",
      "Iteration 1515, loss = 0.61645841\n",
      "Iteration 1516, loss = 0.61646260\n",
      "Iteration 1517, loss = 0.61646469\n",
      "Iteration 1518, loss = 0.61646156\n",
      "Iteration 1519, loss = 0.61646175\n",
      "Iteration 1520, loss = 0.61646150\n",
      "Iteration 1521, loss = 0.61645968\n",
      "Iteration 1522, loss = 0.61645985\n",
      "Iteration 1523, loss = 0.61646006\n",
      "Iteration 1524, loss = 0.61646100\n",
      "Iteration 1525, loss = 0.61646245\n",
      "Iteration 1526, loss = 0.61646176\n",
      "Iteration 1527, loss = 0.61646236\n",
      "Iteration 1528, loss = 0.61645853\n",
      "Iteration 1529, loss = 0.61645456\n",
      "Iteration 1530, loss = 0.61645861\n",
      "Iteration 1531, loss = 0.61646518\n",
      "Iteration 1532, loss = 0.61645845\n",
      "Iteration 1533, loss = 0.61645590\n",
      "Iteration 1534, loss = 0.61645517\n",
      "Iteration 1535, loss = 0.61644915\n",
      "Iteration 1536, loss = 0.61645888\n",
      "Iteration 1537, loss = 0.61646316\n",
      "Iteration 1538, loss = 0.61646204\n",
      "Iteration 1539, loss = 0.61647107\n",
      "Iteration 1540, loss = 0.61646448\n",
      "Iteration 1541, loss = 0.61647039\n",
      "Iteration 1542, loss = 0.61646570\n",
      "Iteration 1543, loss = 0.61646612\n",
      "Iteration 1544, loss = 0.61646567\n",
      "Iteration 1545, loss = 0.61646425\n",
      "Iteration 1546, loss = 0.61647021\n",
      "Iteration 1547, loss = 0.61646791\n",
      "Iteration 1548, loss = 0.61647095\n",
      "Iteration 1549, loss = 0.61647334\n",
      "Iteration 1550, loss = 0.61648329\n",
      "Iteration 1551, loss = 0.61646795\n",
      "Iteration 1552, loss = 0.61647045\n",
      "Iteration 1553, loss = 0.61647348\n",
      "Iteration 1554, loss = 0.61647079\n",
      "Iteration 1555, loss = 0.61646925\n",
      "Iteration 1556, loss = 0.61647217\n",
      "Iteration 1557, loss = 0.61646815\n",
      "Iteration 1558, loss = 0.61646948\n",
      "Iteration 1559, loss = 0.61646768\n",
      "Iteration 1560, loss = 0.61646947\n",
      "Iteration 1561, loss = 0.61647001\n",
      "Iteration 1562, loss = 0.61646735\n",
      "Iteration 1563, loss = 0.61646699\n",
      "Iteration 1564, loss = 0.61646732\n",
      "Iteration 1565, loss = 0.61646411\n",
      "Iteration 1566, loss = 0.61647127\n",
      "Iteration 1567, loss = 0.61646826\n",
      "Iteration 1568, loss = 0.61647570\n",
      "Iteration 1569, loss = 0.61648254\n",
      "Iteration 1570, loss = 0.61647817\n",
      "Iteration 1571, loss = 0.61647798\n",
      "Iteration 1572, loss = 0.61647858\n",
      "Iteration 1573, loss = 0.61648612\n",
      "Iteration 1574, loss = 0.61648045\n",
      "Iteration 1575, loss = 0.61648006\n",
      "Iteration 1576, loss = 0.61647971\n",
      "Iteration 1577, loss = 0.61647807\n",
      "Iteration 1578, loss = 0.61647955\n",
      "Iteration 1579, loss = 0.61647746\n",
      "Iteration 1580, loss = 0.61647656\n",
      "Iteration 1581, loss = 0.61648178\n",
      "Iteration 1582, loss = 0.61647947\n",
      "Iteration 1583, loss = 0.61647987\n",
      "Iteration 1584, loss = 0.61648077\n",
      "Iteration 1585, loss = 0.61647843\n",
      "Iteration 1586, loss = 0.61647589\n",
      "Iteration 1587, loss = 0.61647252\n",
      "Iteration 1588, loss = 0.61648586\n",
      "Iteration 1589, loss = 0.61647234\n",
      "Iteration 1590, loss = 0.61647389\n",
      "Iteration 1591, loss = 0.61647138\n",
      "Iteration 1592, loss = 0.61647469\n",
      "Iteration 1593, loss = 0.61647392\n",
      "Iteration 1594, loss = 0.61647491\n",
      "Iteration 1595, loss = 0.61647745\n",
      "Iteration 1596, loss = 0.61647823\n",
      "Iteration 1597, loss = 0.61647814\n",
      "Iteration 1598, loss = 0.61648422\n",
      "Iteration 1599, loss = 0.61648403\n",
      "Iteration 1600, loss = 0.61649418\n",
      "Iteration 1601, loss = 0.61648778\n",
      "Iteration 1602, loss = 0.61648594\n",
      "Iteration 1603, loss = 0.61649125\n",
      "Iteration 1604, loss = 0.61648305\n",
      "Iteration 1605, loss = 0.61647919\n",
      "Iteration 1606, loss = 0.61647878\n",
      "Iteration 1607, loss = 0.61647605\n",
      "Iteration 1608, loss = 0.61647926\n",
      "Iteration 1609, loss = 0.61647185\n",
      "Iteration 1610, loss = 0.61647224\n",
      "Iteration 1611, loss = 0.61648119\n",
      "Iteration 1612, loss = 0.61646755\n",
      "Iteration 1613, loss = 0.61647548\n",
      "Iteration 1614, loss = 0.61647700\n",
      "Iteration 1615, loss = 0.61647552\n",
      "Iteration 1616, loss = 0.61647324\n",
      "Iteration 1617, loss = 0.61647621\n",
      "Iteration 1618, loss = 0.61647822\n",
      "Iteration 1619, loss = 0.61648340\n",
      "Iteration 1620, loss = 0.61648107\n",
      "Iteration 1621, loss = 0.61648119\n",
      "Iteration 1622, loss = 0.61648108\n",
      "Iteration 1623, loss = 0.61648388\n",
      "Iteration 1624, loss = 0.61648276\n",
      "Iteration 1625, loss = 0.61648227\n",
      "Iteration 1626, loss = 0.61648149\n",
      "Iteration 1627, loss = 0.61648033\n",
      "Iteration 1628, loss = 0.61648004\n",
      "Iteration 1629, loss = 0.61647906\n",
      "Iteration 1630, loss = 0.61647930\n",
      "Iteration 1631, loss = 0.61648653\n",
      "Iteration 1632, loss = 0.61648513\n",
      "Iteration 1633, loss = 0.61648310\n",
      "Iteration 1634, loss = 0.61648242\n",
      "Iteration 1635, loss = 0.61648212\n",
      "Iteration 1636, loss = 0.61648189\n",
      "Iteration 1637, loss = 0.61648171\n",
      "Iteration 1638, loss = 0.61648161\n",
      "Iteration 1639, loss = 0.61647924\n",
      "Iteration 1640, loss = 0.61649079\n",
      "Iteration 1641, loss = 0.61648519\n",
      "Iteration 1642, loss = 0.61648651\n",
      "Iteration 1643, loss = 0.61648595\n",
      "Iteration 1644, loss = 0.61647854\n",
      "Iteration 1645, loss = 0.61647251\n",
      "Iteration 1646, loss = 0.61646897\n",
      "Iteration 1647, loss = 0.61646725\n",
      "Iteration 1648, loss = 0.61649591\n",
      "Iteration 1649, loss = 0.61646519\n",
      "Iteration 1650, loss = 0.61646623\n",
      "Iteration 1651, loss = 0.61647233\n",
      "Iteration 1652, loss = 0.61646357\n",
      "Iteration 1653, loss = 0.61646339\n",
      "Iteration 1654, loss = 0.61646535\n",
      "Iteration 1655, loss = 0.61646476\n",
      "Iteration 1656, loss = 0.61646640\n",
      "Iteration 1657, loss = 0.61646501\n",
      "Iteration 1658, loss = 0.61646482\n",
      "Iteration 1659, loss = 0.61646810\n",
      "Iteration 1660, loss = 0.61646723\n",
      "Iteration 1661, loss = 0.61646515\n",
      "Iteration 1662, loss = 0.61646756\n",
      "Iteration 1663, loss = 0.61646472\n",
      "Iteration 1664, loss = 0.61646336\n",
      "Iteration 1665, loss = 0.61646410\n",
      "Iteration 1666, loss = 0.61646239\n",
      "Iteration 1667, loss = 0.61646260\n",
      "Iteration 1668, loss = 0.61646478\n",
      "Iteration 1669, loss = 0.61646150\n",
      "Iteration 1670, loss = 0.61646311\n",
      "Iteration 1671, loss = 0.61646145\n",
      "Iteration 1672, loss = 0.61646233\n",
      "Iteration 1673, loss = 0.61646148\n",
      "Iteration 1674, loss = 0.61646099\n",
      "Iteration 1675, loss = 0.61646314\n",
      "Iteration 1676, loss = 0.61646119\n",
      "Iteration 1677, loss = 0.61646191\n",
      "Iteration 1678, loss = 0.61646094\n",
      "Iteration 1679, loss = 0.61646133\n",
      "Iteration 1680, loss = 0.61646076\n",
      "Iteration 1681, loss = 0.61646187\n",
      "Iteration 1682, loss = 0.61646100\n",
      "Iteration 1683, loss = 0.61646059\n",
      "Iteration 1684, loss = 0.61646173\n",
      "Iteration 1685, loss = 0.61646368\n",
      "Iteration 1686, loss = 0.61645539\n",
      "Iteration 1687, loss = 0.61647009\n",
      "Iteration 1688, loss = 0.61646013\n",
      "Iteration 1689, loss = 0.61646435\n",
      "Iteration 1690, loss = 0.61647327\n",
      "Iteration 1691, loss = 0.61646745\n",
      "Iteration 1692, loss = 0.61646759\n",
      "Iteration 1693, loss = 0.61646739\n",
      "Iteration 1694, loss = 0.61647215\n",
      "Iteration 1695, loss = 0.61646975\n",
      "Iteration 1696, loss = 0.61646941\n",
      "Iteration 1697, loss = 0.61646832\n",
      "Iteration 1698, loss = 0.61646724\n",
      "Iteration 1699, loss = 0.61646719\n",
      "Iteration 1700, loss = 0.61646621\n",
      "Iteration 1701, loss = 0.61646707\n",
      "Iteration 1702, loss = 0.61646537\n",
      "Iteration 1703, loss = 0.61646439\n",
      "Iteration 1704, loss = 0.61646667\n",
      "Iteration 1705, loss = 0.61646402\n",
      "Iteration 1706, loss = 0.61646390\n",
      "Iteration 1707, loss = 0.61646279\n",
      "Iteration 1708, loss = 0.61646795\n",
      "Iteration 1709, loss = 0.61646190\n",
      "Iteration 1710, loss = 0.61646279\n",
      "Iteration 1711, loss = 0.61646200\n",
      "Iteration 1712, loss = 0.61646147\n",
      "Iteration 1713, loss = 0.61645949\n",
      "Iteration 1714, loss = 0.61645892\n",
      "Iteration 1715, loss = 0.61645949\n",
      "Iteration 1716, loss = 0.61645820\n",
      "Iteration 1717, loss = 0.61645871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1718, loss = 0.61646219\n",
      "Iteration 1719, loss = 0.61646155\n",
      "Iteration 1720, loss = 0.61645791\n",
      "Iteration 1721, loss = 0.61645977\n",
      "Iteration 1722, loss = 0.61645797\n",
      "Iteration 1723, loss = 0.61645669\n",
      "Iteration 1724, loss = 0.61646219\n",
      "Iteration 1725, loss = 0.61645845\n",
      "Iteration 1726, loss = 0.61645923\n",
      "Iteration 1727, loss = 0.61645892\n",
      "Iteration 1728, loss = 0.61646034\n",
      "Iteration 1729, loss = 0.61645807\n",
      "Iteration 1730, loss = 0.61646429\n",
      "Iteration 1731, loss = 0.61645819\n",
      "Iteration 1732, loss = 0.61645891\n",
      "Iteration 1733, loss = 0.61645819\n",
      "Iteration 1734, loss = 0.61645894\n",
      "Iteration 1735, loss = 0.61645801\n",
      "Iteration 1736, loss = 0.61645816\n",
      "Iteration 1737, loss = 0.61646018\n",
      "Iteration 1738, loss = 0.61645804\n",
      "Iteration 1739, loss = 0.61645619\n",
      "Iteration 1740, loss = 0.61646182\n",
      "Iteration 1741, loss = 0.61645939\n",
      "Iteration 1742, loss = 0.61645756\n",
      "Iteration 1743, loss = 0.61645732\n",
      "Iteration 1744, loss = 0.61645821\n",
      "Iteration 1745, loss = 0.61645774\n",
      "Iteration 1746, loss = 0.61645748\n",
      "Iteration 1747, loss = 0.61645424\n",
      "Iteration 1748, loss = 0.61646026\n",
      "Iteration 1749, loss = 0.61646242\n",
      "Iteration 1750, loss = 0.61646850\n",
      "Iteration 1751, loss = 0.61646195\n",
      "Iteration 1752, loss = 0.61645821\n",
      "Iteration 1753, loss = 0.61646136\n",
      "Iteration 1754, loss = 0.61645894\n",
      "Iteration 1755, loss = 0.61645741\n",
      "Iteration 1756, loss = 0.61645529\n",
      "Iteration 1757, loss = 0.61646027\n",
      "Iteration 1758, loss = 0.61646361\n",
      "Iteration 1759, loss = 0.61645752\n",
      "Iteration 1760, loss = 0.61645792\n",
      "Iteration 1761, loss = 0.61645836\n",
      "Iteration 1762, loss = 0.61645717\n",
      "Iteration 1763, loss = 0.61646099\n",
      "Iteration 1764, loss = 0.61645815\n",
      "Iteration 1765, loss = 0.61645609\n",
      "Iteration 1766, loss = 0.61645365\n",
      "Iteration 1767, loss = 0.61645788\n",
      "Iteration 1768, loss = 0.61645645\n",
      "Iteration 1769, loss = 0.61645972\n",
      "Iteration 1770, loss = 0.61645827\n",
      "Iteration 1771, loss = 0.61645791\n",
      "Iteration 1772, loss = 0.61646007\n",
      "Iteration 1773, loss = 0.61645861\n",
      "Iteration 1774, loss = 0.61645838\n",
      "Iteration 1775, loss = 0.61645842\n",
      "Iteration 1776, loss = 0.61645610\n",
      "Iteration 1777, loss = 0.61645714\n",
      "Iteration 1778, loss = 0.61645683\n",
      "Iteration 1779, loss = 0.61645971\n",
      "Iteration 1780, loss = 0.61645585\n",
      "Iteration 1781, loss = 0.61645789\n",
      "Iteration 1782, loss = 0.61645972\n",
      "Iteration 1783, loss = 0.61645964\n",
      "Iteration 1784, loss = 0.61645247\n",
      "Iteration 1785, loss = 0.61645793\n",
      "Iteration 1786, loss = 0.61646191\n",
      "Iteration 1787, loss = 0.61645587\n",
      "Iteration 1788, loss = 0.61645423\n",
      "Iteration 1789, loss = 0.61645938\n",
      "Iteration 1790, loss = 0.61645715\n",
      "Iteration 1791, loss = 0.61646466\n",
      "Iteration 1792, loss = 0.61646065\n",
      "Iteration 1793, loss = 0.61646108\n",
      "Iteration 1794, loss = 0.61646305\n",
      "Iteration 1795, loss = 0.61646234\n",
      "Iteration 1796, loss = 0.61646337\n",
      "Iteration 1797, loss = 0.61646207\n",
      "Iteration 1798, loss = 0.61646098\n",
      "Iteration 1799, loss = 0.61646518\n",
      "Iteration 1800, loss = 0.61647284\n",
      "Iteration 1801, loss = 0.61646709\n",
      "Iteration 1802, loss = 0.61646639\n",
      "Iteration 1803, loss = 0.61646639\n",
      "Iteration 1804, loss = 0.61646612\n",
      "Iteration 1805, loss = 0.61646812\n",
      "Iteration 1806, loss = 0.61647228\n",
      "Iteration 1807, loss = 0.61647420\n",
      "Iteration 1808, loss = 0.61647073\n",
      "Iteration 1809, loss = 0.61646093\n",
      "Iteration 1810, loss = 0.61646893\n",
      "Iteration 1811, loss = 0.61646842\n",
      "Iteration 1812, loss = 0.61647136\n",
      "Iteration 1813, loss = 0.61646245\n",
      "Iteration 1814, loss = 0.61646331\n",
      "Iteration 1815, loss = 0.61646387\n",
      "Iteration 1816, loss = 0.61646126\n",
      "Iteration 1817, loss = 0.61646233\n",
      "Iteration 1818, loss = 0.61646148\n",
      "Iteration 1819, loss = 0.61646008\n",
      "Iteration 1820, loss = 0.61645814\n",
      "Iteration 1821, loss = 0.61645883\n",
      "Iteration 1822, loss = 0.61645518\n",
      "Iteration 1823, loss = 0.61645932\n",
      "Iteration 1824, loss = 0.61646009\n",
      "Iteration 1825, loss = 0.61645866\n",
      "Iteration 1826, loss = 0.61645712\n",
      "Iteration 1827, loss = 0.61645630\n",
      "Iteration 1828, loss = 0.61645132\n",
      "Iteration 1829, loss = 0.61645861\n",
      "Iteration 1830, loss = 0.61646722\n",
      "Iteration 1831, loss = 0.61645771\n",
      "Iteration 1832, loss = 0.61646947\n",
      "Iteration 1833, loss = 0.61645647\n",
      "Iteration 1834, loss = 0.61646164\n",
      "Iteration 1835, loss = 0.61645727\n",
      "Iteration 1836, loss = 0.61645726\n",
      "Iteration 1837, loss = 0.61645720\n",
      "Iteration 1838, loss = 0.61645886\n",
      "Iteration 1839, loss = 0.61645213\n",
      "Iteration 1840, loss = 0.61645473\n",
      "Iteration 1841, loss = 0.61645371\n",
      "Iteration 1842, loss = 0.61645895\n",
      "Iteration 1843, loss = 0.61646703\n",
      "Iteration 1844, loss = 0.61646186\n",
      "Iteration 1845, loss = 0.61646658\n",
      "Iteration 1846, loss = 0.61646274\n",
      "Iteration 1847, loss = 0.61646516\n",
      "Iteration 1848, loss = 0.61646287\n",
      "Iteration 1849, loss = 0.61646285\n",
      "Iteration 1850, loss = 0.61646229\n",
      "Iteration 1851, loss = 0.61646346\n",
      "Iteration 1852, loss = 0.61646305\n",
      "Iteration 1853, loss = 0.61646754\n",
      "Iteration 1854, loss = 0.61646602\n",
      "Iteration 1855, loss = 0.61646694\n",
      "Iteration 1856, loss = 0.61646772\n",
      "Iteration 1857, loss = 0.61646512\n",
      "Iteration 1858, loss = 0.61646612\n",
      "Iteration 1859, loss = 0.61646472\n",
      "Iteration 1860, loss = 0.61646476\n",
      "Iteration 1861, loss = 0.61646352\n",
      "Iteration 1862, loss = 0.61646273\n",
      "Iteration 1863, loss = 0.61646168\n",
      "Iteration 1864, loss = 0.61646220\n",
      "Iteration 1865, loss = 0.61646438\n",
      "Iteration 1866, loss = 0.61645821\n",
      "Iteration 1867, loss = 0.61645794\n",
      "Iteration 1868, loss = 0.61645893\n",
      "Iteration 1869, loss = 0.61645827\n",
      "Iteration 1870, loss = 0.61645610\n",
      "Iteration 1871, loss = 0.61646588\n",
      "Iteration 1872, loss = 0.61645974\n",
      "Iteration 1873, loss = 0.61645566\n",
      "Iteration 1874, loss = 0.61645419\n",
      "Iteration 1875, loss = 0.61645564\n",
      "Iteration 1876, loss = 0.61645424\n",
      "Iteration 1877, loss = 0.61645272\n",
      "Iteration 1878, loss = 0.61645837\n",
      "Iteration 1879, loss = 0.61645715\n",
      "Iteration 1880, loss = 0.61646109\n",
      "Iteration 1881, loss = 0.61645978\n",
      "Iteration 1882, loss = 0.61645952\n",
      "Iteration 1883, loss = 0.61645938\n",
      "Iteration 1884, loss = 0.61645871\n",
      "Iteration 1885, loss = 0.61646192\n",
      "Iteration 1886, loss = 0.61646166\n",
      "Iteration 1887, loss = 0.61646455\n",
      "Iteration 1888, loss = 0.61646428\n",
      "Iteration 1889, loss = 0.61646371\n",
      "Iteration 1890, loss = 0.61646879\n",
      "Iteration 1891, loss = 0.61646059\n",
      "Iteration 1892, loss = 0.61646571\n",
      "Iteration 1893, loss = 0.61646254\n",
      "Iteration 1894, loss = 0.61646918\n",
      "Iteration 1895, loss = 0.61648321\n",
      "Iteration 1896, loss = 0.61647087\n",
      "Iteration 1897, loss = 0.61647500\n",
      "Iteration 1898, loss = 0.61647057\n",
      "Iteration 1899, loss = 0.61647113\n",
      "Iteration 1900, loss = 0.61647126\n",
      "Iteration 1901, loss = 0.61647143\n",
      "Iteration 1902, loss = 0.61647752\n",
      "Iteration 1903, loss = 0.61647231\n",
      "Iteration 1904, loss = 0.61647099\n",
      "Iteration 1905, loss = 0.61647683\n",
      "Iteration 1906, loss = 0.61647017\n",
      "Iteration 1907, loss = 0.61647453\n",
      "Iteration 1908, loss = 0.61646971\n",
      "Iteration 1909, loss = 0.61647171\n",
      "Iteration 1910, loss = 0.61646971\n",
      "Iteration 1911, loss = 0.61646855\n",
      "Iteration 1912, loss = 0.61646912\n",
      "Iteration 1913, loss = 0.61646858\n",
      "Iteration 1914, loss = 0.61647337\n",
      "Iteration 1915, loss = 0.61647141\n",
      "Iteration 1916, loss = 0.61646841\n",
      "Iteration 1917, loss = 0.61647266\n",
      "Iteration 1918, loss = 0.61647217\n",
      "Iteration 1919, loss = 0.61647755\n",
      "Iteration 1920, loss = 0.61647664\n",
      "Iteration 1921, loss = 0.61647888\n",
      "Iteration 1922, loss = 0.61648143\n",
      "Iteration 1923, loss = 0.61647179\n",
      "Iteration 1924, loss = 0.61647891\n",
      "Iteration 1925, loss = 0.61648969\n",
      "Iteration 1926, loss = 0.61650646\n",
      "Iteration 1927, loss = 0.61649681\n",
      "Iteration 1928, loss = 0.61649710\n",
      "Iteration 1929, loss = 0.61649665\n",
      "Iteration 1930, loss = 0.61650038\n",
      "Iteration 1931, loss = 0.61650167\n",
      "Iteration 1932, loss = 0.61651652\n",
      "Iteration 1933, loss = 0.61651284\n",
      "Iteration 1934, loss = 0.61651370\n",
      "Iteration 1935, loss = 0.61650264\n",
      "Iteration 1936, loss = 0.61650058\n",
      "Iteration 1937, loss = 0.61650009\n",
      "Iteration 1938, loss = 0.61648972\n",
      "Iteration 1939, loss = 0.61648754\n",
      "Iteration 1940, loss = 0.61648813\n",
      "Iteration 1941, loss = 0.61648399\n",
      "Iteration 1942, loss = 0.61647868\n",
      "Iteration 1943, loss = 0.61647482\n",
      "Iteration 1944, loss = 0.61647736\n",
      "Iteration 1945, loss = 0.61647296\n",
      "Iteration 1946, loss = 0.61647980\n",
      "Iteration 1947, loss = 0.61647072\n",
      "Iteration 1948, loss = 0.61647079\n",
      "Iteration 1949, loss = 0.61647159\n",
      "Iteration 1950, loss = 0.61647097\n",
      "Iteration 1951, loss = 0.61646820\n",
      "Iteration 1952, loss = 0.61646674\n",
      "Iteration 1953, loss = 0.61647025\n",
      "Iteration 1954, loss = 0.61646756\n",
      "Iteration 1955, loss = 0.61646520\n",
      "Iteration 1956, loss = 0.61646447\n",
      "Iteration 1957, loss = 0.61646647\n",
      "Iteration 1958, loss = 0.61646468\n",
      "Iteration 1959, loss = 0.61646469\n",
      "Iteration 1960, loss = 0.61646608\n",
      "Iteration 1961, loss = 0.61646580\n",
      "Iteration 1962, loss = 0.61646327\n",
      "Iteration 1963, loss = 0.61645860\n",
      "Iteration 1964, loss = 0.61645688\n",
      "Iteration 1965, loss = 0.61645462\n",
      "Iteration 1966, loss = 0.61647026\n",
      "Iteration 1967, loss = 0.61645764\n",
      "Iteration 1968, loss = 0.61646464\n",
      "Iteration 1969, loss = 0.61645736\n",
      "Iteration 1970, loss = 0.61645715\n",
      "Iteration 1971, loss = 0.61645596\n",
      "Iteration 1972, loss = 0.61646434\n",
      "Iteration 1973, loss = 0.61645825\n",
      "Iteration 1974, loss = 0.61645915\n",
      "Iteration 1975, loss = 0.61645891\n",
      "Iteration 1976, loss = 0.61646301\n",
      "Iteration 1977, loss = 0.61646086\n",
      "Iteration 1978, loss = 0.61646017\n",
      "Iteration 1979, loss = 0.61645999\n",
      "Iteration 1980, loss = 0.61645932\n",
      "Iteration 1981, loss = 0.61645942\n",
      "Iteration 1982, loss = 0.61645983\n",
      "Iteration 1983, loss = 0.61645848\n",
      "Iteration 1984, loss = 0.61645949\n",
      "Iteration 1985, loss = 0.61645783\n",
      "Iteration 1986, loss = 0.61646233\n",
      "Iteration 1987, loss = 0.61646058\n",
      "Iteration 1988, loss = 0.61646106\n",
      "Iteration 1989, loss = 0.61645776\n",
      "Iteration 1990, loss = 0.61645690\n",
      "Iteration 1991, loss = 0.61646309\n",
      "Iteration 1992, loss = 0.61645731\n",
      "Iteration 1993, loss = 0.61645745\n",
      "Iteration 1994, loss = 0.61645686\n",
      "Iteration 1995, loss = 0.61645708\n",
      "Iteration 1996, loss = 0.61645812\n",
      "Iteration 1997, loss = 0.61645736\n",
      "Iteration 1998, loss = 0.61645609\n",
      "Iteration 1999, loss = 0.61646759\n",
      "Iteration 2000, loss = 0.61645935\n",
      "Iteration 2001, loss = 0.61646043\n",
      "Iteration 2002, loss = 0.61645987\n",
      "Iteration 2003, loss = 0.61645932\n",
      "Iteration 2004, loss = 0.61645828\n",
      "Iteration 2005, loss = 0.61646158\n",
      "Iteration 2006, loss = 0.61646126\n",
      "Iteration 2007, loss = 0.61646125\n",
      "Iteration 2008, loss = 0.61646432\n",
      "Iteration 2009, loss = 0.61646293\n",
      "Iteration 2010, loss = 0.61646379\n",
      "Iteration 2011, loss = 0.61646970\n",
      "Iteration 2012, loss = 0.61646907\n",
      "Iteration 2013, loss = 0.61647717\n",
      "Iteration 2014, loss = 0.61646673\n",
      "Iteration 2015, loss = 0.61646821\n",
      "Iteration 2016, loss = 0.61646097\n",
      "Iteration 2017, loss = 0.61645872\n",
      "Iteration 2018, loss = 0.61645403\n",
      "Iteration 2019, loss = 0.61646522\n",
      "Iteration 2020, loss = 0.61645431\n",
      "Iteration 2021, loss = 0.61646442\n",
      "Iteration 2022, loss = 0.61646418\n",
      "Iteration 2023, loss = 0.61645696\n",
      "Iteration 2024, loss = 0.61646230\n",
      "Iteration 2025, loss = 0.61645873\n",
      "Iteration 2026, loss = 0.61645698\n",
      "Iteration 2027, loss = 0.61646311\n",
      "Iteration 2028, loss = 0.61645980\n",
      "Iteration 2029, loss = 0.61645691\n",
      "Iteration 2030, loss = 0.61645861\n",
      "Iteration 2031, loss = 0.61645833\n",
      "Iteration 2032, loss = 0.61646180\n",
      "Iteration 2033, loss = 0.61646660\n",
      "Iteration 2034, loss = 0.61646292\n",
      "Iteration 2035, loss = 0.61646231\n",
      "Iteration 2036, loss = 0.61646416\n",
      "Iteration 2037, loss = 0.61646351\n",
      "Iteration 2038, loss = 0.61646350\n",
      "Iteration 2039, loss = 0.61646367\n",
      "Iteration 2040, loss = 0.61647135\n",
      "Iteration 2041, loss = 0.61646234\n",
      "Iteration 2042, loss = 0.61646422\n",
      "Iteration 2043, loss = 0.61646381\n",
      "Iteration 2044, loss = 0.61646333\n",
      "Iteration 2045, loss = 0.61647345\n",
      "Iteration 2046, loss = 0.61646398\n",
      "Iteration 2047, loss = 0.61646563\n",
      "Iteration 2048, loss = 0.61646171\n",
      "Iteration 2049, loss = 0.61645910\n",
      "Iteration 2050, loss = 0.61646477\n",
      "Iteration 2051, loss = 0.61646613\n",
      "Iteration 2052, loss = 0.61646621\n",
      "Iteration 2053, loss = 0.61646923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2054, loss = 0.61646817\n",
      "Iteration 2055, loss = 0.61646866\n",
      "Iteration 2056, loss = 0.61646847\n",
      "Iteration 2057, loss = 0.61648106\n",
      "Iteration 2058, loss = 0.61647259\n",
      "Iteration 2059, loss = 0.61647298\n",
      "Iteration 2060, loss = 0.61647350\n",
      "Iteration 2061, loss = 0.61647235\n",
      "Iteration 2062, loss = 0.61647296\n",
      "Iteration 2063, loss = 0.61646858\n",
      "Iteration 2064, loss = 0.61647183\n",
      "Iteration 2065, loss = 0.61646789\n",
      "Iteration 2066, loss = 0.61646984\n",
      "Iteration 2067, loss = 0.61646568\n",
      "Iteration 2068, loss = 0.61646620\n",
      "Iteration 2069, loss = 0.61646758\n",
      "Iteration 2070, loss = 0.61647006\n",
      "Iteration 2071, loss = 0.61646906\n",
      "Iteration 2072, loss = 0.61646822\n",
      "Iteration 2073, loss = 0.61646933\n",
      "Iteration 2074, loss = 0.61646759\n",
      "Iteration 2075, loss = 0.61646712\n",
      "Iteration 2076, loss = 0.61646526\n",
      "Iteration 2077, loss = 0.61646510\n",
      "Iteration 2078, loss = 0.61646382\n",
      "Iteration 2079, loss = 0.61646542\n",
      "Iteration 2080, loss = 0.61646361\n",
      "Iteration 2081, loss = 0.61646220\n",
      "Iteration 2082, loss = 0.61646147\n",
      "Training loss did not improve more than tol=0.000100 for 2000 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.63149067\n",
      "Iteration 2, loss = 0.63117510\n",
      "Iteration 3, loss = 0.63086746\n",
      "Iteration 4, loss = 0.63060758\n",
      "Iteration 5, loss = 0.63036394\n",
      "Iteration 6, loss = 0.63007973\n",
      "Iteration 7, loss = 0.62988252\n",
      "Iteration 8, loss = 0.62963585\n",
      "Iteration 9, loss = 0.62940877\n",
      "Iteration 10, loss = 0.62920762\n",
      "Iteration 11, loss = 0.62902167\n",
      "Iteration 12, loss = 0.62882527\n",
      "Iteration 13, loss = 0.62861993\n",
      "Iteration 14, loss = 0.62841118\n",
      "Iteration 15, loss = 0.62821632\n",
      "Iteration 16, loss = 0.62799748\n",
      "Iteration 17, loss = 0.62781073\n",
      "Iteration 18, loss = 0.62766752\n",
      "Iteration 19, loss = 0.62746591\n",
      "Iteration 20, loss = 0.62727538\n",
      "Iteration 21, loss = 0.62708697\n",
      "Iteration 22, loss = 0.62690276\n",
      "Iteration 23, loss = 0.62679483\n",
      "Iteration 24, loss = 0.62661159\n",
      "Iteration 25, loss = 0.62640823\n",
      "Iteration 26, loss = 0.62624766\n",
      "Iteration 27, loss = 0.62609483\n",
      "Iteration 28, loss = 0.62593329\n",
      "Iteration 29, loss = 0.62577801\n",
      "Iteration 30, loss = 0.62561727\n",
      "Iteration 31, loss = 0.62542675\n",
      "Iteration 32, loss = 0.62526988\n",
      "Iteration 33, loss = 0.62511459\n",
      "Iteration 34, loss = 0.62497241\n",
      "Iteration 35, loss = 0.62481860\n",
      "Iteration 36, loss = 0.62466856\n",
      "Iteration 37, loss = 0.62452212\n",
      "Iteration 38, loss = 0.62441647\n",
      "Iteration 39, loss = 0.62431759\n",
      "Iteration 40, loss = 0.62411996\n",
      "Iteration 41, loss = 0.62398987\n",
      "Iteration 42, loss = 0.62386703\n",
      "Iteration 43, loss = 0.62372977\n",
      "Iteration 44, loss = 0.62360508\n",
      "Iteration 45, loss = 0.62348425\n",
      "Iteration 46, loss = 0.62334662\n",
      "Iteration 47, loss = 0.62322747\n",
      "Iteration 48, loss = 0.62310840\n",
      "Iteration 49, loss = 0.62299017\n",
      "Iteration 50, loss = 0.62286213\n",
      "Iteration 51, loss = 0.62276680\n",
      "Iteration 52, loss = 0.62269668\n",
      "Iteration 53, loss = 0.62260000\n",
      "Iteration 54, loss = 0.62248242\n",
      "Iteration 55, loss = 0.62236197\n",
      "Iteration 56, loss = 0.62223680\n",
      "Iteration 57, loss = 0.62210969\n",
      "Iteration 58, loss = 0.62201989\n",
      "Iteration 59, loss = 0.62186829\n",
      "Iteration 60, loss = 0.62175359\n",
      "Iteration 61, loss = 0.62164846\n",
      "Iteration 62, loss = 0.62156458\n",
      "Iteration 63, loss = 0.62147402\n",
      "Iteration 64, loss = 0.62134012\n",
      "Iteration 65, loss = 0.62124287\n",
      "Iteration 66, loss = 0.62114936\n",
      "Iteration 67, loss = 0.62104740\n",
      "Iteration 68, loss = 0.62095015\n",
      "Iteration 69, loss = 0.62085080\n",
      "Iteration 70, loss = 0.62077430\n",
      "Iteration 71, loss = 0.62066715\n",
      "Iteration 72, loss = 0.62057429\n",
      "Iteration 73, loss = 0.62048045\n",
      "Iteration 74, loss = 0.62039636\n",
      "Iteration 75, loss = 0.62031006\n",
      "Iteration 76, loss = 0.62022578\n",
      "Iteration 77, loss = 0.62014181\n",
      "Iteration 78, loss = 0.62004138\n",
      "Iteration 79, loss = 0.61995420\n",
      "Iteration 80, loss = 0.61995269\n",
      "Iteration 81, loss = 0.61980702\n",
      "Iteration 82, loss = 0.61972820\n",
      "Iteration 83, loss = 0.61964395\n",
      "Iteration 84, loss = 0.61957179\n",
      "Iteration 85, loss = 0.61950747\n",
      "Iteration 86, loss = 0.61942807\n",
      "Iteration 87, loss = 0.61935313\n",
      "Iteration 88, loss = 0.61927554\n",
      "Iteration 89, loss = 0.61922095\n",
      "Iteration 90, loss = 0.61915568\n",
      "Iteration 91, loss = 0.61907312\n",
      "Iteration 92, loss = 0.61898873\n",
      "Iteration 93, loss = 0.61890055\n",
      "Iteration 94, loss = 0.61882613\n",
      "Iteration 95, loss = 0.61873055\n",
      "Iteration 96, loss = 0.61866050\n",
      "Iteration 97, loss = 0.61860026\n",
      "Iteration 98, loss = 0.61859529\n",
      "Iteration 99, loss = 0.61851351\n",
      "Iteration 100, loss = 0.61840896\n",
      "Iteration 101, loss = 0.61833322\n",
      "Iteration 102, loss = 0.61826927\n",
      "Iteration 103, loss = 0.61820729\n",
      "Iteration 104, loss = 0.61814654\n",
      "Iteration 105, loss = 0.61807881\n",
      "Iteration 106, loss = 0.61802852\n",
      "Iteration 107, loss = 0.61796300\n",
      "Iteration 108, loss = 0.61791033\n",
      "Iteration 109, loss = 0.61785309\n",
      "Iteration 110, loss = 0.61779778\n",
      "Iteration 111, loss = 0.61772594\n",
      "Iteration 112, loss = 0.61767346\n",
      "Iteration 113, loss = 0.61761401\n",
      "Iteration 114, loss = 0.61756022\n",
      "Iteration 115, loss = 0.61755087\n",
      "Iteration 116, loss = 0.61746080\n",
      "Iteration 117, loss = 0.61741024\n",
      "Iteration 118, loss = 0.61735508\n",
      "Iteration 119, loss = 0.61729276\n",
      "Iteration 120, loss = 0.61722468\n",
      "Iteration 121, loss = 0.61720289\n",
      "Iteration 122, loss = 0.61714292\n",
      "Iteration 123, loss = 0.61710136\n",
      "Iteration 124, loss = 0.61705236\n",
      "Iteration 125, loss = 0.61700166\n",
      "Iteration 126, loss = 0.61696775\n",
      "Iteration 127, loss = 0.61691198\n",
      "Iteration 128, loss = 0.61687744\n",
      "Iteration 129, loss = 0.61679653\n",
      "Iteration 130, loss = 0.61675010\n",
      "Iteration 131, loss = 0.61671604\n",
      "Iteration 132, loss = 0.61666959\n",
      "Iteration 133, loss = 0.61663115\n",
      "Iteration 134, loss = 0.61658440\n",
      "Iteration 135, loss = 0.61656585\n",
      "Iteration 136, loss = 0.61651586\n",
      "Iteration 137, loss = 0.61649692\n",
      "Iteration 138, loss = 0.61647367\n",
      "Iteration 139, loss = 0.61643427\n",
      "Iteration 140, loss = 0.61647634\n",
      "Iteration 141, loss = 0.61642393\n",
      "Iteration 142, loss = 0.61637314\n",
      "Iteration 143, loss = 0.61632510\n",
      "Iteration 144, loss = 0.61626618\n",
      "Iteration 145, loss = 0.61623426\n",
      "Iteration 146, loss = 0.61616394\n",
      "Iteration 147, loss = 0.61612064\n",
      "Iteration 148, loss = 0.61605918\n",
      "Iteration 149, loss = 0.61601921\n",
      "Iteration 150, loss = 0.61599037\n",
      "Iteration 151, loss = 0.61592783\n",
      "Iteration 152, loss = 0.61588794\n",
      "Iteration 153, loss = 0.61585071\n",
      "Iteration 154, loss = 0.61580788\n",
      "Iteration 155, loss = 0.61576090\n",
      "Iteration 156, loss = 0.61577733\n",
      "Iteration 157, loss = 0.61569326\n",
      "Iteration 158, loss = 0.61566253\n",
      "Iteration 159, loss = 0.61562583\n",
      "Iteration 160, loss = 0.61559628\n",
      "Iteration 161, loss = 0.61557028\n",
      "Iteration 162, loss = 0.61552461\n",
      "Iteration 163, loss = 0.61550980\n",
      "Iteration 164, loss = 0.61547967\n",
      "Iteration 165, loss = 0.61544268\n",
      "Iteration 166, loss = 0.61541925\n",
      "Iteration 167, loss = 0.61540956\n",
      "Iteration 168, loss = 0.61533539\n",
      "Iteration 169, loss = 0.61529923\n",
      "Iteration 170, loss = 0.61526738\n",
      "Iteration 171, loss = 0.61533630\n",
      "Iteration 172, loss = 0.61523661\n",
      "Iteration 173, loss = 0.61520776\n",
      "Iteration 174, loss = 0.61521770\n",
      "Iteration 175, loss = 0.61514428\n",
      "Iteration 176, loss = 0.61512639\n",
      "Iteration 177, loss = 0.61510003\n",
      "Iteration 178, loss = 0.61506821\n",
      "Iteration 179, loss = 0.61503966\n",
      "Iteration 180, loss = 0.61501198\n",
      "Iteration 181, loss = 0.61500475\n",
      "Iteration 182, loss = 0.61494544\n",
      "Iteration 183, loss = 0.61502442\n",
      "Iteration 184, loss = 0.61501685\n",
      "Iteration 185, loss = 0.61489897\n",
      "Iteration 186, loss = 0.61486752\n",
      "Iteration 187, loss = 0.61483839\n",
      "Iteration 188, loss = 0.61481705\n",
      "Iteration 189, loss = 0.61479661\n",
      "Iteration 190, loss = 0.61477794\n",
      "Iteration 191, loss = 0.61479939\n",
      "Iteration 192, loss = 0.61476591\n",
      "Iteration 193, loss = 0.61469363\n",
      "Iteration 194, loss = 0.61466266\n",
      "Iteration 195, loss = 0.61464036\n",
      "Iteration 196, loss = 0.61461119\n",
      "Iteration 197, loss = 0.61462672\n",
      "Iteration 198, loss = 0.61457022\n",
      "Iteration 199, loss = 0.61454869\n",
      "Iteration 200, loss = 0.61452277\n",
      "Iteration 201, loss = 0.61448207\n",
      "Iteration 202, loss = 0.61452190\n",
      "Iteration 203, loss = 0.61445720\n",
      "Iteration 204, loss = 0.61450195\n",
      "Iteration 205, loss = 0.61443131\n",
      "Iteration 206, loss = 0.61441192\n",
      "Iteration 207, loss = 0.61438013\n",
      "Iteration 208, loss = 0.61435548\n",
      "Iteration 209, loss = 0.61435156\n",
      "Iteration 210, loss = 0.61431575\n",
      "Iteration 211, loss = 0.61428492\n",
      "Iteration 212, loss = 0.61427105\n",
      "Iteration 213, loss = 0.61422997\n",
      "Iteration 214, loss = 0.61420641\n",
      "Iteration 215, loss = 0.61419555\n",
      "Iteration 216, loss = 0.61424242\n",
      "Iteration 217, loss = 0.61416188\n",
      "Iteration 218, loss = 0.61416541\n",
      "Iteration 219, loss = 0.61412050\n",
      "Iteration 220, loss = 0.61409446\n",
      "Iteration 221, loss = 0.61408249\n",
      "Iteration 222, loss = 0.61403285\n",
      "Iteration 223, loss = 0.61410178\n",
      "Iteration 224, loss = 0.61402903\n",
      "Iteration 225, loss = 0.61401009\n",
      "Iteration 226, loss = 0.61398194\n",
      "Iteration 227, loss = 0.61392177\n",
      "Iteration 228, loss = 0.61395604\n",
      "Iteration 229, loss = 0.61390330\n",
      "Iteration 230, loss = 0.61387808\n",
      "Iteration 231, loss = 0.61385566\n",
      "Iteration 232, loss = 0.61382729\n",
      "Iteration 233, loss = 0.61381654\n",
      "Iteration 234, loss = 0.61378056\n",
      "Iteration 235, loss = 0.61376712\n",
      "Iteration 236, loss = 0.61374076\n",
      "Iteration 237, loss = 0.61371598\n",
      "Iteration 238, loss = 0.61372035\n",
      "Iteration 239, loss = 0.61364727\n",
      "Iteration 240, loss = 0.61367878\n",
      "Iteration 241, loss = 0.61363757\n",
      "Iteration 242, loss = 0.61362129\n",
      "Iteration 243, loss = 0.61361577\n",
      "Iteration 244, loss = 0.61365910\n",
      "Iteration 245, loss = 0.61369982\n",
      "Iteration 246, loss = 0.61365249\n",
      "Iteration 247, loss = 0.61361912\n",
      "Iteration 248, loss = 0.61358640\n",
      "Iteration 249, loss = 0.61354259\n",
      "Iteration 250, loss = 0.61355043\n",
      "Iteration 251, loss = 0.61348264\n",
      "Iteration 252, loss = 0.61347826\n",
      "Iteration 253, loss = 0.61346360\n",
      "Iteration 254, loss = 0.61338744\n",
      "Iteration 255, loss = 0.61338156\n",
      "Iteration 256, loss = 0.61330389\n",
      "Iteration 257, loss = 0.61327563\n",
      "Iteration 258, loss = 0.61326110\n",
      "Iteration 259, loss = 0.61320525\n",
      "Iteration 260, loss = 0.61322311\n",
      "Iteration 261, loss = 0.61313790\n",
      "Iteration 262, loss = 0.61310766\n",
      "Iteration 263, loss = 0.61305493\n",
      "Iteration 264, loss = 0.61313075\n",
      "Iteration 265, loss = 0.61315369\n",
      "Iteration 266, loss = 0.61311587\n",
      "Iteration 267, loss = 0.61309235\n",
      "Iteration 268, loss = 0.61306888\n",
      "Iteration 269, loss = 0.61302645\n",
      "Iteration 270, loss = 0.61300278\n",
      "Iteration 271, loss = 0.61292896\n",
      "Iteration 272, loss = 0.61293016\n",
      "Iteration 273, loss = 0.61290687\n",
      "Iteration 274, loss = 0.61288226\n",
      "Iteration 275, loss = 0.61289207\n",
      "Iteration 276, loss = 0.61289074\n",
      "Iteration 277, loss = 0.61290019\n",
      "Iteration 278, loss = 0.61286053\n",
      "Iteration 279, loss = 0.61274939\n",
      "Iteration 280, loss = 0.61268739\n",
      "Iteration 281, loss = 0.61256647\n",
      "Iteration 282, loss = 0.61244507\n",
      "Iteration 283, loss = 0.61241756\n",
      "Iteration 284, loss = 0.61225232\n",
      "Iteration 285, loss = 0.61222877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 286, loss = 0.61212068\n",
      "Iteration 287, loss = 0.61205559\n",
      "Iteration 288, loss = 0.61213766\n",
      "Iteration 289, loss = 0.61202534\n",
      "Iteration 290, loss = 0.61196094\n",
      "Iteration 291, loss = 0.61186127\n",
      "Iteration 292, loss = 0.61183397\n",
      "Iteration 293, loss = 0.61179575\n",
      "Iteration 294, loss = 0.61174772\n",
      "Iteration 295, loss = 0.61173609\n",
      "Iteration 296, loss = 0.61167315\n",
      "Iteration 297, loss = 0.61160646\n",
      "Iteration 298, loss = 0.61153277\n",
      "Iteration 299, loss = 0.61145274\n",
      "Iteration 300, loss = 0.61136905\n",
      "Iteration 301, loss = 0.61128312\n",
      "Iteration 302, loss = 0.61116582\n",
      "Iteration 303, loss = 0.61106534\n",
      "Iteration 304, loss = 0.61092518\n",
      "Iteration 305, loss = 0.61081077\n",
      "Iteration 306, loss = 0.61081616\n",
      "Iteration 307, loss = 0.61087442\n",
      "Iteration 308, loss = 0.61068745\n",
      "Iteration 309, loss = 0.61042140\n",
      "Iteration 310, loss = 0.61035036\n",
      "Iteration 311, loss = 0.61031044\n",
      "Iteration 312, loss = 0.61025018\n",
      "Iteration 313, loss = 0.61016080\n",
      "Iteration 314, loss = 0.61006710\n",
      "Iteration 315, loss = 0.60998535\n",
      "Iteration 316, loss = 0.60985523\n",
      "Iteration 317, loss = 0.60971167\n",
      "Iteration 318, loss = 0.60957478\n",
      "Iteration 319, loss = 0.60932894\n",
      "Iteration 320, loss = 0.60892994\n",
      "Iteration 321, loss = 0.60880819\n",
      "Iteration 322, loss = 0.60872564\n",
      "Iteration 323, loss = 0.60832291\n",
      "Iteration 324, loss = 0.60816060\n",
      "Iteration 325, loss = 0.60805028\n",
      "Iteration 326, loss = 0.60831213\n",
      "Iteration 327, loss = 0.60840544\n",
      "Iteration 328, loss = 0.60831954\n",
      "Iteration 329, loss = 0.60816135\n",
      "Iteration 330, loss = 0.60761028\n",
      "Iteration 331, loss = 0.60687049\n",
      "Iteration 332, loss = 0.60631632\n",
      "Iteration 333, loss = 0.60589078\n",
      "Iteration 334, loss = 0.60550395\n",
      "Iteration 335, loss = 0.60512391\n",
      "Iteration 336, loss = 0.60537756\n",
      "Iteration 337, loss = 0.60520003\n",
      "Iteration 338, loss = 0.60474727\n",
      "Iteration 339, loss = 0.60388781\n",
      "Iteration 340, loss = 0.60405201\n",
      "Iteration 341, loss = 0.60429504\n",
      "Iteration 342, loss = 0.60307903\n",
      "Iteration 343, loss = 0.60270182\n",
      "Iteration 344, loss = 0.60272713\n",
      "Iteration 345, loss = 0.60201717\n",
      "Iteration 346, loss = 0.60122904\n",
      "Iteration 347, loss = 0.60182845\n",
      "Iteration 348, loss = 0.60160854\n",
      "Iteration 349, loss = 0.60026960\n",
      "Iteration 350, loss = 0.60035220\n",
      "Iteration 351, loss = 0.60038883\n",
      "Iteration 352, loss = 0.60006223\n",
      "Iteration 353, loss = 0.59938590\n",
      "Iteration 354, loss = 0.59838458\n",
      "Iteration 355, loss = 0.59753910\n",
      "Iteration 356, loss = 0.59707044\n",
      "Iteration 357, loss = 0.59649625\n",
      "Iteration 358, loss = 0.59585495\n",
      "Iteration 359, loss = 0.59546372\n",
      "Iteration 360, loss = 0.59500539\n",
      "Iteration 361, loss = 0.59435747\n",
      "Iteration 362, loss = 0.59378104\n",
      "Iteration 363, loss = 0.59312184\n",
      "Iteration 364, loss = 0.59255387\n",
      "Iteration 365, loss = 0.59260928\n",
      "Iteration 366, loss = 0.59152946\n",
      "Iteration 367, loss = 0.59094153\n",
      "Iteration 368, loss = 0.59135344\n",
      "Iteration 369, loss = 0.59034556\n",
      "Iteration 370, loss = 0.58901370\n",
      "Iteration 371, loss = 0.58864715\n",
      "Iteration 372, loss = 0.58794306\n",
      "Iteration 373, loss = 0.58725919\n",
      "Iteration 374, loss = 0.58664030\n",
      "Iteration 375, loss = 0.58599709\n",
      "Iteration 376, loss = 0.58564723\n",
      "Iteration 377, loss = 0.58512206\n",
      "Iteration 378, loss = 0.58449823\n",
      "Iteration 379, loss = 0.58393505\n",
      "Iteration 380, loss = 0.58346264\n",
      "Iteration 381, loss = 0.58292589\n",
      "Iteration 382, loss = 0.58245941\n",
      "Iteration 383, loss = 0.58182554\n",
      "Iteration 384, loss = 0.58166597\n",
      "Iteration 385, loss = 0.58141197\n",
      "Iteration 386, loss = 0.58069768\n",
      "Iteration 387, loss = 0.58012379\n",
      "Iteration 388, loss = 0.57970383\n",
      "Iteration 389, loss = 0.57969751\n",
      "Iteration 390, loss = 0.57891343\n",
      "Iteration 391, loss = 0.57903823\n",
      "Iteration 392, loss = 0.57888269\n",
      "Iteration 393, loss = 0.57842222\n",
      "Iteration 394, loss = 0.57764628\n",
      "Iteration 395, loss = 0.57679012\n",
      "Iteration 396, loss = 0.57616724\n",
      "Iteration 397, loss = 0.57670236\n",
      "Iteration 398, loss = 0.57622618\n",
      "Iteration 399, loss = 0.57511330\n",
      "Iteration 400, loss = 0.57466618\n",
      "Iteration 401, loss = 0.57426331\n",
      "Iteration 402, loss = 0.57387528\n",
      "Iteration 403, loss = 0.57343510\n",
      "Iteration 404, loss = 0.57326066\n",
      "Iteration 405, loss = 0.57268104\n",
      "Iteration 406, loss = 0.57285167\n",
      "Iteration 407, loss = 0.57314485\n",
      "Iteration 408, loss = 0.57259784\n",
      "Iteration 409, loss = 0.57141858\n",
      "Iteration 410, loss = 0.57116356\n",
      "Iteration 411, loss = 0.57093478\n",
      "Iteration 412, loss = 0.57082786\n",
      "Iteration 413, loss = 0.57029541\n",
      "Iteration 414, loss = 0.57001212\n",
      "Iteration 415, loss = 0.56940174\n",
      "Iteration 416, loss = 0.56982595\n",
      "Iteration 417, loss = 0.57160456\n",
      "Iteration 418, loss = 0.56973372\n",
      "Iteration 419, loss = 0.56827464\n",
      "Iteration 420, loss = 0.56929068\n",
      "Iteration 421, loss = 0.57078173\n",
      "Iteration 422, loss = 0.57094452\n",
      "Iteration 423, loss = 0.56894073\n",
      "Iteration 424, loss = 0.56682894\n",
      "Iteration 425, loss = 0.56769688\n",
      "Iteration 426, loss = 0.57419258\n",
      "Iteration 427, loss = 0.57489837\n",
      "Iteration 428, loss = 0.56833619\n",
      "Iteration 429, loss = 0.56642271\n",
      "Iteration 430, loss = 0.56636255\n",
      "Iteration 431, loss = 0.56688071\n",
      "Iteration 432, loss = 0.56719878\n",
      "Iteration 433, loss = 0.56738750\n",
      "Iteration 434, loss = 0.56682175\n",
      "Iteration 435, loss = 0.56482828\n",
      "Iteration 436, loss = 0.56363048\n",
      "Iteration 437, loss = 0.56495070\n",
      "Iteration 438, loss = 0.56668274\n",
      "Iteration 439, loss = 0.56418435\n",
      "Iteration 440, loss = 0.56228575\n",
      "Iteration 441, loss = 0.56325334\n",
      "Iteration 442, loss = 0.56471813\n",
      "Iteration 443, loss = 0.56578849\n",
      "Iteration 444, loss = 0.56603531\n",
      "Iteration 445, loss = 0.56495693\n",
      "Iteration 446, loss = 0.56263959\n",
      "Iteration 447, loss = 0.56162476\n",
      "Iteration 448, loss = 0.56005035\n",
      "Iteration 449, loss = 0.55990114\n",
      "Iteration 450, loss = 0.56000998\n",
      "Iteration 451, loss = 0.56034584\n",
      "Iteration 452, loss = 0.56146662\n",
      "Iteration 453, loss = 0.56147767\n",
      "Iteration 454, loss = 0.56043782\n",
      "Iteration 455, loss = 0.55865172\n",
      "Iteration 456, loss = 0.55763726\n",
      "Iteration 457, loss = 0.55721492\n",
      "Iteration 458, loss = 0.55809242\n",
      "Iteration 459, loss = 0.55882452\n",
      "Iteration 460, loss = 0.55812351\n",
      "Iteration 461, loss = 0.55701764\n",
      "Iteration 462, loss = 0.55553044\n",
      "Iteration 463, loss = 0.55539760\n",
      "Iteration 464, loss = 0.55820876\n",
      "Iteration 465, loss = 0.55890974\n",
      "Iteration 466, loss = 0.55594411\n",
      "Iteration 467, loss = 0.55393422\n",
      "Iteration 468, loss = 0.55486092\n",
      "Iteration 469, loss = 0.55436524\n",
      "Iteration 470, loss = 0.55383007\n",
      "Iteration 471, loss = 0.55335983\n",
      "Iteration 472, loss = 0.55250221\n",
      "Iteration 473, loss = 0.55202757\n",
      "Iteration 474, loss = 0.55231466\n",
      "Iteration 475, loss = 0.55212690\n",
      "Iteration 476, loss = 0.55123971\n",
      "Iteration 477, loss = 0.55054162\n",
      "Iteration 478, loss = 0.55039462\n",
      "Iteration 479, loss = 0.55128432\n",
      "Iteration 480, loss = 0.55107452\n",
      "Iteration 481, loss = 0.55038162\n",
      "Iteration 482, loss = 0.54954559\n",
      "Iteration 483, loss = 0.54858264\n",
      "Iteration 484, loss = 0.54802700\n",
      "Iteration 485, loss = 0.54762755\n",
      "Iteration 486, loss = 0.54816434\n",
      "Iteration 487, loss = 0.54776945\n",
      "Iteration 488, loss = 0.54627139\n",
      "Iteration 489, loss = 0.54617847\n",
      "Iteration 490, loss = 0.54637900\n",
      "Iteration 491, loss = 0.54608054\n",
      "Iteration 492, loss = 0.54532288\n",
      "Iteration 493, loss = 0.54483982\n",
      "Iteration 494, loss = 0.54399177\n",
      "Iteration 495, loss = 0.54364759\n",
      "Iteration 496, loss = 0.54366437\n",
      "Iteration 497, loss = 0.54322697\n",
      "Iteration 498, loss = 0.54218495\n",
      "Iteration 499, loss = 0.54200851\n",
      "Iteration 500, loss = 0.54211761\n",
      "Iteration 501, loss = 0.54224545\n",
      "Iteration 502, loss = 0.54160208\n",
      "Iteration 503, loss = 0.54036777\n",
      "Iteration 504, loss = 0.53974001\n",
      "Iteration 505, loss = 0.53924723\n",
      "Iteration 506, loss = 0.53913128\n",
      "Iteration 507, loss = 0.53909842\n",
      "Iteration 508, loss = 0.53839431\n",
      "Iteration 509, loss = 0.53751514\n",
      "Iteration 510, loss = 0.53742578\n",
      "Iteration 511, loss = 0.53662246\n",
      "Iteration 512, loss = 0.53719303\n",
      "Iteration 513, loss = 0.53568609\n",
      "Iteration 514, loss = 0.53511748\n",
      "Iteration 515, loss = 0.53781555\n",
      "Iteration 516, loss = 0.54051393\n",
      "Iteration 517, loss = 0.54013590\n",
      "Iteration 518, loss = 0.53719399\n",
      "Iteration 519, loss = 0.53375615\n",
      "Iteration 520, loss = 0.53340753\n",
      "Iteration 521, loss = 0.53513252\n",
      "Iteration 522, loss = 0.53600053\n",
      "Iteration 523, loss = 0.53466765\n",
      "Iteration 524, loss = 0.53122782\n",
      "Iteration 525, loss = 0.53204865\n",
      "Iteration 526, loss = 0.53310504\n",
      "Iteration 527, loss = 0.53202629\n",
      "Iteration 528, loss = 0.53018508\n",
      "Iteration 529, loss = 0.52911372\n",
      "Iteration 530, loss = 0.52933936\n",
      "Iteration 531, loss = 0.52953778\n",
      "Iteration 532, loss = 0.52968150\n",
      "Iteration 533, loss = 0.52754476\n",
      "Iteration 534, loss = 0.52713537\n",
      "Iteration 535, loss = 0.53029917\n",
      "Iteration 536, loss = 0.53540271\n",
      "Iteration 537, loss = 0.53767940\n",
      "Iteration 538, loss = 0.53601645\n",
      "Iteration 539, loss = 0.53251920\n",
      "Iteration 540, loss = 0.52822154\n",
      "Iteration 541, loss = 0.52432567\n",
      "Iteration 542, loss = 0.52395646\n",
      "Iteration 543, loss = 0.52613241\n",
      "Iteration 544, loss = 0.52623934\n",
      "Iteration 545, loss = 0.52416647\n",
      "Iteration 546, loss = 0.52253739\n",
      "Iteration 547, loss = 0.52163665\n",
      "Iteration 548, loss = 0.52133948\n",
      "Iteration 549, loss = 0.52099778\n",
      "Iteration 550, loss = 0.52034965\n",
      "Iteration 551, loss = 0.52004967\n",
      "Iteration 552, loss = 0.51929789\n",
      "Iteration 553, loss = 0.51907765\n",
      "Iteration 554, loss = 0.52004892\n",
      "Iteration 555, loss = 0.51891338\n",
      "Iteration 556, loss = 0.51776099\n",
      "Iteration 557, loss = 0.51810563\n",
      "Iteration 558, loss = 0.51810751\n",
      "Iteration 559, loss = 0.51738840\n",
      "Iteration 560, loss = 0.51617058\n",
      "Iteration 561, loss = 0.51632634\n",
      "Iteration 562, loss = 0.51589599\n",
      "Iteration 563, loss = 0.51510397\n",
      "Iteration 564, loss = 0.51485011\n",
      "Iteration 565, loss = 0.51485624\n",
      "Iteration 566, loss = 0.51484531\n",
      "Iteration 567, loss = 0.51455414\n",
      "Iteration 568, loss = 0.51536917\n",
      "Iteration 569, loss = 0.51404921\n",
      "Iteration 570, loss = 0.51216660\n",
      "Iteration 571, loss = 0.51261350\n",
      "Iteration 572, loss = 0.51423107\n",
      "Iteration 573, loss = 0.51528247\n",
      "Iteration 574, loss = 0.51365350\n",
      "Iteration 575, loss = 0.51175569\n",
      "Iteration 576, loss = 0.51005664\n",
      "Iteration 577, loss = 0.51045193\n",
      "Iteration 578, loss = 0.51064146\n",
      "Iteration 579, loss = 0.51036401\n",
      "Iteration 580, loss = 0.50978476\n",
      "Iteration 581, loss = 0.50842118\n",
      "Iteration 582, loss = 0.50767025\n",
      "Iteration 583, loss = 0.50790974\n",
      "Iteration 584, loss = 0.50801602\n",
      "Iteration 585, loss = 0.50748020\n",
      "Iteration 586, loss = 0.50765881\n",
      "Iteration 587, loss = 0.50725606\n",
      "Iteration 588, loss = 0.50567027\n",
      "Iteration 589, loss = 0.50591310\n",
      "Iteration 590, loss = 0.50838876\n",
      "Iteration 591, loss = 0.50870678\n",
      "Iteration 592, loss = 0.50651254\n",
      "Iteration 593, loss = 0.50380913\n",
      "Iteration 594, loss = 0.50392082\n",
      "Iteration 595, loss = 0.50707019\n",
      "Iteration 596, loss = 0.50529287\n",
      "Iteration 597, loss = 0.50298580\n",
      "Iteration 598, loss = 0.50438952\n",
      "Iteration 599, loss = 0.50509338\n",
      "Iteration 600, loss = 0.50372849\n",
      "Iteration 601, loss = 0.50199942\n",
      "Iteration 602, loss = 0.50175707\n",
      "Iteration 603, loss = 0.50277483\n",
      "Iteration 604, loss = 0.50245957\n",
      "Iteration 605, loss = 0.50095061\n",
      "Iteration 606, loss = 0.50029588\n",
      "Iteration 607, loss = 0.49998642\n",
      "Iteration 608, loss = 0.50118239\n",
      "Iteration 609, loss = 0.50202148\n",
      "Iteration 610, loss = 0.50101757\n",
      "Iteration 611, loss = 0.50002515\n",
      "Iteration 612, loss = 0.49884591\n",
      "Iteration 613, loss = 0.49916614\n",
      "Iteration 614, loss = 0.50007836\n",
      "Iteration 615, loss = 0.50026390\n",
      "Iteration 616, loss = 0.49950144\n",
      "Iteration 617, loss = 0.49810945\n",
      "Iteration 618, loss = 0.49711095\n",
      "Iteration 619, loss = 0.49741615\n",
      "Iteration 620, loss = 0.49775085\n",
      "Iteration 621, loss = 0.49741611\n",
      "Iteration 622, loss = 0.49652806\n",
      "Iteration 623, loss = 0.49592403\n",
      "Iteration 624, loss = 0.49590689\n",
      "Iteration 625, loss = 0.49649209\n",
      "Iteration 626, loss = 0.49712944\n",
      "Iteration 627, loss = 0.49586009\n",
      "Iteration 628, loss = 0.49547798\n",
      "Iteration 629, loss = 0.49527730\n",
      "Iteration 630, loss = 0.49460773\n",
      "Iteration 631, loss = 0.49406628\n",
      "Iteration 632, loss = 0.49476521\n",
      "Iteration 633, loss = 0.49546837\n",
      "Iteration 634, loss = 0.49445133\n",
      "Iteration 635, loss = 0.49284831\n",
      "Iteration 636, loss = 0.49452743\n",
      "Iteration 637, loss = 0.49699679\n",
      "Iteration 638, loss = 0.49734183\n",
      "Iteration 639, loss = 0.49483695\n",
      "Iteration 640, loss = 0.49182729\n",
      "Iteration 641, loss = 0.49382488\n",
      "Iteration 642, loss = 0.49656742\n",
      "Iteration 643, loss = 0.49644531\n",
      "Iteration 644, loss = 0.49298392\n",
      "Iteration 645, loss = 0.49282492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 646, loss = 0.49360265\n",
      "Iteration 647, loss = 0.49372766\n",
      "Iteration 648, loss = 0.49194070\n",
      "Iteration 649, loss = 0.49090015\n",
      "Iteration 650, loss = 0.49118377\n",
      "Iteration 651, loss = 0.49313616\n",
      "Iteration 652, loss = 0.49649757\n",
      "Iteration 653, loss = 0.49548763\n",
      "Iteration 654, loss = 0.49200872\n",
      "Iteration 655, loss = 0.49111372\n",
      "Iteration 656, loss = 0.49094771\n",
      "Iteration 657, loss = 0.49205094\n",
      "Iteration 658, loss = 0.49177423\n",
      "Iteration 659, loss = 0.49045195\n",
      "Iteration 660, loss = 0.48972351\n",
      "Iteration 661, loss = 0.48938505\n",
      "Iteration 662, loss = 0.48921378\n",
      "Iteration 663, loss = 0.48991195\n",
      "Iteration 664, loss = 0.48952878\n",
      "Iteration 665, loss = 0.48875155\n",
      "Iteration 666, loss = 0.48817496\n",
      "Iteration 667, loss = 0.49069326\n",
      "Iteration 668, loss = 0.49038928\n",
      "Iteration 669, loss = 0.48900724\n",
      "Iteration 670, loss = 0.48745916\n",
      "Iteration 671, loss = 0.48749146\n",
      "Iteration 672, loss = 0.49000143\n",
      "Iteration 673, loss = 0.49006323\n",
      "Iteration 674, loss = 0.48848329\n",
      "Iteration 675, loss = 0.48720597\n",
      "Iteration 676, loss = 0.48813904\n",
      "Iteration 677, loss = 0.48691066\n",
      "Iteration 678, loss = 0.48770913\n",
      "Iteration 679, loss = 0.48747732\n",
      "Iteration 680, loss = 0.48652551\n",
      "Iteration 681, loss = 0.48756573\n",
      "Iteration 682, loss = 0.48670973\n",
      "Iteration 683, loss = 0.48616405\n",
      "Iteration 684, loss = 0.48621389\n",
      "Iteration 685, loss = 0.48607912\n",
      "Iteration 686, loss = 0.48600095\n",
      "Iteration 687, loss = 0.48623425\n",
      "Iteration 688, loss = 0.48464980\n",
      "Iteration 689, loss = 0.48927827\n",
      "Iteration 690, loss = 0.48991749\n",
      "Iteration 691, loss = 0.48684320\n",
      "Iteration 692, loss = 0.48451860\n",
      "Iteration 693, loss = 0.48533491\n",
      "Iteration 694, loss = 0.48912423\n",
      "Iteration 695, loss = 0.49199932\n",
      "Iteration 696, loss = 0.49012604\n",
      "Iteration 697, loss = 0.48636809\n",
      "Iteration 698, loss = 0.48453192\n",
      "Iteration 699, loss = 0.48438554\n",
      "Iteration 700, loss = 0.48502612\n",
      "Iteration 701, loss = 0.48539998\n",
      "Iteration 702, loss = 0.48404659\n",
      "Iteration 703, loss = 0.48323990\n",
      "Iteration 704, loss = 0.48488968\n",
      "Iteration 705, loss = 0.48907776\n",
      "Iteration 706, loss = 0.49209893\n",
      "Iteration 707, loss = 0.49004483\n",
      "Iteration 708, loss = 0.48632108\n",
      "Iteration 709, loss = 0.48328559\n",
      "Iteration 710, loss = 0.48407076\n",
      "Iteration 711, loss = 0.48372017\n",
      "Iteration 712, loss = 0.48325188\n",
      "Iteration 713, loss = 0.48283760\n",
      "Iteration 714, loss = 0.48276436\n",
      "Iteration 715, loss = 0.48277525\n",
      "Iteration 716, loss = 0.48264465\n",
      "Iteration 717, loss = 0.48261989\n",
      "Iteration 718, loss = 0.48292218\n",
      "Iteration 719, loss = 0.48245410\n",
      "Iteration 720, loss = 0.48199432\n",
      "Iteration 721, loss = 0.48235952\n",
      "Iteration 722, loss = 0.48374925\n",
      "Iteration 723, loss = 0.48299016\n",
      "Iteration 724, loss = 0.48200146\n",
      "Iteration 725, loss = 0.48207427\n",
      "Iteration 726, loss = 0.48193323\n",
      "Iteration 727, loss = 0.48243244\n",
      "Iteration 728, loss = 0.48178101\n",
      "Iteration 729, loss = 0.48173388\n",
      "Iteration 730, loss = 0.48171811\n",
      "Iteration 731, loss = 0.48203605\n",
      "Iteration 732, loss = 0.48239349\n",
      "Iteration 733, loss = 0.48226989\n",
      "Iteration 734, loss = 0.48158845\n",
      "Iteration 735, loss = 0.48146542\n",
      "Iteration 736, loss = 0.48149828\n",
      "Iteration 737, loss = 0.48149492\n",
      "Iteration 738, loss = 0.48199421\n",
      "Iteration 739, loss = 0.48185816\n",
      "Iteration 740, loss = 0.48150983\n",
      "Iteration 741, loss = 0.48123347\n",
      "Iteration 742, loss = 0.48140511\n",
      "Iteration 743, loss = 0.48082447\n",
      "Iteration 744, loss = 0.48237149\n",
      "Iteration 745, loss = 0.48337423\n",
      "Iteration 746, loss = 0.48267983\n",
      "Iteration 747, loss = 0.48115789\n",
      "Iteration 748, loss = 0.48090197\n",
      "Iteration 749, loss = 0.48237196\n",
      "Iteration 750, loss = 0.48103654\n",
      "Iteration 751, loss = 0.48242747\n",
      "Iteration 752, loss = 0.48229538\n",
      "Iteration 753, loss = 0.48143982\n",
      "Iteration 754, loss = 0.48146818\n",
      "Iteration 755, loss = 0.48167508\n",
      "Iteration 756, loss = 0.48137790\n",
      "Iteration 757, loss = 0.48134422\n",
      "Iteration 758, loss = 0.48073955\n",
      "Iteration 759, loss = 0.48070416\n",
      "Iteration 760, loss = 0.48091008\n",
      "Iteration 761, loss = 0.48112748\n",
      "Iteration 762, loss = 0.48104144\n",
      "Iteration 763, loss = 0.48081362\n",
      "Iteration 764, loss = 0.48048267\n",
      "Iteration 765, loss = 0.48077260\n",
      "Iteration 766, loss = 0.48105097\n",
      "Iteration 767, loss = 0.48079599\n",
      "Iteration 768, loss = 0.48087535\n",
      "Iteration 769, loss = 0.48113959\n",
      "Iteration 770, loss = 0.48297874\n",
      "Iteration 771, loss = 0.48268985\n",
      "Iteration 772, loss = 0.48081062\n",
      "Iteration 773, loss = 0.48018381\n",
      "Iteration 774, loss = 0.48167010\n",
      "Iteration 775, loss = 0.48060986\n",
      "Iteration 776, loss = 0.48073157\n",
      "Iteration 777, loss = 0.48065098\n",
      "Iteration 778, loss = 0.48121010\n",
      "Iteration 779, loss = 0.48233831\n",
      "Iteration 780, loss = 0.48395417\n",
      "Iteration 781, loss = 0.48679136\n",
      "Iteration 782, loss = 0.48576627\n",
      "Iteration 783, loss = 0.48249151\n",
      "Iteration 784, loss = 0.48034413\n",
      "Iteration 785, loss = 0.48026142\n",
      "Iteration 786, loss = 0.48094413\n",
      "Iteration 787, loss = 0.48037018\n",
      "Iteration 788, loss = 0.47996144\n",
      "Iteration 789, loss = 0.48031980\n",
      "Iteration 790, loss = 0.48063509\n",
      "Iteration 791, loss = 0.48051275\n",
      "Iteration 792, loss = 0.48000382\n",
      "Iteration 793, loss = 0.47979722\n",
      "Iteration 794, loss = 0.48047029\n",
      "Iteration 795, loss = 0.48209962\n",
      "Iteration 796, loss = 0.48255798\n",
      "Iteration 797, loss = 0.48212588\n",
      "Iteration 798, loss = 0.48168125\n",
      "Iteration 799, loss = 0.48156022\n",
      "Iteration 800, loss = 0.48139529\n",
      "Iteration 801, loss = 0.48167269\n",
      "Iteration 802, loss = 0.48229612\n",
      "Iteration 803, loss = 0.48172771\n",
      "Iteration 804, loss = 0.48066716\n",
      "Iteration 805, loss = 0.48065674\n",
      "Iteration 806, loss = 0.47971990\n",
      "Iteration 807, loss = 0.48023710\n",
      "Iteration 808, loss = 0.48062504\n",
      "Iteration 809, loss = 0.48007226\n",
      "Iteration 810, loss = 0.48014637\n",
      "Iteration 811, loss = 0.47990920\n",
      "Iteration 812, loss = 0.47961855\n",
      "Iteration 813, loss = 0.47950989\n",
      "Iteration 814, loss = 0.47991579\n",
      "Iteration 815, loss = 0.47985537\n",
      "Iteration 816, loss = 0.47953636\n",
      "Iteration 817, loss = 0.48015368\n",
      "Iteration 818, loss = 0.47973599\n",
      "Iteration 819, loss = 0.47943428\n",
      "Iteration 820, loss = 0.47941659\n",
      "Iteration 821, loss = 0.47946392\n",
      "Iteration 822, loss = 0.47954018\n",
      "Iteration 823, loss = 0.47992590\n",
      "Iteration 824, loss = 0.47988135\n",
      "Iteration 825, loss = 0.47948168\n",
      "Iteration 826, loss = 0.47950581\n",
      "Iteration 827, loss = 0.47949333\n",
      "Iteration 828, loss = 0.47958028\n",
      "Iteration 829, loss = 0.47980690\n",
      "Iteration 830, loss = 0.47990471\n",
      "Iteration 831, loss = 0.47986756\n",
      "Iteration 832, loss = 0.47960660\n",
      "Iteration 833, loss = 0.47972086\n",
      "Iteration 834, loss = 0.47968034\n",
      "Iteration 835, loss = 0.47976819\n",
      "Iteration 836, loss = 0.48007904\n",
      "Iteration 837, loss = 0.47988387\n",
      "Iteration 838, loss = 0.47973726\n",
      "Iteration 839, loss = 0.47946134\n",
      "Iteration 840, loss = 0.47913129\n",
      "Iteration 841, loss = 0.47955948\n",
      "Iteration 842, loss = 0.48017569\n",
      "Iteration 843, loss = 0.48103904\n",
      "Iteration 844, loss = 0.48003958\n",
      "Iteration 845, loss = 0.47884939\n",
      "Iteration 846, loss = 0.48081325\n",
      "Iteration 847, loss = 0.48060555\n",
      "Iteration 848, loss = 0.47972432\n",
      "Iteration 849, loss = 0.47937740\n",
      "Iteration 850, loss = 0.47910315\n",
      "Iteration 851, loss = 0.47906547\n",
      "Iteration 852, loss = 0.47910322\n",
      "Iteration 853, loss = 0.47878878\n",
      "Iteration 854, loss = 0.47922275\n",
      "Iteration 855, loss = 0.48081321\n",
      "Iteration 856, loss = 0.48342909\n",
      "Iteration 857, loss = 0.48857810\n",
      "Iteration 858, loss = 0.48734548\n",
      "Iteration 859, loss = 0.48219576\n",
      "Iteration 860, loss = 0.47887664\n",
      "Iteration 861, loss = 0.48113656\n",
      "Iteration 862, loss = 0.48234570\n",
      "Iteration 863, loss = 0.48196705\n",
      "Iteration 864, loss = 0.48070894\n",
      "Iteration 865, loss = 0.47894015\n",
      "Iteration 866, loss = 0.47938327\n",
      "Iteration 867, loss = 0.47928346\n",
      "Iteration 868, loss = 0.47887867\n",
      "Iteration 869, loss = 0.47894144\n",
      "Iteration 870, loss = 0.48021513\n",
      "Iteration 871, loss = 0.48116085\n",
      "Iteration 872, loss = 0.48156155\n",
      "Iteration 873, loss = 0.48296305\n",
      "Iteration 874, loss = 0.48291127\n",
      "Iteration 875, loss = 0.48207622\n",
      "Iteration 876, loss = 0.48005391\n",
      "Iteration 877, loss = 0.47965538\n",
      "Iteration 878, loss = 0.47984300\n",
      "Iteration 879, loss = 0.47960117\n",
      "Iteration 880, loss = 0.47933934\n",
      "Iteration 881, loss = 0.47911647\n",
      "Iteration 882, loss = 0.47912942\n",
      "Iteration 883, loss = 0.47929156\n",
      "Iteration 884, loss = 0.47907395\n",
      "Iteration 885, loss = 0.47851477\n",
      "Iteration 886, loss = 0.47946524\n",
      "Iteration 887, loss = 0.48028827\n",
      "Iteration 888, loss = 0.48106706\n",
      "Iteration 889, loss = 0.48111322\n",
      "Iteration 890, loss = 0.47969250\n",
      "Iteration 891, loss = 0.47992841\n",
      "Iteration 892, loss = 0.47968116\n",
      "Iteration 893, loss = 0.47977682\n",
      "Iteration 894, loss = 0.47936488\n",
      "Iteration 895, loss = 0.47875211\n",
      "Iteration 896, loss = 0.47903524\n",
      "Iteration 897, loss = 0.48007318\n",
      "Iteration 898, loss = 0.48004602\n",
      "Iteration 899, loss = 0.47926834\n",
      "Iteration 900, loss = 0.47865582\n",
      "Iteration 901, loss = 0.47941004\n",
      "Iteration 902, loss = 0.47986671\n",
      "Iteration 903, loss = 0.47872604\n",
      "Iteration 904, loss = 0.47930854\n",
      "Iteration 905, loss = 0.48029937\n",
      "Iteration 906, loss = 0.48047698\n",
      "Iteration 907, loss = 0.48001247\n",
      "Iteration 908, loss = 0.47885592\n",
      "Iteration 909, loss = 0.47836614\n",
      "Iteration 910, loss = 0.47919700\n",
      "Iteration 911, loss = 0.48108077\n",
      "Iteration 912, loss = 0.48031990\n",
      "Iteration 913, loss = 0.47892291\n",
      "Iteration 914, loss = 0.47899194\n",
      "Iteration 915, loss = 0.47983406\n",
      "Iteration 916, loss = 0.48030890\n",
      "Iteration 917, loss = 0.48027380\n",
      "Iteration 918, loss = 0.47942857\n",
      "Iteration 919, loss = 0.47911080\n",
      "Iteration 920, loss = 0.47878654\n",
      "Iteration 921, loss = 0.47929878\n",
      "Iteration 922, loss = 0.48034304\n",
      "Iteration 923, loss = 0.48016878\n",
      "Iteration 924, loss = 0.47942968\n",
      "Iteration 925, loss = 0.47890364\n",
      "Iteration 926, loss = 0.47868336\n",
      "Iteration 927, loss = 0.47857581\n",
      "Iteration 928, loss = 0.47871537\n",
      "Iteration 929, loss = 0.47903550\n",
      "Iteration 930, loss = 0.47894004\n",
      "Iteration 931, loss = 0.47858346\n",
      "Iteration 932, loss = 0.47932055\n",
      "Iteration 933, loss = 0.47997637\n",
      "Iteration 934, loss = 0.48005660\n",
      "Iteration 935, loss = 0.47942182\n",
      "Iteration 936, loss = 0.47956482\n",
      "Iteration 937, loss = 0.47862769\n",
      "Iteration 938, loss = 0.47869840\n",
      "Iteration 939, loss = 0.47891633\n",
      "Iteration 940, loss = 0.47938897\n",
      "Iteration 941, loss = 0.47998570\n",
      "Iteration 942, loss = 0.48032756\n",
      "Iteration 943, loss = 0.48051705\n",
      "Iteration 944, loss = 0.47947587\n",
      "Iteration 945, loss = 0.47946868\n",
      "Iteration 946, loss = 0.47874762\n",
      "Iteration 947, loss = 0.47882297\n",
      "Iteration 948, loss = 0.47860590\n",
      "Iteration 949, loss = 0.47865073\n",
      "Iteration 950, loss = 0.47919652\n",
      "Iteration 951, loss = 0.48042411\n",
      "Iteration 952, loss = 0.48151289\n",
      "Iteration 953, loss = 0.48233135\n",
      "Iteration 954, loss = 0.48233024\n",
      "Iteration 955, loss = 0.48065737\n",
      "Iteration 956, loss = 0.47986179\n",
      "Iteration 957, loss = 0.47907667\n",
      "Iteration 958, loss = 0.47953376\n",
      "Iteration 959, loss = 0.47902529\n",
      "Iteration 960, loss = 0.47867922\n",
      "Iteration 961, loss = 0.47877942\n",
      "Iteration 962, loss = 0.47890469\n",
      "Iteration 963, loss = 0.47835367\n",
      "Iteration 964, loss = 0.47835203\n",
      "Iteration 965, loss = 0.48019179\n",
      "Iteration 966, loss = 0.48364820\n",
      "Iteration 967, loss = 0.48543606\n",
      "Iteration 968, loss = 0.48321184\n",
      "Iteration 969, loss = 0.47924142\n",
      "Iteration 970, loss = 0.47881976\n",
      "Iteration 971, loss = 0.48145144\n",
      "Iteration 972, loss = 0.48173698\n",
      "Iteration 973, loss = 0.48042591\n",
      "Iteration 974, loss = 0.47926108\n",
      "Iteration 975, loss = 0.47931117\n",
      "Iteration 976, loss = 0.48023952\n",
      "Iteration 977, loss = 0.48109776\n",
      "Iteration 978, loss = 0.48127318\n",
      "Iteration 979, loss = 0.48091274\n",
      "Iteration 980, loss = 0.47943741\n",
      "Iteration 981, loss = 0.47916601\n",
      "Iteration 982, loss = 0.47875200\n",
      "Iteration 983, loss = 0.47859353\n",
      "Iteration 984, loss = 0.47941474\n",
      "Iteration 985, loss = 0.47955847\n",
      "Iteration 986, loss = 0.47926946\n",
      "Iteration 987, loss = 0.47895574\n",
      "Iteration 988, loss = 0.47837931\n",
      "Iteration 989, loss = 0.47834289\n",
      "Iteration 990, loss = 0.48018151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 991, loss = 0.48018522\n",
      "Iteration 992, loss = 0.48013809\n",
      "Iteration 993, loss = 0.47827341\n",
      "Iteration 994, loss = 0.47895430\n",
      "Iteration 995, loss = 0.47908041\n",
      "Iteration 996, loss = 0.47920264\n",
      "Iteration 997, loss = 0.47931540\n",
      "Iteration 998, loss = 0.47939423\n",
      "Iteration 999, loss = 0.47927051\n",
      "Iteration 1000, loss = 0.47865124\n",
      "Iteration 1001, loss = 0.47834953\n",
      "Iteration 1002, loss = 0.47863793\n",
      "Iteration 1003, loss = 0.47946675\n",
      "Iteration 1004, loss = 0.47963603\n",
      "Iteration 1005, loss = 0.47916549\n",
      "Iteration 1006, loss = 0.47867936\n",
      "Iteration 1007, loss = 0.47835542\n",
      "Iteration 1008, loss = 0.47830262\n",
      "Iteration 1009, loss = 0.47833782\n",
      "Iteration 1010, loss = 0.47839269\n",
      "Iteration 1011, loss = 0.47842712\n",
      "Iteration 1012, loss = 0.47848021\n",
      "Iteration 1013, loss = 0.47852326\n",
      "Iteration 1014, loss = 0.47837871\n",
      "Iteration 1015, loss = 0.47822778\n",
      "Iteration 1016, loss = 0.47839508\n",
      "Iteration 1017, loss = 0.47899839\n",
      "Iteration 1018, loss = 0.47982770\n",
      "Iteration 1019, loss = 0.48040580\n",
      "Iteration 1020, loss = 0.48051363\n",
      "Iteration 1021, loss = 0.48020147\n",
      "Iteration 1022, loss = 0.47898165\n",
      "Iteration 1023, loss = 0.47867326\n",
      "Iteration 1024, loss = 0.47854529\n",
      "Iteration 1025, loss = 0.47903907\n",
      "Iteration 1026, loss = 0.47990661\n",
      "Iteration 1027, loss = 0.47907822\n",
      "Iteration 1028, loss = 0.47834147\n",
      "Iteration 1029, loss = 0.47839074\n",
      "Iteration 1030, loss = 0.47903135\n",
      "Iteration 1031, loss = 0.47883337\n",
      "Iteration 1032, loss = 0.47831379\n",
      "Iteration 1033, loss = 0.47829705\n",
      "Iteration 1034, loss = 0.47853616\n",
      "Iteration 1035, loss = 0.47852243\n",
      "Iteration 1036, loss = 0.47840753\n",
      "Iteration 1037, loss = 0.47861482\n",
      "Iteration 1038, loss = 0.47857406\n",
      "Iteration 1039, loss = 0.47833975\n",
      "Iteration 1040, loss = 0.47842337\n",
      "Iteration 1041, loss = 0.47893662\n",
      "Iteration 1042, loss = 0.47858406\n",
      "Iteration 1043, loss = 0.47813086\n",
      "Iteration 1044, loss = 0.47874444\n",
      "Iteration 1045, loss = 0.47817394\n",
      "Iteration 1046, loss = 0.47807992\n",
      "Iteration 1047, loss = 0.47828877\n",
      "Iteration 1048, loss = 0.47927925\n",
      "Iteration 1049, loss = 0.48152223\n",
      "Iteration 1050, loss = 0.48087793\n",
      "Iteration 1051, loss = 0.47890297\n",
      "Iteration 1052, loss = 0.47918492\n",
      "Iteration 1053, loss = 0.47894761\n",
      "Iteration 1054, loss = 0.47930604\n",
      "Iteration 1055, loss = 0.47911135\n",
      "Iteration 1056, loss = 0.47800704\n",
      "Iteration 1057, loss = 0.47912898\n",
      "Iteration 1058, loss = 0.47947918\n",
      "Iteration 1059, loss = 0.48010560\n",
      "Iteration 1060, loss = 0.47899509\n",
      "Iteration 1061, loss = 0.47849811\n",
      "Iteration 1062, loss = 0.47819509\n",
      "Iteration 1063, loss = 0.47855232\n",
      "Iteration 1064, loss = 0.47977556\n",
      "Iteration 1065, loss = 0.48008609\n",
      "Iteration 1066, loss = 0.47944782\n",
      "Iteration 1067, loss = 0.47864320\n",
      "Iteration 1068, loss = 0.47850986\n",
      "Iteration 1069, loss = 0.47807976\n",
      "Iteration 1070, loss = 0.47813483\n",
      "Iteration 1071, loss = 0.47796441\n",
      "Iteration 1072, loss = 0.47824375\n",
      "Iteration 1073, loss = 0.47865728\n",
      "Iteration 1074, loss = 0.47848242\n",
      "Iteration 1075, loss = 0.47832827\n",
      "Iteration 1076, loss = 0.47804370\n",
      "Iteration 1077, loss = 0.47816113\n",
      "Iteration 1078, loss = 0.47848593\n",
      "Iteration 1079, loss = 0.47821403\n",
      "Iteration 1080, loss = 0.47866945\n",
      "Iteration 1081, loss = 0.47841679\n",
      "Iteration 1082, loss = 0.47829491\n",
      "Iteration 1083, loss = 0.47806701\n",
      "Iteration 1084, loss = 0.47819435\n",
      "Iteration 1085, loss = 0.47797062\n",
      "Iteration 1086, loss = 0.47833112\n",
      "Iteration 1087, loss = 0.47857897\n",
      "Iteration 1088, loss = 0.47846587\n",
      "Iteration 1089, loss = 0.47898790\n",
      "Iteration 1090, loss = 0.47975057\n",
      "Iteration 1091, loss = 0.48026939\n",
      "Iteration 1092, loss = 0.47963184\n",
      "Iteration 1093, loss = 0.47850957\n",
      "Iteration 1094, loss = 0.47788419\n",
      "Iteration 1095, loss = 0.47928924\n",
      "Iteration 1096, loss = 0.47991756\n",
      "Iteration 1097, loss = 0.47997372\n",
      "Iteration 1098, loss = 0.47960648\n",
      "Iteration 1099, loss = 0.47856538\n",
      "Iteration 1100, loss = 0.47810789\n",
      "Iteration 1101, loss = 0.47807311\n",
      "Iteration 1102, loss = 0.47811333\n",
      "Iteration 1103, loss = 0.47812788\n",
      "Iteration 1104, loss = 0.47812017\n",
      "Iteration 1105, loss = 0.47786837\n",
      "Iteration 1106, loss = 0.47875652\n",
      "Iteration 1107, loss = 0.47798424\n",
      "Iteration 1108, loss = 0.47796501\n",
      "Iteration 1109, loss = 0.47833758\n",
      "Iteration 1110, loss = 0.47839128\n",
      "Iteration 1111, loss = 0.47821246\n",
      "Iteration 1112, loss = 0.47818306\n",
      "Iteration 1113, loss = 0.47804885\n",
      "Iteration 1114, loss = 0.47839860\n",
      "Iteration 1115, loss = 0.47921194\n",
      "Iteration 1116, loss = 0.47804445\n",
      "Iteration 1117, loss = 0.47792536\n",
      "Iteration 1118, loss = 0.48005088\n",
      "Iteration 1119, loss = 0.48075119\n",
      "Iteration 1120, loss = 0.47930566\n",
      "Iteration 1121, loss = 0.47844430\n",
      "Iteration 1122, loss = 0.47996875\n",
      "Iteration 1123, loss = 0.47882406\n",
      "Iteration 1124, loss = 0.47814384\n",
      "Iteration 1125, loss = 0.47790002\n",
      "Iteration 1126, loss = 0.47825812\n",
      "Iteration 1127, loss = 0.47891888\n",
      "Iteration 1128, loss = 0.47890376\n",
      "Iteration 1129, loss = 0.47831046\n",
      "Iteration 1130, loss = 0.47827155\n",
      "Iteration 1131, loss = 0.47792691\n",
      "Iteration 1132, loss = 0.47793312\n",
      "Iteration 1133, loss = 0.47794690\n",
      "Iteration 1134, loss = 0.47800511\n",
      "Iteration 1135, loss = 0.47820482\n",
      "Iteration 1136, loss = 0.47832262\n",
      "Iteration 1137, loss = 0.47825769\n",
      "Iteration 1138, loss = 0.47800329\n",
      "Iteration 1139, loss = 0.47806046\n",
      "Iteration 1140, loss = 0.47803811\n",
      "Iteration 1141, loss = 0.47821721\n",
      "Iteration 1142, loss = 0.47820483\n",
      "Iteration 1143, loss = 0.47802821\n",
      "Iteration 1144, loss = 0.47758261\n",
      "Iteration 1145, loss = 0.47804189\n",
      "Iteration 1146, loss = 0.47992865\n",
      "Iteration 1147, loss = 0.48172035\n",
      "Iteration 1148, loss = 0.48122302\n",
      "Iteration 1149, loss = 0.47956402\n",
      "Iteration 1150, loss = 0.47830606\n",
      "Iteration 1151, loss = 0.47792919\n",
      "Iteration 1152, loss = 0.47842189\n",
      "Iteration 1153, loss = 0.47806976\n",
      "Iteration 1154, loss = 0.47805991\n",
      "Iteration 1155, loss = 0.47822780\n",
      "Iteration 1156, loss = 0.47814536\n",
      "Iteration 1157, loss = 0.47801879\n",
      "Iteration 1158, loss = 0.47776524\n",
      "Iteration 1159, loss = 0.47764806\n",
      "Iteration 1160, loss = 0.48055205\n",
      "Iteration 1161, loss = 0.48103570\n",
      "Iteration 1162, loss = 0.48050397\n",
      "Iteration 1163, loss = 0.47870738\n",
      "Iteration 1164, loss = 0.47792327\n",
      "Iteration 1165, loss = 0.47854226\n",
      "Iteration 1166, loss = 0.48073148\n",
      "Iteration 1167, loss = 0.48275169\n",
      "Iteration 1168, loss = 0.48195915\n",
      "Iteration 1169, loss = 0.47969265\n",
      "Iteration 1170, loss = 0.47770686\n",
      "Iteration 1171, loss = 0.47867790\n",
      "Iteration 1172, loss = 0.47987131\n",
      "Iteration 1173, loss = 0.48055154\n",
      "Iteration 1174, loss = 0.48011857\n",
      "Iteration 1175, loss = 0.47883818\n",
      "Iteration 1176, loss = 0.47883526\n",
      "Iteration 1177, loss = 0.47889803\n",
      "Iteration 1178, loss = 0.47900509\n",
      "Iteration 1179, loss = 0.47835597\n",
      "Iteration 1180, loss = 0.47845891\n",
      "Iteration 1181, loss = 0.47842652\n",
      "Iteration 1182, loss = 0.47811674\n",
      "Iteration 1183, loss = 0.47838180\n",
      "Iteration 1184, loss = 0.47829314\n",
      "Iteration 1185, loss = 0.47851356\n",
      "Iteration 1186, loss = 0.47895872\n",
      "Iteration 1187, loss = 0.47924949\n",
      "Iteration 1188, loss = 0.47934857\n",
      "Iteration 1189, loss = 0.47951874\n",
      "Iteration 1190, loss = 0.47933433\n",
      "Iteration 1191, loss = 0.47929443\n",
      "Iteration 1192, loss = 0.47917685\n",
      "Iteration 1193, loss = 0.47879636\n",
      "Iteration 1194, loss = 0.47790184\n",
      "Iteration 1195, loss = 0.47790439\n",
      "Iteration 1196, loss = 0.47868102\n",
      "Iteration 1197, loss = 0.48103293\n",
      "Iteration 1198, loss = 0.48082951\n",
      "Iteration 1199, loss = 0.47913197\n",
      "Iteration 1200, loss = 0.47870992\n",
      "Iteration 1201, loss = 0.47824108\n",
      "Iteration 1202, loss = 0.47844087\n",
      "Iteration 1203, loss = 0.47871214\n",
      "Iteration 1204, loss = 0.47938888\n",
      "Iteration 1205, loss = 0.47950477\n",
      "Iteration 1206, loss = 0.47799232\n",
      "Iteration 1207, loss = 0.47797554\n",
      "Iteration 1208, loss = 0.48108965\n",
      "Iteration 1209, loss = 0.48396953\n",
      "Iteration 1210, loss = 0.48389239\n",
      "Iteration 1211, loss = 0.48123871\n",
      "Iteration 1212, loss = 0.47786654\n",
      "Iteration 1213, loss = 0.47935163\n",
      "Iteration 1214, loss = 0.47891114\n",
      "Iteration 1215, loss = 0.47939790\n",
      "Iteration 1216, loss = 0.47861362\n",
      "Iteration 1217, loss = 0.47774048\n",
      "Iteration 1218, loss = 0.47768510\n",
      "Iteration 1219, loss = 0.47971017\n",
      "Iteration 1220, loss = 0.47979199\n",
      "Iteration 1221, loss = 0.47827238\n",
      "Iteration 1222, loss = 0.47774219\n",
      "Iteration 1223, loss = 0.47849806\n",
      "Iteration 1224, loss = 0.47997973\n",
      "Iteration 1225, loss = 0.48236608\n",
      "Iteration 1226, loss = 0.48198452\n",
      "Iteration 1227, loss = 0.48024506\n",
      "Iteration 1228, loss = 0.47894663\n",
      "Iteration 1229, loss = 0.47818688\n",
      "Iteration 1230, loss = 0.47764508\n",
      "Iteration 1231, loss = 0.47772534\n",
      "Iteration 1232, loss = 0.47805805\n",
      "Iteration 1233, loss = 0.47785033\n",
      "Iteration 1234, loss = 0.47820168\n",
      "Iteration 1235, loss = 0.47775860\n",
      "Iteration 1236, loss = 0.47772390\n",
      "Iteration 1237, loss = 0.47787116\n",
      "Iteration 1238, loss = 0.47818655\n",
      "Iteration 1239, loss = 0.47841370\n",
      "Iteration 1240, loss = 0.47833977\n",
      "Iteration 1241, loss = 0.47780987\n",
      "Iteration 1242, loss = 0.47763073\n",
      "Iteration 1243, loss = 0.47778336\n",
      "Iteration 1244, loss = 0.47838537\n",
      "Iteration 1245, loss = 0.47980499\n",
      "Iteration 1246, loss = 0.48409452\n",
      "Iteration 1247, loss = 0.48427792\n",
      "Iteration 1248, loss = 0.48152514\n",
      "Iteration 1249, loss = 0.47892620\n",
      "Iteration 1250, loss = 0.47712324\n",
      "Iteration 1251, loss = 0.47809251\n",
      "Iteration 1252, loss = 0.48134276\n",
      "Iteration 1253, loss = 0.48180951\n",
      "Iteration 1254, loss = 0.47967147\n",
      "Iteration 1255, loss = 0.47774762\n",
      "Iteration 1256, loss = 0.47875261\n",
      "Iteration 1257, loss = 0.47883190\n",
      "Iteration 1258, loss = 0.47891206\n",
      "Iteration 1259, loss = 0.47846253\n",
      "Iteration 1260, loss = 0.47802897\n",
      "Iteration 1261, loss = 0.47779499\n",
      "Iteration 1262, loss = 0.47780455\n",
      "Iteration 1263, loss = 0.47790254\n",
      "Iteration 1264, loss = 0.47772390\n",
      "Iteration 1265, loss = 0.47807107\n",
      "Iteration 1266, loss = 0.47776157\n",
      "Iteration 1267, loss = 0.47771242\n",
      "Iteration 1268, loss = 0.47771453\n",
      "Iteration 1269, loss = 0.47766746\n",
      "Iteration 1270, loss = 0.47760283\n",
      "Iteration 1271, loss = 0.47830747\n",
      "Iteration 1272, loss = 0.47826960\n",
      "Iteration 1273, loss = 0.47845620\n",
      "Iteration 1274, loss = 0.47838477\n",
      "Iteration 1275, loss = 0.47794256\n",
      "Iteration 1276, loss = 0.47738194\n",
      "Iteration 1277, loss = 0.47798424\n",
      "Iteration 1278, loss = 0.47856963\n",
      "Iteration 1279, loss = 0.47883369\n",
      "Iteration 1280, loss = 0.47889073\n",
      "Iteration 1281, loss = 0.47916768\n",
      "Iteration 1282, loss = 0.47999920\n",
      "Iteration 1283, loss = 0.47965755\n",
      "Iteration 1284, loss = 0.47859178\n",
      "Iteration 1285, loss = 0.47789346\n",
      "Iteration 1286, loss = 0.47761340\n",
      "Iteration 1287, loss = 0.47826848\n",
      "Iteration 1288, loss = 0.47983975\n",
      "Iteration 1289, loss = 0.47943560\n",
      "Iteration 1290, loss = 0.47838067\n",
      "Iteration 1291, loss = 0.47769491\n",
      "Iteration 1292, loss = 0.47902995\n",
      "Iteration 1293, loss = 0.47999410\n",
      "Iteration 1294, loss = 0.47899114\n",
      "Iteration 1295, loss = 0.47751408\n",
      "Iteration 1296, loss = 0.47774099\n",
      "Iteration 1297, loss = 0.47903956\n",
      "Iteration 1298, loss = 0.48026386\n",
      "Iteration 1299, loss = 0.47981758\n",
      "Iteration 1300, loss = 0.47808933\n",
      "Iteration 1301, loss = 0.47725904\n",
      "Iteration 1302, loss = 0.47831308\n",
      "Iteration 1303, loss = 0.48212214\n",
      "Iteration 1304, loss = 0.48576141\n",
      "Iteration 1305, loss = 0.48598050\n",
      "Iteration 1306, loss = 0.48325696\n",
      "Iteration 1307, loss = 0.47975261\n",
      "Iteration 1308, loss = 0.47885670\n",
      "Iteration 1309, loss = 0.47737038\n",
      "Iteration 1310, loss = 0.47930620\n",
      "Iteration 1311, loss = 0.47935343\n",
      "Iteration 1312, loss = 0.47927128\n",
      "Iteration 1313, loss = 0.47908800\n",
      "Iteration 1314, loss = 0.47863375\n",
      "Iteration 1315, loss = 0.47820421\n",
      "Iteration 1316, loss = 0.47770319\n",
      "Iteration 1317, loss = 0.47762284\n",
      "Iteration 1318, loss = 0.47798789\n",
      "Iteration 1319, loss = 0.47788796\n",
      "Iteration 1320, loss = 0.47774464\n",
      "Iteration 1321, loss = 0.47755765\n",
      "Iteration 1322, loss = 0.47764077\n",
      "Iteration 1323, loss = 0.47780982\n",
      "Iteration 1324, loss = 0.47773008\n",
      "Iteration 1325, loss = 0.47749686\n",
      "Iteration 1326, loss = 0.47745290\n",
      "Iteration 1327, loss = 0.47756395\n",
      "Iteration 1328, loss = 0.47841097\n",
      "Iteration 1329, loss = 0.47926287\n",
      "Iteration 1330, loss = 0.48017292\n",
      "Iteration 1331, loss = 0.47979609\n",
      "Iteration 1332, loss = 0.47820858\n",
      "Iteration 1333, loss = 0.47758038\n",
      "Iteration 1334, loss = 0.47723801\n",
      "Iteration 1335, loss = 0.47761007\n",
      "Iteration 1336, loss = 0.47818616\n",
      "Iteration 1337, loss = 0.47897587\n",
      "Iteration 1338, loss = 0.47931609\n",
      "Iteration 1339, loss = 0.47944880\n",
      "Iteration 1340, loss = 0.47924732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1341, loss = 0.47943530\n",
      "Iteration 1342, loss = 0.47902117\n",
      "Iteration 1343, loss = 0.47793543\n",
      "Iteration 1344, loss = 0.47705468\n",
      "Iteration 1345, loss = 0.47836673\n",
      "Iteration 1346, loss = 0.47919645\n",
      "Iteration 1347, loss = 0.47915784\n",
      "Iteration 1348, loss = 0.47827215\n",
      "Iteration 1349, loss = 0.47749786\n",
      "Iteration 1350, loss = 0.47765604\n",
      "Iteration 1351, loss = 0.47798332\n",
      "Iteration 1352, loss = 0.47847906\n",
      "Iteration 1353, loss = 0.47793483\n",
      "Iteration 1354, loss = 0.47765233\n",
      "Iteration 1355, loss = 0.47766551\n",
      "Iteration 1356, loss = 0.47811220\n",
      "Iteration 1357, loss = 0.47761122\n",
      "Iteration 1358, loss = 0.47729278\n",
      "Iteration 1359, loss = 0.47768027\n",
      "Iteration 1360, loss = 0.47772593\n",
      "Iteration 1361, loss = 0.47762601\n",
      "Iteration 1362, loss = 0.47740455\n",
      "Iteration 1363, loss = 0.47740004\n",
      "Iteration 1364, loss = 0.47726804\n",
      "Iteration 1365, loss = 0.47738256\n",
      "Iteration 1366, loss = 0.47743427\n",
      "Iteration 1367, loss = 0.47731859\n",
      "Iteration 1368, loss = 0.47801523\n",
      "Iteration 1369, loss = 0.47828554\n",
      "Iteration 1370, loss = 0.47818724\n",
      "Iteration 1371, loss = 0.47813655\n",
      "Iteration 1372, loss = 0.47771549\n",
      "Iteration 1373, loss = 0.47719773\n",
      "Iteration 1374, loss = 0.47758370\n",
      "Iteration 1375, loss = 0.47811541\n",
      "Iteration 1376, loss = 0.47836335\n",
      "Iteration 1377, loss = 0.47840511\n",
      "Iteration 1378, loss = 0.47790467\n",
      "Iteration 1379, loss = 0.47781322\n",
      "Iteration 1380, loss = 0.47766641\n",
      "Iteration 1381, loss = 0.47747698\n",
      "Iteration 1382, loss = 0.47758101\n",
      "Iteration 1383, loss = 0.47778045\n",
      "Iteration 1384, loss = 0.47793664\n",
      "Iteration 1385, loss = 0.47783478\n",
      "Iteration 1386, loss = 0.47754437\n",
      "Iteration 1387, loss = 0.47730276\n",
      "Iteration 1388, loss = 0.47736063\n",
      "Iteration 1389, loss = 0.47763483\n",
      "Iteration 1390, loss = 0.47741311\n",
      "Iteration 1391, loss = 0.47725914\n",
      "Iteration 1392, loss = 0.47731993\n",
      "Iteration 1393, loss = 0.47772113\n",
      "Iteration 1394, loss = 0.47785010\n",
      "Iteration 1395, loss = 0.47782520\n",
      "Iteration 1396, loss = 0.47781510\n",
      "Iteration 1397, loss = 0.47764044\n",
      "Iteration 1398, loss = 0.47744760\n",
      "Iteration 1399, loss = 0.47766502\n",
      "Iteration 1400, loss = 0.47757398\n",
      "Iteration 1401, loss = 0.47770405\n",
      "Iteration 1402, loss = 0.47801582\n",
      "Iteration 1403, loss = 0.47780638\n",
      "Iteration 1404, loss = 0.47808788\n",
      "Iteration 1405, loss = 0.47756068\n",
      "Iteration 1406, loss = 0.47772423\n",
      "Iteration 1407, loss = 0.47825016\n",
      "Iteration 1408, loss = 0.47871222\n",
      "Iteration 1409, loss = 0.47851772\n",
      "Iteration 1410, loss = 0.47805112\n",
      "Iteration 1411, loss = 0.47769963\n",
      "Iteration 1412, loss = 0.47755445\n",
      "Iteration 1413, loss = 0.47753068\n",
      "Iteration 1414, loss = 0.47765375\n",
      "Iteration 1415, loss = 0.47815629\n",
      "Iteration 1416, loss = 0.47831098\n",
      "Iteration 1417, loss = 0.47797498\n",
      "Iteration 1418, loss = 0.47746772\n",
      "Iteration 1419, loss = 0.47728484\n",
      "Iteration 1420, loss = 0.47794490\n",
      "Iteration 1421, loss = 0.47806558\n",
      "Iteration 1422, loss = 0.47788006\n",
      "Iteration 1423, loss = 0.47785449\n",
      "Iteration 1424, loss = 0.47744982\n",
      "Iteration 1425, loss = 0.47715895\n",
      "Iteration 1426, loss = 0.47737524\n",
      "Iteration 1427, loss = 0.47839972\n",
      "Iteration 1428, loss = 0.47795668\n",
      "Iteration 1429, loss = 0.47719125\n",
      "Iteration 1430, loss = 0.47763658\n",
      "Iteration 1431, loss = 0.47900399\n",
      "Iteration 1432, loss = 0.47861630\n",
      "Iteration 1433, loss = 0.47807374\n",
      "Iteration 1434, loss = 0.47745812\n",
      "Iteration 1435, loss = 0.47730680\n",
      "Iteration 1436, loss = 0.47751683\n",
      "Iteration 1437, loss = 0.47745709\n",
      "Iteration 1438, loss = 0.47735135\n",
      "Iteration 1439, loss = 0.47723305\n",
      "Iteration 1440, loss = 0.47755732\n",
      "Iteration 1441, loss = 0.47842580\n",
      "Iteration 1442, loss = 0.47850921\n",
      "Iteration 1443, loss = 0.47813130\n",
      "Iteration 1444, loss = 0.47772673\n",
      "Iteration 1445, loss = 0.47720685\n",
      "Iteration 1446, loss = 0.47776034\n",
      "Iteration 1447, loss = 0.47960021\n",
      "Iteration 1448, loss = 0.48078705\n",
      "Iteration 1449, loss = 0.48097651\n",
      "Iteration 1450, loss = 0.47995725\n",
      "Iteration 1451, loss = 0.47784559\n",
      "Iteration 1452, loss = 0.47748781\n",
      "Iteration 1453, loss = 0.47886267\n",
      "Iteration 1454, loss = 0.48066109\n",
      "Iteration 1455, loss = 0.47994201\n",
      "Iteration 1456, loss = 0.47843922\n",
      "Iteration 1457, loss = 0.47780932\n",
      "Iteration 1458, loss = 0.47753657\n",
      "Iteration 1459, loss = 0.47802152\n",
      "Iteration 1460, loss = 0.47747568\n",
      "Iteration 1461, loss = 0.47764134\n",
      "Iteration 1462, loss = 0.47791142\n",
      "Iteration 1463, loss = 0.47801928\n",
      "Iteration 1464, loss = 0.47804828\n",
      "Iteration 1465, loss = 0.47765105\n",
      "Iteration 1466, loss = 0.47710431\n",
      "Iteration 1467, loss = 0.47736199\n",
      "Iteration 1468, loss = 0.47795999\n",
      "Iteration 1469, loss = 0.47952398\n",
      "Iteration 1470, loss = 0.48023589\n",
      "Iteration 1471, loss = 0.47965152\n",
      "Iteration 1472, loss = 0.47809492\n",
      "Iteration 1473, loss = 0.47664091\n",
      "Iteration 1474, loss = 0.47789229\n",
      "Iteration 1475, loss = 0.48117486\n",
      "Iteration 1476, loss = 0.48124674\n",
      "Iteration 1477, loss = 0.47952758\n",
      "Iteration 1478, loss = 0.47838730\n",
      "Iteration 1479, loss = 0.47753782\n",
      "Iteration 1480, loss = 0.47705246\n",
      "Iteration 1481, loss = 0.47749199\n",
      "Iteration 1482, loss = 0.47786526\n",
      "Iteration 1483, loss = 0.47783736\n",
      "Iteration 1484, loss = 0.47729604\n",
      "Iteration 1485, loss = 0.47699373\n",
      "Iteration 1486, loss = 0.47801062\n",
      "Iteration 1487, loss = 0.47883391\n",
      "Iteration 1488, loss = 0.47893387\n",
      "Iteration 1489, loss = 0.47838192\n",
      "Iteration 1490, loss = 0.47768069\n",
      "Iteration 1491, loss = 0.47728861\n",
      "Iteration 1492, loss = 0.47756152\n",
      "Iteration 1493, loss = 0.47757317\n",
      "Iteration 1494, loss = 0.47749296\n",
      "Iteration 1495, loss = 0.47732383\n",
      "Iteration 1496, loss = 0.47702054\n",
      "Iteration 1497, loss = 0.47754210\n",
      "Iteration 1498, loss = 0.47755384\n",
      "Iteration 1499, loss = 0.47763923\n",
      "Iteration 1500, loss = 0.47794827\n",
      "Iteration 1501, loss = 0.47761675\n",
      "Iteration 1502, loss = 0.47733199\n",
      "Iteration 1503, loss = 0.47713925\n",
      "Iteration 1504, loss = 0.47848188\n",
      "Iteration 1505, loss = 0.47866196\n",
      "Iteration 1506, loss = 0.47780572\n",
      "Iteration 1507, loss = 0.47717004\n",
      "Iteration 1508, loss = 0.47763528\n",
      "Iteration 1509, loss = 0.47818425\n",
      "Iteration 1510, loss = 0.47825998\n",
      "Iteration 1511, loss = 0.47815385\n",
      "Iteration 1512, loss = 0.47777884\n",
      "Iteration 1513, loss = 0.47777250\n",
      "Iteration 1514, loss = 0.47754500\n",
      "Iteration 1515, loss = 0.47721381\n",
      "Iteration 1516, loss = 0.47702149\n",
      "Iteration 1517, loss = 0.47720206\n",
      "Iteration 1518, loss = 0.47779157\n",
      "Iteration 1519, loss = 0.47769056\n",
      "Iteration 1520, loss = 0.47718183\n",
      "Iteration 1521, loss = 0.47713839\n",
      "Iteration 1522, loss = 0.47779762\n",
      "Iteration 1523, loss = 0.47800243\n",
      "Iteration 1524, loss = 0.47787403\n",
      "Iteration 1525, loss = 0.47749215\n",
      "Iteration 1526, loss = 0.47736447\n",
      "Iteration 1527, loss = 0.47736415\n",
      "Iteration 1528, loss = 0.47740614\n",
      "Iteration 1529, loss = 0.47722659\n",
      "Iteration 1530, loss = 0.47801723\n",
      "Iteration 1531, loss = 0.47808404\n",
      "Iteration 1532, loss = 0.47777754\n",
      "Iteration 1533, loss = 0.47721994\n",
      "Iteration 1534, loss = 0.47715859\n",
      "Iteration 1535, loss = 0.47715577\n",
      "Iteration 1536, loss = 0.47789381\n",
      "Iteration 1537, loss = 0.47859567\n",
      "Iteration 1538, loss = 0.47857133\n",
      "Iteration 1539, loss = 0.47814078\n",
      "Iteration 1540, loss = 0.47794628\n",
      "Iteration 1541, loss = 0.47719908\n",
      "Iteration 1542, loss = 0.47727938\n",
      "Iteration 1543, loss = 0.47721639\n",
      "Iteration 1544, loss = 0.47719978\n",
      "Iteration 1545, loss = 0.47729564\n",
      "Iteration 1546, loss = 0.47771862\n",
      "Iteration 1547, loss = 0.47730321\n",
      "Iteration 1548, loss = 0.47717129\n",
      "Iteration 1549, loss = 0.47694197\n",
      "Iteration 1550, loss = 0.47783252\n",
      "Iteration 1551, loss = 0.47801214\n",
      "Iteration 1552, loss = 0.47843215\n",
      "Iteration 1553, loss = 0.47870017\n",
      "Iteration 1554, loss = 0.47846667\n",
      "Iteration 1555, loss = 0.47802263\n",
      "Iteration 1556, loss = 0.47738664\n",
      "Iteration 1557, loss = 0.47710348\n",
      "Iteration 1558, loss = 0.47704737\n",
      "Iteration 1559, loss = 0.47708891\n",
      "Iteration 1560, loss = 0.47691817\n",
      "Iteration 1561, loss = 0.47742813\n",
      "Iteration 1562, loss = 0.47695730\n",
      "Iteration 1563, loss = 0.47692822\n",
      "Iteration 1564, loss = 0.47696964\n",
      "Iteration 1565, loss = 0.47710288\n",
      "Iteration 1566, loss = 0.47985896\n",
      "Iteration 1567, loss = 0.47995467\n",
      "Iteration 1568, loss = 0.47861467\n",
      "Iteration 1569, loss = 0.47763661\n",
      "Iteration 1570, loss = 0.47702198\n",
      "Iteration 1571, loss = 0.47728953\n",
      "Iteration 1572, loss = 0.47758573\n",
      "Iteration 1573, loss = 0.47739230\n",
      "Iteration 1574, loss = 0.47691350\n",
      "Iteration 1575, loss = 0.47687258\n",
      "Iteration 1576, loss = 0.47745117\n",
      "Iteration 1577, loss = 0.47777612\n",
      "Iteration 1578, loss = 0.47750260\n",
      "Iteration 1579, loss = 0.47708135\n",
      "Iteration 1580, loss = 0.47731840\n",
      "Iteration 1581, loss = 0.47694251\n",
      "Iteration 1582, loss = 0.47713073\n",
      "Iteration 1583, loss = 0.47714056\n",
      "Iteration 1584, loss = 0.47716802\n",
      "Iteration 1585, loss = 0.47721540\n",
      "Iteration 1586, loss = 0.47699572\n",
      "Iteration 1587, loss = 0.47702143\n",
      "Iteration 1588, loss = 0.47726919\n",
      "Iteration 1589, loss = 0.47701286\n",
      "Iteration 1590, loss = 0.47671544\n",
      "Iteration 1591, loss = 0.47740361\n",
      "Iteration 1592, loss = 0.47948293\n",
      "Iteration 1593, loss = 0.48163710\n",
      "Iteration 1594, loss = 0.48235782\n",
      "Iteration 1595, loss = 0.48061068\n",
      "Iteration 1596, loss = 0.47836794\n",
      "Iteration 1597, loss = 0.47693767\n",
      "Iteration 1598, loss = 0.47742535\n",
      "Iteration 1599, loss = 0.47829978\n",
      "Iteration 1600, loss = 0.47928412\n",
      "Iteration 1601, loss = 0.48068113\n",
      "Iteration 1602, loss = 0.48118849\n",
      "Iteration 1603, loss = 0.48041776\n",
      "Iteration 1604, loss = 0.47892807\n",
      "Iteration 1605, loss = 0.47746282\n",
      "Iteration 1606, loss = 0.47758597\n",
      "Iteration 1607, loss = 0.47806593\n",
      "Iteration 1608, loss = 0.47804160\n",
      "Iteration 1609, loss = 0.47756974\n",
      "Iteration 1610, loss = 0.47709536\n",
      "Iteration 1611, loss = 0.47787274\n",
      "Iteration 1612, loss = 0.47730250\n",
      "Iteration 1613, loss = 0.47723138\n",
      "Iteration 1614, loss = 0.47739978\n",
      "Iteration 1615, loss = 0.47813181\n",
      "Iteration 1616, loss = 0.47887830\n",
      "Iteration 1617, loss = 0.47936316\n",
      "Iteration 1618, loss = 0.47939289\n",
      "Iteration 1619, loss = 0.47875461\n",
      "Iteration 1620, loss = 0.47796008\n",
      "Iteration 1621, loss = 0.47725139\n",
      "Iteration 1622, loss = 0.47702183\n",
      "Iteration 1623, loss = 0.47728504\n",
      "Iteration 1624, loss = 0.47825159\n",
      "Iteration 1625, loss = 0.47822866\n",
      "Iteration 1626, loss = 0.47806662\n",
      "Iteration 1627, loss = 0.47780835\n",
      "Iteration 1628, loss = 0.47754755\n",
      "Iteration 1629, loss = 0.47715135\n",
      "Iteration 1630, loss = 0.47743800\n",
      "Iteration 1631, loss = 0.47773157\n",
      "Iteration 1632, loss = 0.47815346\n",
      "Iteration 1633, loss = 0.47917534\n",
      "Iteration 1634, loss = 0.47938116\n",
      "Iteration 1635, loss = 0.47885586\n",
      "Iteration 1636, loss = 0.47837010\n",
      "Iteration 1637, loss = 0.47826232\n",
      "Iteration 1638, loss = 0.47806150\n",
      "Iteration 1639, loss = 0.47719647\n",
      "Iteration 1640, loss = 0.47688211\n",
      "Iteration 1641, loss = 0.47715082\n",
      "Iteration 1642, loss = 0.47702229\n",
      "Iteration 1643, loss = 0.47682775\n",
      "Iteration 1644, loss = 0.47678593\n",
      "Iteration 1645, loss = 0.47734009\n",
      "Iteration 1646, loss = 0.47772672\n",
      "Iteration 1647, loss = 0.47804304\n",
      "Iteration 1648, loss = 0.47797141\n",
      "Iteration 1649, loss = 0.47765480\n",
      "Iteration 1650, loss = 0.47695634\n",
      "Iteration 1651, loss = 0.47719576\n",
      "Iteration 1652, loss = 0.47730371\n",
      "Iteration 1653, loss = 0.47732755\n",
      "Iteration 1654, loss = 0.47749043\n",
      "Iteration 1655, loss = 0.47739457\n",
      "Iteration 1656, loss = 0.47729156\n",
      "Iteration 1657, loss = 0.47720708\n",
      "Iteration 1658, loss = 0.47754046\n",
      "Iteration 1659, loss = 0.47766882\n",
      "Iteration 1660, loss = 0.47805167\n",
      "Iteration 1661, loss = 0.47789504\n",
      "Iteration 1662, loss = 0.47730508\n",
      "Iteration 1663, loss = 0.47700358\n",
      "Iteration 1664, loss = 0.47702024\n",
      "Iteration 1665, loss = 0.47750300\n",
      "Iteration 1666, loss = 0.47749521\n",
      "Iteration 1667, loss = 0.47736791\n",
      "Iteration 1668, loss = 0.47713036\n",
      "Iteration 1669, loss = 0.47714665\n",
      "Iteration 1670, loss = 0.47687717\n",
      "Iteration 1671, loss = 0.47685763\n",
      "Iteration 1672, loss = 0.47683790\n",
      "Iteration 1673, loss = 0.47689999\n",
      "Iteration 1674, loss = 0.47683300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1675, loss = 0.47755545\n",
      "Iteration 1676, loss = 0.47686880\n",
      "Iteration 1677, loss = 0.47693162\n",
      "Iteration 1678, loss = 0.47695257\n",
      "Iteration 1679, loss = 0.47674932\n",
      "Iteration 1680, loss = 0.47681374\n",
      "Iteration 1681, loss = 0.47731401\n",
      "Iteration 1682, loss = 0.47773764\n",
      "Iteration 1683, loss = 0.47721220\n",
      "Iteration 1684, loss = 0.47717480\n",
      "Iteration 1685, loss = 0.47684387\n",
      "Iteration 1686, loss = 0.47697252\n",
      "Iteration 1687, loss = 0.47714499\n",
      "Iteration 1688, loss = 0.47759268\n",
      "Iteration 1689, loss = 0.47939002\n",
      "Iteration 1690, loss = 0.48041033\n",
      "Iteration 1691, loss = 0.47883353\n",
      "Iteration 1692, loss = 0.47870747\n",
      "Iteration 1693, loss = 0.47746050\n",
      "Iteration 1694, loss = 0.47704197\n",
      "Iteration 1695, loss = 0.47693882\n",
      "Iteration 1696, loss = 0.47711484\n",
      "Iteration 1697, loss = 0.47736584\n",
      "Iteration 1698, loss = 0.47727691\n",
      "Iteration 1699, loss = 0.47707769\n",
      "Iteration 1700, loss = 0.47687373\n",
      "Iteration 1701, loss = 0.47698691\n",
      "Iteration 1702, loss = 0.47694923\n",
      "Iteration 1703, loss = 0.47691662\n",
      "Iteration 1704, loss = 0.47698842\n",
      "Iteration 1705, loss = 0.47677823\n",
      "Iteration 1706, loss = 0.47653941\n",
      "Iteration 1707, loss = 0.47793879\n",
      "Iteration 1708, loss = 0.47742042\n",
      "Iteration 1709, loss = 0.47658026\n",
      "Iteration 1710, loss = 0.47754367\n",
      "Iteration 1711, loss = 0.47720849\n",
      "Iteration 1712, loss = 0.47736775\n",
      "Iteration 1713, loss = 0.47774146\n",
      "Iteration 1714, loss = 0.47776195\n",
      "Iteration 1715, loss = 0.47802275\n",
      "Iteration 1716, loss = 0.47725037\n",
      "Iteration 1717, loss = 0.47638043\n",
      "Iteration 1718, loss = 0.47655819\n",
      "Iteration 1719, loss = 0.48057326\n",
      "Iteration 1720, loss = 0.48247248\n",
      "Iteration 1721, loss = 0.48194388\n",
      "Iteration 1722, loss = 0.47972601\n",
      "Iteration 1723, loss = 0.47761952\n",
      "Iteration 1724, loss = 0.47633661\n",
      "Iteration 1725, loss = 0.47714883\n",
      "Iteration 1726, loss = 0.47930365\n",
      "Iteration 1727, loss = 0.48124831\n",
      "Iteration 1728, loss = 0.48075377\n",
      "Iteration 1729, loss = 0.47983191\n",
      "Iteration 1730, loss = 0.47736365\n",
      "Iteration 1731, loss = 0.47671693\n",
      "Iteration 1732, loss = 0.47660921\n",
      "Iteration 1733, loss = 0.47740729\n",
      "Iteration 1734, loss = 0.47845695\n",
      "Iteration 1735, loss = 0.47900589\n",
      "Iteration 1736, loss = 0.47898143\n",
      "Iteration 1737, loss = 0.47811760\n",
      "Iteration 1738, loss = 0.47711040\n",
      "Iteration 1739, loss = 0.47672781\n",
      "Iteration 1740, loss = 0.47907777\n",
      "Iteration 1741, loss = 0.47943850\n",
      "Iteration 1742, loss = 0.47879344\n",
      "Iteration 1743, loss = 0.47815121\n",
      "Iteration 1744, loss = 0.47708293\n",
      "Iteration 1745, loss = 0.47671958\n",
      "Iteration 1746, loss = 0.47652456\n",
      "Iteration 1747, loss = 0.47677054\n",
      "Iteration 1748, loss = 0.47767128\n",
      "Iteration 1749, loss = 0.47788190\n",
      "Iteration 1750, loss = 0.47756649\n",
      "Iteration 1751, loss = 0.47756741\n",
      "Iteration 1752, loss = 0.47728450\n",
      "Iteration 1753, loss = 0.47692993\n",
      "Iteration 1754, loss = 0.47662069\n",
      "Iteration 1755, loss = 0.47673666\n",
      "Iteration 1756, loss = 0.47689094\n",
      "Iteration 1757, loss = 0.47717870\n",
      "Iteration 1758, loss = 0.47793170\n",
      "Iteration 1759, loss = 0.47890589\n",
      "Iteration 1760, loss = 0.47934064\n",
      "Iteration 1761, loss = 0.47899703\n",
      "Iteration 1762, loss = 0.47852554\n",
      "Iteration 1763, loss = 0.47696521\n",
      "Iteration 1764, loss = 0.47654978\n",
      "Iteration 1765, loss = 0.47667396\n",
      "Iteration 1766, loss = 0.47792286\n",
      "Iteration 1767, loss = 0.47815748\n",
      "Iteration 1768, loss = 0.47786990\n",
      "Iteration 1769, loss = 0.47733045\n",
      "Iteration 1770, loss = 0.47699217\n",
      "Iteration 1771, loss = 0.47683776\n",
      "Iteration 1772, loss = 0.47687865\n",
      "Iteration 1773, loss = 0.47710414\n",
      "Iteration 1774, loss = 0.47734328\n",
      "Iteration 1775, loss = 0.47691952\n",
      "Iteration 1776, loss = 0.47668390\n",
      "Iteration 1777, loss = 0.47744938\n",
      "Iteration 1778, loss = 0.47775176\n",
      "Iteration 1779, loss = 0.47767142\n",
      "Iteration 1780, loss = 0.47712387\n",
      "Iteration 1781, loss = 0.47698161\n",
      "Iteration 1782, loss = 0.47676839\n",
      "Iteration 1783, loss = 0.47651475\n",
      "Iteration 1784, loss = 0.47714369\n",
      "Iteration 1785, loss = 0.47936362\n",
      "Iteration 1786, loss = 0.48000377\n",
      "Iteration 1787, loss = 0.47928893\n",
      "Iteration 1788, loss = 0.47796353\n",
      "Iteration 1789, loss = 0.47696433\n",
      "Iteration 1790, loss = 0.47661409\n",
      "Iteration 1791, loss = 0.47694940\n",
      "Iteration 1792, loss = 0.47702390\n",
      "Iteration 1793, loss = 0.47703605\n",
      "Iteration 1794, loss = 0.47702037\n",
      "Iteration 1795, loss = 0.47686942\n",
      "Iteration 1796, loss = 0.47673924\n",
      "Iteration 1797, loss = 0.47679215\n",
      "Iteration 1798, loss = 0.47706725\n",
      "Iteration 1799, loss = 0.47730895\n",
      "Iteration 1800, loss = 0.47698266\n",
      "Iteration 1801, loss = 0.47661911\n",
      "Iteration 1802, loss = 0.47808445\n",
      "Iteration 1803, loss = 0.47744993\n",
      "Iteration 1804, loss = 0.47659196\n",
      "Iteration 1805, loss = 0.47677508\n",
      "Iteration 1806, loss = 0.47844053\n",
      "Iteration 1807, loss = 0.47895451\n",
      "Iteration 1808, loss = 0.47814487\n",
      "Iteration 1809, loss = 0.47653270\n",
      "Iteration 1810, loss = 0.47775491\n",
      "Iteration 1811, loss = 0.47816165\n",
      "Iteration 1812, loss = 0.47866876\n",
      "Iteration 1813, loss = 0.47808716\n",
      "Iteration 1814, loss = 0.47712005\n",
      "Iteration 1815, loss = 0.47676631\n",
      "Iteration 1816, loss = 0.47757264\n",
      "Iteration 1817, loss = 0.47774351\n",
      "Iteration 1818, loss = 0.47756797\n",
      "Iteration 1819, loss = 0.47683539\n",
      "Iteration 1820, loss = 0.47642108\n",
      "Iteration 1821, loss = 0.47705004\n",
      "Iteration 1822, loss = 0.47996249\n",
      "Iteration 1823, loss = 0.48326248\n",
      "Iteration 1824, loss = 0.48417084\n",
      "Iteration 1825, loss = 0.48263678\n",
      "Iteration 1826, loss = 0.48028948\n",
      "Iteration 1827, loss = 0.47978910\n",
      "Iteration 1828, loss = 0.47793719\n",
      "Iteration 1829, loss = 0.47840509\n",
      "Iteration 1830, loss = 0.47797412\n",
      "Iteration 1831, loss = 0.47784549\n",
      "Iteration 1832, loss = 0.47678058\n",
      "Iteration 1833, loss = 0.47772191\n",
      "Iteration 1834, loss = 0.47819624\n",
      "Iteration 1835, loss = 0.47763227\n",
      "Iteration 1836, loss = 0.47701130\n",
      "Iteration 1837, loss = 0.47674502\n",
      "Iteration 1838, loss = 0.47675104\n",
      "Iteration 1839, loss = 0.47665329\n",
      "Iteration 1840, loss = 0.47722604\n",
      "Iteration 1841, loss = 0.47773036\n",
      "Iteration 1842, loss = 0.47789473\n",
      "Iteration 1843, loss = 0.47743616\n",
      "Iteration 1844, loss = 0.47753687\n",
      "Iteration 1845, loss = 0.47670573\n",
      "Iteration 1846, loss = 0.47671522\n",
      "Iteration 1847, loss = 0.47672646\n",
      "Iteration 1848, loss = 0.47689851\n",
      "Iteration 1849, loss = 0.47717046\n",
      "Iteration 1850, loss = 0.47740550\n",
      "Iteration 1851, loss = 0.47763346\n",
      "Iteration 1852, loss = 0.47674401\n",
      "Iteration 1853, loss = 0.47654908\n",
      "Iteration 1854, loss = 0.47772036\n",
      "Iteration 1855, loss = 0.47857162\n",
      "Iteration 1856, loss = 0.47906068\n",
      "Iteration 1857, loss = 0.48008429\n",
      "Iteration 1858, loss = 0.48025151\n",
      "Iteration 1859, loss = 0.47937507\n",
      "Iteration 1860, loss = 0.47772748\n",
      "Iteration 1861, loss = 0.47618025\n",
      "Iteration 1862, loss = 0.47741957\n",
      "Iteration 1863, loss = 0.47918939\n",
      "Iteration 1864, loss = 0.47911515\n",
      "Iteration 1865, loss = 0.47757129\n",
      "Iteration 1866, loss = 0.47626091\n",
      "Iteration 1867, loss = 0.47778983\n",
      "Iteration 1868, loss = 0.48164104\n",
      "Iteration 1869, loss = 0.48327945\n",
      "Iteration 1870, loss = 0.48244250\n",
      "Iteration 1871, loss = 0.47922102\n",
      "Iteration 1872, loss = 0.47852936\n",
      "Iteration 1873, loss = 0.47867279\n",
      "Iteration 1874, loss = 0.47813257\n",
      "Iteration 1875, loss = 0.47737377\n",
      "Iteration 1876, loss = 0.47651697\n",
      "Iteration 1877, loss = 0.47734584\n",
      "Iteration 1878, loss = 0.47736349\n",
      "Iteration 1879, loss = 0.47697164\n",
      "Iteration 1880, loss = 0.47656474\n",
      "Iteration 1881, loss = 0.47636480\n",
      "Iteration 1882, loss = 0.47745101\n",
      "Iteration 1883, loss = 0.47831016\n",
      "Iteration 1884, loss = 0.47839334\n",
      "Iteration 1885, loss = 0.47771630\n",
      "Iteration 1886, loss = 0.47720692\n",
      "Iteration 1887, loss = 0.47645868\n",
      "Iteration 1888, loss = 0.47651687\n",
      "Iteration 1889, loss = 0.47685965\n",
      "Iteration 1890, loss = 0.47719628\n",
      "Iteration 1891, loss = 0.47735956\n",
      "Iteration 1892, loss = 0.47890616\n",
      "Iteration 1893, loss = 0.48090067\n",
      "Iteration 1894, loss = 0.48176624\n",
      "Iteration 1895, loss = 0.48104281\n",
      "Iteration 1896, loss = 0.47898372\n",
      "Iteration 1897, loss = 0.47766189\n",
      "Iteration 1898, loss = 0.47720468\n",
      "Iteration 1899, loss = 0.47666349\n",
      "Iteration 1900, loss = 0.47644727\n",
      "Iteration 1901, loss = 0.47626121\n",
      "Iteration 1902, loss = 0.47696756\n",
      "Iteration 1903, loss = 0.47708681\n",
      "Iteration 1904, loss = 0.47710821\n",
      "Iteration 1905, loss = 0.47695243\n",
      "Iteration 1906, loss = 0.47732742\n",
      "Iteration 1907, loss = 0.47738199\n",
      "Iteration 1908, loss = 0.47691294\n",
      "Iteration 1909, loss = 0.47690250\n",
      "Iteration 1910, loss = 0.47646478\n",
      "Iteration 1911, loss = 0.47654602\n",
      "Iteration 1912, loss = 0.47659064\n",
      "Iteration 1913, loss = 0.47644250\n",
      "Iteration 1914, loss = 0.47662450\n",
      "Iteration 1915, loss = 0.47633342\n",
      "Iteration 1916, loss = 0.47616425\n",
      "Iteration 1917, loss = 0.47734590\n",
      "Iteration 1918, loss = 0.47789537\n",
      "Iteration 1919, loss = 0.47780315\n",
      "Iteration 1920, loss = 0.47684770\n",
      "Iteration 1921, loss = 0.47677833\n",
      "Iteration 1922, loss = 0.47785215\n",
      "Iteration 1923, loss = 0.47670622\n",
      "Iteration 1924, loss = 0.47625423\n",
      "Iteration 1925, loss = 0.47671510\n",
      "Iteration 1926, loss = 0.47727714\n",
      "Iteration 1927, loss = 0.47716960\n",
      "Iteration 1928, loss = 0.47651434\n",
      "Iteration 1929, loss = 0.47641193\n",
      "Iteration 1930, loss = 0.47635757\n",
      "Iteration 1931, loss = 0.47637575\n",
      "Iteration 1932, loss = 0.47693409\n",
      "Iteration 1933, loss = 0.47707703\n",
      "Iteration 1934, loss = 0.47643322\n",
      "Iteration 1935, loss = 0.47665849\n",
      "Iteration 1936, loss = 0.47656804\n",
      "Iteration 1937, loss = 0.47660739\n",
      "Iteration 1938, loss = 0.47672057\n",
      "Iteration 1939, loss = 0.47649855\n",
      "Iteration 1940, loss = 0.47661192\n",
      "Iteration 1941, loss = 0.47662760\n",
      "Iteration 1942, loss = 0.47640855\n",
      "Iteration 1943, loss = 0.47681912\n",
      "Iteration 1944, loss = 0.47644161\n",
      "Iteration 1945, loss = 0.47654798\n",
      "Iteration 1946, loss = 0.47633506\n",
      "Iteration 1947, loss = 0.47628078\n",
      "Iteration 1948, loss = 0.47642681\n",
      "Iteration 1949, loss = 0.47662955\n",
      "Iteration 1950, loss = 0.47643061\n",
      "Iteration 1951, loss = 0.47627564\n",
      "Iteration 1952, loss = 0.47692443\n",
      "Iteration 1953, loss = 0.47753483\n",
      "Iteration 1954, loss = 0.47748929\n",
      "Iteration 1955, loss = 0.47687614\n",
      "Iteration 1956, loss = 0.47635728\n",
      "Iteration 1957, loss = 0.47679807\n",
      "Iteration 1958, loss = 0.47730065\n",
      "Iteration 1959, loss = 0.47771899\n",
      "Iteration 1960, loss = 0.47769767\n",
      "Iteration 1961, loss = 0.47733608\n",
      "Iteration 1962, loss = 0.47658355\n",
      "Iteration 1963, loss = 0.47608741\n",
      "Iteration 1964, loss = 0.47658975\n",
      "Iteration 1965, loss = 0.47907192\n",
      "Iteration 1966, loss = 0.48304702\n",
      "Iteration 1967, loss = 0.48523876\n",
      "Iteration 1968, loss = 0.48265854\n",
      "Iteration 1969, loss = 0.47803852\n",
      "Iteration 1970, loss = 0.47609719\n",
      "Iteration 1971, loss = 0.47813434\n",
      "Iteration 1972, loss = 0.48133433\n",
      "Iteration 1973, loss = 0.48350176\n",
      "Iteration 1974, loss = 0.48391679\n",
      "Iteration 1975, loss = 0.48229254\n",
      "Iteration 1976, loss = 0.47951644\n",
      "Iteration 1977, loss = 0.47717306\n",
      "Iteration 1978, loss = 0.47742866\n",
      "Iteration 1979, loss = 0.47732542\n",
      "Iteration 1980, loss = 0.47808803\n",
      "Iteration 1981, loss = 0.47896216\n",
      "Iteration 1982, loss = 0.47934416\n",
      "Iteration 1983, loss = 0.47819694\n",
      "Iteration 1984, loss = 0.47704271\n",
      "Iteration 1985, loss = 0.47622863\n",
      "Iteration 1986, loss = 0.47699821\n",
      "Iteration 1987, loss = 0.47875456\n",
      "Iteration 1988, loss = 0.48058363\n",
      "Iteration 1989, loss = 0.48192846\n",
      "Iteration 1990, loss = 0.48246279\n",
      "Iteration 1991, loss = 0.48169141\n",
      "Iteration 1992, loss = 0.47983642\n",
      "Iteration 1993, loss = 0.47802788\n",
      "Iteration 1994, loss = 0.47718889\n",
      "Iteration 1995, loss = 0.47690634\n",
      "Iteration 1996, loss = 0.47677042\n",
      "Iteration 1997, loss = 0.47687824\n",
      "Iteration 1998, loss = 0.47697562\n",
      "Iteration 1999, loss = 0.47688706\n",
      "Iteration 2000, loss = 0.47638428\n",
      "Iteration 2001, loss = 0.47639959\n",
      "Iteration 2002, loss = 0.47689605\n",
      "Iteration 2003, loss = 0.47732809\n",
      "Iteration 2004, loss = 0.47750366\n",
      "Iteration 2005, loss = 0.47727528\n",
      "Iteration 2006, loss = 0.47690075\n",
      "Iteration 2007, loss = 0.47688393\n",
      "Iteration 2008, loss = 0.47677526\n",
      "Iteration 2009, loss = 0.47664195\n",
      "Iteration 2010, loss = 0.47678141\n",
      "Iteration 2011, loss = 0.47678373\n",
      "Iteration 2012, loss = 0.47677622\n",
      "Iteration 2013, loss = 0.47674073\n",
      "Iteration 2014, loss = 0.47644858\n",
      "Iteration 2015, loss = 0.47670759\n",
      "Iteration 2016, loss = 0.47764057\n",
      "Iteration 2017, loss = 0.47744343\n",
      "Iteration 2018, loss = 0.47695977\n",
      "Iteration 2019, loss = 0.47630740\n",
      "Iteration 2020, loss = 0.47764707\n",
      "Iteration 2021, loss = 0.47677621\n",
      "Iteration 2022, loss = 0.47641219\n",
      "Iteration 2023, loss = 0.47642386\n",
      "Iteration 2024, loss = 0.47642714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2025, loss = 0.47663882\n",
      "Iteration 2026, loss = 0.47717898\n",
      "Iteration 2027, loss = 0.47838821\n",
      "Iteration 2028, loss = 0.47941592\n",
      "Iteration 2029, loss = 0.47852891\n",
      "Iteration 2030, loss = 0.47689949\n",
      "Iteration 2031, loss = 0.47651818\n",
      "Iteration 2032, loss = 0.47635035\n",
      "Iteration 2033, loss = 0.47646987\n",
      "Iteration 2034, loss = 0.47661269\n",
      "Iteration 2035, loss = 0.47679125\n",
      "Iteration 2036, loss = 0.47698429\n",
      "Iteration 2037, loss = 0.47672894\n",
      "Iteration 2038, loss = 0.47684175\n",
      "Iteration 2039, loss = 0.47627415\n",
      "Iteration 2040, loss = 0.47639084\n",
      "Iteration 2041, loss = 0.47618215\n",
      "Iteration 2042, loss = 0.47631899\n",
      "Iteration 2043, loss = 0.47683074\n",
      "Iteration 2044, loss = 0.47653968\n",
      "Iteration 2045, loss = 0.47659089\n",
      "Iteration 2046, loss = 0.47632398\n",
      "Iteration 2047, loss = 0.47692993\n",
      "Iteration 2048, loss = 0.47724197\n",
      "Iteration 2049, loss = 0.47709237\n",
      "Iteration 2050, loss = 0.47656680\n",
      "Iteration 2051, loss = 0.47628150\n",
      "Iteration 2052, loss = 0.47640563\n",
      "Iteration 2053, loss = 0.47663886\n",
      "Iteration 2054, loss = 0.47705497\n",
      "Iteration 2055, loss = 0.47784882\n",
      "Iteration 2056, loss = 0.47815340\n",
      "Iteration 2057, loss = 0.47773233\n",
      "Iteration 2058, loss = 0.47732541\n",
      "Iteration 2059, loss = 0.47683063\n",
      "Iteration 2060, loss = 0.47635482\n",
      "Iteration 2061, loss = 0.47658647\n",
      "Iteration 2062, loss = 0.47625670\n",
      "Iteration 2063, loss = 0.47631126\n",
      "Iteration 2064, loss = 0.47623641\n",
      "Iteration 2065, loss = 0.47625656\n",
      "Iteration 2066, loss = 0.47623542\n",
      "Iteration 2067, loss = 0.47605921\n",
      "Iteration 2068, loss = 0.47634088\n",
      "Iteration 2069, loss = 0.47691547\n",
      "Iteration 2070, loss = 0.47753024\n",
      "Iteration 2071, loss = 0.47707272\n",
      "Iteration 2072, loss = 0.47681724\n",
      "Iteration 2073, loss = 0.47623326\n",
      "Iteration 2074, loss = 0.47636599\n",
      "Iteration 2075, loss = 0.47655259\n",
      "Iteration 2076, loss = 0.47687148\n",
      "Iteration 2077, loss = 0.47642476\n",
      "Iteration 2078, loss = 0.47652108\n",
      "Iteration 2079, loss = 0.47657138\n",
      "Iteration 2080, loss = 0.47682886\n",
      "Iteration 2081, loss = 0.47698053\n",
      "Iteration 2082, loss = 0.47684422\n",
      "Iteration 2083, loss = 0.47708917\n",
      "Iteration 2084, loss = 0.47646787\n",
      "Iteration 2085, loss = 0.47675155\n",
      "Iteration 2086, loss = 0.47651720\n",
      "Iteration 2087, loss = 0.47648418\n",
      "Iteration 2088, loss = 0.47608806\n",
      "Iteration 2089, loss = 0.47634450\n",
      "Iteration 2090, loss = 0.47673072\n",
      "Iteration 2091, loss = 0.47635482\n",
      "Iteration 2092, loss = 0.47617963\n",
      "Iteration 2093, loss = 0.47626453\n",
      "Iteration 2094, loss = 0.47662861\n",
      "Iteration 2095, loss = 0.47715468\n",
      "Iteration 2096, loss = 0.47763862\n",
      "Iteration 2097, loss = 0.47741904\n",
      "Iteration 2098, loss = 0.47666656\n",
      "Iteration 2099, loss = 0.47638501\n",
      "Iteration 2100, loss = 0.47613380\n",
      "Iteration 2101, loss = 0.47619514\n",
      "Iteration 2102, loss = 0.47628858\n",
      "Iteration 2103, loss = 0.47582926\n",
      "Iteration 2104, loss = 0.47580842\n",
      "Iteration 2105, loss = 0.47725558\n",
      "Iteration 2106, loss = 0.47909670\n",
      "Iteration 2107, loss = 0.48055274\n",
      "Iteration 2108, loss = 0.48167621\n",
      "Iteration 2109, loss = 0.48096623\n",
      "Iteration 2110, loss = 0.47873379\n",
      "Iteration 2111, loss = 0.47623734\n",
      "Iteration 2112, loss = 0.47573110\n",
      "Iteration 2113, loss = 0.47724902\n",
      "Iteration 2114, loss = 0.48100381\n",
      "Iteration 2115, loss = 0.48366066\n",
      "Iteration 2116, loss = 0.48256797\n",
      "Iteration 2117, loss = 0.47912182\n",
      "Iteration 2118, loss = 0.47648279\n",
      "Iteration 2119, loss = 0.47696724\n",
      "Iteration 2120, loss = 0.47884030\n",
      "Iteration 2121, loss = 0.47911210\n",
      "Iteration 2122, loss = 0.47911394\n",
      "Iteration 2123, loss = 0.47885836\n",
      "Iteration 2124, loss = 0.47816956\n",
      "Iteration 2125, loss = 0.47781091\n",
      "Iteration 2126, loss = 0.47728117\n",
      "Iteration 2127, loss = 0.47718366\n",
      "Iteration 2128, loss = 0.47753322\n",
      "Iteration 2129, loss = 0.47808369\n",
      "Iteration 2130, loss = 0.47850779\n",
      "Iteration 2131, loss = 0.47892883\n",
      "Iteration 2132, loss = 0.47849672\n",
      "Iteration 2133, loss = 0.47690933\n",
      "Iteration 2134, loss = 0.47663169\n",
      "Iteration 2135, loss = 0.47679330\n",
      "Iteration 2136, loss = 0.47728750\n",
      "Iteration 2137, loss = 0.47757753\n",
      "Iteration 2138, loss = 0.47737869\n",
      "Iteration 2139, loss = 0.47704110\n",
      "Iteration 2140, loss = 0.47650813\n",
      "Iteration 2141, loss = 0.47622694\n",
      "Iteration 2142, loss = 0.47741755\n",
      "Iteration 2143, loss = 0.47734792\n",
      "Iteration 2144, loss = 0.47695857\n",
      "Iteration 2145, loss = 0.47733522\n",
      "Iteration 2146, loss = 0.47758436\n",
      "Iteration 2147, loss = 0.47856759\n",
      "Iteration 2148, loss = 0.47979627\n",
      "Iteration 2149, loss = 0.48011774\n",
      "Iteration 2150, loss = 0.47919642\n",
      "Iteration 2151, loss = 0.47769785\n",
      "Iteration 2152, loss = 0.47698699\n",
      "Iteration 2153, loss = 0.47640534\n",
      "Iteration 2154, loss = 0.47679492\n",
      "Iteration 2155, loss = 0.47715417\n",
      "Iteration 2156, loss = 0.47725544\n",
      "Iteration 2157, loss = 0.47720348\n",
      "Iteration 2158, loss = 0.47724704\n",
      "Iteration 2159, loss = 0.47784672\n",
      "Iteration 2160, loss = 0.47799285\n",
      "Iteration 2161, loss = 0.47758801\n",
      "Iteration 2162, loss = 0.47663716\n",
      "Iteration 2163, loss = 0.47595344\n",
      "Iteration 2164, loss = 0.47807112\n",
      "Iteration 2165, loss = 0.47886788\n",
      "Iteration 2166, loss = 0.47853969\n",
      "Iteration 2167, loss = 0.47683247\n",
      "Iteration 2168, loss = 0.47551629\n",
      "Iteration 2169, loss = 0.47678023\n",
      "Iteration 2170, loss = 0.47962427\n",
      "Iteration 2171, loss = 0.48171734\n",
      "Iteration 2172, loss = 0.48261251\n",
      "Iteration 2173, loss = 0.48163019\n",
      "Iteration 2174, loss = 0.47948459\n",
      "Iteration 2175, loss = 0.47842844\n",
      "Iteration 2176, loss = 0.47659669\n",
      "Iteration 2177, loss = 0.47622919\n",
      "Iteration 2178, loss = 0.47580036\n",
      "Iteration 2179, loss = 0.47659468\n",
      "Iteration 2180, loss = 0.47704699\n",
      "Iteration 2181, loss = 0.47722298\n",
      "Iteration 2182, loss = 0.47701681\n",
      "Iteration 2183, loss = 0.47658435\n",
      "Iteration 2184, loss = 0.47597660\n",
      "Iteration 2185, loss = 0.47594872\n",
      "Iteration 2186, loss = 0.47681759\n",
      "Iteration 2187, loss = 0.47736858\n",
      "Iteration 2188, loss = 0.47719110\n",
      "Iteration 2189, loss = 0.47653247\n",
      "Iteration 2190, loss = 0.47621597\n",
      "Iteration 2191, loss = 0.47618226\n",
      "Iteration 2192, loss = 0.47616420\n",
      "Iteration 2193, loss = 0.47614337\n",
      "Iteration 2194, loss = 0.47609721\n",
      "Iteration 2195, loss = 0.47631177\n",
      "Iteration 2196, loss = 0.47606269\n",
      "Iteration 2197, loss = 0.47616237\n",
      "Iteration 2198, loss = 0.47606270\n",
      "Iteration 2199, loss = 0.47600884\n",
      "Iteration 2200, loss = 0.47602598\n",
      "Iteration 2201, loss = 0.47605638\n",
      "Iteration 2202, loss = 0.47614106\n",
      "Iteration 2203, loss = 0.47622607\n",
      "Iteration 2204, loss = 0.47604053\n",
      "Iteration 2205, loss = 0.47601098\n",
      "Iteration 2206, loss = 0.47596719\n",
      "Iteration 2207, loss = 0.47622761\n",
      "Iteration 2208, loss = 0.47606540\n",
      "Iteration 2209, loss = 0.47595971\n",
      "Iteration 2210, loss = 0.47600817\n",
      "Iteration 2211, loss = 0.47634906\n",
      "Iteration 2212, loss = 0.47619386\n",
      "Iteration 2213, loss = 0.47589336\n",
      "Iteration 2214, loss = 0.47617182\n",
      "Iteration 2215, loss = 0.47642587\n",
      "Iteration 2216, loss = 0.47622049\n",
      "Iteration 2217, loss = 0.47583237\n",
      "Iteration 2218, loss = 0.47614257\n",
      "Iteration 2219, loss = 0.47648985\n",
      "Iteration 2220, loss = 0.47676487\n",
      "Iteration 2221, loss = 0.47669566\n",
      "Iteration 2222, loss = 0.47636524\n",
      "Iteration 2223, loss = 0.47596601\n",
      "Iteration 2224, loss = 0.47666311\n",
      "Iteration 2225, loss = 0.47623761\n",
      "Iteration 2226, loss = 0.47602877\n",
      "Iteration 2227, loss = 0.47603907\n",
      "Iteration 2228, loss = 0.47602046\n",
      "Iteration 2229, loss = 0.47609393\n",
      "Iteration 2230, loss = 0.47632936\n",
      "Iteration 2231, loss = 0.47678214\n",
      "Iteration 2232, loss = 0.47725932\n",
      "Iteration 2233, loss = 0.47707769\n",
      "Iteration 2234, loss = 0.47640150\n",
      "Iteration 2235, loss = 0.47597571\n",
      "Iteration 2236, loss = 0.47605622\n",
      "Iteration 2237, loss = 0.47689247\n",
      "Iteration 2238, loss = 0.47671918\n",
      "Iteration 2239, loss = 0.47630055\n",
      "Iteration 2240, loss = 0.47592155\n",
      "Iteration 2241, loss = 0.47568984\n",
      "Iteration 2242, loss = 0.47613333\n",
      "Iteration 2243, loss = 0.47719440\n",
      "Iteration 2244, loss = 0.47846692\n",
      "Iteration 2245, loss = 0.47884523\n",
      "Iteration 2246, loss = 0.47821022\n",
      "Iteration 2247, loss = 0.47756587\n",
      "Iteration 2248, loss = 0.47712766\n",
      "Iteration 2249, loss = 0.47664662\n",
      "Iteration 2250, loss = 0.47632540\n",
      "Iteration 2251, loss = 0.47586850\n",
      "Iteration 2252, loss = 0.47688495\n",
      "Iteration 2253, loss = 0.47699419\n",
      "Iteration 2254, loss = 0.47720165\n",
      "Iteration 2255, loss = 0.47703801\n",
      "Iteration 2256, loss = 0.47661846\n",
      "Iteration 2257, loss = 0.47609661\n",
      "Iteration 2258, loss = 0.47575477\n",
      "Iteration 2259, loss = 0.47601464\n",
      "Iteration 2260, loss = 0.47635064\n",
      "Iteration 2261, loss = 0.47658281\n",
      "Iteration 2262, loss = 0.47667008\n",
      "Iteration 2263, loss = 0.47758543\n",
      "Iteration 2264, loss = 0.47645717\n",
      "Iteration 2265, loss = 0.47647586\n",
      "Iteration 2266, loss = 0.47627941\n",
      "Iteration 2267, loss = 0.47621874\n",
      "Iteration 2268, loss = 0.47617469\n",
      "Iteration 2269, loss = 0.47609152\n",
      "Iteration 2270, loss = 0.47598326\n",
      "Iteration 2271, loss = 0.47595950\n",
      "Iteration 2272, loss = 0.47595233\n",
      "Iteration 2273, loss = 0.47587487\n",
      "Iteration 2274, loss = 0.47597059\n",
      "Iteration 2275, loss = 0.47598842\n",
      "Iteration 2276, loss = 0.47610648\n",
      "Iteration 2277, loss = 0.47608464\n",
      "Iteration 2278, loss = 0.47603896\n",
      "Iteration 2279, loss = 0.47625420\n",
      "Iteration 2280, loss = 0.47641036\n",
      "Iteration 2281, loss = 0.47650242\n",
      "Iteration 2282, loss = 0.47667886\n",
      "Iteration 2283, loss = 0.47688377\n",
      "Iteration 2284, loss = 0.47666333\n",
      "Iteration 2285, loss = 0.47641448\n",
      "Iteration 2286, loss = 0.47594709\n",
      "Iteration 2287, loss = 0.47614909\n",
      "Iteration 2288, loss = 0.47648089\n",
      "Iteration 2289, loss = 0.47708557\n",
      "Iteration 2290, loss = 0.47812158\n",
      "Iteration 2291, loss = 0.47846646\n",
      "Iteration 2292, loss = 0.47721753\n",
      "Iteration 2293, loss = 0.47613562\n",
      "Iteration 2294, loss = 0.47616853\n",
      "Iteration 2295, loss = 0.47715818\n",
      "Iteration 2296, loss = 0.47867179\n",
      "Iteration 2297, loss = 0.47933359\n",
      "Iteration 2298, loss = 0.47791842\n",
      "Iteration 2299, loss = 0.47789901\n",
      "Iteration 2300, loss = 0.47595500\n",
      "Iteration 2301, loss = 0.47631004\n",
      "Iteration 2302, loss = 0.47580854\n",
      "Iteration 2303, loss = 0.47591472\n",
      "Iteration 2304, loss = 0.47697310\n",
      "Iteration 2305, loss = 0.47711262\n",
      "Iteration 2306, loss = 0.47610237\n",
      "Iteration 2307, loss = 0.47615614\n",
      "Iteration 2308, loss = 0.47646683\n",
      "Iteration 2309, loss = 0.47797382\n",
      "Iteration 2310, loss = 0.48152127\n",
      "Iteration 2311, loss = 0.48348502\n",
      "Iteration 2312, loss = 0.48191969\n",
      "Iteration 2313, loss = 0.47864217\n",
      "Iteration 2314, loss = 0.47641205\n",
      "Iteration 2315, loss = 0.47622180\n",
      "Iteration 2316, loss = 0.47732484\n",
      "Iteration 2317, loss = 0.47855096\n",
      "Iteration 2318, loss = 0.47924512\n",
      "Iteration 2319, loss = 0.47900097\n",
      "Iteration 2320, loss = 0.47852161\n",
      "Iteration 2321, loss = 0.47735413\n",
      "Iteration 2322, loss = 0.47686117\n",
      "Iteration 2323, loss = 0.47641714\n",
      "Iteration 2324, loss = 0.47606844\n",
      "Iteration 2325, loss = 0.47669096\n",
      "Iteration 2326, loss = 0.47714035\n",
      "Iteration 2327, loss = 0.47703461\n",
      "Iteration 2328, loss = 0.47647745\n",
      "Iteration 2329, loss = 0.47627335\n",
      "Iteration 2330, loss = 0.47654895\n",
      "Iteration 2331, loss = 0.47600041\n",
      "Iteration 2332, loss = 0.47691219\n",
      "Iteration 2333, loss = 0.47615333\n",
      "Iteration 2334, loss = 0.47593808\n",
      "Iteration 2335, loss = 0.47635687\n",
      "Iteration 2336, loss = 0.47634208\n",
      "Iteration 2337, loss = 0.47622827\n",
      "Iteration 2338, loss = 0.47633755\n",
      "Iteration 2339, loss = 0.47607368\n",
      "Iteration 2340, loss = 0.47603135\n",
      "Iteration 2341, loss = 0.47590414\n",
      "Iteration 2342, loss = 0.47592769\n",
      "Iteration 2343, loss = 0.47603769\n",
      "Iteration 2344, loss = 0.47595888\n",
      "Iteration 2345, loss = 0.47597583\n",
      "Iteration 2346, loss = 0.47602205\n",
      "Iteration 2347, loss = 0.47608010\n",
      "Iteration 2348, loss = 0.47632179\n",
      "Iteration 2349, loss = 0.47614892\n",
      "Iteration 2350, loss = 0.47665285\n",
      "Iteration 2351, loss = 0.47712893\n",
      "Iteration 2352, loss = 0.47734425\n",
      "Iteration 2353, loss = 0.47714568\n",
      "Iteration 2354, loss = 0.47638959\n",
      "Iteration 2355, loss = 0.47580439\n",
      "Iteration 2356, loss = 0.47653684\n",
      "Iteration 2357, loss = 0.47654793\n",
      "Iteration 2358, loss = 0.47641590\n",
      "Iteration 2359, loss = 0.47594605\n",
      "Iteration 2360, loss = 0.47570192\n",
      "Iteration 2361, loss = 0.47576703\n",
      "Iteration 2362, loss = 0.47628331\n",
      "Iteration 2363, loss = 0.47641195\n",
      "Iteration 2364, loss = 0.47640901\n",
      "Iteration 2365, loss = 0.47617133\n",
      "Iteration 2366, loss = 0.47619919\n",
      "Iteration 2367, loss = 0.47635834\n",
      "Iteration 2368, loss = 0.47567229\n",
      "Iteration 2369, loss = 0.47525829\n",
      "Iteration 2370, loss = 0.47757275\n",
      "Iteration 2371, loss = 0.48084767\n",
      "Iteration 2372, loss = 0.48174285\n",
      "Iteration 2373, loss = 0.47964968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2374, loss = 0.47660025\n",
      "Iteration 2375, loss = 0.47614568\n",
      "Iteration 2376, loss = 0.47634878\n",
      "Iteration 2377, loss = 0.47691782\n",
      "Iteration 2378, loss = 0.47679553\n",
      "Iteration 2379, loss = 0.47604091\n",
      "Iteration 2380, loss = 0.47579281\n",
      "Iteration 2381, loss = 0.47623461\n",
      "Iteration 2382, loss = 0.47764683\n",
      "Iteration 2383, loss = 0.47718892\n",
      "Iteration 2384, loss = 0.47641997\n",
      "Iteration 2385, loss = 0.47589037\n",
      "Iteration 2386, loss = 0.47617606\n",
      "Iteration 2387, loss = 0.47722693\n",
      "Iteration 2388, loss = 0.47867476\n",
      "Iteration 2389, loss = 0.47831755\n",
      "Iteration 2390, loss = 0.47696800\n",
      "Iteration 2391, loss = 0.47636491\n",
      "Iteration 2392, loss = 0.47575739\n",
      "Iteration 2393, loss = 0.47686521\n",
      "Iteration 2394, loss = 0.47616423\n",
      "Iteration 2395, loss = 0.47541598\n",
      "Iteration 2396, loss = 0.47653097\n",
      "Iteration 2397, loss = 0.47726365\n",
      "Iteration 2398, loss = 0.47801528\n",
      "Iteration 2399, loss = 0.47770926\n",
      "Iteration 2400, loss = 0.47683308\n",
      "Iteration 2401, loss = 0.47601173\n",
      "Iteration 2402, loss = 0.47575951\n",
      "Iteration 2403, loss = 0.47615300\n",
      "Iteration 2404, loss = 0.47618848\n",
      "Iteration 2405, loss = 0.47616555\n",
      "Iteration 2406, loss = 0.47589484\n",
      "Iteration 2407, loss = 0.47607405\n",
      "Iteration 2408, loss = 0.47634678\n",
      "Iteration 2409, loss = 0.47668373\n",
      "Iteration 2410, loss = 0.47677414\n",
      "Iteration 2411, loss = 0.47646708\n",
      "Iteration 2412, loss = 0.47675283\n",
      "Iteration 2413, loss = 0.47615249\n",
      "Iteration 2414, loss = 0.47632615\n",
      "Iteration 2415, loss = 0.47623629\n",
      "Iteration 2416, loss = 0.47591814\n",
      "Iteration 2417, loss = 0.47615369\n",
      "Iteration 2418, loss = 0.47632835\n",
      "Iteration 2419, loss = 0.47650153\n",
      "Iteration 2420, loss = 0.47629411\n",
      "Iteration 2421, loss = 0.47607437\n",
      "Iteration 2422, loss = 0.47583109\n",
      "Iteration 2423, loss = 0.47580469\n",
      "Iteration 2424, loss = 0.47582404\n",
      "Iteration 2425, loss = 0.47579787\n",
      "Iteration 2426, loss = 0.47615132\n",
      "Iteration 2427, loss = 0.47594954\n",
      "Iteration 2428, loss = 0.47594751\n",
      "Iteration 2429, loss = 0.47623058\n",
      "Iteration 2430, loss = 0.47636683\n",
      "Iteration 2431, loss = 0.47671892\n",
      "Iteration 2432, loss = 0.47699348\n",
      "Iteration 2433, loss = 0.47723026\n",
      "Iteration 2434, loss = 0.47702715\n",
      "Iteration 2435, loss = 0.47659805\n",
      "Iteration 2436, loss = 0.47624658\n",
      "Iteration 2437, loss = 0.47591163\n",
      "Iteration 2438, loss = 0.47587635\n",
      "Iteration 2439, loss = 0.47579302\n",
      "Iteration 2440, loss = 0.47593021\n",
      "Iteration 2441, loss = 0.47679711\n",
      "Iteration 2442, loss = 0.47621643\n",
      "Iteration 2443, loss = 0.47575373\n",
      "Iteration 2444, loss = 0.47620881\n",
      "Iteration 2445, loss = 0.47660901\n",
      "Iteration 2446, loss = 0.47629356\n",
      "Iteration 2447, loss = 0.47569787\n",
      "Iteration 2448, loss = 0.47603267\n",
      "Iteration 2449, loss = 0.47651121\n",
      "Iteration 2450, loss = 0.47727405\n",
      "Iteration 2451, loss = 0.47742137\n",
      "Iteration 2452, loss = 0.47722379\n",
      "Iteration 2453, loss = 0.47712226\n",
      "Iteration 2454, loss = 0.47677604\n",
      "Iteration 2455, loss = 0.47702482\n",
      "Iteration 2456, loss = 0.47783359\n",
      "Iteration 2457, loss = 0.47761823\n",
      "Iteration 2458, loss = 0.47683674\n",
      "Iteration 2459, loss = 0.47622918\n",
      "Iteration 2460, loss = 0.47619165\n",
      "Iteration 2461, loss = 0.47617757\n",
      "Iteration 2462, loss = 0.47627873\n",
      "Iteration 2463, loss = 0.47591539\n",
      "Iteration 2464, loss = 0.47610509\n",
      "Iteration 2465, loss = 0.47612342\n",
      "Iteration 2466, loss = 0.47621742\n",
      "Iteration 2467, loss = 0.47618717\n",
      "Iteration 2468, loss = 0.47584403\n",
      "Iteration 2469, loss = 0.47570773\n",
      "Iteration 2470, loss = 0.47572570\n",
      "Iteration 2471, loss = 0.47601932\n",
      "Iteration 2472, loss = 0.47662680\n",
      "Iteration 2473, loss = 0.47655169\n",
      "Iteration 2474, loss = 0.47608673\n",
      "Iteration 2475, loss = 0.47584413\n",
      "Iteration 2476, loss = 0.47575771\n",
      "Iteration 2477, loss = 0.47590250\n",
      "Iteration 2478, loss = 0.47591424\n",
      "Iteration 2479, loss = 0.47604616\n",
      "Iteration 2480, loss = 0.47589113\n",
      "Iteration 2481, loss = 0.47607045\n",
      "Iteration 2482, loss = 0.47624496\n",
      "Iteration 2483, loss = 0.47632891\n",
      "Iteration 2484, loss = 0.47634555\n",
      "Iteration 2485, loss = 0.47623627\n",
      "Iteration 2486, loss = 0.47608974\n",
      "Iteration 2487, loss = 0.47598021\n",
      "Iteration 2488, loss = 0.47618034\n",
      "Iteration 2489, loss = 0.47600060\n",
      "Iteration 2490, loss = 0.47572353\n",
      "Iteration 2491, loss = 0.47634730\n",
      "Iteration 2492, loss = 0.47597173\n",
      "Iteration 2493, loss = 0.47581407\n",
      "Iteration 2494, loss = 0.47594714\n",
      "Iteration 2495, loss = 0.47608451\n",
      "Iteration 2496, loss = 0.47613593\n",
      "Iteration 2497, loss = 0.47610339\n",
      "Iteration 2498, loss = 0.47608767\n",
      "Iteration 2499, loss = 0.47642346\n",
      "Iteration 2500, loss = 0.47765943\n",
      "Iteration 2501, loss = 0.47692921\n",
      "Iteration 2502, loss = 0.47610396\n",
      "Iteration 2503, loss = 0.47607325\n",
      "Iteration 2504, loss = 0.47622258\n",
      "Iteration 2505, loss = 0.47758454\n",
      "Iteration 2506, loss = 0.47945765\n",
      "Iteration 2507, loss = 0.47997079\n",
      "Iteration 2508, loss = 0.47813012\n",
      "Iteration 2509, loss = 0.47634234\n",
      "Iteration 2510, loss = 0.47681369\n",
      "Iteration 2511, loss = 0.47674121\n",
      "Iteration 2512, loss = 0.47699063\n",
      "Iteration 2513, loss = 0.47733856\n",
      "Iteration 2514, loss = 0.47751280\n",
      "Iteration 2515, loss = 0.47828916\n",
      "Iteration 2516, loss = 0.47900465\n",
      "Iteration 2517, loss = 0.47801357\n",
      "Iteration 2518, loss = 0.47584632\n",
      "Iteration 2519, loss = 0.47518888\n",
      "Iteration 2520, loss = 0.47854838\n",
      "Iteration 2521, loss = 0.48205758\n",
      "Iteration 2522, loss = 0.48410639\n",
      "Iteration 2523, loss = 0.48260823\n",
      "Iteration 2524, loss = 0.47898073\n",
      "Iteration 2525, loss = 0.47620119\n",
      "Iteration 2526, loss = 0.47533922\n",
      "Iteration 2527, loss = 0.47735210\n",
      "Iteration 2528, loss = 0.47997613\n",
      "Iteration 2529, loss = 0.48362189\n",
      "Iteration 2530, loss = 0.48314113\n",
      "Iteration 2531, loss = 0.48026734\n",
      "Iteration 2532, loss = 0.47834907\n",
      "Iteration 2533, loss = 0.47669656\n",
      "Iteration 2534, loss = 0.47600933\n",
      "Iteration 2535, loss = 0.47598486\n",
      "Iteration 2536, loss = 0.47579463\n",
      "Iteration 2537, loss = 0.47561676\n",
      "Iteration 2538, loss = 0.47579985\n",
      "Iteration 2539, loss = 0.47606131\n",
      "Iteration 2540, loss = 0.47603884\n",
      "Iteration 2541, loss = 0.47593888\n",
      "Iteration 2542, loss = 0.47581729\n",
      "Iteration 2543, loss = 0.47588855\n",
      "Iteration 2544, loss = 0.47580719\n",
      "Iteration 2545, loss = 0.47589363\n",
      "Iteration 2546, loss = 0.47628760\n",
      "Iteration 2547, loss = 0.47649145\n",
      "Iteration 2548, loss = 0.47626266\n",
      "Iteration 2549, loss = 0.47592919\n",
      "Iteration 2550, loss = 0.47592604\n",
      "Iteration 2551, loss = 0.47590765\n",
      "Iteration 2552, loss = 0.47592029\n",
      "Iteration 2553, loss = 0.47589022\n",
      "Iteration 2554, loss = 0.47592410\n",
      "Iteration 2555, loss = 0.47599605\n",
      "Iteration 2556, loss = 0.47605509\n",
      "Iteration 2557, loss = 0.47642206\n",
      "Iteration 2558, loss = 0.47663359\n",
      "Iteration 2559, loss = 0.47673591\n",
      "Iteration 2560, loss = 0.47690808\n",
      "Iteration 2561, loss = 0.47675000\n",
      "Iteration 2562, loss = 0.47655148\n",
      "Iteration 2563, loss = 0.47616116\n",
      "Iteration 2564, loss = 0.47568520\n",
      "Iteration 2565, loss = 0.47554518\n",
      "Iteration 2566, loss = 0.47553284\n",
      "Iteration 2567, loss = 0.47584327\n",
      "Iteration 2568, loss = 0.47710546\n",
      "Iteration 2569, loss = 0.47787785\n",
      "Iteration 2570, loss = 0.47782065\n",
      "Iteration 2571, loss = 0.47687227\n",
      "Iteration 2572, loss = 0.47638965\n",
      "Iteration 2573, loss = 0.47597210\n",
      "Iteration 2574, loss = 0.47584479\n",
      "Iteration 2575, loss = 0.47576043\n",
      "Iteration 2576, loss = 0.47613995\n",
      "Iteration 2577, loss = 0.47736105\n",
      "Iteration 2578, loss = 0.47673654\n",
      "Iteration 2579, loss = 0.47569842\n",
      "Iteration 2580, loss = 0.47576144\n",
      "Iteration 2581, loss = 0.47647004\n",
      "Iteration 2582, loss = 0.47739544\n",
      "Iteration 2583, loss = 0.47862642\n",
      "Iteration 2584, loss = 0.47818496\n",
      "Iteration 2585, loss = 0.47723980\n",
      "Iteration 2586, loss = 0.47586572\n",
      "Iteration 2587, loss = 0.47643987\n",
      "Iteration 2588, loss = 0.47689709\n",
      "Iteration 2589, loss = 0.47717703\n",
      "Iteration 2590, loss = 0.47716028\n",
      "Iteration 2591, loss = 0.47698427\n",
      "Iteration 2592, loss = 0.47653004\n",
      "Iteration 2593, loss = 0.47598074\n",
      "Iteration 2594, loss = 0.47580834\n",
      "Iteration 2595, loss = 0.47553093\n",
      "Iteration 2596, loss = 0.47565520\n",
      "Iteration 2597, loss = 0.47573000\n",
      "Iteration 2598, loss = 0.47590222\n",
      "Iteration 2599, loss = 0.47595532\n",
      "Iteration 2600, loss = 0.47592344\n",
      "Iteration 2601, loss = 0.47732951\n",
      "Iteration 2602, loss = 0.47815547\n",
      "Iteration 2603, loss = 0.47764532\n",
      "Iteration 2604, loss = 0.47609900\n",
      "Iteration 2605, loss = 0.47527617\n",
      "Iteration 2606, loss = 0.47677573\n",
      "Iteration 2607, loss = 0.47874900\n",
      "Iteration 2608, loss = 0.48008092\n",
      "Iteration 2609, loss = 0.48087868\n",
      "Iteration 2610, loss = 0.48132492\n",
      "Iteration 2611, loss = 0.48045132\n",
      "Iteration 2612, loss = 0.47726880\n",
      "Iteration 2613, loss = 0.47571462\n",
      "Iteration 2614, loss = 0.47579558\n",
      "Iteration 2615, loss = 0.47969028\n",
      "Iteration 2616, loss = 0.48266027\n",
      "Iteration 2617, loss = 0.48303451\n",
      "Iteration 2618, loss = 0.48107004\n",
      "Iteration 2619, loss = 0.47966748\n",
      "Iteration 2620, loss = 0.47732636\n",
      "Iteration 2621, loss = 0.47552154\n",
      "Iteration 2622, loss = 0.47550347\n",
      "Iteration 2623, loss = 0.47718367\n",
      "Iteration 2624, loss = 0.47898178\n",
      "Iteration 2625, loss = 0.47999329\n",
      "Iteration 2626, loss = 0.48008865\n",
      "Iteration 2627, loss = 0.47926615\n",
      "Iteration 2628, loss = 0.47788495\n",
      "Iteration 2629, loss = 0.47649335\n",
      "Iteration 2630, loss = 0.47567180\n",
      "Iteration 2631, loss = 0.47591579\n",
      "Iteration 2632, loss = 0.47602923\n",
      "Iteration 2633, loss = 0.47677558\n",
      "Iteration 2634, loss = 0.47709635\n",
      "Iteration 2635, loss = 0.47700061\n",
      "Iteration 2636, loss = 0.47667681\n",
      "Iteration 2637, loss = 0.47628480\n",
      "Iteration 2638, loss = 0.47561805\n",
      "Iteration 2639, loss = 0.47554218\n",
      "Iteration 2640, loss = 0.47603884\n",
      "Iteration 2641, loss = 0.47600229\n",
      "Iteration 2642, loss = 0.47593610\n",
      "Iteration 2643, loss = 0.47531181\n",
      "Iteration 2644, loss = 0.47575225\n",
      "Iteration 2645, loss = 0.47674030\n",
      "Iteration 2646, loss = 0.47723173\n",
      "Iteration 2647, loss = 0.47643105\n",
      "Iteration 2648, loss = 0.47524508\n",
      "Iteration 2649, loss = 0.47546498\n",
      "Iteration 2650, loss = 0.47781688\n",
      "Iteration 2651, loss = 0.47932520\n",
      "Iteration 2652, loss = 0.47961950\n",
      "Iteration 2653, loss = 0.47902917\n",
      "Iteration 2654, loss = 0.47736506\n",
      "Iteration 2655, loss = 0.47673439\n",
      "Iteration 2656, loss = 0.47535527\n",
      "Iteration 2657, loss = 0.47616215\n",
      "Iteration 2658, loss = 0.47650741\n",
      "Iteration 2659, loss = 0.47700168\n",
      "Iteration 2660, loss = 0.47791361\n",
      "Iteration 2661, loss = 0.47762060\n",
      "Iteration 2662, loss = 0.47663818\n",
      "Iteration 2663, loss = 0.47602454\n",
      "Iteration 2664, loss = 0.47577840\n",
      "Iteration 2665, loss = 0.47590832\n",
      "Iteration 2666, loss = 0.47717267\n",
      "Iteration 2667, loss = 0.47751459\n",
      "Iteration 2668, loss = 0.47707019\n",
      "Iteration 2669, loss = 0.47666861\n",
      "Iteration 2670, loss = 0.47626805\n",
      "Iteration 2671, loss = 0.47580232\n",
      "Iteration 2672, loss = 0.47572134\n",
      "Iteration 2673, loss = 0.47569346\n",
      "Iteration 2674, loss = 0.47551540\n",
      "Iteration 2675, loss = 0.47549971\n",
      "Iteration 2676, loss = 0.47556974\n",
      "Iteration 2677, loss = 0.47596997\n",
      "Iteration 2678, loss = 0.47629482\n",
      "Iteration 2679, loss = 0.47625634\n",
      "Iteration 2680, loss = 0.47612822\n",
      "Iteration 2681, loss = 0.47603087\n",
      "Iteration 2682, loss = 0.47597873\n",
      "Iteration 2683, loss = 0.47575974\n",
      "Iteration 2684, loss = 0.47585730\n",
      "Iteration 2685, loss = 0.47554271\n",
      "Iteration 2686, loss = 0.47556914\n",
      "Iteration 2687, loss = 0.47564664\n",
      "Iteration 2688, loss = 0.47632430\n",
      "Iteration 2689, loss = 0.47704036\n",
      "Iteration 2690, loss = 0.47613471\n",
      "Iteration 2691, loss = 0.47520120\n",
      "Iteration 2692, loss = 0.47537933\n",
      "Iteration 2693, loss = 0.47798915\n",
      "Iteration 2694, loss = 0.48215718\n",
      "Iteration 2695, loss = 0.48411971\n",
      "Iteration 2696, loss = 0.48340174\n",
      "Iteration 2697, loss = 0.48116179\n",
      "Iteration 2698, loss = 0.47860554\n",
      "Iteration 2699, loss = 0.47572975\n",
      "Iteration 2700, loss = 0.47562686\n",
      "Iteration 2701, loss = 0.47863847\n",
      "Iteration 2702, loss = 0.48053185\n",
      "Iteration 2703, loss = 0.48076951\n",
      "Iteration 2704, loss = 0.47978605\n",
      "Iteration 2705, loss = 0.47844457\n",
      "Iteration 2706, loss = 0.47668539\n",
      "Iteration 2707, loss = 0.47557345\n",
      "Iteration 2708, loss = 0.47567120\n",
      "Iteration 2709, loss = 0.47575376\n",
      "Iteration 2710, loss = 0.47639290\n",
      "Iteration 2711, loss = 0.47632531\n",
      "Iteration 2712, loss = 0.47578776\n",
      "Iteration 2713, loss = 0.47563426\n",
      "Iteration 2714, loss = 0.47538842\n",
      "Iteration 2715, loss = 0.47624020\n",
      "Iteration 2716, loss = 0.47650278\n",
      "Iteration 2717, loss = 0.47642168\n",
      "Iteration 2718, loss = 0.47613287\n",
      "Iteration 2719, loss = 0.47578313\n",
      "Iteration 2720, loss = 0.47553416\n",
      "Iteration 2721, loss = 0.47523276\n",
      "Iteration 2722, loss = 0.47642588\n",
      "Iteration 2723, loss = 0.47681891\n",
      "Iteration 2724, loss = 0.47657106\n",
      "Iteration 2725, loss = 0.47631782\n",
      "Iteration 2726, loss = 0.47585078\n",
      "Iteration 2727, loss = 0.47571917\n",
      "Iteration 2728, loss = 0.47581799\n",
      "Iteration 2729, loss = 0.47552273\n",
      "Iteration 2730, loss = 0.47628539\n",
      "Iteration 2731, loss = 0.47743635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2732, loss = 0.47761680\n",
      "Iteration 2733, loss = 0.47688743\n",
      "Iteration 2734, loss = 0.47615966\n",
      "Iteration 2735, loss = 0.47560253\n",
      "Iteration 2736, loss = 0.47550132\n",
      "Iteration 2737, loss = 0.47593475\n",
      "Iteration 2738, loss = 0.47598693\n",
      "Iteration 2739, loss = 0.47586511\n",
      "Iteration 2740, loss = 0.47604321\n",
      "Iteration 2741, loss = 0.47587290\n",
      "Iteration 2742, loss = 0.47568185\n",
      "Iteration 2743, loss = 0.47577002\n",
      "Iteration 2744, loss = 0.47589557\n",
      "Iteration 2745, loss = 0.47644680\n",
      "Iteration 2746, loss = 0.47688211\n",
      "Iteration 2747, loss = 0.47719131\n",
      "Iteration 2748, loss = 0.47736286\n",
      "Iteration 2749, loss = 0.47688099\n",
      "Iteration 2750, loss = 0.47626443\n",
      "Iteration 2751, loss = 0.47573779\n",
      "Iteration 2752, loss = 0.47558994\n",
      "Iteration 2753, loss = 0.47557482\n",
      "Iteration 2754, loss = 0.47558569\n",
      "Iteration 2755, loss = 0.47533268\n",
      "Iteration 2756, loss = 0.47621381\n",
      "Iteration 2757, loss = 0.47608874\n",
      "Iteration 2758, loss = 0.47598894\n",
      "Iteration 2759, loss = 0.47593045\n",
      "Iteration 2760, loss = 0.47593805\n",
      "Iteration 2761, loss = 0.47594019\n",
      "Iteration 2762, loss = 0.47562760\n",
      "Iteration 2763, loss = 0.47600370\n",
      "Iteration 2764, loss = 0.47532618\n",
      "Iteration 2765, loss = 0.47570574\n",
      "Iteration 2766, loss = 0.47566250\n",
      "Iteration 2767, loss = 0.47555761\n",
      "Iteration 2768, loss = 0.47543076\n",
      "Iteration 2769, loss = 0.47555329\n",
      "Iteration 2770, loss = 0.47540778\n",
      "Iteration 2771, loss = 0.47523605\n",
      "Iteration 2772, loss = 0.47612091\n",
      "Iteration 2773, loss = 0.47640743\n",
      "Iteration 2774, loss = 0.47660540\n",
      "Iteration 2775, loss = 0.47696849\n",
      "Iteration 2776, loss = 0.47714595\n",
      "Iteration 2777, loss = 0.47681335\n",
      "Iteration 2778, loss = 0.47628464\n",
      "Iteration 2779, loss = 0.47602777\n",
      "Iteration 2780, loss = 0.47551693\n",
      "Iteration 2781, loss = 0.47564193\n",
      "Iteration 2782, loss = 0.47604748\n",
      "Iteration 2783, loss = 0.47588647\n",
      "Iteration 2784, loss = 0.47555196\n",
      "Iteration 2785, loss = 0.47558024\n",
      "Iteration 2786, loss = 0.47557357\n",
      "Iteration 2787, loss = 0.47564552\n",
      "Iteration 2788, loss = 0.47642629\n",
      "Iteration 2789, loss = 0.47790603\n",
      "Iteration 2790, loss = 0.47824505\n",
      "Iteration 2791, loss = 0.47655399\n",
      "Iteration 2792, loss = 0.47658325\n",
      "Iteration 2793, loss = 0.47660795\n",
      "Iteration 2794, loss = 0.47596756\n",
      "Iteration 2795, loss = 0.47604147\n",
      "Iteration 2796, loss = 0.47545375\n",
      "Iteration 2797, loss = 0.47562725\n",
      "Iteration 2798, loss = 0.47561633\n",
      "Iteration 2799, loss = 0.47546511\n",
      "Iteration 2800, loss = 0.47547958\n",
      "Iteration 2801, loss = 0.47536706\n",
      "Iteration 2802, loss = 0.47536082\n",
      "Iteration 2803, loss = 0.47578619\n",
      "Iteration 2804, loss = 0.47687626\n",
      "Iteration 2805, loss = 0.47814893\n",
      "Iteration 2806, loss = 0.47836050\n",
      "Iteration 2807, loss = 0.47767620\n",
      "Iteration 2808, loss = 0.47654610\n",
      "Iteration 2809, loss = 0.47604442\n",
      "Iteration 2810, loss = 0.47625496\n",
      "Iteration 2811, loss = 0.47579219\n",
      "Iteration 2812, loss = 0.47589570\n",
      "Iteration 2813, loss = 0.47626161\n",
      "Iteration 2814, loss = 0.47621544\n",
      "Iteration 2815, loss = 0.47576282\n",
      "Iteration 2816, loss = 0.47525174\n",
      "Iteration 2817, loss = 0.47592427\n",
      "Iteration 2818, loss = 0.47743850\n",
      "Iteration 2819, loss = 0.47843034\n",
      "Iteration 2820, loss = 0.47822552\n",
      "Iteration 2821, loss = 0.47707406\n",
      "Iteration 2822, loss = 0.47632059\n",
      "Iteration 2823, loss = 0.47536800\n",
      "Iteration 2824, loss = 0.47544796\n",
      "Iteration 2825, loss = 0.47534315\n",
      "Iteration 2826, loss = 0.47536223\n",
      "Iteration 2827, loss = 0.47575110\n",
      "Iteration 2828, loss = 0.47551852\n",
      "Iteration 2829, loss = 0.47537894\n",
      "Iteration 2830, loss = 0.47535713\n",
      "Iteration 2831, loss = 0.47535425\n",
      "Iteration 2832, loss = 0.47573848\n",
      "Iteration 2833, loss = 0.47520576\n",
      "Iteration 2834, loss = 0.47512378\n",
      "Iteration 2835, loss = 0.47633563\n",
      "Iteration 2836, loss = 0.47735267\n",
      "Iteration 2837, loss = 0.47757514\n",
      "Iteration 2838, loss = 0.47703056\n",
      "Iteration 2839, loss = 0.47633435\n",
      "Iteration 2840, loss = 0.47621776\n",
      "Iteration 2841, loss = 0.47560093\n",
      "Iteration 2842, loss = 0.47751581\n",
      "Iteration 2843, loss = 0.47748461\n",
      "Iteration 2844, loss = 0.47576452\n",
      "Iteration 2845, loss = 0.47545611\n",
      "Iteration 2846, loss = 0.47601181\n",
      "Iteration 2847, loss = 0.47836879\n",
      "Iteration 2848, loss = 0.47903279\n",
      "Iteration 2849, loss = 0.47856931\n",
      "Iteration 2850, loss = 0.47849791\n",
      "Iteration 2851, loss = 0.47768692\n",
      "Iteration 2852, loss = 0.47675596\n",
      "Iteration 2853, loss = 0.47570460\n",
      "Iteration 2854, loss = 0.47583028\n",
      "Iteration 2855, loss = 0.47539929\n",
      "Iteration 2856, loss = 0.47525740\n",
      "Iteration 2857, loss = 0.47527587\n",
      "Iteration 2858, loss = 0.47572017\n",
      "Iteration 2859, loss = 0.47594412\n",
      "Iteration 2860, loss = 0.47613971\n",
      "Iteration 2861, loss = 0.47637744\n",
      "Iteration 2862, loss = 0.47649282\n",
      "Iteration 2863, loss = 0.47674241\n",
      "Iteration 2864, loss = 0.47612397\n",
      "Iteration 2865, loss = 0.47538908\n",
      "Iteration 2866, loss = 0.47601379\n",
      "Iteration 2867, loss = 0.47595211\n",
      "Iteration 2868, loss = 0.47594801\n",
      "Iteration 2869, loss = 0.47592984\n",
      "Iteration 2870, loss = 0.47591086\n",
      "Iteration 2871, loss = 0.47593279\n",
      "Iteration 2872, loss = 0.47595593\n",
      "Iteration 2873, loss = 0.47547002\n",
      "Iteration 2874, loss = 0.47521739\n",
      "Iteration 2875, loss = 0.47603012\n",
      "Iteration 2876, loss = 0.47613845\n",
      "Iteration 2877, loss = 0.47598218\n",
      "Iteration 2878, loss = 0.47558877\n",
      "Iteration 2879, loss = 0.47493683\n",
      "Iteration 2880, loss = 0.47544965\n",
      "Iteration 2881, loss = 0.47667448\n",
      "Iteration 2882, loss = 0.47732605\n",
      "Iteration 2883, loss = 0.47671706\n",
      "Iteration 2884, loss = 0.47582206\n",
      "Iteration 2885, loss = 0.47512127\n",
      "Iteration 2886, loss = 0.47507269\n",
      "Iteration 2887, loss = 0.47680807\n",
      "Iteration 2888, loss = 0.47716921\n",
      "Iteration 2889, loss = 0.47590232\n",
      "Iteration 2890, loss = 0.47545212\n",
      "Iteration 2891, loss = 0.47621968\n",
      "Iteration 2892, loss = 0.47634907\n",
      "Iteration 2893, loss = 0.47613166\n",
      "Iteration 2894, loss = 0.47583307\n",
      "Iteration 2895, loss = 0.47557235\n",
      "Iteration 2896, loss = 0.47527854\n",
      "Iteration 2897, loss = 0.47535487\n",
      "Iteration 2898, loss = 0.47520734\n",
      "Iteration 2899, loss = 0.47579037\n",
      "Iteration 2900, loss = 0.47641643\n",
      "Iteration 2901, loss = 0.47609462\n",
      "Iteration 2902, loss = 0.47555155\n",
      "Iteration 2903, loss = 0.47502120\n",
      "Iteration 2904, loss = 0.47622473\n",
      "Iteration 2905, loss = 0.47765958\n",
      "Iteration 2906, loss = 0.47823461\n",
      "Iteration 2907, loss = 0.47735307\n",
      "Iteration 2908, loss = 0.47563540\n",
      "Iteration 2909, loss = 0.47486635\n",
      "Iteration 2910, loss = 0.47539298\n",
      "Iteration 2911, loss = 0.47822163\n",
      "Iteration 2912, loss = 0.47885084\n",
      "Iteration 2913, loss = 0.47825806\n",
      "Iteration 2914, loss = 0.47709552\n",
      "Iteration 2915, loss = 0.47595733\n",
      "Iteration 2916, loss = 0.47540619\n",
      "Iteration 2917, loss = 0.47510989\n",
      "Iteration 2918, loss = 0.47544248\n",
      "Iteration 2919, loss = 0.47552497\n",
      "Iteration 2920, loss = 0.47571303\n",
      "Iteration 2921, loss = 0.47573757\n",
      "Iteration 2922, loss = 0.47598001\n",
      "Iteration 2923, loss = 0.47571189\n",
      "Iteration 2924, loss = 0.47544122\n",
      "Iteration 2925, loss = 0.47519825\n",
      "Iteration 2926, loss = 0.47514430\n",
      "Iteration 2927, loss = 0.47498079\n",
      "Iteration 2928, loss = 0.47583847\n",
      "Iteration 2929, loss = 0.47729424\n",
      "Iteration 2930, loss = 0.47881215\n",
      "Iteration 2931, loss = 0.47847061\n",
      "Iteration 2932, loss = 0.47655308\n",
      "Iteration 2933, loss = 0.47577894\n",
      "Iteration 2934, loss = 0.47554438\n",
      "Iteration 2935, loss = 0.47656660\n",
      "Iteration 2936, loss = 0.47602675\n",
      "Iteration 2937, loss = 0.47582051\n",
      "Iteration 2938, loss = 0.47510845\n",
      "Iteration 2939, loss = 0.47526683\n",
      "Iteration 2940, loss = 0.47567233\n",
      "Iteration 2941, loss = 0.47602103\n",
      "Iteration 2942, loss = 0.47591406\n",
      "Iteration 2943, loss = 0.47533000\n",
      "Iteration 2944, loss = 0.47524991\n",
      "Iteration 2945, loss = 0.47553798\n",
      "Iteration 2946, loss = 0.47561315\n",
      "Iteration 2947, loss = 0.47554146\n",
      "Iteration 2948, loss = 0.47525242\n",
      "Iteration 2949, loss = 0.47511361\n",
      "Iteration 2950, loss = 0.47495881\n",
      "Iteration 2951, loss = 0.47545924\n",
      "Iteration 2952, loss = 0.47558201\n",
      "Iteration 2953, loss = 0.47573356\n",
      "Iteration 2954, loss = 0.47589035\n",
      "Iteration 2955, loss = 0.47587526\n",
      "Iteration 2956, loss = 0.47606761\n",
      "Iteration 2957, loss = 0.47582063\n",
      "Iteration 2958, loss = 0.47605098\n",
      "Iteration 2959, loss = 0.47525284\n",
      "Iteration 2960, loss = 0.47536896\n",
      "Iteration 2961, loss = 0.47539274\n",
      "Iteration 2962, loss = 0.47523666\n",
      "Iteration 2963, loss = 0.47502990\n",
      "Iteration 2964, loss = 0.47559025\n",
      "Iteration 2965, loss = 0.47586264\n",
      "Iteration 2966, loss = 0.47563235\n",
      "Iteration 2967, loss = 0.47629079\n",
      "Iteration 2968, loss = 0.47700591\n",
      "Iteration 2969, loss = 0.47705700\n",
      "Iteration 2970, loss = 0.47611789\n",
      "Iteration 2971, loss = 0.47665012\n",
      "Iteration 2972, loss = 0.47514333\n",
      "Iteration 2973, loss = 0.47508680\n",
      "Iteration 2974, loss = 0.47561292\n",
      "Iteration 2975, loss = 0.47512044\n",
      "Iteration 2976, loss = 0.47502274\n",
      "Iteration 2977, loss = 0.47506969\n",
      "Iteration 2978, loss = 0.47542976\n",
      "Iteration 2979, loss = 0.47616609\n",
      "Iteration 2980, loss = 0.47665415\n",
      "Iteration 2981, loss = 0.47563322\n",
      "Iteration 2982, loss = 0.47579795\n",
      "Iteration 2983, loss = 0.47573220\n",
      "Iteration 2984, loss = 0.47576836\n",
      "Iteration 2985, loss = 0.47547342\n",
      "Iteration 2986, loss = 0.47510843\n",
      "Iteration 2987, loss = 0.47498475\n",
      "Iteration 2988, loss = 0.47552096\n",
      "Iteration 2989, loss = 0.47600748\n",
      "Iteration 2990, loss = 0.47534415\n",
      "Iteration 2991, loss = 0.47507800\n",
      "Iteration 2992, loss = 0.47518410\n",
      "Iteration 2993, loss = 0.47614287\n",
      "Iteration 2994, loss = 0.47682253\n",
      "Iteration 2995, loss = 0.47657353\n",
      "Iteration 2996, loss = 0.47606811\n",
      "Iteration 2997, loss = 0.47500657\n",
      "Iteration 2998, loss = 0.47557789\n",
      "Iteration 2999, loss = 0.47519431\n",
      "Iteration 3000, loss = 0.47503273\n",
      "Iteration 3001, loss = 0.47507890\n",
      "Iteration 3002, loss = 0.47519801\n",
      "Iteration 3003, loss = 0.47590580\n",
      "Iteration 3004, loss = 0.47544058\n",
      "Iteration 3005, loss = 0.47534119\n",
      "Iteration 3006, loss = 0.47490006\n",
      "Iteration 3007, loss = 0.47552897\n",
      "Iteration 3008, loss = 0.47641035\n",
      "Iteration 3009, loss = 0.47674789\n",
      "Iteration 3010, loss = 0.47647774\n",
      "Iteration 3011, loss = 0.47574910\n",
      "Iteration 3012, loss = 0.47536658\n",
      "Iteration 3013, loss = 0.47492277\n",
      "Iteration 3014, loss = 0.47542298\n",
      "Iteration 3015, loss = 0.47591981\n",
      "Iteration 3016, loss = 0.47586135\n",
      "Iteration 3017, loss = 0.47563421\n",
      "Iteration 3018, loss = 0.47528426\n",
      "Iteration 3019, loss = 0.47555711\n",
      "Iteration 3020, loss = 0.47620934\n",
      "Iteration 3021, loss = 0.47606426\n",
      "Iteration 3022, loss = 0.47624378\n",
      "Iteration 3023, loss = 0.47512723\n",
      "Iteration 3024, loss = 0.47508642\n",
      "Iteration 3025, loss = 0.47557056\n",
      "Iteration 3026, loss = 0.47558387\n",
      "Iteration 3027, loss = 0.47559552\n",
      "Iteration 3028, loss = 0.47582908\n",
      "Iteration 3029, loss = 0.47585016\n",
      "Iteration 3030, loss = 0.47541585\n",
      "Iteration 3031, loss = 0.47520323\n",
      "Iteration 3032, loss = 0.47510545\n",
      "Iteration 3033, loss = 0.47525396\n",
      "Iteration 3034, loss = 0.47555278\n",
      "Iteration 3035, loss = 0.47546477\n",
      "Iteration 3036, loss = 0.47539691\n",
      "Iteration 3037, loss = 0.47505104\n",
      "Iteration 3038, loss = 0.47496838\n",
      "Iteration 3039, loss = 0.47530923\n",
      "Iteration 3040, loss = 0.47483407\n",
      "Iteration 3041, loss = 0.47559558\n",
      "Iteration 3042, loss = 0.47554506\n",
      "Iteration 3043, loss = 0.47528735\n",
      "Iteration 3044, loss = 0.47557897\n",
      "Iteration 3045, loss = 0.47539236\n",
      "Iteration 3046, loss = 0.47506968\n",
      "Iteration 3047, loss = 0.47508848\n",
      "Iteration 3048, loss = 0.47522379\n",
      "Iteration 3049, loss = 0.47562396\n",
      "Iteration 3050, loss = 0.47603673\n",
      "Iteration 3051, loss = 0.47598423\n",
      "Iteration 3052, loss = 0.47525199\n",
      "Iteration 3053, loss = 0.47473039\n",
      "Iteration 3054, loss = 0.47572985\n",
      "Iteration 3055, loss = 0.47634898\n",
      "Iteration 3056, loss = 0.47651350\n",
      "Iteration 3057, loss = 0.47563941\n",
      "Iteration 3058, loss = 0.47567517\n",
      "Iteration 3059, loss = 0.47551090\n",
      "Iteration 3060, loss = 0.47538718\n",
      "Iteration 3061, loss = 0.47497274\n",
      "Iteration 3062, loss = 0.47512285\n",
      "Iteration 3063, loss = 0.47504595\n",
      "Iteration 3064, loss = 0.47500897\n",
      "Iteration 3065, loss = 0.47500482\n",
      "Iteration 3066, loss = 0.47510063\n",
      "Iteration 3067, loss = 0.47533411\n",
      "Iteration 3068, loss = 0.47552463\n",
      "Iteration 3069, loss = 0.47559430\n",
      "Iteration 3070, loss = 0.47545218\n",
      "Iteration 3071, loss = 0.47590101\n",
      "Iteration 3072, loss = 0.47606026\n",
      "Iteration 3073, loss = 0.47549706\n",
      "Iteration 3074, loss = 0.47477477\n",
      "Iteration 3075, loss = 0.47522477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3076, loss = 0.47758836\n",
      "Iteration 3077, loss = 0.47881506\n",
      "Iteration 3078, loss = 0.47897386\n",
      "Iteration 3079, loss = 0.47829989\n",
      "Iteration 3080, loss = 0.47691498\n",
      "Iteration 3081, loss = 0.47560491\n",
      "Iteration 3082, loss = 0.47520152\n",
      "Iteration 3083, loss = 0.47498195\n",
      "Iteration 3084, loss = 0.47517551\n",
      "Iteration 3085, loss = 0.47655497\n",
      "Iteration 3086, loss = 0.47726986\n",
      "Iteration 3087, loss = 0.47692519\n",
      "Iteration 3088, loss = 0.47644656\n",
      "Iteration 3089, loss = 0.47556531\n",
      "Iteration 3090, loss = 0.47539243\n",
      "Iteration 3091, loss = 0.47533283\n",
      "Iteration 3092, loss = 0.47502727\n",
      "Iteration 3093, loss = 0.47499755\n",
      "Iteration 3094, loss = 0.47500181\n",
      "Iteration 3095, loss = 0.47501521\n",
      "Iteration 3096, loss = 0.47504090\n",
      "Iteration 3097, loss = 0.47501441\n",
      "Iteration 3098, loss = 0.47528253\n",
      "Iteration 3099, loss = 0.47493479\n",
      "Iteration 3100, loss = 0.47527362\n",
      "Iteration 3101, loss = 0.47524775\n",
      "Iteration 3102, loss = 0.47519830\n",
      "Iteration 3103, loss = 0.47512639\n",
      "Iteration 3104, loss = 0.47498063\n",
      "Iteration 3105, loss = 0.47539729\n",
      "Iteration 3106, loss = 0.47501237\n",
      "Iteration 3107, loss = 0.47504501\n",
      "Iteration 3108, loss = 0.47546591\n",
      "Iteration 3109, loss = 0.47522438\n",
      "Iteration 3110, loss = 0.47479305\n",
      "Iteration 3111, loss = 0.47513822\n",
      "Iteration 3112, loss = 0.47583251\n",
      "Iteration 3113, loss = 0.47648042\n",
      "Iteration 3114, loss = 0.47695758\n",
      "Iteration 3115, loss = 0.47616813\n",
      "Iteration 3116, loss = 0.47570956\n",
      "Iteration 3117, loss = 0.47522793\n",
      "Iteration 3118, loss = 0.47512036\n",
      "Iteration 3119, loss = 0.47520831\n",
      "Iteration 3120, loss = 0.47568436\n",
      "Iteration 3121, loss = 0.47587295\n",
      "Iteration 3122, loss = 0.47558634\n",
      "Iteration 3123, loss = 0.47585074\n",
      "Iteration 3124, loss = 0.47546339\n",
      "Iteration 3125, loss = 0.47536178\n",
      "Iteration 3126, loss = 0.47521972\n",
      "Iteration 3127, loss = 0.47501776\n",
      "Iteration 3128, loss = 0.47501279\n",
      "Iteration 3129, loss = 0.47514435\n",
      "Iteration 3130, loss = 0.47527576\n",
      "Iteration 3131, loss = 0.47527020\n",
      "Iteration 3132, loss = 0.47530834\n",
      "Iteration 3133, loss = 0.47523783\n",
      "Iteration 3134, loss = 0.47546220\n",
      "Iteration 3135, loss = 0.47563926\n",
      "Iteration 3136, loss = 0.47573299\n",
      "Iteration 3137, loss = 0.47536409\n",
      "Iteration 3138, loss = 0.47492417\n",
      "Iteration 3139, loss = 0.47515935\n",
      "Iteration 3140, loss = 0.47596270\n",
      "Iteration 3141, loss = 0.47643505\n",
      "Iteration 3142, loss = 0.47608406\n",
      "Iteration 3143, loss = 0.47537370\n",
      "Iteration 3144, loss = 0.47524589\n",
      "Iteration 3145, loss = 0.47513925\n",
      "Iteration 3146, loss = 0.47512716\n",
      "Iteration 3147, loss = 0.47513064\n",
      "Iteration 3148, loss = 0.47510674\n",
      "Iteration 3149, loss = 0.47590794\n",
      "Iteration 3150, loss = 0.47642517\n",
      "Iteration 3151, loss = 0.47626337\n",
      "Iteration 3152, loss = 0.47575932\n",
      "Iteration 3153, loss = 0.47527178\n",
      "Iteration 3154, loss = 0.47565871\n",
      "Iteration 3155, loss = 0.47705205\n",
      "Iteration 3156, loss = 0.47774289\n",
      "Iteration 3157, loss = 0.47705633\n",
      "Iteration 3158, loss = 0.47544250\n",
      "Iteration 3159, loss = 0.47498867\n",
      "Iteration 3160, loss = 0.47560499\n",
      "Iteration 3161, loss = 0.47666337\n",
      "Iteration 3162, loss = 0.47760184\n",
      "Iteration 3163, loss = 0.47851538\n",
      "Iteration 3164, loss = 0.47707831\n",
      "Iteration 3165, loss = 0.47455716\n",
      "Iteration 3166, loss = 0.47587067\n",
      "Iteration 3167, loss = 0.47733744\n",
      "Iteration 3168, loss = 0.47790847\n",
      "Iteration 3169, loss = 0.47727294\n",
      "Iteration 3170, loss = 0.47626567\n",
      "Iteration 3171, loss = 0.47583383\n",
      "Iteration 3172, loss = 0.47507212\n",
      "Iteration 3173, loss = 0.47500953\n",
      "Iteration 3174, loss = 0.47503179\n",
      "Iteration 3175, loss = 0.47508625\n",
      "Iteration 3176, loss = 0.47560151\n",
      "Iteration 3177, loss = 0.47513099\n",
      "Iteration 3178, loss = 0.47485215\n",
      "Iteration 3179, loss = 0.47502629\n",
      "Iteration 3180, loss = 0.47543570\n",
      "Iteration 3181, loss = 0.47660456\n",
      "Iteration 3182, loss = 0.47690980\n",
      "Iteration 3183, loss = 0.47630849\n",
      "Iteration 3184, loss = 0.47515171\n",
      "Iteration 3185, loss = 0.47504838\n",
      "Iteration 3186, loss = 0.47595170\n",
      "Iteration 3187, loss = 0.47634300\n",
      "Iteration 3188, loss = 0.47618006\n",
      "Iteration 3189, loss = 0.47553626\n",
      "Iteration 3190, loss = 0.47509808\n",
      "Iteration 3191, loss = 0.47543549\n",
      "Iteration 3192, loss = 0.47524617\n",
      "Iteration 3193, loss = 0.47506842\n",
      "Iteration 3194, loss = 0.47480122\n",
      "Iteration 3195, loss = 0.47486688\n",
      "Iteration 3196, loss = 0.47558047\n",
      "Iteration 3197, loss = 0.47553734\n",
      "Iteration 3198, loss = 0.47523110\n",
      "Iteration 3199, loss = 0.47482935\n",
      "Iteration 3200, loss = 0.47489061\n",
      "Iteration 3201, loss = 0.47562768\n",
      "Iteration 3202, loss = 0.47565971\n",
      "Iteration 3203, loss = 0.47521794\n",
      "Iteration 3204, loss = 0.47525074\n",
      "Iteration 3205, loss = 0.47544026\n",
      "Iteration 3206, loss = 0.47519234\n",
      "Iteration 3207, loss = 0.47486178\n",
      "Iteration 3208, loss = 0.47517085\n",
      "Iteration 3209, loss = 0.47534663\n",
      "Iteration 3210, loss = 0.47542543\n",
      "Iteration 3211, loss = 0.47538689\n",
      "Iteration 3212, loss = 0.47518786\n",
      "Iteration 3213, loss = 0.47502000\n",
      "Iteration 3214, loss = 0.47478716\n",
      "Iteration 3215, loss = 0.47511098\n",
      "Iteration 3216, loss = 0.47501907\n",
      "Iteration 3217, loss = 0.47497982\n",
      "Iteration 3218, loss = 0.47494463\n",
      "Iteration 3219, loss = 0.47505927\n",
      "Iteration 3220, loss = 0.47544274\n",
      "Iteration 3221, loss = 0.47525874\n",
      "Iteration 3222, loss = 0.47573810\n",
      "Iteration 3223, loss = 0.47529189\n",
      "Iteration 3224, loss = 0.47531787\n",
      "Iteration 3225, loss = 0.47522932\n",
      "Iteration 3226, loss = 0.47525077\n",
      "Iteration 3227, loss = 0.47501849\n",
      "Iteration 3228, loss = 0.47514785\n",
      "Iteration 3229, loss = 0.47506034\n",
      "Iteration 3230, loss = 0.47508762\n",
      "Iteration 3231, loss = 0.47484222\n",
      "Iteration 3232, loss = 0.47503576\n",
      "Iteration 3233, loss = 0.47704898\n",
      "Iteration 3234, loss = 0.47856294\n",
      "Iteration 3235, loss = 0.47816622\n",
      "Iteration 3236, loss = 0.47673197\n",
      "Iteration 3237, loss = 0.47579699\n",
      "Iteration 3238, loss = 0.47556486\n",
      "Iteration 3239, loss = 0.47504843\n",
      "Iteration 3240, loss = 0.47465975\n",
      "Iteration 3241, loss = 0.47562645\n",
      "Iteration 3242, loss = 0.47573137\n",
      "Iteration 3243, loss = 0.47557552\n",
      "Iteration 3244, loss = 0.47532774\n",
      "Iteration 3245, loss = 0.47492536\n",
      "Iteration 3246, loss = 0.47506962\n",
      "Iteration 3247, loss = 0.47481186\n",
      "Iteration 3248, loss = 0.47508237\n",
      "Iteration 3249, loss = 0.47574780\n",
      "Iteration 3250, loss = 0.47631786\n",
      "Iteration 3251, loss = 0.47652705\n",
      "Iteration 3252, loss = 0.47568970\n",
      "Iteration 3253, loss = 0.47516249\n",
      "Iteration 3254, loss = 0.47528677\n",
      "Iteration 3255, loss = 0.47521055\n",
      "Iteration 3256, loss = 0.47442280\n",
      "Iteration 3257, loss = 0.47467081\n",
      "Iteration 3258, loss = 0.47657614\n",
      "Iteration 3259, loss = 0.47895910\n",
      "Iteration 3260, loss = 0.47954206\n",
      "Iteration 3261, loss = 0.47847004\n",
      "Iteration 3262, loss = 0.47619550\n",
      "Iteration 3263, loss = 0.47461830\n",
      "Iteration 3264, loss = 0.47523354\n",
      "Iteration 3265, loss = 0.47760760\n",
      "Iteration 3266, loss = 0.47815095\n",
      "Iteration 3267, loss = 0.47755284\n",
      "Iteration 3268, loss = 0.47636369\n",
      "Iteration 3269, loss = 0.47594478\n",
      "Iteration 3270, loss = 0.47509724\n",
      "Iteration 3271, loss = 0.47542711\n",
      "Iteration 3272, loss = 0.47476877\n",
      "Iteration 3273, loss = 0.47490398\n",
      "Iteration 3274, loss = 0.47494130\n",
      "Iteration 3275, loss = 0.47479482\n",
      "Iteration 3276, loss = 0.47514222\n",
      "Iteration 3277, loss = 0.47515814\n",
      "Iteration 3278, loss = 0.47511494\n",
      "Iteration 3279, loss = 0.47514475\n",
      "Iteration 3280, loss = 0.47490336\n",
      "Iteration 3281, loss = 0.47485974\n",
      "Iteration 3282, loss = 0.47526631\n",
      "Iteration 3283, loss = 0.47532384\n",
      "Iteration 3284, loss = 0.47537062\n",
      "Iteration 3285, loss = 0.47548596\n",
      "Iteration 3286, loss = 0.47544352\n",
      "Iteration 3287, loss = 0.47475777\n",
      "Iteration 3288, loss = 0.47419426\n",
      "Iteration 3289, loss = 0.47727697\n",
      "Iteration 3290, loss = 0.48011673\n",
      "Iteration 3291, loss = 0.48154506\n",
      "Iteration 3292, loss = 0.48032097\n",
      "Iteration 3293, loss = 0.47672456\n",
      "Iteration 3294, loss = 0.47575632\n",
      "Iteration 3295, loss = 0.47552324\n",
      "Iteration 3296, loss = 0.47570427\n",
      "Iteration 3297, loss = 0.47495822\n",
      "Iteration 3298, loss = 0.47436351\n",
      "Iteration 3299, loss = 0.47478691\n",
      "Iteration 3300, loss = 0.47743077\n",
      "Iteration 3301, loss = 0.48210529\n",
      "Iteration 3302, loss = 0.48617377\n",
      "Iteration 3303, loss = 0.48726023\n",
      "Iteration 3304, loss = 0.48370540\n",
      "Iteration 3305, loss = 0.47848924\n",
      "Iteration 3306, loss = 0.47540131\n",
      "Iteration 3307, loss = 0.47561324\n",
      "Iteration 3308, loss = 0.47639541\n",
      "Iteration 3309, loss = 0.47684007\n",
      "Iteration 3310, loss = 0.47694169\n",
      "Iteration 3311, loss = 0.47682417\n",
      "Iteration 3312, loss = 0.47649857\n",
      "Iteration 3313, loss = 0.47635080\n",
      "Iteration 3314, loss = 0.47626619\n",
      "Iteration 3315, loss = 0.47607146\n",
      "Iteration 3316, loss = 0.47587208\n",
      "Iteration 3317, loss = 0.47598872\n",
      "Iteration 3318, loss = 0.47559287\n",
      "Iteration 3319, loss = 0.47582047\n",
      "Iteration 3320, loss = 0.47617534\n",
      "Iteration 3321, loss = 0.47720489\n",
      "Iteration 3322, loss = 0.47736820\n",
      "Iteration 3323, loss = 0.47659159\n",
      "Iteration 3324, loss = 0.47609785\n",
      "Iteration 3325, loss = 0.47582070\n",
      "Iteration 3326, loss = 0.47545976\n",
      "Iteration 3327, loss = 0.47585998\n",
      "Iteration 3328, loss = 0.47615307\n",
      "Iteration 3329, loss = 0.47634274\n",
      "Iteration 3330, loss = 0.47788903\n",
      "Iteration 3331, loss = 0.47773288\n",
      "Iteration 3332, loss = 0.47685476\n",
      "Iteration 3333, loss = 0.47616431\n",
      "Iteration 3334, loss = 0.47596909\n",
      "Iteration 3335, loss = 0.47549257\n",
      "Iteration 3336, loss = 0.47670466\n",
      "Iteration 3337, loss = 0.47675375\n",
      "Iteration 3338, loss = 0.47619644\n",
      "Iteration 3339, loss = 0.47586377\n",
      "Iteration 3340, loss = 0.47548849\n",
      "Iteration 3341, loss = 0.47563791\n",
      "Iteration 3342, loss = 0.47573374\n",
      "Iteration 3343, loss = 0.47585374\n",
      "Iteration 3344, loss = 0.47592237\n",
      "Iteration 3345, loss = 0.47583803\n",
      "Iteration 3346, loss = 0.47590263\n",
      "Iteration 3347, loss = 0.47588999\n",
      "Iteration 3348, loss = 0.47596859\n",
      "Iteration 3349, loss = 0.47568135\n",
      "Iteration 3350, loss = 0.47531285\n",
      "Iteration 3351, loss = 0.47596600\n",
      "Iteration 3352, loss = 0.47715298\n",
      "Iteration 3353, loss = 0.47833943\n",
      "Iteration 3354, loss = 0.47856113\n",
      "Iteration 3355, loss = 0.47754004\n",
      "Iteration 3356, loss = 0.47634053\n",
      "Iteration 3357, loss = 0.47493685\n",
      "Iteration 3358, loss = 0.47666298\n",
      "Iteration 3359, loss = 0.47632811\n",
      "Iteration 3360, loss = 0.47598164\n",
      "Iteration 3361, loss = 0.47606436\n",
      "Iteration 3362, loss = 0.47596427\n",
      "Iteration 3363, loss = 0.47553541\n",
      "Iteration 3364, loss = 0.47541803\n",
      "Iteration 3365, loss = 0.47532456\n",
      "Iteration 3366, loss = 0.47528358\n",
      "Iteration 3367, loss = 0.47541674\n",
      "Iteration 3368, loss = 0.47542662\n",
      "Iteration 3369, loss = 0.47542359\n",
      "Iteration 3370, loss = 0.47551510\n",
      "Iteration 3371, loss = 0.47605989\n",
      "Iteration 3372, loss = 0.47733840\n",
      "Iteration 3373, loss = 0.47857912\n",
      "Iteration 3374, loss = 0.47934757\n",
      "Iteration 3375, loss = 0.47933667\n",
      "Iteration 3376, loss = 0.47863818\n",
      "Iteration 3377, loss = 0.47711618\n",
      "Iteration 3378, loss = 0.47585786\n",
      "Iteration 3379, loss = 0.47543190\n",
      "Iteration 3380, loss = 0.47525997\n",
      "Iteration 3381, loss = 0.47508028\n",
      "Iteration 3382, loss = 0.47513873\n",
      "Iteration 3383, loss = 0.47521016\n",
      "Iteration 3384, loss = 0.47536308\n",
      "Iteration 3385, loss = 0.47542583\n",
      "Iteration 3386, loss = 0.47526675\n",
      "Iteration 3387, loss = 0.47557039\n",
      "Iteration 3388, loss = 0.47647800\n",
      "Iteration 3389, loss = 0.47703070\n",
      "Iteration 3390, loss = 0.47657535\n",
      "Iteration 3391, loss = 0.47531305\n",
      "Iteration 3392, loss = 0.47488806\n",
      "Iteration 3393, loss = 0.47583806\n",
      "Iteration 3394, loss = 0.47746769\n",
      "Iteration 3395, loss = 0.47893143\n",
      "Iteration 3396, loss = 0.47902166\n",
      "Iteration 3397, loss = 0.47865908\n",
      "Iteration 3398, loss = 0.47712799\n",
      "Iteration 3399, loss = 0.47679456\n",
      "Iteration 3400, loss = 0.47610164\n",
      "Iteration 3401, loss = 0.47530961\n",
      "Iteration 3402, loss = 0.47473405\n",
      "Iteration 3403, loss = 0.47572571\n",
      "Iteration 3404, loss = 0.47762385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3405, loss = 0.47822210\n",
      "Iteration 3406, loss = 0.47771020\n",
      "Iteration 3407, loss = 0.47628723\n",
      "Iteration 3408, loss = 0.47527436\n",
      "Iteration 3409, loss = 0.47497141\n",
      "Iteration 3410, loss = 0.47562235\n",
      "Iteration 3411, loss = 0.47630039\n",
      "Iteration 3412, loss = 0.47701806\n",
      "Iteration 3413, loss = 0.47807423\n",
      "Iteration 3414, loss = 0.47919824\n",
      "Iteration 3415, loss = 0.47882215\n",
      "Iteration 3416, loss = 0.47734296\n",
      "Iteration 3417, loss = 0.47533946\n",
      "Iteration 3418, loss = 0.47472260\n",
      "Iteration 3419, loss = 0.47611412\n",
      "Iteration 3420, loss = 0.47621357\n",
      "Iteration 3421, loss = 0.47613628\n",
      "Iteration 3422, loss = 0.47614883\n",
      "Iteration 3423, loss = 0.47563354\n",
      "Iteration 3424, loss = 0.47559769\n",
      "Iteration 3425, loss = 0.47539536\n",
      "Iteration 3426, loss = 0.47512517\n",
      "Iteration 3427, loss = 0.47512423\n",
      "Iteration 3428, loss = 0.47529428\n",
      "Iteration 3429, loss = 0.47517734\n",
      "Iteration 3430, loss = 0.47500411\n",
      "Iteration 3431, loss = 0.47485767\n",
      "Iteration 3432, loss = 0.47534435\n",
      "Iteration 3433, loss = 0.47508274\n",
      "Iteration 3434, loss = 0.47506755\n",
      "Iteration 3435, loss = 0.47493796\n",
      "Iteration 3436, loss = 0.47517694\n",
      "Iteration 3437, loss = 0.47513234\n",
      "Iteration 3438, loss = 0.47502569\n",
      "Iteration 3439, loss = 0.47497451\n",
      "Iteration 3440, loss = 0.47497613\n",
      "Iteration 3441, loss = 0.47503805\n",
      "Iteration 3442, loss = 0.47498633\n",
      "Iteration 3443, loss = 0.47492300\n",
      "Iteration 3444, loss = 0.47492764\n",
      "Iteration 3445, loss = 0.47513600\n",
      "Iteration 3446, loss = 0.47544007\n",
      "Iteration 3447, loss = 0.47555445\n",
      "Iteration 3448, loss = 0.47502348\n",
      "Iteration 3449, loss = 0.47542526\n",
      "Iteration 3450, loss = 0.47609259\n",
      "Iteration 3451, loss = 0.47780198\n",
      "Iteration 3452, loss = 0.47872044\n",
      "Iteration 3453, loss = 0.47883544\n",
      "Iteration 3454, loss = 0.47858025\n",
      "Iteration 3455, loss = 0.47727491\n",
      "Iteration 3456, loss = 0.47642633\n",
      "Iteration 3457, loss = 0.47457101\n",
      "Iteration 3458, loss = 0.47676002\n",
      "Iteration 3459, loss = 0.47892043\n",
      "Iteration 3460, loss = 0.48010294\n",
      "Iteration 3461, loss = 0.48011508\n",
      "Iteration 3462, loss = 0.47910036\n",
      "Iteration 3463, loss = 0.47800495\n",
      "Iteration 3464, loss = 0.47558152\n",
      "Iteration 3465, loss = 0.47509764\n",
      "Iteration 3466, loss = 0.47506504\n",
      "Iteration 3467, loss = 0.47626583\n",
      "Iteration 3468, loss = 0.47658725\n",
      "Iteration 3469, loss = 0.47654664\n",
      "Iteration 3470, loss = 0.47595223\n",
      "Iteration 3471, loss = 0.47519149\n",
      "Iteration 3472, loss = 0.47464352\n",
      "Iteration 3473, loss = 0.47618360\n",
      "Iteration 3474, loss = 0.47662634\n",
      "Iteration 3475, loss = 0.47601820\n",
      "Iteration 3476, loss = 0.47584266\n",
      "Iteration 3477, loss = 0.47492393\n",
      "Iteration 3478, loss = 0.47510559\n",
      "Iteration 3479, loss = 0.47490579\n",
      "Iteration 3480, loss = 0.47482666\n",
      "Iteration 3481, loss = 0.47531501\n",
      "Iteration 3482, loss = 0.47520101\n",
      "Iteration 3483, loss = 0.47512572\n",
      "Iteration 3484, loss = 0.47516550\n",
      "Iteration 3485, loss = 0.47503776\n",
      "Iteration 3486, loss = 0.47550665\n",
      "Iteration 3487, loss = 0.47657632\n",
      "Iteration 3488, loss = 0.47707920\n",
      "Iteration 3489, loss = 0.47619223\n",
      "Iteration 3490, loss = 0.47496677\n",
      "Iteration 3491, loss = 0.47495729\n",
      "Iteration 3492, loss = 0.47641024\n",
      "Iteration 3493, loss = 0.47739065\n",
      "Iteration 3494, loss = 0.47759108\n",
      "Iteration 3495, loss = 0.47708325\n",
      "Iteration 3496, loss = 0.47613103\n",
      "Iteration 3497, loss = 0.47522545\n",
      "Iteration 3498, loss = 0.47463156\n",
      "Iteration 3499, loss = 0.47607194\n",
      "Iteration 3500, loss = 0.47728317\n",
      "Iteration 3501, loss = 0.47817038\n",
      "Iteration 3502, loss = 0.47867734\n",
      "Iteration 3503, loss = 0.47831640\n",
      "Iteration 3504, loss = 0.47713056\n",
      "Iteration 3505, loss = 0.47561041\n",
      "Iteration 3506, loss = 0.47498515\n",
      "Iteration 3507, loss = 0.47579875\n",
      "Iteration 3508, loss = 0.47778647\n",
      "Iteration 3509, loss = 0.47929939\n",
      "Iteration 3510, loss = 0.48003670\n",
      "Iteration 3511, loss = 0.47988999\n",
      "Iteration 3512, loss = 0.47925420\n",
      "Iteration 3513, loss = 0.47774039\n",
      "Iteration 3514, loss = 0.47687212\n",
      "Iteration 3515, loss = 0.47563000\n",
      "Iteration 3516, loss = 0.47465496\n",
      "Iteration 3517, loss = 0.47497115\n",
      "Iteration 3518, loss = 0.47638714\n",
      "Iteration 3519, loss = 0.47715934\n",
      "Iteration 3520, loss = 0.47746677\n",
      "Iteration 3521, loss = 0.47755437\n",
      "Iteration 3522, loss = 0.47716173\n",
      "Iteration 3523, loss = 0.47641525\n",
      "Iteration 3524, loss = 0.47568825\n",
      "Iteration 3525, loss = 0.47507167\n",
      "Iteration 3526, loss = 0.47490899\n",
      "Iteration 3527, loss = 0.47503310\n",
      "Iteration 3528, loss = 0.47550057\n",
      "Iteration 3529, loss = 0.47613871\n",
      "Iteration 3530, loss = 0.47625731\n",
      "Iteration 3531, loss = 0.47540153\n",
      "Iteration 3532, loss = 0.47544172\n",
      "Iteration 3533, loss = 0.47540504\n",
      "Iteration 3534, loss = 0.47656572\n",
      "Iteration 3535, loss = 0.47610988\n",
      "Iteration 3536, loss = 0.47547810\n",
      "Iteration 3537, loss = 0.47514085\n",
      "Iteration 3538, loss = 0.47519571\n",
      "Iteration 3539, loss = 0.47506504\n",
      "Iteration 3540, loss = 0.47504645\n",
      "Iteration 3541, loss = 0.47513114\n",
      "Iteration 3542, loss = 0.47495097\n",
      "Iteration 3543, loss = 0.47518801\n",
      "Iteration 3544, loss = 0.47518402\n",
      "Iteration 3545, loss = 0.47485610\n",
      "Iteration 3546, loss = 0.47487253\n",
      "Iteration 3547, loss = 0.47489067\n",
      "Iteration 3548, loss = 0.47526272\n",
      "Iteration 3549, loss = 0.47534533\n",
      "Iteration 3550, loss = 0.47553093\n",
      "Iteration 3551, loss = 0.47586067\n",
      "Iteration 3552, loss = 0.47619871\n",
      "Iteration 3553, loss = 0.47596957\n",
      "Iteration 3554, loss = 0.47617866\n",
      "Iteration 3555, loss = 0.47568589\n",
      "Iteration 3556, loss = 0.47531765\n",
      "Iteration 3557, loss = 0.47538455\n",
      "Iteration 3558, loss = 0.47542375\n",
      "Iteration 3559, loss = 0.47556907\n",
      "Iteration 3560, loss = 0.47537854\n",
      "Iteration 3561, loss = 0.47516620\n",
      "Iteration 3562, loss = 0.47514023\n",
      "Iteration 3563, loss = 0.47506215\n",
      "Iteration 3564, loss = 0.47504014\n",
      "Iteration 3565, loss = 0.47504852\n",
      "Iteration 3566, loss = 0.47481990\n",
      "Iteration 3567, loss = 0.47492230\n",
      "Iteration 3568, loss = 0.47489173\n",
      "Iteration 3569, loss = 0.47485195\n",
      "Iteration 3570, loss = 0.47478756\n",
      "Iteration 3571, loss = 0.47466218\n",
      "Iteration 3572, loss = 0.47507646\n",
      "Iteration 3573, loss = 0.47470905\n",
      "Iteration 3574, loss = 0.47466856\n",
      "Iteration 3575, loss = 0.47467535\n",
      "Iteration 3576, loss = 0.47465554\n",
      "Iteration 3577, loss = 0.47505560\n",
      "Iteration 3578, loss = 0.47486073\n",
      "Iteration 3579, loss = 0.47445915\n",
      "Iteration 3580, loss = 0.47516435\n",
      "Iteration 3581, loss = 0.47515306\n",
      "Iteration 3582, loss = 0.47494503\n",
      "Iteration 3583, loss = 0.47456829\n",
      "Iteration 3584, loss = 0.47509767\n",
      "Iteration 3585, loss = 0.47497260\n",
      "Iteration 3586, loss = 0.47536619\n",
      "Iteration 3587, loss = 0.47536776\n",
      "Iteration 3588, loss = 0.47520154\n",
      "Iteration 3589, loss = 0.47494164\n",
      "Iteration 3590, loss = 0.47498544\n",
      "Iteration 3591, loss = 0.47463753\n",
      "Iteration 3592, loss = 0.47491590\n",
      "Iteration 3593, loss = 0.47517866\n",
      "Iteration 3594, loss = 0.47493295\n",
      "Iteration 3595, loss = 0.47464990\n",
      "Iteration 3596, loss = 0.47449615\n",
      "Iteration 3597, loss = 0.47497719\n",
      "Iteration 3598, loss = 0.47574419\n",
      "Iteration 3599, loss = 0.47597384\n",
      "Iteration 3600, loss = 0.47496988\n",
      "Iteration 3601, loss = 0.47505491\n",
      "Iteration 3602, loss = 0.47544191\n",
      "Iteration 3603, loss = 0.47534224\n",
      "Iteration 3604, loss = 0.47520566\n",
      "Iteration 3605, loss = 0.47494418\n",
      "Iteration 3606, loss = 0.47503519\n",
      "Iteration 3607, loss = 0.47523923\n",
      "Iteration 3608, loss = 0.47552707\n",
      "Iteration 3609, loss = 0.47503511\n",
      "Iteration 3610, loss = 0.47470496\n",
      "Iteration 3611, loss = 0.47493980\n",
      "Iteration 3612, loss = 0.47591102\n",
      "Iteration 3613, loss = 0.47685732\n",
      "Iteration 3614, loss = 0.47741773\n",
      "Iteration 3615, loss = 0.47784329\n",
      "Iteration 3616, loss = 0.47700301\n",
      "Iteration 3617, loss = 0.47637443\n",
      "Iteration 3618, loss = 0.47493641\n",
      "Iteration 3619, loss = 0.47484597\n",
      "Iteration 3620, loss = 0.47500749\n",
      "Iteration 3621, loss = 0.47477972\n",
      "Iteration 3622, loss = 0.47479447\n",
      "Iteration 3623, loss = 0.47484331\n",
      "Iteration 3624, loss = 0.47484372\n",
      "Iteration 3625, loss = 0.47484730\n",
      "Iteration 3626, loss = 0.47477704\n",
      "Iteration 3627, loss = 0.47467516\n",
      "Iteration 3628, loss = 0.47477539\n",
      "Iteration 3629, loss = 0.47484014\n",
      "Iteration 3630, loss = 0.47490572\n",
      "Iteration 3631, loss = 0.47495137\n",
      "Iteration 3632, loss = 0.47455366\n",
      "Iteration 3633, loss = 0.47528183\n",
      "Iteration 3634, loss = 0.47502807\n",
      "Iteration 3635, loss = 0.47501526\n",
      "Iteration 3636, loss = 0.47501271\n",
      "Iteration 3637, loss = 0.47484521\n",
      "Iteration 3638, loss = 0.47486469\n",
      "Iteration 3639, loss = 0.47464956\n",
      "Iteration 3640, loss = 0.47470297\n",
      "Iteration 3641, loss = 0.47480811\n",
      "Iteration 3642, loss = 0.47539953\n",
      "Iteration 3643, loss = 0.47645796\n",
      "Iteration 3644, loss = 0.47615277\n",
      "Iteration 3645, loss = 0.47477522\n",
      "Iteration 3646, loss = 0.47468959\n",
      "Iteration 3647, loss = 0.47550960\n",
      "Iteration 3648, loss = 0.47783361\n",
      "Iteration 3649, loss = 0.47873343\n",
      "Iteration 3650, loss = 0.47823494\n",
      "Iteration 3651, loss = 0.47669730\n",
      "Iteration 3652, loss = 0.47496225\n",
      "Iteration 3653, loss = 0.47466319\n",
      "Iteration 3654, loss = 0.47630080\n",
      "Iteration 3655, loss = 0.47843096\n",
      "Iteration 3656, loss = 0.48003081\n",
      "Iteration 3657, loss = 0.47971542\n",
      "Iteration 3658, loss = 0.47813762\n",
      "Iteration 3659, loss = 0.47624282\n",
      "Iteration 3660, loss = 0.47533266\n",
      "Iteration 3661, loss = 0.47496988\n",
      "Iteration 3662, loss = 0.47487492\n",
      "Iteration 3663, loss = 0.47488965\n",
      "Iteration 3664, loss = 0.47480827\n",
      "Iteration 3665, loss = 0.47497641\n",
      "Iteration 3666, loss = 0.47471860\n",
      "Iteration 3667, loss = 0.47484427\n",
      "Iteration 3668, loss = 0.47463184\n",
      "Iteration 3669, loss = 0.47468387\n",
      "Iteration 3670, loss = 0.47475381\n",
      "Iteration 3671, loss = 0.47468471\n",
      "Iteration 3672, loss = 0.47459712\n",
      "Iteration 3673, loss = 0.47447504\n",
      "Iteration 3674, loss = 0.47448463\n",
      "Iteration 3675, loss = 0.47593480\n",
      "Iteration 3676, loss = 0.47574304\n",
      "Iteration 3677, loss = 0.47553290\n",
      "Iteration 3678, loss = 0.47474337\n",
      "Iteration 3679, loss = 0.47488465\n",
      "Iteration 3680, loss = 0.47458601\n",
      "Iteration 3681, loss = 0.47460466\n",
      "Iteration 3682, loss = 0.47478743\n",
      "Iteration 3683, loss = 0.47471376\n",
      "Iteration 3684, loss = 0.47457470\n",
      "Iteration 3685, loss = 0.47450003\n",
      "Iteration 3686, loss = 0.47484397\n",
      "Iteration 3687, loss = 0.47526315\n",
      "Iteration 3688, loss = 0.47589657\n",
      "Iteration 3689, loss = 0.47616541\n",
      "Iteration 3690, loss = 0.47564201\n",
      "Iteration 3691, loss = 0.47552467\n",
      "Iteration 3692, loss = 0.47505899\n",
      "Iteration 3693, loss = 0.47500256\n",
      "Iteration 3694, loss = 0.47456067\n",
      "Iteration 3695, loss = 0.47466559\n",
      "Iteration 3696, loss = 0.47520097\n",
      "Iteration 3697, loss = 0.47547332\n",
      "Iteration 3698, loss = 0.47535974\n",
      "Iteration 3699, loss = 0.47462297\n",
      "Iteration 3700, loss = 0.47448151\n",
      "Iteration 3701, loss = 0.47491980\n",
      "Iteration 3702, loss = 0.47489816\n",
      "Iteration 3703, loss = 0.47488045\n",
      "Iteration 3704, loss = 0.47474550\n",
      "Iteration 3705, loss = 0.47456244\n",
      "Iteration 3706, loss = 0.47462740\n",
      "Iteration 3707, loss = 0.47473916\n",
      "Iteration 3708, loss = 0.47502287\n",
      "Iteration 3709, loss = 0.47510722\n",
      "Iteration 3710, loss = 0.47489889\n",
      "Iteration 3711, loss = 0.47461012\n",
      "Iteration 3712, loss = 0.47444771\n",
      "Iteration 3713, loss = 0.47451476\n",
      "Iteration 3714, loss = 0.47459186\n",
      "Iteration 3715, loss = 0.47465205\n",
      "Iteration 3716, loss = 0.47446207\n",
      "Iteration 3717, loss = 0.47469701\n",
      "Iteration 3718, loss = 0.47496511\n",
      "Iteration 3719, loss = 0.47520303\n",
      "Iteration 3720, loss = 0.47528649\n",
      "Iteration 3721, loss = 0.47483594\n",
      "Iteration 3722, loss = 0.47456245\n",
      "Iteration 3723, loss = 0.47478468\n",
      "Iteration 3724, loss = 0.47470364\n",
      "Iteration 3725, loss = 0.47423769\n",
      "Iteration 3726, loss = 0.47505801\n",
      "Iteration 3727, loss = 0.47559290\n",
      "Iteration 3728, loss = 0.47589704\n",
      "Iteration 3729, loss = 0.47544125\n",
      "Iteration 3730, loss = 0.47476850\n",
      "Iteration 3731, loss = 0.47517361\n",
      "Iteration 3732, loss = 0.47503842\n",
      "Iteration 3733, loss = 0.47527195\n",
      "Iteration 3734, loss = 0.47672902\n",
      "Iteration 3735, loss = 0.47733783\n",
      "Iteration 3736, loss = 0.47687092\n",
      "Iteration 3737, loss = 0.47579356\n",
      "Iteration 3738, loss = 0.47483183\n",
      "Iteration 3739, loss = 0.47430514\n",
      "Iteration 3740, loss = 0.47467951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3741, loss = 0.47749347\n",
      "Iteration 3742, loss = 0.47934795\n",
      "Iteration 3743, loss = 0.47871561\n",
      "Iteration 3744, loss = 0.47627684\n",
      "Iteration 3745, loss = 0.47541189\n",
      "Iteration 3746, loss = 0.47444753\n",
      "Iteration 3747, loss = 0.47475915\n",
      "Iteration 3748, loss = 0.47513019\n",
      "Iteration 3749, loss = 0.47547493\n",
      "Iteration 3750, loss = 0.47484835\n",
      "Iteration 3751, loss = 0.47449280\n",
      "Iteration 3752, loss = 0.47482134\n",
      "Iteration 3753, loss = 0.47556366\n",
      "Iteration 3754, loss = 0.47543157\n",
      "Iteration 3755, loss = 0.47474538\n",
      "Iteration 3756, loss = 0.47476291\n",
      "Iteration 3757, loss = 0.47522469\n",
      "Iteration 3758, loss = 0.47492015\n",
      "Iteration 3759, loss = 0.47478950\n",
      "Iteration 3760, loss = 0.47455694\n",
      "Iteration 3761, loss = 0.47487227\n",
      "Iteration 3762, loss = 0.47501003\n",
      "Iteration 3763, loss = 0.47496141\n",
      "Iteration 3764, loss = 0.47471481\n",
      "Iteration 3765, loss = 0.47417653\n",
      "Iteration 3766, loss = 0.47498450\n",
      "Iteration 3767, loss = 0.47567060\n",
      "Iteration 3768, loss = 0.47583179\n",
      "Iteration 3769, loss = 0.47540153\n",
      "Iteration 3770, loss = 0.47517768\n",
      "Iteration 3771, loss = 0.47458716\n",
      "Iteration 3772, loss = 0.47474002\n",
      "Iteration 3773, loss = 0.47481887\n",
      "Iteration 3774, loss = 0.47487415\n",
      "Iteration 3775, loss = 0.47485273\n",
      "Iteration 3776, loss = 0.47470841\n",
      "Iteration 3777, loss = 0.47434540\n",
      "Iteration 3778, loss = 0.47469319\n",
      "Iteration 3779, loss = 0.47510135\n",
      "Iteration 3780, loss = 0.47498937\n",
      "Iteration 3781, loss = 0.47446121\n",
      "Iteration 3782, loss = 0.47444576\n",
      "Iteration 3783, loss = 0.47442088\n",
      "Iteration 3784, loss = 0.47433459\n",
      "Iteration 3785, loss = 0.47503216\n",
      "Iteration 3786, loss = 0.47499963\n",
      "Iteration 3787, loss = 0.47481731\n",
      "Iteration 3788, loss = 0.47442271\n",
      "Iteration 3789, loss = 0.47449970\n",
      "Iteration 3790, loss = 0.47438440\n",
      "Iteration 3791, loss = 0.47431303\n",
      "Iteration 3792, loss = 0.47454100\n",
      "Iteration 3793, loss = 0.47497434\n",
      "Iteration 3794, loss = 0.47475596\n",
      "Iteration 3795, loss = 0.47452291\n",
      "Iteration 3796, loss = 0.47476051\n",
      "Iteration 3797, loss = 0.47470253\n",
      "Iteration 3798, loss = 0.47456383\n",
      "Iteration 3799, loss = 0.47436911\n",
      "Iteration 3800, loss = 0.47440598\n",
      "Iteration 3801, loss = 0.47464806\n",
      "Iteration 3802, loss = 0.47481112\n",
      "Iteration 3803, loss = 0.47485743\n",
      "Iteration 3804, loss = 0.47463972\n",
      "Iteration 3805, loss = 0.47463790\n",
      "Iteration 3806, loss = 0.47471784\n",
      "Iteration 3807, loss = 0.47473530\n",
      "Iteration 3808, loss = 0.47478782\n",
      "Iteration 3809, loss = 0.47483707\n",
      "Iteration 3810, loss = 0.47482865\n",
      "Iteration 3811, loss = 0.47469059\n",
      "Iteration 3812, loss = 0.47469422\n",
      "Iteration 3813, loss = 0.47456433\n",
      "Iteration 3814, loss = 0.47453159\n",
      "Iteration 3815, loss = 0.47482478\n",
      "Iteration 3816, loss = 0.47486804\n",
      "Iteration 3817, loss = 0.47465151\n",
      "Iteration 3818, loss = 0.47484435\n",
      "Iteration 3819, loss = 0.47449933\n",
      "Iteration 3820, loss = 0.47451606\n",
      "Iteration 3821, loss = 0.47450345\n",
      "Iteration 3822, loss = 0.47453319\n",
      "Iteration 3823, loss = 0.47416002\n",
      "Iteration 3824, loss = 0.47471330\n",
      "Iteration 3825, loss = 0.47571496\n",
      "Iteration 3826, loss = 0.47659200\n",
      "Iteration 3827, loss = 0.47741968\n",
      "Iteration 3828, loss = 0.47693404\n",
      "Iteration 3829, loss = 0.47530580\n",
      "Iteration 3830, loss = 0.47482685\n",
      "Iteration 3831, loss = 0.47522534\n",
      "Iteration 3832, loss = 0.47542543\n",
      "Iteration 3833, loss = 0.47523434\n",
      "Iteration 3834, loss = 0.47486724\n",
      "Iteration 3835, loss = 0.47452356\n",
      "Iteration 3836, loss = 0.47470985\n",
      "Iteration 3837, loss = 0.47501700\n",
      "Iteration 3838, loss = 0.47454874\n",
      "Iteration 3839, loss = 0.47453887\n",
      "Iteration 3840, loss = 0.47445276\n",
      "Iteration 3841, loss = 0.47447550\n",
      "Iteration 3842, loss = 0.47481576\n",
      "Iteration 3843, loss = 0.47477948\n",
      "Iteration 3844, loss = 0.47459041\n",
      "Iteration 3845, loss = 0.47434158\n",
      "Iteration 3846, loss = 0.47474157\n",
      "Iteration 3847, loss = 0.47478032\n",
      "Iteration 3848, loss = 0.47445690\n",
      "Iteration 3849, loss = 0.47424364\n",
      "Iteration 3850, loss = 0.47479277\n",
      "Iteration 3851, loss = 0.47535667\n",
      "Iteration 3852, loss = 0.47559558\n",
      "Iteration 3853, loss = 0.47563996\n",
      "Iteration 3854, loss = 0.47533764\n",
      "Iteration 3855, loss = 0.47520839\n",
      "Iteration 3856, loss = 0.47463918\n",
      "Iteration 3857, loss = 0.47453216\n",
      "Iteration 3858, loss = 0.47446319\n",
      "Iteration 3859, loss = 0.47475288\n",
      "Iteration 3860, loss = 0.47470444\n",
      "Iteration 3861, loss = 0.47482824\n",
      "Iteration 3862, loss = 0.47501089\n",
      "Iteration 3863, loss = 0.47549928\n",
      "Iteration 3864, loss = 0.47569329\n",
      "Iteration 3865, loss = 0.47580598\n",
      "Iteration 3866, loss = 0.47514859\n",
      "Iteration 3867, loss = 0.47412832\n",
      "Iteration 3868, loss = 0.47468374\n",
      "Iteration 3869, loss = 0.47535667\n",
      "Iteration 3870, loss = 0.47550611\n",
      "Iteration 3871, loss = 0.47492119\n",
      "Iteration 3872, loss = 0.47466978\n",
      "Iteration 3873, loss = 0.47459834\n",
      "Iteration 3874, loss = 0.47463689\n",
      "Iteration 3875, loss = 0.47446551\n",
      "Iteration 3876, loss = 0.47453813\n",
      "Iteration 3877, loss = 0.47451997\n",
      "Iteration 3878, loss = 0.47448220\n",
      "Iteration 3879, loss = 0.47479280\n",
      "Iteration 3880, loss = 0.47517024\n",
      "Iteration 3881, loss = 0.47550042\n",
      "Iteration 3882, loss = 0.47519639\n",
      "Iteration 3883, loss = 0.47462356\n",
      "Iteration 3884, loss = 0.47442434\n",
      "Iteration 3885, loss = 0.47433517\n",
      "Iteration 3886, loss = 0.47444750\n",
      "Iteration 3887, loss = 0.47503283\n",
      "Iteration 3888, loss = 0.47562805\n",
      "Iteration 3889, loss = 0.47510970\n",
      "Iteration 3890, loss = 0.47415625\n",
      "Iteration 3891, loss = 0.47525884\n",
      "Iteration 3892, loss = 0.47573019\n",
      "Iteration 3893, loss = 0.47560972\n",
      "Iteration 3894, loss = 0.47523657\n",
      "Iteration 3895, loss = 0.47482157\n",
      "Iteration 3896, loss = 0.47490510\n",
      "Iteration 3897, loss = 0.47472576\n",
      "Iteration 3898, loss = 0.47499941\n",
      "Iteration 3899, loss = 0.47628228\n",
      "Iteration 3900, loss = 0.47614816\n",
      "Iteration 3901, loss = 0.47600337\n",
      "Iteration 3902, loss = 0.47513398\n",
      "Iteration 3903, loss = 0.47542130\n",
      "Iteration 3904, loss = 0.47509838\n",
      "Iteration 3905, loss = 0.47449508\n",
      "Iteration 3906, loss = 0.47430238\n",
      "Iteration 3907, loss = 0.47463197\n",
      "Iteration 3908, loss = 0.47442740\n",
      "Iteration 3909, loss = 0.47428120\n",
      "Iteration 3910, loss = 0.47450941\n",
      "Iteration 3911, loss = 0.47461586\n",
      "Iteration 3912, loss = 0.47469304\n",
      "Iteration 3913, loss = 0.47470190\n",
      "Iteration 3914, loss = 0.47450556\n",
      "Iteration 3915, loss = 0.47444572\n",
      "Iteration 3916, loss = 0.47437295\n",
      "Iteration 3917, loss = 0.47449490\n",
      "Iteration 3918, loss = 0.47445088\n",
      "Iteration 3919, loss = 0.47435180\n",
      "Iteration 3920, loss = 0.47442993\n",
      "Iteration 3921, loss = 0.47481146\n",
      "Iteration 3922, loss = 0.47465347\n",
      "Iteration 3923, loss = 0.47450504\n",
      "Iteration 3924, loss = 0.47446137\n",
      "Iteration 3925, loss = 0.47449519\n",
      "Iteration 3926, loss = 0.47483356\n",
      "Iteration 3927, loss = 0.47460279\n",
      "Iteration 3928, loss = 0.47445505\n",
      "Iteration 3929, loss = 0.47440600\n",
      "Iteration 3930, loss = 0.47451128\n",
      "Iteration 3931, loss = 0.47425117\n",
      "Iteration 3932, loss = 0.47494287\n",
      "Iteration 3933, loss = 0.47565328\n",
      "Iteration 3934, loss = 0.47522407\n",
      "Iteration 3935, loss = 0.47428128\n",
      "Iteration 3936, loss = 0.47495950\n",
      "Iteration 3937, loss = 0.47514963\n",
      "Iteration 3938, loss = 0.47436838\n",
      "Iteration 3939, loss = 0.47512670\n",
      "Iteration 3940, loss = 0.47469709\n",
      "Iteration 3941, loss = 0.47430368\n",
      "Iteration 3942, loss = 0.47508172\n",
      "Iteration 3943, loss = 0.47463816\n",
      "Iteration 3944, loss = 0.47441926\n",
      "Iteration 3945, loss = 0.47446083\n",
      "Iteration 3946, loss = 0.47443284\n",
      "Iteration 3947, loss = 0.47425334\n",
      "Iteration 3948, loss = 0.47377847\n",
      "Iteration 3949, loss = 0.47676588\n",
      "Iteration 3950, loss = 0.47704753\n",
      "Iteration 3951, loss = 0.47661167\n",
      "Iteration 3952, loss = 0.47538307\n",
      "Iteration 3953, loss = 0.47422885\n",
      "Iteration 3954, loss = 0.47435949\n",
      "Iteration 3955, loss = 0.47588867\n",
      "Iteration 3956, loss = 0.47618084\n",
      "Iteration 3957, loss = 0.47534457\n",
      "Iteration 3958, loss = 0.47459308\n",
      "Iteration 3959, loss = 0.47460993\n",
      "Iteration 3960, loss = 0.47454469\n",
      "Iteration 3961, loss = 0.47504065\n",
      "Iteration 3962, loss = 0.47640823\n",
      "Iteration 3963, loss = 0.47633685\n",
      "Iteration 3964, loss = 0.47553688\n",
      "Iteration 3965, loss = 0.47523821\n",
      "Iteration 3966, loss = 0.47507734\n",
      "Iteration 3967, loss = 0.47465264\n",
      "Iteration 3968, loss = 0.47423106\n",
      "Iteration 3969, loss = 0.47499949\n",
      "Iteration 3970, loss = 0.47528695\n",
      "Iteration 3971, loss = 0.47463862\n",
      "Iteration 3972, loss = 0.47458654\n",
      "Iteration 3973, loss = 0.47486724\n",
      "Iteration 3974, loss = 0.47469575\n",
      "Iteration 3975, loss = 0.47434732\n",
      "Iteration 3976, loss = 0.47441383\n",
      "Iteration 3977, loss = 0.47454307\n",
      "Iteration 3978, loss = 0.47445719\n",
      "Iteration 3979, loss = 0.47430812\n",
      "Iteration 3980, loss = 0.47442643\n",
      "Iteration 3981, loss = 0.47483667\n",
      "Iteration 3982, loss = 0.47543860\n",
      "Iteration 3983, loss = 0.47587389\n",
      "Iteration 3984, loss = 0.47503798\n",
      "Iteration 3985, loss = 0.47474717\n",
      "Iteration 3986, loss = 0.47460861\n",
      "Iteration 3987, loss = 0.47506470\n",
      "Iteration 3988, loss = 0.47522100\n",
      "Iteration 3989, loss = 0.47480723\n",
      "Iteration 3990, loss = 0.47458335\n",
      "Iteration 3991, loss = 0.47427651\n",
      "Iteration 3992, loss = 0.47425561\n",
      "Iteration 3993, loss = 0.47417072\n",
      "Iteration 3994, loss = 0.47413555\n",
      "Iteration 3995, loss = 0.47418209\n",
      "Iteration 3996, loss = 0.47439753\n",
      "Iteration 3997, loss = 0.47475263\n",
      "Iteration 3998, loss = 0.47498351\n",
      "Iteration 3999, loss = 0.47501270\n",
      "Iteration 4000, loss = 0.47502351\n",
      "Iteration 4001, loss = 0.47540026\n",
      "Iteration 4002, loss = 0.47477979\n",
      "Iteration 4003, loss = 0.47498380\n",
      "Iteration 4004, loss = 0.47503811\n",
      "Iteration 4005, loss = 0.47488080\n",
      "Iteration 4006, loss = 0.47454524\n",
      "Iteration 4007, loss = 0.47441149\n",
      "Iteration 4008, loss = 0.47479868\n",
      "Iteration 4009, loss = 0.47553992\n",
      "Iteration 4010, loss = 0.47579882\n",
      "Iteration 4011, loss = 0.47622555\n",
      "Iteration 4012, loss = 0.47494721\n",
      "Iteration 4013, loss = 0.47407422\n",
      "Iteration 4014, loss = 0.47554825\n",
      "Iteration 4015, loss = 0.47585639\n",
      "Iteration 4016, loss = 0.47537190\n",
      "Iteration 4017, loss = 0.47458727\n",
      "Iteration 4018, loss = 0.47417915\n",
      "Iteration 4019, loss = 0.47448947\n",
      "Iteration 4020, loss = 0.47456634\n",
      "Iteration 4021, loss = 0.47443013\n",
      "Iteration 4022, loss = 0.47461446\n",
      "Iteration 4023, loss = 0.47421721\n",
      "Iteration 4024, loss = 0.47425630\n",
      "Iteration 4025, loss = 0.47432354\n",
      "Iteration 4026, loss = 0.47443928\n",
      "Iteration 4027, loss = 0.47485151\n",
      "Iteration 4028, loss = 0.47505329\n",
      "Iteration 4029, loss = 0.47469676\n",
      "Iteration 4030, loss = 0.47429470\n",
      "Iteration 4031, loss = 0.47471581\n",
      "Iteration 4032, loss = 0.47491132\n",
      "Iteration 4033, loss = 0.47446302\n",
      "Iteration 4034, loss = 0.47417417\n",
      "Iteration 4035, loss = 0.47468431\n",
      "Iteration 4036, loss = 0.47521260\n",
      "Iteration 4037, loss = 0.47555108\n",
      "Iteration 4038, loss = 0.47539570\n",
      "Iteration 4039, loss = 0.47489285\n",
      "Iteration 4040, loss = 0.47463425\n",
      "Iteration 4041, loss = 0.47435633\n",
      "Iteration 4042, loss = 0.47453156\n",
      "Iteration 4043, loss = 0.47476046\n",
      "Iteration 4044, loss = 0.47501649\n",
      "Iteration 4045, loss = 0.47489318\n",
      "Iteration 4046, loss = 0.47499294\n",
      "Iteration 4047, loss = 0.47441600\n",
      "Iteration 4048, loss = 0.47472194\n",
      "Iteration 4049, loss = 0.47561639\n",
      "Iteration 4050, loss = 0.47727234\n",
      "Iteration 4051, loss = 0.47758050\n",
      "Iteration 4052, loss = 0.47628373\n",
      "Iteration 4053, loss = 0.47468619\n",
      "Iteration 4054, loss = 0.47443768\n",
      "Iteration 4055, loss = 0.47489021\n",
      "Iteration 4056, loss = 0.47544254\n",
      "Iteration 4057, loss = 0.47563516\n",
      "Iteration 4058, loss = 0.47530469\n",
      "Iteration 4059, loss = 0.47479331\n",
      "Iteration 4060, loss = 0.47423092\n",
      "Iteration 4061, loss = 0.47462684\n",
      "Iteration 4062, loss = 0.47465100\n",
      "Iteration 4063, loss = 0.47453691\n",
      "Iteration 4064, loss = 0.47454303\n",
      "Iteration 4065, loss = 0.47508401\n",
      "Iteration 4066, loss = 0.47620914\n",
      "Iteration 4067, loss = 0.47665437\n",
      "Iteration 4068, loss = 0.47646725\n",
      "Iteration 4069, loss = 0.47599368\n",
      "Iteration 4070, loss = 0.47548018\n",
      "Iteration 4071, loss = 0.47456511\n",
      "Iteration 4072, loss = 0.47437300\n",
      "Iteration 4073, loss = 0.47512802\n",
      "Iteration 4074, loss = 0.47639995\n",
      "Iteration 4075, loss = 0.47672283\n",
      "Iteration 4076, loss = 0.47597670\n",
      "Iteration 4077, loss = 0.47481313\n",
      "Iteration 4078, loss = 0.47511019\n",
      "Iteration 4079, loss = 0.47467620\n",
      "Iteration 4080, loss = 0.47516513\n",
      "Iteration 4081, loss = 0.47529502\n",
      "Iteration 4082, loss = 0.47501962\n",
      "Iteration 4083, loss = 0.47453812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4084, loss = 0.47446601\n",
      "Iteration 4085, loss = 0.47444642\n",
      "Iteration 4086, loss = 0.47449359\n",
      "Iteration 4087, loss = 0.47437614\n",
      "Iteration 4088, loss = 0.47451462\n",
      "Iteration 4089, loss = 0.47471690\n",
      "Iteration 4090, loss = 0.47490430\n",
      "Iteration 4091, loss = 0.47416624\n",
      "Iteration 4092, loss = 0.47477883\n",
      "Iteration 4093, loss = 0.47598078\n",
      "Iteration 4094, loss = 0.47564500\n",
      "Iteration 4095, loss = 0.47479921\n",
      "Iteration 4096, loss = 0.47442954\n",
      "Iteration 4097, loss = 0.47500718\n",
      "Iteration 4098, loss = 0.47682372\n",
      "Iteration 4099, loss = 0.47942998\n",
      "Iteration 4100, loss = 0.47787856\n",
      "Iteration 4101, loss = 0.47520721\n",
      "Iteration 4102, loss = 0.47398502\n",
      "Iteration 4103, loss = 0.47646440\n",
      "Iteration 4104, loss = 0.47734578\n",
      "Iteration 4105, loss = 0.47737360\n",
      "Iteration 4106, loss = 0.47629548\n",
      "Iteration 4107, loss = 0.47460622\n",
      "Iteration 4108, loss = 0.47464302\n",
      "Iteration 4109, loss = 0.47522938\n",
      "Iteration 4110, loss = 0.47558150\n",
      "Iteration 4111, loss = 0.47548844\n",
      "Iteration 4112, loss = 0.47512154\n",
      "Iteration 4113, loss = 0.47450544\n",
      "Iteration 4114, loss = 0.47404794\n",
      "Iteration 4115, loss = 0.47509193\n",
      "Iteration 4116, loss = 0.47499928\n",
      "Iteration 4117, loss = 0.47498568\n",
      "Iteration 4118, loss = 0.47486598\n",
      "Iteration 4119, loss = 0.47466471\n",
      "Iteration 4120, loss = 0.47437070\n",
      "Iteration 4121, loss = 0.47425183\n",
      "Iteration 4122, loss = 0.47430366\n",
      "Iteration 4123, loss = 0.47489203\n",
      "Iteration 4124, loss = 0.47492325\n",
      "Iteration 4125, loss = 0.47483385\n",
      "Iteration 4126, loss = 0.47434560\n",
      "Iteration 4127, loss = 0.47427755\n",
      "Iteration 4128, loss = 0.47422440\n",
      "Iteration 4129, loss = 0.47426758\n",
      "Iteration 4130, loss = 0.47443817\n",
      "Iteration 4131, loss = 0.47472530\n",
      "Iteration 4132, loss = 0.47501729\n",
      "Iteration 4133, loss = 0.47479068\n",
      "Iteration 4134, loss = 0.47450780\n",
      "Iteration 4135, loss = 0.47435538\n",
      "Iteration 4136, loss = 0.47467742\n",
      "Iteration 4137, loss = 0.47447779\n",
      "Iteration 4138, loss = 0.47439340\n",
      "Iteration 4139, loss = 0.47425025\n",
      "Iteration 4140, loss = 0.47426535\n",
      "Iteration 4141, loss = 0.47428437\n",
      "Iteration 4142, loss = 0.47429423\n",
      "Iteration 4143, loss = 0.47445650\n",
      "Iteration 4144, loss = 0.47499840\n",
      "Iteration 4145, loss = 0.47571669\n",
      "Iteration 4146, loss = 0.47507098\n",
      "Iteration 4147, loss = 0.47459318\n",
      "Iteration 4148, loss = 0.47426518\n",
      "Iteration 4149, loss = 0.47493637\n",
      "Iteration 4150, loss = 0.47557368\n",
      "Iteration 4151, loss = 0.47547348\n",
      "Iteration 4152, loss = 0.47490803\n",
      "Iteration 4153, loss = 0.47460483\n",
      "Iteration 4154, loss = 0.47432692\n",
      "Iteration 4155, loss = 0.47447825\n",
      "Iteration 4156, loss = 0.47471314\n",
      "Iteration 4157, loss = 0.47487413\n",
      "Iteration 4158, loss = 0.47495436\n",
      "Iteration 4159, loss = 0.47459323\n",
      "Iteration 4160, loss = 0.47423344\n",
      "Iteration 4161, loss = 0.47429628\n",
      "Iteration 4162, loss = 0.47545281\n",
      "Iteration 4163, loss = 0.47683663\n",
      "Iteration 4164, loss = 0.47620284\n",
      "Iteration 4165, loss = 0.47449574\n",
      "Iteration 4166, loss = 0.47416629\n",
      "Iteration 4167, loss = 0.47458072\n",
      "Iteration 4168, loss = 0.47673648\n",
      "Iteration 4169, loss = 0.47801638\n",
      "Iteration 4170, loss = 0.47622992\n",
      "Iteration 4171, loss = 0.47411541\n",
      "Iteration 4172, loss = 0.47474478\n",
      "Iteration 4173, loss = 0.47639400\n",
      "Iteration 4174, loss = 0.47662647\n",
      "Iteration 4175, loss = 0.47630821\n",
      "Iteration 4176, loss = 0.47539986\n",
      "Iteration 4177, loss = 0.47495453\n",
      "Iteration 4178, loss = 0.47407226\n",
      "Iteration 4179, loss = 0.47428513\n",
      "Iteration 4180, loss = 0.47460854\n",
      "Iteration 4181, loss = 0.47475749\n",
      "Iteration 4182, loss = 0.47490721\n",
      "Iteration 4183, loss = 0.47485820\n",
      "Iteration 4184, loss = 0.47465739\n",
      "Iteration 4185, loss = 0.47455299\n",
      "Iteration 4186, loss = 0.47436388\n",
      "Iteration 4187, loss = 0.47440027\n",
      "Iteration 4188, loss = 0.47465287\n",
      "Iteration 4189, loss = 0.47473388\n",
      "Iteration 4190, loss = 0.47467679\n",
      "Iteration 4191, loss = 0.47447655\n",
      "Iteration 4192, loss = 0.47447270\n",
      "Iteration 4193, loss = 0.47425603\n",
      "Iteration 4194, loss = 0.47499599\n",
      "Iteration 4195, loss = 0.47526016\n",
      "Iteration 4196, loss = 0.47531649\n",
      "Iteration 4197, loss = 0.47497794\n",
      "Iteration 4198, loss = 0.47518752\n",
      "Iteration 4199, loss = 0.47714833\n",
      "Iteration 4200, loss = 0.47746464\n",
      "Iteration 4201, loss = 0.47635301\n",
      "Iteration 4202, loss = 0.47458516\n",
      "Iteration 4203, loss = 0.47427374\n",
      "Iteration 4204, loss = 0.47485051\n",
      "Iteration 4205, loss = 0.47530410\n",
      "Iteration 4206, loss = 0.47446389\n",
      "Iteration 4207, loss = 0.47441592\n",
      "Iteration 4208, loss = 0.47455157\n",
      "Iteration 4209, loss = 0.47419678\n",
      "Iteration 4210, loss = 0.47501117\n",
      "Iteration 4211, loss = 0.47544365\n",
      "Iteration 4212, loss = 0.47525201\n",
      "Iteration 4213, loss = 0.47462791\n",
      "Iteration 4214, loss = 0.47443485\n",
      "Iteration 4215, loss = 0.47451198\n",
      "Iteration 4216, loss = 0.47549991\n",
      "Iteration 4217, loss = 0.47607786\n",
      "Iteration 4218, loss = 0.47499601\n",
      "Iteration 4219, loss = 0.47440572\n",
      "Iteration 4220, loss = 0.47451311\n",
      "Iteration 4221, loss = 0.47460905\n",
      "Iteration 4222, loss = 0.47436850\n",
      "Iteration 4223, loss = 0.47413445\n",
      "Iteration 4224, loss = 0.47423094\n",
      "Iteration 4225, loss = 0.47412831\n",
      "Iteration 4226, loss = 0.47411436\n",
      "Iteration 4227, loss = 0.47407619\n",
      "Iteration 4228, loss = 0.47492178\n",
      "Iteration 4229, loss = 0.47424693\n",
      "Iteration 4230, loss = 0.47393816\n",
      "Iteration 4231, loss = 0.47491756\n",
      "Iteration 4232, loss = 0.47618022\n",
      "Iteration 4233, loss = 0.47729191\n",
      "Iteration 4234, loss = 0.47572356\n",
      "Iteration 4235, loss = 0.47451031\n",
      "Iteration 4236, loss = 0.47476857\n",
      "Iteration 4237, loss = 0.47560251\n",
      "Iteration 4238, loss = 0.47658450\n",
      "Iteration 4239, loss = 0.47609379\n",
      "Iteration 4240, loss = 0.47534411\n",
      "Iteration 4241, loss = 0.47420586\n",
      "Iteration 4242, loss = 0.47420885\n",
      "Iteration 4243, loss = 0.47406096\n",
      "Iteration 4244, loss = 0.47426561\n",
      "Iteration 4245, loss = 0.47463464\n",
      "Iteration 4246, loss = 0.47485728\n",
      "Iteration 4247, loss = 0.47484144\n",
      "Iteration 4248, loss = 0.47463548\n",
      "Iteration 4249, loss = 0.47449343\n",
      "Iteration 4250, loss = 0.47421689\n",
      "Iteration 4251, loss = 0.47414285\n",
      "Iteration 4252, loss = 0.47416942\n",
      "Iteration 4253, loss = 0.47417586\n",
      "Iteration 4254, loss = 0.47437036\n",
      "Iteration 4255, loss = 0.47410483\n",
      "Iteration 4256, loss = 0.47423927\n",
      "Iteration 4257, loss = 0.47405490\n",
      "Iteration 4258, loss = 0.47413411\n",
      "Iteration 4259, loss = 0.47419387\n",
      "Iteration 4260, loss = 0.47429231\n",
      "Iteration 4261, loss = 0.47428635\n",
      "Iteration 4262, loss = 0.47403780\n",
      "Iteration 4263, loss = 0.47395671\n",
      "Iteration 4264, loss = 0.47437190\n",
      "Iteration 4265, loss = 0.47532214\n",
      "Iteration 4266, loss = 0.47597834\n",
      "Iteration 4267, loss = 0.47642848\n",
      "Iteration 4268, loss = 0.47672297\n",
      "Iteration 4269, loss = 0.47596744\n",
      "Iteration 4270, loss = 0.47453342\n",
      "Iteration 4271, loss = 0.47536211\n",
      "Iteration 4272, loss = 0.47480570\n",
      "Iteration 4273, loss = 0.47565744\n",
      "Iteration 4274, loss = 0.47631629\n",
      "Iteration 4275, loss = 0.47680765\n",
      "Iteration 4276, loss = 0.47759596\n",
      "Iteration 4277, loss = 0.47600057\n",
      "Iteration 4278, loss = 0.47463859\n",
      "Iteration 4279, loss = 0.47449657\n",
      "Iteration 4280, loss = 0.47505264\n",
      "Iteration 4281, loss = 0.47477576\n",
      "Iteration 4282, loss = 0.47430501\n",
      "Iteration 4283, loss = 0.47433630\n",
      "Iteration 4284, loss = 0.47428728\n",
      "Iteration 4285, loss = 0.47435912\n",
      "Iteration 4286, loss = 0.47415305\n",
      "Iteration 4287, loss = 0.47407666\n",
      "Iteration 4288, loss = 0.47435217\n",
      "Iteration 4289, loss = 0.47526818\n",
      "Iteration 4290, loss = 0.47731106\n",
      "Iteration 4291, loss = 0.47800244\n",
      "Iteration 4292, loss = 0.47692313\n",
      "Iteration 4293, loss = 0.47705225\n",
      "Iteration 4294, loss = 0.47707479\n",
      "Iteration 4295, loss = 0.47717462\n",
      "Iteration 4296, loss = 0.47670164\n",
      "Iteration 4297, loss = 0.47599489\n",
      "Iteration 4298, loss = 0.47409978\n",
      "Iteration 4299, loss = 0.47363839\n",
      "Iteration 4300, loss = 0.47518436\n",
      "Iteration 4301, loss = 0.47791738\n",
      "Iteration 4302, loss = 0.47979180\n",
      "Iteration 4303, loss = 0.47985390\n",
      "Iteration 4304, loss = 0.47819641\n",
      "Iteration 4305, loss = 0.47610367\n",
      "Iteration 4306, loss = 0.47521275\n",
      "Iteration 4307, loss = 0.47420984\n",
      "Iteration 4308, loss = 0.47420499\n",
      "Iteration 4309, loss = 0.47443757\n",
      "Iteration 4310, loss = 0.47427014\n",
      "Iteration 4311, loss = 0.47429322\n",
      "Iteration 4312, loss = 0.47494256\n",
      "Iteration 4313, loss = 0.47455759\n",
      "Iteration 4314, loss = 0.47406857\n",
      "Iteration 4315, loss = 0.47424748\n",
      "Iteration 4316, loss = 0.47575940\n",
      "Iteration 4317, loss = 0.47632478\n",
      "Iteration 4318, loss = 0.47566879\n",
      "Iteration 4319, loss = 0.47419080\n",
      "Iteration 4320, loss = 0.47380489\n",
      "Iteration 4321, loss = 0.47498390\n",
      "Iteration 4322, loss = 0.47753314\n",
      "Iteration 4323, loss = 0.47950172\n",
      "Iteration 4324, loss = 0.48130118\n",
      "Iteration 4325, loss = 0.48077476\n",
      "Iteration 4326, loss = 0.47887001\n",
      "Iteration 4327, loss = 0.47613506\n",
      "Iteration 4328, loss = 0.47499392\n",
      "Iteration 4329, loss = 0.47401030\n",
      "Iteration 4330, loss = 0.47397552\n",
      "Iteration 4331, loss = 0.47465982\n",
      "Iteration 4332, loss = 0.47552351\n",
      "Iteration 4333, loss = 0.47513539\n",
      "Iteration 4334, loss = 0.47476357\n",
      "Iteration 4335, loss = 0.47415572\n",
      "Iteration 4336, loss = 0.47409713\n",
      "Iteration 4337, loss = 0.47430678\n",
      "Iteration 4338, loss = 0.47462644\n",
      "Iteration 4339, loss = 0.47450661\n",
      "Iteration 4340, loss = 0.47433336\n",
      "Iteration 4341, loss = 0.47426277\n",
      "Iteration 4342, loss = 0.47416388\n",
      "Iteration 4343, loss = 0.47561602\n",
      "Iteration 4344, loss = 0.47588033\n",
      "Iteration 4345, loss = 0.47553518\n",
      "Iteration 4346, loss = 0.47448540\n",
      "Iteration 4347, loss = 0.47484895\n",
      "Iteration 4348, loss = 0.47476597\n",
      "Iteration 4349, loss = 0.47512674\n",
      "Iteration 4350, loss = 0.47504117\n",
      "Iteration 4351, loss = 0.47450084\n",
      "Iteration 4352, loss = 0.47459581\n",
      "Iteration 4353, loss = 0.47449109\n",
      "Iteration 4354, loss = 0.47445290\n",
      "Iteration 4355, loss = 0.47473363\n",
      "Iteration 4356, loss = 0.47478742\n",
      "Iteration 4357, loss = 0.47450528\n",
      "Iteration 4358, loss = 0.47442538\n",
      "Iteration 4359, loss = 0.47405040\n",
      "Iteration 4360, loss = 0.47408683\n",
      "Iteration 4361, loss = 0.47417432\n",
      "Iteration 4362, loss = 0.47442207\n",
      "Iteration 4363, loss = 0.47434912\n",
      "Iteration 4364, loss = 0.47507529\n",
      "Iteration 4365, loss = 0.47573417\n",
      "Iteration 4366, loss = 0.47619884\n",
      "Iteration 4367, loss = 0.47696175\n",
      "Iteration 4368, loss = 0.47698064\n",
      "Iteration 4369, loss = 0.47609600\n",
      "Iteration 4370, loss = 0.47477063\n",
      "Iteration 4371, loss = 0.47433256\n",
      "Iteration 4372, loss = 0.47393512\n",
      "Iteration 4373, loss = 0.47425482\n",
      "Iteration 4374, loss = 0.47435670\n",
      "Iteration 4375, loss = 0.47444157\n",
      "Iteration 4376, loss = 0.47434293\n",
      "Iteration 4377, loss = 0.47414541\n",
      "Iteration 4378, loss = 0.47397257\n",
      "Iteration 4379, loss = 0.47396782\n",
      "Iteration 4380, loss = 0.47409135\n",
      "Iteration 4381, loss = 0.47427286\n",
      "Iteration 4382, loss = 0.47431312\n",
      "Iteration 4383, loss = 0.47406093\n",
      "Iteration 4384, loss = 0.47402353\n",
      "Iteration 4385, loss = 0.47407122\n",
      "Iteration 4386, loss = 0.47479367\n",
      "Iteration 4387, loss = 0.47461254\n",
      "Iteration 4388, loss = 0.47407041\n",
      "Iteration 4389, loss = 0.47359376\n",
      "Iteration 4390, loss = 0.47459452\n",
      "Iteration 4391, loss = 0.47605631\n",
      "Iteration 4392, loss = 0.47657400\n",
      "Iteration 4393, loss = 0.47634109\n",
      "Iteration 4394, loss = 0.47601614\n",
      "Iteration 4395, loss = 0.47556544\n",
      "Iteration 4396, loss = 0.47460524\n",
      "Iteration 4397, loss = 0.47414369\n",
      "Iteration 4398, loss = 0.47519769\n",
      "Iteration 4399, loss = 0.47566189\n",
      "Iteration 4400, loss = 0.47546538\n",
      "Iteration 4401, loss = 0.47480670\n",
      "Iteration 4402, loss = 0.47417888\n",
      "Iteration 4403, loss = 0.47448136\n",
      "Iteration 4404, loss = 0.47528136\n",
      "Iteration 4405, loss = 0.47613648\n",
      "Iteration 4406, loss = 0.47658896\n",
      "Iteration 4407, loss = 0.47665743\n",
      "Iteration 4408, loss = 0.47601058\n",
      "Iteration 4409, loss = 0.47466028\n",
      "Iteration 4410, loss = 0.47426418\n",
      "Iteration 4411, loss = 0.47425336\n",
      "Iteration 4412, loss = 0.47542622\n",
      "Iteration 4413, loss = 0.47644215\n",
      "Iteration 4414, loss = 0.47617108\n",
      "Iteration 4415, loss = 0.47481921\n",
      "Iteration 4416, loss = 0.47356405\n",
      "Iteration 4417, loss = 0.47373524\n",
      "Iteration 4418, loss = 0.47644196\n",
      "Iteration 4419, loss = 0.47989883\n",
      "Iteration 4420, loss = 0.48154659\n",
      "Iteration 4421, loss = 0.48110693\n",
      "Iteration 4422, loss = 0.47971590\n",
      "Iteration 4423, loss = 0.47760068\n",
      "Iteration 4424, loss = 0.47614004\n",
      "Iteration 4425, loss = 0.47551616\n",
      "Iteration 4426, loss = 0.47498034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4427, loss = 0.47463924\n",
      "Iteration 4428, loss = 0.47443116\n",
      "Iteration 4429, loss = 0.47424170\n",
      "Iteration 4430, loss = 0.47491502\n",
      "Iteration 4431, loss = 0.47425751\n",
      "Iteration 4432, loss = 0.47415561\n",
      "Iteration 4433, loss = 0.47431323\n",
      "Iteration 4434, loss = 0.47560632\n",
      "Iteration 4435, loss = 0.47651557\n",
      "Iteration 4436, loss = 0.47617453\n",
      "Iteration 4437, loss = 0.47506862\n",
      "Iteration 4438, loss = 0.47446959\n",
      "Iteration 4439, loss = 0.47425400\n",
      "Iteration 4440, loss = 0.47405146\n",
      "Iteration 4441, loss = 0.47411951\n",
      "Iteration 4442, loss = 0.47414633\n",
      "Iteration 4443, loss = 0.47410836\n",
      "Iteration 4444, loss = 0.47407819\n",
      "Iteration 4445, loss = 0.47406573\n",
      "Iteration 4446, loss = 0.47409281\n",
      "Iteration 4447, loss = 0.47412685\n",
      "Iteration 4448, loss = 0.47395967\n",
      "Iteration 4449, loss = 0.47408524\n",
      "Iteration 4450, loss = 0.47459464\n",
      "Iteration 4451, loss = 0.47706377\n",
      "Iteration 4452, loss = 0.47798832\n",
      "Iteration 4453, loss = 0.47744313\n",
      "Iteration 4454, loss = 0.47536074\n",
      "Iteration 4455, loss = 0.47422264\n",
      "Iteration 4456, loss = 0.47509045\n",
      "Iteration 4457, loss = 0.48264660\n",
      "Iteration 4458, loss = 0.48661168\n",
      "Iteration 4459, loss = 0.48402006\n",
      "Iteration 4460, loss = 0.48005096\n",
      "Iteration 4461, loss = 0.47581082\n",
      "Iteration 4462, loss = 0.47437983\n",
      "Iteration 4463, loss = 0.47381247\n",
      "Iteration 4464, loss = 0.47432672\n",
      "Iteration 4465, loss = 0.47578197\n",
      "Iteration 4466, loss = 0.47634892\n",
      "Iteration 4467, loss = 0.47574009\n",
      "Iteration 4468, loss = 0.47459867\n",
      "Iteration 4469, loss = 0.47376931\n",
      "Iteration 4470, loss = 0.47408500\n",
      "Iteration 4471, loss = 0.47533164\n",
      "Iteration 4472, loss = 0.47540562\n",
      "Iteration 4473, loss = 0.47494854\n",
      "Iteration 4474, loss = 0.47422048\n",
      "Iteration 4475, loss = 0.47413496\n",
      "Iteration 4476, loss = 0.47409924\n",
      "Iteration 4477, loss = 0.47419258\n",
      "Iteration 4478, loss = 0.47425390\n",
      "Iteration 4479, loss = 0.47421515\n",
      "Iteration 4480, loss = 0.47420385\n",
      "Iteration 4481, loss = 0.47422167\n",
      "Iteration 4482, loss = 0.47420669\n",
      "Iteration 4483, loss = 0.47426184\n",
      "Iteration 4484, loss = 0.47415125\n",
      "Iteration 4485, loss = 0.47393379\n",
      "Iteration 4486, loss = 0.47394408\n",
      "Iteration 4487, loss = 0.47391486\n",
      "Iteration 4488, loss = 0.47392045\n",
      "Iteration 4489, loss = 0.47521012\n",
      "Iteration 4490, loss = 0.47520804\n",
      "Iteration 4491, loss = 0.47455688\n",
      "Iteration 4492, loss = 0.47403272\n",
      "Iteration 4493, loss = 0.47382239\n",
      "Iteration 4494, loss = 0.47419126\n",
      "Iteration 4495, loss = 0.47493014\n",
      "Iteration 4496, loss = 0.47578864\n",
      "Iteration 4497, loss = 0.47600746\n",
      "Iteration 4498, loss = 0.47560763\n",
      "Iteration 4499, loss = 0.47450332\n",
      "Iteration 4500, loss = 0.47405301\n",
      "Iteration 4501, loss = 0.47379443\n",
      "Iteration 4502, loss = 0.47426098\n",
      "Iteration 4503, loss = 0.47579765\n",
      "Iteration 4504, loss = 0.47584190\n",
      "Iteration 4505, loss = 0.47497131\n",
      "Iteration 4506, loss = 0.47408170\n",
      "Iteration 4507, loss = 0.47383978\n",
      "Iteration 4508, loss = 0.47453524\n",
      "Iteration 4509, loss = 0.47576195\n",
      "Iteration 4510, loss = 0.47710663\n",
      "Iteration 4511, loss = 0.47705528\n",
      "Iteration 4512, loss = 0.47599564\n",
      "Iteration 4513, loss = 0.47464028\n",
      "Iteration 4514, loss = 0.47426763\n",
      "Iteration 4515, loss = 0.47439184\n",
      "Iteration 4516, loss = 0.47472773\n",
      "Iteration 4517, loss = 0.47481111\n",
      "Iteration 4518, loss = 0.47469358\n",
      "Iteration 4519, loss = 0.47438328\n",
      "Iteration 4520, loss = 0.47430020\n",
      "Iteration 4521, loss = 0.47395090\n",
      "Iteration 4522, loss = 0.47391968\n",
      "Iteration 4523, loss = 0.47419352\n",
      "Iteration 4524, loss = 0.47452166\n",
      "Iteration 4525, loss = 0.47430065\n",
      "Iteration 4526, loss = 0.47459037\n",
      "Iteration 4527, loss = 0.47378595\n",
      "Iteration 4528, loss = 0.47412234\n",
      "Iteration 4529, loss = 0.47517257\n",
      "Iteration 4530, loss = 0.47610645\n",
      "Iteration 4531, loss = 0.47690978\n",
      "Iteration 4532, loss = 0.47767676\n",
      "Iteration 4533, loss = 0.47708892\n",
      "Iteration 4534, loss = 0.47556742\n",
      "Iteration 4535, loss = 0.47459092\n",
      "Iteration 4536, loss = 0.47410139\n",
      "Iteration 4537, loss = 0.47396500\n",
      "Iteration 4538, loss = 0.47392984\n",
      "Iteration 4539, loss = 0.47403444\n",
      "Iteration 4540, loss = 0.47409559\n",
      "Iteration 4541, loss = 0.47447902\n",
      "Iteration 4542, loss = 0.47430753\n",
      "Iteration 4543, loss = 0.47414534\n",
      "Iteration 4544, loss = 0.47405653\n",
      "Iteration 4545, loss = 0.47449155\n",
      "Iteration 4546, loss = 0.47497290\n",
      "Iteration 4547, loss = 0.47538177\n",
      "Iteration 4548, loss = 0.47531040\n",
      "Iteration 4549, loss = 0.47473665\n",
      "Iteration 4550, loss = 0.47410889\n",
      "Iteration 4551, loss = 0.47398989\n",
      "Iteration 4552, loss = 0.47422558\n",
      "Iteration 4553, loss = 0.47536059\n",
      "Iteration 4554, loss = 0.47553403\n",
      "Iteration 4555, loss = 0.47503253\n",
      "Iteration 4556, loss = 0.47447621\n",
      "Iteration 4557, loss = 0.47406434\n",
      "Iteration 4558, loss = 0.47415275\n",
      "Iteration 4559, loss = 0.47481266\n",
      "Iteration 4560, loss = 0.47460237\n",
      "Iteration 4561, loss = 0.47406177\n",
      "Iteration 4562, loss = 0.47370391\n",
      "Iteration 4563, loss = 0.47438129\n",
      "Iteration 4564, loss = 0.47467878\n",
      "Iteration 4565, loss = 0.47508613\n",
      "Iteration 4566, loss = 0.47531642\n",
      "Iteration 4567, loss = 0.47527609\n",
      "Iteration 4568, loss = 0.47526141\n",
      "Iteration 4569, loss = 0.47542129\n",
      "Iteration 4570, loss = 0.47527230\n",
      "Iteration 4571, loss = 0.47549955\n",
      "Iteration 4572, loss = 0.47540701\n",
      "Iteration 4573, loss = 0.47553694\n",
      "Iteration 4574, loss = 0.47526150\n",
      "Iteration 4575, loss = 0.47441769\n",
      "Iteration 4576, loss = 0.47434671\n",
      "Iteration 4577, loss = 0.47404085\n",
      "Iteration 4578, loss = 0.47396695\n",
      "Iteration 4579, loss = 0.47392727\n",
      "Iteration 4580, loss = 0.47394267\n",
      "Iteration 4581, loss = 0.47443428\n",
      "Iteration 4582, loss = 0.47388064\n",
      "Iteration 4583, loss = 0.47400204\n",
      "Iteration 4584, loss = 0.47438414\n",
      "Iteration 4585, loss = 0.47522799\n",
      "Iteration 4586, loss = 0.47533746\n",
      "Iteration 4587, loss = 0.47500549\n",
      "Iteration 4588, loss = 0.47485833\n",
      "Iteration 4589, loss = 0.47440795\n",
      "Iteration 4590, loss = 0.47420741\n",
      "Iteration 4591, loss = 0.47440189\n",
      "Iteration 4592, loss = 0.47465825\n",
      "Iteration 4593, loss = 0.47495146\n",
      "Iteration 4594, loss = 0.47522824\n",
      "Iteration 4595, loss = 0.47515945\n",
      "Iteration 4596, loss = 0.47479136\n",
      "Iteration 4597, loss = 0.47429829\n",
      "Iteration 4598, loss = 0.47417356\n",
      "Iteration 4599, loss = 0.47478162\n",
      "Iteration 4600, loss = 0.47515232\n",
      "Iteration 4601, loss = 0.47507497\n",
      "Iteration 4602, loss = 0.47467207\n",
      "Iteration 4603, loss = 0.47445539\n",
      "Iteration 4604, loss = 0.47411238\n",
      "Iteration 4605, loss = 0.47415598\n",
      "Iteration 4606, loss = 0.47411690\n",
      "Iteration 4607, loss = 0.47410971\n",
      "Iteration 4608, loss = 0.47418427\n",
      "Iteration 4609, loss = 0.47425915\n",
      "Iteration 4610, loss = 0.47415560\n",
      "Iteration 4611, loss = 0.47420984\n",
      "Iteration 4612, loss = 0.47435318\n",
      "Iteration 4613, loss = 0.47468814\n",
      "Iteration 4614, loss = 0.47467104\n",
      "Iteration 4615, loss = 0.47436221\n",
      "Iteration 4616, loss = 0.47402878\n",
      "Iteration 4617, loss = 0.47398679\n",
      "Iteration 4618, loss = 0.47393937\n",
      "Iteration 4619, loss = 0.47391269\n",
      "Iteration 4620, loss = 0.47392850\n",
      "Iteration 4621, loss = 0.47400970\n",
      "Iteration 4622, loss = 0.47399021\n",
      "Iteration 4623, loss = 0.47396850\n",
      "Iteration 4624, loss = 0.47393601\n",
      "Iteration 4625, loss = 0.47404201\n",
      "Iteration 4626, loss = 0.47453431\n",
      "Iteration 4627, loss = 0.47580589\n",
      "Iteration 4628, loss = 0.47579390\n",
      "Iteration 4629, loss = 0.47469231\n",
      "Iteration 4630, loss = 0.47359709\n",
      "Iteration 4631, loss = 0.47379288\n",
      "Iteration 4632, loss = 0.47672757\n",
      "Iteration 4633, loss = 0.47765509\n",
      "Iteration 4634, loss = 0.47519265\n",
      "Iteration 4635, loss = 0.47368711\n",
      "Iteration 4636, loss = 0.47548960\n",
      "Iteration 4637, loss = 0.47751162\n",
      "Iteration 4638, loss = 0.47770558\n",
      "Iteration 4639, loss = 0.47620565\n",
      "Iteration 4640, loss = 0.47455513\n",
      "Iteration 4641, loss = 0.47496213\n",
      "Iteration 4642, loss = 0.47468814\n",
      "Iteration 4643, loss = 0.47578008\n",
      "Iteration 4644, loss = 0.47528655\n",
      "Iteration 4645, loss = 0.47489004\n",
      "Iteration 4646, loss = 0.47390692\n",
      "Iteration 4647, loss = 0.47396998\n",
      "Iteration 4648, loss = 0.47509493\n",
      "Iteration 4649, loss = 0.47573844\n",
      "Iteration 4650, loss = 0.47548716\n",
      "Iteration 4651, loss = 0.47465979\n",
      "Iteration 4652, loss = 0.47416004\n",
      "Iteration 4653, loss = 0.47408669\n",
      "Iteration 4654, loss = 0.47463059\n",
      "Iteration 4655, loss = 0.47541424\n",
      "Iteration 4656, loss = 0.47483215\n",
      "Iteration 4657, loss = 0.47406499\n",
      "Iteration 4658, loss = 0.47375421\n",
      "Iteration 4659, loss = 0.47437635\n",
      "Iteration 4660, loss = 0.47528211\n",
      "Iteration 4661, loss = 0.47516341\n",
      "Iteration 4662, loss = 0.47470337\n",
      "Iteration 4663, loss = 0.47441831\n",
      "Iteration 4664, loss = 0.47448093\n",
      "Iteration 4665, loss = 0.47476921\n",
      "Iteration 4666, loss = 0.47466135\n",
      "Iteration 4667, loss = 0.47467506\n",
      "Iteration 4668, loss = 0.47430516\n",
      "Iteration 4669, loss = 0.47433477\n",
      "Iteration 4670, loss = 0.47460412\n",
      "Iteration 4671, loss = 0.47430217\n",
      "Iteration 4672, loss = 0.47425919\n",
      "Iteration 4673, loss = 0.47415730\n",
      "Iteration 4674, loss = 0.47415336\n",
      "Iteration 4675, loss = 0.47417029\n",
      "Iteration 4676, loss = 0.47414835\n",
      "Iteration 4677, loss = 0.47414519\n",
      "Iteration 4678, loss = 0.47410557\n",
      "Iteration 4679, loss = 0.47402231\n",
      "Iteration 4680, loss = 0.47396614\n",
      "Iteration 4681, loss = 0.47385575\n",
      "Iteration 4682, loss = 0.47425241\n",
      "Iteration 4683, loss = 0.47492952\n",
      "Iteration 4684, loss = 0.47520112\n",
      "Iteration 4685, loss = 0.47502439\n",
      "Iteration 4686, loss = 0.47467540\n",
      "Iteration 4687, loss = 0.47468034\n",
      "Iteration 4688, loss = 0.47457032\n",
      "Iteration 4689, loss = 0.47458324\n",
      "Iteration 4690, loss = 0.47466907\n",
      "Iteration 4691, loss = 0.47471171\n",
      "Iteration 4692, loss = 0.47470873\n",
      "Iteration 4693, loss = 0.47465580\n",
      "Iteration 4694, loss = 0.47498222\n",
      "Iteration 4695, loss = 0.47608379\n",
      "Iteration 4696, loss = 0.47758415\n",
      "Iteration 4697, loss = 0.47824710\n",
      "Iteration 4698, loss = 0.47750659\n",
      "Iteration 4699, loss = 0.47573141\n",
      "Iteration 4700, loss = 0.47435027\n",
      "Iteration 4701, loss = 0.47390300\n",
      "Iteration 4702, loss = 0.47471543\n",
      "Iteration 4703, loss = 0.47539823\n",
      "Iteration 4704, loss = 0.47528246\n",
      "Iteration 4705, loss = 0.47475390\n",
      "Iteration 4706, loss = 0.47387591\n",
      "Iteration 4707, loss = 0.47443852\n",
      "Iteration 4708, loss = 0.47420739\n",
      "Iteration 4709, loss = 0.47423773\n",
      "Iteration 4710, loss = 0.47418877\n",
      "Iteration 4711, loss = 0.47462166\n",
      "Iteration 4712, loss = 0.47576829\n",
      "Iteration 4713, loss = 0.47561600\n",
      "Iteration 4714, loss = 0.47491935\n",
      "Iteration 4715, loss = 0.47399971\n",
      "Iteration 4716, loss = 0.47414470\n",
      "Iteration 4717, loss = 0.47402722\n",
      "Iteration 4718, loss = 0.47409329\n",
      "Iteration 4719, loss = 0.47449444\n",
      "Iteration 4720, loss = 0.47433323\n",
      "Iteration 4721, loss = 0.47418767\n",
      "Iteration 4722, loss = 0.47405691\n",
      "Iteration 4723, loss = 0.47408321\n",
      "Iteration 4724, loss = 0.47411889\n",
      "Iteration 4725, loss = 0.47414493\n",
      "Iteration 4726, loss = 0.47439659\n",
      "Iteration 4727, loss = 0.47463693\n",
      "Iteration 4728, loss = 0.47467683\n",
      "Iteration 4729, loss = 0.47498051\n",
      "Iteration 4730, loss = 0.47492991\n",
      "Iteration 4731, loss = 0.47500434\n",
      "Iteration 4732, loss = 0.47525398\n",
      "Iteration 4733, loss = 0.47537901\n",
      "Iteration 4734, loss = 0.47507906\n",
      "Iteration 4735, loss = 0.47438539\n",
      "Iteration 4736, loss = 0.47384784\n",
      "Iteration 4737, loss = 0.47396334\n",
      "Iteration 4738, loss = 0.47455777\n",
      "Iteration 4739, loss = 0.47494325\n",
      "Iteration 4740, loss = 0.47572975\n",
      "Iteration 4741, loss = 0.47486707\n",
      "Iteration 4742, loss = 0.47420615\n",
      "Iteration 4743, loss = 0.47392833\n",
      "Iteration 4744, loss = 0.47409102\n",
      "Iteration 4745, loss = 0.47452337\n",
      "Iteration 4746, loss = 0.47474694\n",
      "Iteration 4747, loss = 0.47466626\n",
      "Iteration 4748, loss = 0.47461117\n",
      "Iteration 4749, loss = 0.47422733\n",
      "Iteration 4750, loss = 0.47394328\n",
      "Iteration 4751, loss = 0.47369146\n",
      "Iteration 4752, loss = 0.47435176\n",
      "Iteration 4753, loss = 0.47491941\n",
      "Iteration 4754, loss = 0.47563800\n",
      "Iteration 4755, loss = 0.47586881\n",
      "Iteration 4756, loss = 0.47533102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4757, loss = 0.47423245\n",
      "Iteration 4758, loss = 0.47380298\n",
      "Iteration 4759, loss = 0.47487289\n",
      "Iteration 4760, loss = 0.47578878\n",
      "Iteration 4761, loss = 0.47595583\n",
      "Iteration 4762, loss = 0.47550760\n",
      "Iteration 4763, loss = 0.47524115\n",
      "Iteration 4764, loss = 0.47491690\n",
      "Iteration 4765, loss = 0.47537380\n",
      "Iteration 4766, loss = 0.47525848\n",
      "Iteration 4767, loss = 0.47474735\n",
      "Iteration 4768, loss = 0.47431933\n",
      "Iteration 4769, loss = 0.47345405\n",
      "Iteration 4770, loss = 0.47427446\n",
      "Iteration 4771, loss = 0.47507688\n",
      "Iteration 4772, loss = 0.47697745\n",
      "Iteration 4773, loss = 0.47645368\n",
      "Iteration 4774, loss = 0.47518764\n",
      "Iteration 4775, loss = 0.47416602\n",
      "Iteration 4776, loss = 0.47421270\n",
      "Iteration 4777, loss = 0.47427701\n",
      "Iteration 4778, loss = 0.47424485\n",
      "Iteration 4779, loss = 0.47404180\n",
      "Iteration 4780, loss = 0.47376991\n",
      "Iteration 4781, loss = 0.47389284\n",
      "Iteration 4782, loss = 0.47422169\n",
      "Iteration 4783, loss = 0.47465006\n",
      "Iteration 4784, loss = 0.47512902\n",
      "Iteration 4785, loss = 0.47602487\n",
      "Iteration 4786, loss = 0.47630491\n",
      "Iteration 4787, loss = 0.47575535\n",
      "Iteration 4788, loss = 0.47529816\n",
      "Iteration 4789, loss = 0.47477473\n",
      "Iteration 4790, loss = 0.47409080\n",
      "Iteration 4791, loss = 0.47410501\n",
      "Iteration 4792, loss = 0.47410589\n",
      "Iteration 4793, loss = 0.47410393\n",
      "Iteration 4794, loss = 0.47403535\n",
      "Iteration 4795, loss = 0.47390482\n",
      "Iteration 4796, loss = 0.47391247\n",
      "Iteration 4797, loss = 0.47399469\n",
      "Iteration 4798, loss = 0.47394261\n",
      "Iteration 4799, loss = 0.47389484\n",
      "Iteration 4800, loss = 0.47387465\n",
      "Iteration 4801, loss = 0.47383925\n",
      "Iteration 4802, loss = 0.47385916\n",
      "Iteration 4803, loss = 0.47476920\n",
      "Iteration 4804, loss = 0.47471212\n",
      "Iteration 4805, loss = 0.47421625\n",
      "Iteration 4806, loss = 0.47419632\n",
      "Iteration 4807, loss = 0.47415361\n",
      "Iteration 4808, loss = 0.47428166\n",
      "Iteration 4809, loss = 0.47427083\n",
      "Iteration 4810, loss = 0.47455338\n",
      "Iteration 4811, loss = 0.47426612\n",
      "Iteration 4812, loss = 0.47380136\n",
      "Iteration 4813, loss = 0.47419425\n",
      "Iteration 4814, loss = 0.47418403\n",
      "Iteration 4815, loss = 0.47451258\n",
      "Iteration 4816, loss = 0.47464680\n",
      "Iteration 4817, loss = 0.47499409\n",
      "Iteration 4818, loss = 0.47493343\n",
      "Iteration 4819, loss = 0.47436110\n",
      "Iteration 4820, loss = 0.47469624\n",
      "Iteration 4821, loss = 0.47465328\n",
      "Iteration 4822, loss = 0.47453641\n",
      "Iteration 4823, loss = 0.47424620\n",
      "Iteration 4824, loss = 0.47392035\n",
      "Iteration 4825, loss = 0.47392716\n",
      "Iteration 4826, loss = 0.47396370\n",
      "Iteration 4827, loss = 0.47396246\n",
      "Iteration 4828, loss = 0.47384685\n",
      "Iteration 4829, loss = 0.47476511\n",
      "Iteration 4830, loss = 0.47505859\n",
      "Iteration 4831, loss = 0.47438210\n",
      "Iteration 4832, loss = 0.47428811\n",
      "Iteration 4833, loss = 0.47471030\n",
      "Iteration 4834, loss = 0.47456113\n",
      "Iteration 4835, loss = 0.47440022\n",
      "Iteration 4836, loss = 0.47431702\n",
      "Iteration 4837, loss = 0.47512248\n",
      "Iteration 4838, loss = 0.47483510\n",
      "Iteration 4839, loss = 0.47432809\n",
      "Iteration 4840, loss = 0.47443812\n",
      "Iteration 4841, loss = 0.47488782\n",
      "Iteration 4842, loss = 0.47513452\n",
      "Iteration 4843, loss = 0.47431182\n",
      "Iteration 4844, loss = 0.47381291\n",
      "Iteration 4845, loss = 0.47369676\n",
      "Iteration 4846, loss = 0.47486574\n",
      "Iteration 4847, loss = 0.47657924\n",
      "Iteration 4848, loss = 0.47750310\n",
      "Iteration 4849, loss = 0.47709889\n",
      "Iteration 4850, loss = 0.47549349\n",
      "Iteration 4851, loss = 0.47381299\n",
      "Iteration 4852, loss = 0.47376303\n",
      "Iteration 4853, loss = 0.47508280\n",
      "Iteration 4854, loss = 0.47624706\n",
      "Iteration 4855, loss = 0.47674564\n",
      "Iteration 4856, loss = 0.47668962\n",
      "Iteration 4857, loss = 0.47595900\n",
      "Iteration 4858, loss = 0.47597252\n",
      "Iteration 4859, loss = 0.47579548\n",
      "Iteration 4860, loss = 0.47506601\n",
      "Iteration 4861, loss = 0.47458677\n",
      "Iteration 4862, loss = 0.47403095\n",
      "Iteration 4863, loss = 0.47351852\n",
      "Iteration 4864, loss = 0.47408499\n",
      "Iteration 4865, loss = 0.47482493\n",
      "Iteration 4866, loss = 0.47534048\n",
      "Iteration 4867, loss = 0.47543830\n",
      "Iteration 4868, loss = 0.47481317\n",
      "Iteration 4869, loss = 0.47403094\n",
      "Iteration 4870, loss = 0.47394236\n",
      "Iteration 4871, loss = 0.47413925\n",
      "Iteration 4872, loss = 0.47412610\n",
      "Iteration 4873, loss = 0.47396917\n",
      "Iteration 4874, loss = 0.47366482\n",
      "Iteration 4875, loss = 0.47446724\n",
      "Iteration 4876, loss = 0.47556961\n",
      "Iteration 4877, loss = 0.47593422\n",
      "Iteration 4878, loss = 0.47563036\n",
      "Iteration 4879, loss = 0.47509674\n",
      "Iteration 4880, loss = 0.47475333\n",
      "Iteration 4881, loss = 0.47420762\n",
      "Iteration 4882, loss = 0.47401286\n",
      "Iteration 4883, loss = 0.47387962\n",
      "Iteration 4884, loss = 0.47364026\n",
      "Iteration 4885, loss = 0.47433973\n",
      "Iteration 4886, loss = 0.47397379\n",
      "Iteration 4887, loss = 0.47384181\n",
      "Iteration 4888, loss = 0.47395708\n",
      "Iteration 4889, loss = 0.47371825\n",
      "Iteration 4890, loss = 0.47372113\n",
      "Iteration 4891, loss = 0.47424880\n",
      "Iteration 4892, loss = 0.47443195\n",
      "Iteration 4893, loss = 0.47443376\n",
      "Iteration 4894, loss = 0.47414109\n",
      "Iteration 4895, loss = 0.47361945\n",
      "Iteration 4896, loss = 0.47478467\n",
      "Iteration 4897, loss = 0.47507277\n",
      "Iteration 4898, loss = 0.47460037\n",
      "Iteration 4899, loss = 0.47436939\n",
      "Iteration 4900, loss = 0.47406549\n",
      "Iteration 4901, loss = 0.47416625\n",
      "Iteration 4902, loss = 0.47384387\n",
      "Iteration 4903, loss = 0.47463312\n",
      "Iteration 4904, loss = 0.47517563\n",
      "Iteration 4905, loss = 0.47508339\n",
      "Iteration 4906, loss = 0.47462028\n",
      "Iteration 4907, loss = 0.47438423\n",
      "Iteration 4908, loss = 0.47419586\n",
      "Iteration 4909, loss = 0.47449418\n",
      "Iteration 4910, loss = 0.47431694\n",
      "Iteration 4911, loss = 0.47399611\n",
      "Iteration 4912, loss = 0.47393768\n",
      "Iteration 4913, loss = 0.47367597\n",
      "Iteration 4914, loss = 0.47376037\n",
      "Iteration 4915, loss = 0.47432928\n",
      "Iteration 4916, loss = 0.47484520\n",
      "Iteration 4917, loss = 0.47462057\n",
      "Iteration 4918, loss = 0.47402074\n",
      "Iteration 4919, loss = 0.47383832\n",
      "Iteration 4920, loss = 0.47369127\n",
      "Iteration 4921, loss = 0.47472041\n",
      "Iteration 4922, loss = 0.47453038\n",
      "Iteration 4923, loss = 0.47413287\n",
      "Iteration 4924, loss = 0.47368157\n",
      "Iteration 4925, loss = 0.47376811\n",
      "Iteration 4926, loss = 0.47484241\n",
      "Iteration 4927, loss = 0.47486752\n",
      "Iteration 4928, loss = 0.47476931\n",
      "Iteration 4929, loss = 0.47465129\n",
      "Iteration 4930, loss = 0.47472721\n",
      "Iteration 4931, loss = 0.47421458\n",
      "Iteration 4932, loss = 0.47409236\n",
      "Iteration 4933, loss = 0.47383528\n",
      "Iteration 4934, loss = 0.47361557\n",
      "Iteration 4935, loss = 0.47367901\n",
      "Iteration 4936, loss = 0.47515962\n",
      "Iteration 4937, loss = 0.47606560\n",
      "Iteration 4938, loss = 0.47583475\n",
      "Iteration 4939, loss = 0.47460371\n",
      "Iteration 4940, loss = 0.47359707\n",
      "Iteration 4941, loss = 0.47389971\n",
      "Iteration 4942, loss = 0.47561754\n",
      "Iteration 4943, loss = 0.47569602\n",
      "Iteration 4944, loss = 0.47479947\n",
      "Iteration 4945, loss = 0.47434701\n",
      "Iteration 4946, loss = 0.47374069\n",
      "Iteration 4947, loss = 0.47369495\n",
      "Iteration 4948, loss = 0.47391571\n",
      "Iteration 4949, loss = 0.47385220\n",
      "Iteration 4950, loss = 0.47364860\n",
      "Iteration 4951, loss = 0.47386715\n",
      "Iteration 4952, loss = 0.47365916\n",
      "Iteration 4953, loss = 0.47361577\n",
      "Iteration 4954, loss = 0.47423503\n",
      "Iteration 4955, loss = 0.47364615\n",
      "Iteration 4956, loss = 0.47352831\n",
      "Iteration 4957, loss = 0.47403282\n",
      "Iteration 4958, loss = 0.47421822\n",
      "Iteration 4959, loss = 0.47431478\n",
      "Iteration 4960, loss = 0.47460209\n",
      "Iteration 4961, loss = 0.47424183\n",
      "Iteration 4962, loss = 0.47410012\n",
      "Iteration 4963, loss = 0.47399811\n",
      "Iteration 4964, loss = 0.47391870\n",
      "Iteration 4965, loss = 0.47389039\n",
      "Iteration 4966, loss = 0.47384515\n",
      "Iteration 4967, loss = 0.47385063\n",
      "Iteration 4968, loss = 0.47437910\n",
      "Iteration 4969, loss = 0.47419564\n",
      "Iteration 4970, loss = 0.47398510\n",
      "Iteration 4971, loss = 0.47375215\n",
      "Iteration 4972, loss = 0.47389786\n",
      "Iteration 4973, loss = 0.47427814\n",
      "Iteration 4974, loss = 0.47449850\n",
      "Iteration 4975, loss = 0.47412820\n",
      "Iteration 4976, loss = 0.47439729\n",
      "Iteration 4977, loss = 0.47450904\n",
      "Iteration 4978, loss = 0.47444404\n",
      "Iteration 4979, loss = 0.47380537\n",
      "Iteration 4980, loss = 0.47372368\n",
      "Iteration 4981, loss = 0.47456091\n",
      "Iteration 4982, loss = 0.47467892\n",
      "Iteration 4983, loss = 0.47468893\n",
      "Iteration 4984, loss = 0.47359191\n",
      "Iteration 4985, loss = 0.47394999\n",
      "Iteration 4986, loss = 0.47482617\n",
      "Iteration 4987, loss = 0.47494518\n",
      "Iteration 4988, loss = 0.47435938\n",
      "Iteration 4989, loss = 0.47440222\n",
      "Iteration 4990, loss = 0.47464046\n",
      "Iteration 4991, loss = 0.47455326\n",
      "Iteration 4992, loss = 0.47436407\n",
      "Iteration 4993, loss = 0.47427372\n",
      "Iteration 4994, loss = 0.47434667\n",
      "Iteration 4995, loss = 0.47430626\n",
      "Iteration 4996, loss = 0.47423693\n",
      "Iteration 4997, loss = 0.47397125\n",
      "Iteration 4998, loss = 0.47408268\n",
      "Iteration 4999, loss = 0.47378589\n",
      "Iteration 5000, loss = 0.47368765\n",
      "Iteration 5001, loss = 0.47388254\n",
      "Iteration 5002, loss = 0.47398759\n",
      "Iteration 5003, loss = 0.47464491\n",
      "Iteration 5004, loss = 0.47442442\n",
      "Iteration 5005, loss = 0.47390800\n",
      "Iteration 5006, loss = 0.47399324\n",
      "Iteration 5007, loss = 0.47402928\n",
      "Iteration 5008, loss = 0.47388869\n",
      "Iteration 5009, loss = 0.47384991\n",
      "Iteration 5010, loss = 0.47370980\n",
      "Iteration 5011, loss = 0.47362960\n",
      "Iteration 5012, loss = 0.47351500\n",
      "Iteration 5013, loss = 0.47376070\n",
      "Iteration 5014, loss = 0.47416543\n",
      "Iteration 5015, loss = 0.47436400\n",
      "Iteration 5016, loss = 0.47421890\n",
      "Iteration 5017, loss = 0.47421652\n",
      "Iteration 5018, loss = 0.47401450\n",
      "Iteration 5019, loss = 0.47401192\n",
      "Iteration 5020, loss = 0.47400478\n",
      "Iteration 5021, loss = 0.47370714\n",
      "Iteration 5022, loss = 0.47371469\n",
      "Iteration 5023, loss = 0.47371499\n",
      "Iteration 5024, loss = 0.47364267\n",
      "Iteration 5025, loss = 0.47379398\n",
      "Iteration 5026, loss = 0.47339111\n",
      "Iteration 5027, loss = 0.47446790\n",
      "Iteration 5028, loss = 0.47503590\n",
      "Iteration 5029, loss = 0.47422114\n",
      "Iteration 5030, loss = 0.47435157\n",
      "Iteration 5031, loss = 0.47374730\n",
      "Iteration 5032, loss = 0.47401720\n",
      "Iteration 5033, loss = 0.47451213\n",
      "Iteration 5034, loss = 0.47518668\n",
      "Iteration 5035, loss = 0.47584653\n",
      "Iteration 5036, loss = 0.47577120\n",
      "Iteration 5037, loss = 0.47440862\n",
      "Iteration 5038, loss = 0.47362090\n",
      "Iteration 5039, loss = 0.47396418\n",
      "Iteration 5040, loss = 0.47518790\n",
      "Iteration 5041, loss = 0.47572247\n",
      "Iteration 5042, loss = 0.47538353\n",
      "Iteration 5043, loss = 0.47422275\n",
      "Iteration 5044, loss = 0.47323246\n",
      "Iteration 5045, loss = 0.47418787\n",
      "Iteration 5046, loss = 0.47581853\n",
      "Iteration 5047, loss = 0.47640453\n",
      "Iteration 5048, loss = 0.47586616\n",
      "Iteration 5049, loss = 0.47508512\n",
      "Iteration 5050, loss = 0.47438247\n",
      "Iteration 5051, loss = 0.47381924\n",
      "Iteration 5052, loss = 0.47353460\n",
      "Iteration 5053, loss = 0.47369753\n",
      "Iteration 5054, loss = 0.47388990\n",
      "Iteration 5055, loss = 0.47447314\n",
      "Iteration 5056, loss = 0.47472812\n",
      "Iteration 5057, loss = 0.47452612\n",
      "Iteration 5058, loss = 0.47416271\n",
      "Iteration 5059, loss = 0.47371421\n",
      "Iteration 5060, loss = 0.47411077\n",
      "Iteration 5061, loss = 0.47425633\n",
      "Iteration 5062, loss = 0.47409361\n",
      "Iteration 5063, loss = 0.47369399\n",
      "Iteration 5064, loss = 0.47348123\n",
      "Iteration 5065, loss = 0.47342502\n",
      "Iteration 5066, loss = 0.47464131\n",
      "Iteration 5067, loss = 0.47655939\n",
      "Iteration 5068, loss = 0.47891875\n",
      "Iteration 5069, loss = 0.47980032\n",
      "Iteration 5070, loss = 0.47853208\n",
      "Iteration 5071, loss = 0.47672734\n",
      "Iteration 5072, loss = 0.47449648\n",
      "Iteration 5073, loss = 0.47327087\n",
      "Iteration 5074, loss = 0.47411725\n",
      "Iteration 5075, loss = 0.47606610\n",
      "Iteration 5076, loss = 0.47841032\n",
      "Iteration 5077, loss = 0.48022036\n",
      "Iteration 5078, loss = 0.47998220\n",
      "Iteration 5079, loss = 0.47837745\n",
      "Iteration 5080, loss = 0.47636151\n",
      "Iteration 5081, loss = 0.47519514\n",
      "Iteration 5082, loss = 0.47425288\n",
      "Iteration 5083, loss = 0.47399734\n",
      "Iteration 5084, loss = 0.47412294\n",
      "Iteration 5085, loss = 0.47552635\n",
      "Iteration 5086, loss = 0.48002444\n",
      "Iteration 5087, loss = 0.48121247\n",
      "Iteration 5088, loss = 0.47876718\n",
      "Iteration 5089, loss = 0.47568219\n",
      "Iteration 5090, loss = 0.47345458\n",
      "Iteration 5091, loss = 0.47376265\n",
      "Iteration 5092, loss = 0.47611435\n",
      "Iteration 5093, loss = 0.47746816\n",
      "Iteration 5094, loss = 0.47782406\n",
      "Iteration 5095, loss = 0.47723599\n",
      "Iteration 5096, loss = 0.47575687\n",
      "Iteration 5097, loss = 0.47450903\n",
      "Iteration 5098, loss = 0.47331012\n",
      "Iteration 5099, loss = 0.47351755\n",
      "Iteration 5100, loss = 0.47630023\n",
      "Iteration 5101, loss = 0.48088786\n",
      "Iteration 5102, loss = 0.48248501\n",
      "Iteration 5103, loss = 0.48091213\n",
      "Iteration 5104, loss = 0.47838110\n",
      "Iteration 5105, loss = 0.47600144\n",
      "Iteration 5106, loss = 0.47487189\n",
      "Iteration 5107, loss = 0.47408257\n",
      "Iteration 5108, loss = 0.47507570\n",
      "Iteration 5109, loss = 0.47528509\n",
      "Iteration 5110, loss = 0.47474920\n",
      "Iteration 5111, loss = 0.47421162\n",
      "Iteration 5112, loss = 0.47376881\n",
      "Iteration 5113, loss = 0.47415916\n",
      "Iteration 5114, loss = 0.47527118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5115, loss = 0.47774529\n",
      "Iteration 5116, loss = 0.47890465\n",
      "Iteration 5117, loss = 0.47783360\n",
      "Iteration 5118, loss = 0.47609286\n",
      "Iteration 5119, loss = 0.47406488\n",
      "Iteration 5120, loss = 0.47483967\n",
      "Iteration 5121, loss = 0.47396928\n",
      "Iteration 5122, loss = 0.47397705\n",
      "Iteration 5123, loss = 0.47335240\n",
      "Iteration 5124, loss = 0.47367514\n",
      "Iteration 5125, loss = 0.47526055\n",
      "Iteration 5126, loss = 0.47599616\n",
      "Iteration 5127, loss = 0.47571190\n",
      "Iteration 5128, loss = 0.47442074\n",
      "Iteration 5129, loss = 0.47409068\n",
      "Iteration 5130, loss = 0.47375860\n",
      "Iteration 5131, loss = 0.47379142\n",
      "Iteration 5132, loss = 0.47359368\n",
      "Iteration 5133, loss = 0.47436427\n",
      "Iteration 5134, loss = 0.47405364\n",
      "Iteration 5135, loss = 0.47357422\n",
      "Iteration 5136, loss = 0.47372627\n",
      "Iteration 5137, loss = 0.47383116\n",
      "Iteration 5138, loss = 0.47368639\n",
      "Iteration 5139, loss = 0.47396930\n",
      "Iteration 5140, loss = 0.47385077\n",
      "Iteration 5141, loss = 0.47367252\n",
      "Iteration 5142, loss = 0.47357572\n",
      "Iteration 5143, loss = 0.47356966\n",
      "Iteration 5144, loss = 0.47369786\n",
      "Iteration 5145, loss = 0.47380498\n",
      "Iteration 5146, loss = 0.47385255\n",
      "Iteration 5147, loss = 0.47403734\n",
      "Iteration 5148, loss = 0.47360186\n",
      "Iteration 5149, loss = 0.47421573\n",
      "Iteration 5150, loss = 0.47432001\n",
      "Iteration 5151, loss = 0.47589086\n",
      "Iteration 5152, loss = 0.47565771\n",
      "Iteration 5153, loss = 0.47475942\n",
      "Iteration 5154, loss = 0.47436193\n",
      "Iteration 5155, loss = 0.47386366\n",
      "Iteration 5156, loss = 0.47402180\n",
      "Iteration 5157, loss = 0.47408124\n",
      "Iteration 5158, loss = 0.47403253\n",
      "Iteration 5159, loss = 0.47390440\n",
      "Iteration 5160, loss = 0.47386067\n",
      "Iteration 5161, loss = 0.47372121\n",
      "Iteration 5162, loss = 0.47360618\n",
      "Iteration 5163, loss = 0.47411722\n",
      "Iteration 5164, loss = 0.47379867\n",
      "Iteration 5165, loss = 0.47379571\n",
      "Iteration 5166, loss = 0.47384330\n",
      "Iteration 5167, loss = 0.47382434\n",
      "Iteration 5168, loss = 0.47377309\n",
      "Iteration 5169, loss = 0.47374878\n",
      "Iteration 5170, loss = 0.47376716\n",
      "Iteration 5171, loss = 0.47380532\n",
      "Iteration 5172, loss = 0.47363981\n",
      "Iteration 5173, loss = 0.47380217\n",
      "Iteration 5174, loss = 0.47383772\n",
      "Iteration 5175, loss = 0.47364640\n",
      "Iteration 5176, loss = 0.47373882\n",
      "Iteration 5177, loss = 0.47403263\n",
      "Iteration 5178, loss = 0.47489458\n",
      "Iteration 5179, loss = 0.47425947\n",
      "Iteration 5180, loss = 0.47344073\n",
      "Iteration 5181, loss = 0.47434510\n",
      "Iteration 5182, loss = 0.47430760\n",
      "Iteration 5183, loss = 0.47396303\n",
      "Iteration 5184, loss = 0.47349269\n",
      "Iteration 5185, loss = 0.47356200\n",
      "Iteration 5186, loss = 0.47508219\n",
      "Iteration 5187, loss = 0.47729641\n",
      "Iteration 5188, loss = 0.47690782\n",
      "Iteration 5189, loss = 0.47508442\n",
      "Iteration 5190, loss = 0.47442214\n",
      "Iteration 5191, loss = 0.47453450\n",
      "Iteration 5192, loss = 0.47437441\n",
      "Iteration 5193, loss = 0.47446468\n",
      "Iteration 5194, loss = 0.47401102\n",
      "Iteration 5195, loss = 0.47396173\n",
      "Iteration 5196, loss = 0.47579117\n",
      "Iteration 5197, loss = 0.47698902\n",
      "Iteration 5198, loss = 0.47656663\n",
      "Iteration 5199, loss = 0.47504444\n",
      "Iteration 5200, loss = 0.47459239\n",
      "Iteration 5201, loss = 0.47393739\n",
      "Iteration 5202, loss = 0.47383780\n",
      "Iteration 5203, loss = 0.47374331\n",
      "Iteration 5204, loss = 0.47358845\n",
      "Iteration 5205, loss = 0.47352372\n",
      "Iteration 5206, loss = 0.47368020\n",
      "Iteration 5207, loss = 0.47412286\n",
      "Iteration 5208, loss = 0.47483959\n",
      "Iteration 5209, loss = 0.47510519\n",
      "Iteration 5210, loss = 0.47480216\n",
      "Iteration 5211, loss = 0.47391019\n",
      "Iteration 5212, loss = 0.47322798\n",
      "Iteration 5213, loss = 0.47343572\n",
      "Iteration 5214, loss = 0.47594183\n",
      "Iteration 5215, loss = 0.47852842\n",
      "Iteration 5216, loss = 0.47860637\n",
      "Iteration 5217, loss = 0.47659048\n",
      "Iteration 5218, loss = 0.47469573\n",
      "Iteration 5219, loss = 0.47396526\n",
      "Iteration 5220, loss = 0.47434923\n",
      "Iteration 5221, loss = 0.47479895\n",
      "Iteration 5222, loss = 0.47516886\n",
      "Iteration 5223, loss = 0.47550874\n",
      "Iteration 5224, loss = 0.47579930\n",
      "Iteration 5225, loss = 0.47577274\n",
      "Iteration 5226, loss = 0.47499113\n",
      "Iteration 5227, loss = 0.47440750\n",
      "Iteration 5228, loss = 0.47387830\n",
      "Iteration 5229, loss = 0.47371256\n",
      "Iteration 5230, loss = 0.47456528\n",
      "Iteration 5231, loss = 0.47499903\n",
      "Iteration 5232, loss = 0.47491769\n",
      "Iteration 5233, loss = 0.47414531\n",
      "Iteration 5234, loss = 0.47313244\n",
      "Iteration 5235, loss = 0.47380251\n",
      "Iteration 5236, loss = 0.47564040\n",
      "Iteration 5237, loss = 0.47790709\n",
      "Iteration 5238, loss = 0.47898683\n",
      "Iteration 5239, loss = 0.47825801\n",
      "Iteration 5240, loss = 0.47593489\n",
      "Iteration 5241, loss = 0.47391149\n",
      "Iteration 5242, loss = 0.47433265\n",
      "Iteration 5243, loss = 0.47548788\n",
      "Iteration 5244, loss = 0.47543146\n",
      "Iteration 5245, loss = 0.47484106\n",
      "Iteration 5246, loss = 0.47416059\n",
      "Iteration 5247, loss = 0.47410657\n",
      "Iteration 5248, loss = 0.47398481\n",
      "Iteration 5249, loss = 0.47394637\n",
      "Iteration 5250, loss = 0.47361367\n",
      "Iteration 5251, loss = 0.47380872\n",
      "Iteration 5252, loss = 0.47463225\n",
      "Iteration 5253, loss = 0.47562573\n",
      "Iteration 5254, loss = 0.47609410\n",
      "Iteration 5255, loss = 0.47587950\n",
      "Iteration 5256, loss = 0.47518752\n",
      "Iteration 5257, loss = 0.47455758\n",
      "Iteration 5258, loss = 0.47385597\n",
      "Iteration 5259, loss = 0.47367494\n",
      "Iteration 5260, loss = 0.47365764\n",
      "Iteration 5261, loss = 0.47404967\n",
      "Iteration 5262, loss = 0.47513559\n",
      "Iteration 5263, loss = 0.47520204\n",
      "Iteration 5264, loss = 0.47446317\n",
      "Iteration 5265, loss = 0.47407291\n",
      "Iteration 5266, loss = 0.47360793\n",
      "Iteration 5267, loss = 0.47416233\n",
      "Iteration 5268, loss = 0.47402724\n",
      "Iteration 5269, loss = 0.47399038\n",
      "Iteration 5270, loss = 0.47395764\n",
      "Iteration 5271, loss = 0.47409111\n",
      "Iteration 5272, loss = 0.47416525\n",
      "Iteration 5273, loss = 0.47362392\n",
      "Iteration 5274, loss = 0.47349826\n",
      "Iteration 5275, loss = 0.47425807\n",
      "Iteration 5276, loss = 0.47607309\n",
      "Iteration 5277, loss = 0.47907292\n",
      "Iteration 5278, loss = 0.48145150\n",
      "Iteration 5279, loss = 0.48118493\n",
      "Iteration 5280, loss = 0.47886927\n",
      "Iteration 5281, loss = 0.47585759\n",
      "Iteration 5282, loss = 0.47389423\n",
      "Iteration 5283, loss = 0.47339186\n",
      "Iteration 5284, loss = 0.47623635\n",
      "Iteration 5285, loss = 0.47988614\n",
      "Iteration 5286, loss = 0.48032758\n",
      "Iteration 5287, loss = 0.47838362\n",
      "Iteration 5288, loss = 0.47650494\n",
      "Iteration 5289, loss = 0.47481305\n",
      "Iteration 5290, loss = 0.47402761\n",
      "Iteration 5291, loss = 0.47409275\n",
      "Iteration 5292, loss = 0.47403240\n",
      "Iteration 5293, loss = 0.47384338\n",
      "Iteration 5294, loss = 0.47358467\n",
      "Iteration 5295, loss = 0.47352040\n",
      "Iteration 5296, loss = 0.47420808\n",
      "Iteration 5297, loss = 0.47356656\n",
      "Iteration 5298, loss = 0.47382990\n",
      "Iteration 5299, loss = 0.47421827\n",
      "Iteration 5300, loss = 0.47420738\n",
      "Iteration 5301, loss = 0.47382524\n",
      "Iteration 5302, loss = 0.47331152\n",
      "Iteration 5303, loss = 0.47417907\n",
      "Iteration 5304, loss = 0.47460116\n",
      "Iteration 5305, loss = 0.47599498\n",
      "Iteration 5306, loss = 0.47440188\n",
      "Iteration 5307, loss = 0.47339923\n",
      "Iteration 5308, loss = 0.47464343\n",
      "Iteration 5309, loss = 0.47570435\n",
      "Iteration 5310, loss = 0.47574831\n",
      "Iteration 5311, loss = 0.47498962\n",
      "Iteration 5312, loss = 0.47408005\n",
      "Iteration 5313, loss = 0.47342855\n",
      "Iteration 5314, loss = 0.47445299\n",
      "Iteration 5315, loss = 0.47483622\n",
      "Iteration 5316, loss = 0.47505241\n",
      "Iteration 5317, loss = 0.47486491\n",
      "Iteration 5318, loss = 0.47466592\n",
      "Iteration 5319, loss = 0.47393570\n",
      "Iteration 5320, loss = 0.47374828\n",
      "Iteration 5321, loss = 0.47366216\n",
      "Iteration 5322, loss = 0.47368887\n",
      "Iteration 5323, loss = 0.47350537\n",
      "Iteration 5324, loss = 0.47351638\n",
      "Iteration 5325, loss = 0.47349369\n",
      "Iteration 5326, loss = 0.47373576\n",
      "Iteration 5327, loss = 0.47367989\n",
      "Iteration 5328, loss = 0.47357431\n",
      "Iteration 5329, loss = 0.47352089\n",
      "Iteration 5330, loss = 0.47349247\n",
      "Iteration 5331, loss = 0.47387279\n",
      "Iteration 5332, loss = 0.47395611\n",
      "Iteration 5333, loss = 0.47340250\n",
      "Iteration 5334, loss = 0.47349267\n",
      "Iteration 5335, loss = 0.47450530\n",
      "Iteration 5336, loss = 0.47645065\n",
      "Iteration 5337, loss = 0.47529993\n",
      "Iteration 5338, loss = 0.47486892\n",
      "Iteration 5339, loss = 0.47348587\n",
      "Iteration 5340, loss = 0.47355566\n",
      "Iteration 5341, loss = 0.47380784\n",
      "Iteration 5342, loss = 0.47418185\n",
      "Iteration 5343, loss = 0.47461813\n",
      "Iteration 5344, loss = 0.47475730\n",
      "Iteration 5345, loss = 0.47456047\n",
      "Iteration 5346, loss = 0.47440852\n",
      "Iteration 5347, loss = 0.47404732\n",
      "Iteration 5348, loss = 0.47366577\n",
      "Iteration 5349, loss = 0.47363704\n",
      "Iteration 5350, loss = 0.47397752\n",
      "Iteration 5351, loss = 0.47428395\n",
      "Iteration 5352, loss = 0.47498579\n",
      "Iteration 5353, loss = 0.47493025\n",
      "Iteration 5354, loss = 0.47433144\n",
      "Iteration 5355, loss = 0.47382422\n",
      "Iteration 5356, loss = 0.47434110\n",
      "Iteration 5357, loss = 0.47453375\n",
      "Iteration 5358, loss = 0.47466357\n",
      "Iteration 5359, loss = 0.47451536\n",
      "Iteration 5360, loss = 0.47412755\n",
      "Iteration 5361, loss = 0.47359357\n",
      "Iteration 5362, loss = 0.47441291\n",
      "Iteration 5363, loss = 0.47511788\n",
      "Iteration 5364, loss = 0.47576814\n",
      "Iteration 5365, loss = 0.47572708\n",
      "Iteration 5366, loss = 0.47572566\n",
      "Iteration 5367, loss = 0.47489697\n",
      "Iteration 5368, loss = 0.47439741\n",
      "Iteration 5369, loss = 0.47348096\n",
      "Iteration 5370, loss = 0.47419272\n",
      "Iteration 5371, loss = 0.47442940\n",
      "Iteration 5372, loss = 0.47471512\n",
      "Iteration 5373, loss = 0.47428944\n",
      "Iteration 5374, loss = 0.47363105\n",
      "Iteration 5375, loss = 0.47347725\n",
      "Iteration 5376, loss = 0.47405960\n",
      "Iteration 5377, loss = 0.47428693\n",
      "Iteration 5378, loss = 0.47436065\n",
      "Iteration 5379, loss = 0.47416356\n",
      "Iteration 5380, loss = 0.47395131\n",
      "Iteration 5381, loss = 0.47371053\n",
      "Iteration 5382, loss = 0.47357673\n",
      "Iteration 5383, loss = 0.47355805\n",
      "Iteration 5384, loss = 0.47356961\n",
      "Iteration 5385, loss = 0.47386730\n",
      "Iteration 5386, loss = 0.47403990\n",
      "Iteration 5387, loss = 0.47447349\n",
      "Iteration 5388, loss = 0.47376470\n",
      "Iteration 5389, loss = 0.47333252\n",
      "Iteration 5390, loss = 0.47334473\n",
      "Iteration 5391, loss = 0.47507295\n",
      "Iteration 5392, loss = 0.47597701\n",
      "Iteration 5393, loss = 0.47489447\n",
      "Iteration 5394, loss = 0.47438560\n",
      "Iteration 5395, loss = 0.47371832\n",
      "Iteration 5396, loss = 0.47426010\n",
      "Iteration 5397, loss = 0.47537654\n",
      "Iteration 5398, loss = 0.47582161\n",
      "Iteration 5399, loss = 0.47552689\n",
      "Iteration 5400, loss = 0.47514252\n",
      "Iteration 5401, loss = 0.47413187\n",
      "Iteration 5402, loss = 0.47380410\n",
      "Iteration 5403, loss = 0.47358368\n",
      "Iteration 5404, loss = 0.47416793\n",
      "Iteration 5405, loss = 0.47552478\n",
      "Iteration 5406, loss = 0.47544742\n",
      "Iteration 5407, loss = 0.47486992\n",
      "Iteration 5408, loss = 0.47425543\n",
      "Iteration 5409, loss = 0.47401963\n",
      "Iteration 5410, loss = 0.47347935\n",
      "Iteration 5411, loss = 0.47352579\n",
      "Iteration 5412, loss = 0.47377863\n",
      "Iteration 5413, loss = 0.47498515\n",
      "Iteration 5414, loss = 0.47499521\n",
      "Iteration 5415, loss = 0.47451516\n",
      "Iteration 5416, loss = 0.47382536\n",
      "Iteration 5417, loss = 0.47370631\n",
      "Iteration 5418, loss = 0.47387714\n",
      "Iteration 5419, loss = 0.47492299\n",
      "Iteration 5420, loss = 0.47483134\n",
      "Iteration 5421, loss = 0.47415206\n",
      "Iteration 5422, loss = 0.47388649\n",
      "Iteration 5423, loss = 0.47372136\n",
      "Iteration 5424, loss = 0.47355446\n",
      "Iteration 5425, loss = 0.47395679\n",
      "Iteration 5426, loss = 0.47404103\n",
      "Iteration 5427, loss = 0.47387292\n",
      "Iteration 5428, loss = 0.47404008\n",
      "Iteration 5429, loss = 0.47407971\n",
      "Iteration 5430, loss = 0.47392721\n",
      "Iteration 5431, loss = 0.47360901\n",
      "Iteration 5432, loss = 0.47363178\n",
      "Iteration 5433, loss = 0.47351452\n",
      "Iteration 5434, loss = 0.47378027\n",
      "Iteration 5435, loss = 0.47365622\n",
      "Iteration 5436, loss = 0.47364341\n",
      "Iteration 5437, loss = 0.47392599\n",
      "Iteration 5438, loss = 0.47394073\n",
      "Iteration 5439, loss = 0.47366510\n",
      "Iteration 5440, loss = 0.47352234\n",
      "Iteration 5441, loss = 0.47358505\n",
      "Iteration 5442, loss = 0.47390889\n",
      "Iteration 5443, loss = 0.47382714\n",
      "Iteration 5444, loss = 0.47367158\n",
      "Iteration 5445, loss = 0.47363482\n",
      "Iteration 5446, loss = 0.47358373\n",
      "Iteration 5447, loss = 0.47349169\n",
      "Iteration 5448, loss = 0.47344099\n",
      "Iteration 5449, loss = 0.47346245\n",
      "Iteration 5450, loss = 0.47398047\n",
      "Iteration 5451, loss = 0.47418552\n",
      "Iteration 5452, loss = 0.47424083\n",
      "Iteration 5453, loss = 0.47427167\n",
      "Iteration 5454, loss = 0.47410453\n",
      "Iteration 5455, loss = 0.47382070\n",
      "Iteration 5456, loss = 0.47368727\n",
      "Iteration 5457, loss = 0.47351685\n",
      "Iteration 5458, loss = 0.47353865\n",
      "Iteration 5459, loss = 0.47351513\n",
      "Iteration 5460, loss = 0.47348789\n",
      "Iteration 5461, loss = 0.47344829\n",
      "Iteration 5462, loss = 0.47380671\n",
      "Iteration 5463, loss = 0.47364781\n",
      "Iteration 5464, loss = 0.47348117\n",
      "Iteration 5465, loss = 0.47347286\n",
      "Iteration 5466, loss = 0.47395838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5467, loss = 0.47372752\n",
      "Iteration 5468, loss = 0.47334265\n",
      "Iteration 5469, loss = 0.47398352\n",
      "Iteration 5470, loss = 0.47433455\n",
      "Iteration 5471, loss = 0.47439536\n",
      "Iteration 5472, loss = 0.47388673\n",
      "Iteration 5473, loss = 0.47415617\n",
      "Iteration 5474, loss = 0.47408288\n",
      "Iteration 5475, loss = 0.47372531\n",
      "Iteration 5476, loss = 0.47322105\n",
      "Iteration 5477, loss = 0.47385679\n",
      "Iteration 5478, loss = 0.47438136\n",
      "Iteration 5479, loss = 0.47515728\n",
      "Iteration 5480, loss = 0.47515995\n",
      "Iteration 5481, loss = 0.47442112\n",
      "Iteration 5482, loss = 0.47433302\n",
      "Iteration 5483, loss = 0.47372076\n",
      "Iteration 5484, loss = 0.47350595\n",
      "Iteration 5485, loss = 0.47351315\n",
      "Iteration 5486, loss = 0.47346048\n",
      "Iteration 5487, loss = 0.47344190\n",
      "Iteration 5488, loss = 0.47356685\n",
      "Iteration 5489, loss = 0.47371698\n",
      "Iteration 5490, loss = 0.47353603\n",
      "Iteration 5491, loss = 0.47365924\n",
      "Iteration 5492, loss = 0.47377854\n",
      "Iteration 5493, loss = 0.47389160\n",
      "Iteration 5494, loss = 0.47379699\n",
      "Iteration 5495, loss = 0.47365835\n",
      "Iteration 5496, loss = 0.47356261\n",
      "Iteration 5497, loss = 0.47359238\n",
      "Iteration 5498, loss = 0.47385114\n",
      "Iteration 5499, loss = 0.47410781\n",
      "Iteration 5500, loss = 0.47435390\n",
      "Iteration 5501, loss = 0.47443116\n",
      "Iteration 5502, loss = 0.47426220\n",
      "Iteration 5503, loss = 0.47400725\n",
      "Iteration 5504, loss = 0.47356430\n",
      "Iteration 5505, loss = 0.47398776\n",
      "Iteration 5506, loss = 0.47360747\n",
      "Iteration 5507, loss = 0.47357537\n",
      "Iteration 5508, loss = 0.47358967\n",
      "Iteration 5509, loss = 0.47350428\n",
      "Iteration 5510, loss = 0.47365455\n",
      "Iteration 5511, loss = 0.47408434\n",
      "Iteration 5512, loss = 0.47396639\n",
      "Iteration 5513, loss = 0.47374853\n",
      "Iteration 5514, loss = 0.47366125\n",
      "Iteration 5515, loss = 0.47350274\n",
      "Iteration 5516, loss = 0.47340579\n",
      "Iteration 5517, loss = 0.47359767\n",
      "Iteration 5518, loss = 0.47467359\n",
      "Iteration 5519, loss = 0.47493664\n",
      "Iteration 5520, loss = 0.47482241\n",
      "Iteration 5521, loss = 0.47437492\n",
      "Iteration 5522, loss = 0.47393585\n",
      "Iteration 5523, loss = 0.47415850\n",
      "Iteration 5524, loss = 0.47348556\n",
      "Iteration 5525, loss = 0.47348285\n",
      "Iteration 5526, loss = 0.47362277\n",
      "Iteration 5527, loss = 0.47352315\n",
      "Iteration 5528, loss = 0.47377202\n",
      "Iteration 5529, loss = 0.47343774\n",
      "Iteration 5530, loss = 0.47343066\n",
      "Iteration 5531, loss = 0.47357950\n",
      "Iteration 5532, loss = 0.47343631\n",
      "Iteration 5533, loss = 0.47351450\n",
      "Iteration 5534, loss = 0.47418841\n",
      "Iteration 5535, loss = 0.47495917\n",
      "Iteration 5536, loss = 0.47451483\n",
      "Iteration 5537, loss = 0.47417322\n",
      "Iteration 5538, loss = 0.47348606\n",
      "Iteration 5539, loss = 0.47341733\n",
      "Iteration 5540, loss = 0.47343065\n",
      "Iteration 5541, loss = 0.47358141\n",
      "Iteration 5542, loss = 0.47374138\n",
      "Iteration 5543, loss = 0.47372475\n",
      "Iteration 5544, loss = 0.47358681\n",
      "Iteration 5545, loss = 0.47353075\n",
      "Iteration 5546, loss = 0.47359931\n",
      "Iteration 5547, loss = 0.47379104\n",
      "Iteration 5548, loss = 0.47338005\n",
      "Iteration 5549, loss = 0.47342721\n",
      "Iteration 5550, loss = 0.47442552\n",
      "Iteration 5551, loss = 0.47502510\n",
      "Iteration 5552, loss = 0.47441898\n",
      "Iteration 5553, loss = 0.47371544\n",
      "Iteration 5554, loss = 0.47377635\n",
      "Iteration 5555, loss = 0.47406596\n",
      "Iteration 5556, loss = 0.47431912\n",
      "Iteration 5557, loss = 0.47429677\n",
      "Iteration 5558, loss = 0.47396122\n",
      "Iteration 5559, loss = 0.47354598\n",
      "Iteration 5560, loss = 0.47352749\n",
      "Iteration 5561, loss = 0.47363712\n",
      "Iteration 5562, loss = 0.47399492\n",
      "Iteration 5563, loss = 0.47420970\n",
      "Iteration 5564, loss = 0.47415773\n",
      "Iteration 5565, loss = 0.47441192\n",
      "Iteration 5566, loss = 0.47381307\n",
      "Iteration 5567, loss = 0.47430089\n",
      "Iteration 5568, loss = 0.47397851\n",
      "Iteration 5569, loss = 0.47366252\n",
      "Iteration 5570, loss = 0.47338979\n",
      "Iteration 5571, loss = 0.47361186\n",
      "Iteration 5572, loss = 0.47490315\n",
      "Iteration 5573, loss = 0.47566207\n",
      "Iteration 5574, loss = 0.47619292\n",
      "Iteration 5575, loss = 0.47660723\n",
      "Iteration 5576, loss = 0.47631322\n",
      "Iteration 5577, loss = 0.47531368\n",
      "Iteration 5578, loss = 0.47444210\n",
      "Iteration 5579, loss = 0.47352531\n",
      "Iteration 5580, loss = 0.47398969\n",
      "Iteration 5581, loss = 0.47415459\n",
      "Iteration 5582, loss = 0.47406191\n",
      "Iteration 5583, loss = 0.47451793\n",
      "Iteration 5584, loss = 0.47390649\n",
      "Iteration 5585, loss = 0.47354330\n",
      "Iteration 5586, loss = 0.47375319\n",
      "Iteration 5587, loss = 0.47345696\n",
      "Iteration 5588, loss = 0.47366595\n",
      "Iteration 5589, loss = 0.47351712\n",
      "Iteration 5590, loss = 0.47361042\n",
      "Iteration 5591, loss = 0.47383624\n",
      "Iteration 5592, loss = 0.47415945\n",
      "Iteration 5593, loss = 0.47445427\n",
      "Iteration 5594, loss = 0.47432418\n",
      "Iteration 5595, loss = 0.47342725\n",
      "Iteration 5596, loss = 0.47316881\n",
      "Iteration 5597, loss = 0.47466902\n",
      "Iteration 5598, loss = 0.47627203\n",
      "Iteration 5599, loss = 0.47612136\n",
      "Iteration 5600, loss = 0.47501282\n",
      "Iteration 5601, loss = 0.47401048\n",
      "Iteration 5602, loss = 0.47352218\n",
      "Iteration 5603, loss = 0.47358121\n",
      "Iteration 5604, loss = 0.47345447\n",
      "Iteration 5605, loss = 0.47383378\n",
      "Iteration 5606, loss = 0.47359825\n",
      "Iteration 5607, loss = 0.47371272\n",
      "Iteration 5608, loss = 0.47372707\n",
      "Iteration 5609, loss = 0.47417131\n",
      "Iteration 5610, loss = 0.47413388\n",
      "Iteration 5611, loss = 0.47369824\n",
      "Iteration 5612, loss = 0.47364135\n",
      "Iteration 5613, loss = 0.47358706\n",
      "Iteration 5614, loss = 0.47362544\n",
      "Iteration 5615, loss = 0.47362120\n",
      "Iteration 5616, loss = 0.47354714\n",
      "Iteration 5617, loss = 0.47353714\n",
      "Iteration 5618, loss = 0.47420702\n",
      "Iteration 5619, loss = 0.47347655\n",
      "Iteration 5620, loss = 0.47368801\n",
      "Iteration 5621, loss = 0.47410875\n",
      "Iteration 5622, loss = 0.47392531\n",
      "Iteration 5623, loss = 0.47397629\n",
      "Iteration 5624, loss = 0.47353887\n",
      "Iteration 5625, loss = 0.47359977\n",
      "Iteration 5626, loss = 0.47382290\n",
      "Iteration 5627, loss = 0.47377226\n",
      "Iteration 5628, loss = 0.47356562\n",
      "Iteration 5629, loss = 0.47348390\n",
      "Iteration 5630, loss = 0.47421378\n",
      "Iteration 5631, loss = 0.47416367\n",
      "Iteration 5632, loss = 0.47407648\n",
      "Iteration 5633, loss = 0.47405455\n",
      "Iteration 5634, loss = 0.47376667\n",
      "Iteration 5635, loss = 0.47374386\n",
      "Iteration 5636, loss = 0.47349135\n",
      "Iteration 5637, loss = 0.47327850\n",
      "Iteration 5638, loss = 0.47323142\n",
      "Iteration 5639, loss = 0.47435773\n",
      "Iteration 5640, loss = 0.47425797\n",
      "Iteration 5641, loss = 0.47385490\n",
      "Iteration 5642, loss = 0.47427987\n",
      "Iteration 5643, loss = 0.47395610\n",
      "Iteration 5644, loss = 0.47452691\n",
      "Iteration 5645, loss = 0.47446642\n",
      "Iteration 5646, loss = 0.47424048\n",
      "Iteration 5647, loss = 0.47394290\n",
      "Iteration 5648, loss = 0.47425337\n",
      "Iteration 5649, loss = 0.47441711\n",
      "Iteration 5650, loss = 0.47423947\n",
      "Iteration 5651, loss = 0.47388573\n",
      "Iteration 5652, loss = 0.47396142\n",
      "Iteration 5653, loss = 0.47366872\n",
      "Iteration 5654, loss = 0.47350487\n",
      "Iteration 5655, loss = 0.47425171\n",
      "Iteration 5656, loss = 0.47399755\n",
      "Iteration 5657, loss = 0.47386131\n",
      "Iteration 5658, loss = 0.47435363\n",
      "Iteration 5659, loss = 0.47421567\n",
      "Iteration 5660, loss = 0.47432342\n",
      "Iteration 5661, loss = 0.47448372\n",
      "Iteration 5662, loss = 0.47460996\n",
      "Iteration 5663, loss = 0.47472618\n",
      "Iteration 5664, loss = 0.47493907\n",
      "Iteration 5665, loss = 0.47466363\n",
      "Iteration 5666, loss = 0.47427576\n",
      "Iteration 5667, loss = 0.47395070\n",
      "Iteration 5668, loss = 0.47400926\n",
      "Iteration 5669, loss = 0.47405071\n",
      "Iteration 5670, loss = 0.47371441\n",
      "Iteration 5671, loss = 0.47365850\n",
      "Iteration 5672, loss = 0.47347393\n",
      "Iteration 5673, loss = 0.47462387\n",
      "Iteration 5674, loss = 0.47511399\n",
      "Iteration 5675, loss = 0.47488249\n",
      "Iteration 5676, loss = 0.47491396\n",
      "Iteration 5677, loss = 0.47405614\n",
      "Iteration 5678, loss = 0.47361423\n",
      "Iteration 5679, loss = 0.47382589\n",
      "Iteration 5680, loss = 0.47361405\n",
      "Iteration 5681, loss = 0.47344455\n",
      "Iteration 5682, loss = 0.47339600\n",
      "Iteration 5683, loss = 0.47415963\n",
      "Iteration 5684, loss = 0.47420470\n",
      "Iteration 5685, loss = 0.47381364\n",
      "Iteration 5686, loss = 0.47364833\n",
      "Iteration 5687, loss = 0.47356862\n",
      "Iteration 5688, loss = 0.47395451\n",
      "Iteration 5689, loss = 0.47599438\n",
      "Iteration 5690, loss = 0.47651766\n",
      "Iteration 5691, loss = 0.47633369\n",
      "Iteration 5692, loss = 0.47608915\n",
      "Iteration 5693, loss = 0.47501939\n",
      "Iteration 5694, loss = 0.47434823\n",
      "Iteration 5695, loss = 0.47364417\n",
      "Iteration 5696, loss = 0.47356337\n",
      "Iteration 5697, loss = 0.47399570\n",
      "Iteration 5698, loss = 0.47410086\n",
      "Iteration 5699, loss = 0.47408352\n",
      "Iteration 5700, loss = 0.47364092\n",
      "Iteration 5701, loss = 0.47360951\n",
      "Iteration 5702, loss = 0.47351164\n",
      "Iteration 5703, loss = 0.47333952\n",
      "Iteration 5704, loss = 0.47356547\n",
      "Iteration 5705, loss = 0.47374807\n",
      "Iteration 5706, loss = 0.47426510\n",
      "Iteration 5707, loss = 0.47413559\n",
      "Iteration 5708, loss = 0.47366901\n",
      "Iteration 5709, loss = 0.47338522\n",
      "Iteration 5710, loss = 0.47357251\n",
      "Iteration 5711, loss = 0.47417103\n",
      "Iteration 5712, loss = 0.47373884\n",
      "Iteration 5713, loss = 0.47340345\n",
      "Iteration 5714, loss = 0.47364437\n",
      "Iteration 5715, loss = 0.47390517\n",
      "Iteration 5716, loss = 0.47420447\n",
      "Iteration 5717, loss = 0.47448952\n",
      "Iteration 5718, loss = 0.47392154\n",
      "Iteration 5719, loss = 0.47331643\n",
      "Iteration 5720, loss = 0.47402479\n",
      "Iteration 5721, loss = 0.47440210\n",
      "Iteration 5722, loss = 0.47490193\n",
      "Iteration 5723, loss = 0.47524170\n",
      "Iteration 5724, loss = 0.47529185\n",
      "Iteration 5725, loss = 0.47483869\n",
      "Iteration 5726, loss = 0.47424470\n",
      "Iteration 5727, loss = 0.47361328\n",
      "Iteration 5728, loss = 0.47363734\n",
      "Iteration 5729, loss = 0.47354729\n",
      "Iteration 5730, loss = 0.47362691\n",
      "Iteration 5731, loss = 0.47348323\n",
      "Iteration 5732, loss = 0.47348417\n",
      "Iteration 5733, loss = 0.47322762\n",
      "Iteration 5734, loss = 0.47382607\n",
      "Iteration 5735, loss = 0.47599721\n",
      "Iteration 5736, loss = 0.47926051\n",
      "Iteration 5737, loss = 0.48032020\n",
      "Iteration 5738, loss = 0.47955734\n",
      "Iteration 5739, loss = 0.47750221\n",
      "Iteration 5740, loss = 0.47547290\n",
      "Iteration 5741, loss = 0.47383246\n",
      "Iteration 5742, loss = 0.47345791\n",
      "Iteration 5743, loss = 0.47404439\n",
      "Iteration 5744, loss = 0.47485651\n",
      "Iteration 5745, loss = 0.47485438\n",
      "Iteration 5746, loss = 0.47447297\n",
      "Iteration 5747, loss = 0.47404146\n",
      "Iteration 5748, loss = 0.47382245\n",
      "Iteration 5749, loss = 0.47359516\n",
      "Iteration 5750, loss = 0.47377938\n",
      "Iteration 5751, loss = 0.47449903\n",
      "Iteration 5752, loss = 0.47407269\n",
      "Iteration 5753, loss = 0.47342049\n",
      "Iteration 5754, loss = 0.47378172\n",
      "Iteration 5755, loss = 0.47532459\n",
      "Iteration 5756, loss = 0.47586807\n",
      "Iteration 5757, loss = 0.47573340\n",
      "Iteration 5758, loss = 0.47472989\n",
      "Iteration 5759, loss = 0.47419107\n",
      "Iteration 5760, loss = 0.47363723\n",
      "Iteration 5761, loss = 0.47377172\n",
      "Iteration 5762, loss = 0.47475086\n",
      "Iteration 5763, loss = 0.47560738\n",
      "Iteration 5764, loss = 0.47499387\n",
      "Iteration 5765, loss = 0.47448743\n",
      "Iteration 5766, loss = 0.47350578\n",
      "Iteration 5767, loss = 0.47345498\n",
      "Iteration 5768, loss = 0.47343143\n",
      "Iteration 5769, loss = 0.47347477\n",
      "Iteration 5770, loss = 0.47348045\n",
      "Iteration 5771, loss = 0.47412594\n",
      "Iteration 5772, loss = 0.47456616\n",
      "Iteration 5773, loss = 0.47464712\n",
      "Iteration 5774, loss = 0.47460449\n",
      "Iteration 5775, loss = 0.47453806\n",
      "Iteration 5776, loss = 0.47433290\n",
      "Iteration 5777, loss = 0.47402549\n",
      "Iteration 5778, loss = 0.47379868\n",
      "Iteration 5779, loss = 0.47354859\n",
      "Iteration 5780, loss = 0.47339086\n",
      "Iteration 5781, loss = 0.47330676\n",
      "Iteration 5782, loss = 0.47371676\n",
      "Iteration 5783, loss = 0.47387750\n",
      "Iteration 5784, loss = 0.47404164\n",
      "Iteration 5785, loss = 0.47407392\n",
      "Iteration 5786, loss = 0.47444223\n",
      "Iteration 5787, loss = 0.47473262\n",
      "Iteration 5788, loss = 0.47496182\n",
      "Iteration 5789, loss = 0.47513350\n",
      "Iteration 5790, loss = 0.47550949\n",
      "Iteration 5791, loss = 0.47503741\n",
      "Iteration 5792, loss = 0.47437353\n",
      "Iteration 5793, loss = 0.47306002\n",
      "Iteration 5794, loss = 0.47292286\n",
      "Iteration 5795, loss = 0.47479048\n",
      "Iteration 5796, loss = 0.47685416\n",
      "Iteration 5797, loss = 0.47853465\n",
      "Iteration 5798, loss = 0.47904445\n",
      "Iteration 5799, loss = 0.47821380\n",
      "Iteration 5800, loss = 0.47723623\n",
      "Iteration 5801, loss = 0.47517242\n",
      "Iteration 5802, loss = 0.47364504\n",
      "Iteration 5803, loss = 0.47412705\n",
      "Iteration 5804, loss = 0.47695734\n",
      "Iteration 5805, loss = 0.47824503\n",
      "Iteration 5806, loss = 0.47840396\n",
      "Iteration 5807, loss = 0.47763114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5808, loss = 0.47538077\n",
      "Iteration 5809, loss = 0.47453225\n",
      "Iteration 5810, loss = 0.47440183\n",
      "Iteration 5811, loss = 0.47568170\n",
      "Iteration 5812, loss = 0.47690411\n",
      "Iteration 5813, loss = 0.47740391\n",
      "Iteration 5814, loss = 0.47693902\n",
      "Iteration 5815, loss = 0.47640647\n",
      "Iteration 5816, loss = 0.47487325\n",
      "Iteration 5817, loss = 0.47484972\n",
      "Iteration 5818, loss = 0.47375439\n",
      "Iteration 5819, loss = 0.47364871\n",
      "Iteration 5820, loss = 0.47404855\n",
      "Iteration 5821, loss = 0.47383937\n",
      "Iteration 5822, loss = 0.47379581\n",
      "Iteration 5823, loss = 0.47399733\n",
      "Iteration 5824, loss = 0.47439850\n",
      "Iteration 5825, loss = 0.47579051\n",
      "Iteration 5826, loss = 0.47717854\n",
      "Iteration 5827, loss = 0.47689118\n",
      "Iteration 5828, loss = 0.47587720\n",
      "Iteration 5829, loss = 0.47461206\n",
      "Iteration 5830, loss = 0.47413028\n",
      "Iteration 5831, loss = 0.47409299\n",
      "Iteration 5832, loss = 0.47379474\n",
      "Iteration 5833, loss = 0.47382344\n",
      "Iteration 5834, loss = 0.47388428\n",
      "Iteration 5835, loss = 0.47376103\n",
      "Iteration 5836, loss = 0.47344669\n",
      "Iteration 5837, loss = 0.47369186\n",
      "Iteration 5838, loss = 0.47439548\n",
      "Iteration 5839, loss = 0.47449917\n",
      "Iteration 5840, loss = 0.47417963\n",
      "Iteration 5841, loss = 0.47413049\n",
      "Iteration 5842, loss = 0.47371860\n",
      "Iteration 5843, loss = 0.47357647\n",
      "Iteration 5844, loss = 0.47340212\n",
      "Iteration 5845, loss = 0.47341433\n",
      "Iteration 5846, loss = 0.47329028\n",
      "Iteration 5847, loss = 0.47335007\n",
      "Iteration 5848, loss = 0.47339543\n",
      "Iteration 5849, loss = 0.47338249\n",
      "Iteration 5850, loss = 0.47345721\n",
      "Iteration 5851, loss = 0.47346251\n",
      "Iteration 5852, loss = 0.47347070\n",
      "Iteration 5853, loss = 0.47387091\n",
      "Iteration 5854, loss = 0.47343276\n",
      "Iteration 5855, loss = 0.47343621\n",
      "Iteration 5856, loss = 0.47357797\n",
      "Iteration 5857, loss = 0.47341458\n",
      "Iteration 5858, loss = 0.47320911\n",
      "Iteration 5859, loss = 0.47327342\n",
      "Iteration 5860, loss = 0.47431127\n",
      "Iteration 5861, loss = 0.47575925\n",
      "Iteration 5862, loss = 0.47693195\n",
      "Iteration 5863, loss = 0.47595832\n",
      "Iteration 5864, loss = 0.47324733\n",
      "Iteration 5865, loss = 0.47369112\n",
      "Iteration 5866, loss = 0.47574156\n",
      "Iteration 5867, loss = 0.47891088\n",
      "Iteration 5868, loss = 0.47869345\n",
      "Iteration 5869, loss = 0.47596891\n",
      "Iteration 5870, loss = 0.47412108\n",
      "Iteration 5871, loss = 0.47334240\n",
      "Iteration 5872, loss = 0.47448263\n",
      "Iteration 5873, loss = 0.47497000\n",
      "Iteration 5874, loss = 0.47501592\n",
      "Iteration 5875, loss = 0.47472639\n",
      "Iteration 5876, loss = 0.47436760\n",
      "Iteration 5877, loss = 0.47359340\n",
      "Iteration 5878, loss = 0.47354925\n",
      "Iteration 5879, loss = 0.47391846\n",
      "Iteration 5880, loss = 0.47429577\n",
      "Iteration 5881, loss = 0.47414862\n",
      "Iteration 5882, loss = 0.47369359\n",
      "Iteration 5883, loss = 0.47411418\n",
      "Iteration 5884, loss = 0.47362358\n",
      "Iteration 5885, loss = 0.47360320\n",
      "Iteration 5886, loss = 0.47380971\n",
      "Iteration 5887, loss = 0.47356476\n",
      "Iteration 5888, loss = 0.47356276\n",
      "Iteration 5889, loss = 0.47481195\n",
      "Iteration 5890, loss = 0.47525853\n",
      "Iteration 5891, loss = 0.47493498\n",
      "Iteration 5892, loss = 0.47385525\n",
      "Iteration 5893, loss = 0.47414770\n",
      "Iteration 5894, loss = 0.47339663\n",
      "Iteration 5895, loss = 0.47343261\n",
      "Iteration 5896, loss = 0.47356239\n",
      "Iteration 5897, loss = 0.47344973\n",
      "Iteration 5898, loss = 0.47335254\n",
      "Iteration 5899, loss = 0.47329539\n",
      "Iteration 5900, loss = 0.47338654\n",
      "Iteration 5901, loss = 0.47323980\n",
      "Iteration 5902, loss = 0.47385659\n",
      "Iteration 5903, loss = 0.47540276\n",
      "Iteration 5904, loss = 0.47541257\n",
      "Iteration 5905, loss = 0.47456204\n",
      "Iteration 5906, loss = 0.47322900\n",
      "Iteration 5907, loss = 0.47333411\n",
      "Iteration 5908, loss = 0.47438403\n",
      "Iteration 5909, loss = 0.47485429\n",
      "Iteration 5910, loss = 0.47440528\n",
      "Iteration 5911, loss = 0.47348540\n",
      "Iteration 5912, loss = 0.47324454\n",
      "Iteration 5913, loss = 0.47336482\n",
      "Iteration 5914, loss = 0.47407309\n",
      "Iteration 5915, loss = 0.47476615\n",
      "Iteration 5916, loss = 0.47455471\n",
      "Iteration 5917, loss = 0.47329329\n",
      "Iteration 5918, loss = 0.47396380\n",
      "Iteration 5919, loss = 0.47533405\n",
      "Iteration 5920, loss = 0.47628024\n",
      "Iteration 5921, loss = 0.47659503\n",
      "Iteration 5922, loss = 0.47551061\n",
      "Iteration 5923, loss = 0.47354347\n",
      "Iteration 5924, loss = 0.47295612\n",
      "Iteration 5925, loss = 0.47521710\n",
      "Iteration 5926, loss = 0.47647091\n",
      "Iteration 5927, loss = 0.47689603\n",
      "Iteration 5928, loss = 0.47670939\n",
      "Iteration 5929, loss = 0.47621866\n",
      "Iteration 5930, loss = 0.47564130\n",
      "Iteration 5931, loss = 0.47486331\n",
      "Iteration 5932, loss = 0.47399976\n",
      "Iteration 5933, loss = 0.47390028\n",
      "Iteration 5934, loss = 0.47325892\n",
      "Iteration 5935, loss = 0.47329043\n",
      "Iteration 5936, loss = 0.47350901\n",
      "Iteration 5937, loss = 0.47353914\n",
      "Iteration 5938, loss = 0.47348775\n",
      "Iteration 5939, loss = 0.47347259\n",
      "Iteration 5940, loss = 0.47335206\n",
      "Iteration 5941, loss = 0.47341528\n",
      "Iteration 5942, loss = 0.47327917\n",
      "Iteration 5943, loss = 0.47351683\n",
      "Iteration 5944, loss = 0.47355603\n",
      "Iteration 5945, loss = 0.47365567\n",
      "Iteration 5946, loss = 0.47316952\n",
      "Iteration 5947, loss = 0.47337374\n",
      "Iteration 5948, loss = 0.47416443\n",
      "Iteration 5949, loss = 0.47496784\n",
      "Iteration 5950, loss = 0.47422674\n",
      "Iteration 5951, loss = 0.47376558\n",
      "Iteration 5952, loss = 0.47323180\n",
      "Iteration 5953, loss = 0.47383041\n",
      "Iteration 5954, loss = 0.47491255\n",
      "Iteration 5955, loss = 0.47596021\n",
      "Iteration 5956, loss = 0.47694298\n",
      "Iteration 5957, loss = 0.47606053\n",
      "Iteration 5958, loss = 0.47465536\n",
      "Iteration 5959, loss = 0.47300439\n",
      "Iteration 5960, loss = 0.47331273\n",
      "Iteration 5961, loss = 0.47463096\n",
      "Iteration 5962, loss = 0.47556922\n",
      "Iteration 5963, loss = 0.47594396\n",
      "Iteration 5964, loss = 0.47496794\n",
      "Iteration 5965, loss = 0.47359114\n",
      "Iteration 5966, loss = 0.47302772\n",
      "Iteration 5967, loss = 0.47373475\n",
      "Iteration 5968, loss = 0.47455845\n",
      "Iteration 5969, loss = 0.47528624\n",
      "Iteration 5970, loss = 0.47508906\n",
      "Iteration 5971, loss = 0.47404535\n",
      "Iteration 5972, loss = 0.47319928\n",
      "Iteration 5973, loss = 0.47346166\n",
      "Iteration 5974, loss = 0.47401541\n",
      "Iteration 5975, loss = 0.47458795\n",
      "Iteration 5976, loss = 0.47517749\n",
      "Iteration 5977, loss = 0.47616821\n",
      "Iteration 5978, loss = 0.47639032\n",
      "Iteration 5979, loss = 0.47569764\n",
      "Iteration 5980, loss = 0.47553160\n",
      "Iteration 5981, loss = 0.47402587\n",
      "Iteration 5982, loss = 0.47431453\n",
      "Iteration 5983, loss = 0.47498282\n",
      "Iteration 5984, loss = 0.47493120\n",
      "Iteration 5985, loss = 0.47417621\n",
      "Iteration 5986, loss = 0.47380399\n",
      "Iteration 5987, loss = 0.47406613\n",
      "Iteration 5988, loss = 0.47372642\n",
      "Iteration 5989, loss = 0.47356848\n",
      "Iteration 5990, loss = 0.47378241\n",
      "Iteration 5991, loss = 0.47373451\n",
      "Iteration 5992, loss = 0.47365527\n",
      "Iteration 5993, loss = 0.47358458\n",
      "Iteration 5994, loss = 0.47346577\n",
      "Iteration 5995, loss = 0.47360035\n",
      "Iteration 5996, loss = 0.47402161\n",
      "Iteration 5997, loss = 0.47423067\n",
      "Iteration 5998, loss = 0.47357731\n",
      "Iteration 5999, loss = 0.47371421\n",
      "Iteration 6000, loss = 0.47317416\n",
      "Iteration 6001, loss = 0.47374239\n",
      "Iteration 6002, loss = 0.47421767\n",
      "Iteration 6003, loss = 0.47426962\n",
      "Iteration 6004, loss = 0.47366583\n",
      "Iteration 6005, loss = 0.47342551\n",
      "Iteration 6006, loss = 0.47354335\n",
      "Iteration 6007, loss = 0.47364817\n",
      "Iteration 6008, loss = 0.47332008\n",
      "Iteration 6009, loss = 0.47308214\n",
      "Iteration 6010, loss = 0.47346319\n",
      "Iteration 6011, loss = 0.47490225\n",
      "Iteration 6012, loss = 0.47606729\n",
      "Iteration 6013, loss = 0.47605947\n",
      "Iteration 6014, loss = 0.47523993\n",
      "Iteration 6015, loss = 0.47437572\n",
      "Iteration 6016, loss = 0.47347255\n",
      "Iteration 6017, loss = 0.47393451\n",
      "Iteration 6018, loss = 0.47414961\n",
      "Iteration 6019, loss = 0.47444500\n",
      "Iteration 6020, loss = 0.47438333\n",
      "Iteration 6021, loss = 0.47375871\n",
      "Iteration 6022, loss = 0.47352456\n",
      "Iteration 6023, loss = 0.47368347\n",
      "Iteration 6024, loss = 0.47384384\n",
      "Iteration 6025, loss = 0.47376342\n",
      "Iteration 6026, loss = 0.47364424\n",
      "Iteration 6027, loss = 0.47345530\n",
      "Iteration 6028, loss = 0.47319228\n",
      "Iteration 6029, loss = 0.47309729\n",
      "Iteration 6030, loss = 0.47346156\n",
      "Iteration 6031, loss = 0.47451442\n",
      "Iteration 6032, loss = 0.47505326\n",
      "Iteration 6033, loss = 0.47521294\n",
      "Iteration 6034, loss = 0.47446664\n",
      "Iteration 6035, loss = 0.47380806\n",
      "Iteration 6036, loss = 0.47343949\n",
      "Iteration 6037, loss = 0.47332063\n",
      "Iteration 6038, loss = 0.47322768\n",
      "Iteration 6039, loss = 0.47327616\n",
      "Iteration 6040, loss = 0.47368159\n",
      "Iteration 6041, loss = 0.47443119\n",
      "Iteration 6042, loss = 0.47448618\n",
      "Iteration 6043, loss = 0.47443496\n",
      "Iteration 6044, loss = 0.47402432\n",
      "Iteration 6045, loss = 0.47409675\n",
      "Iteration 6046, loss = 0.47381569\n",
      "Iteration 6047, loss = 0.47388991\n",
      "Iteration 6048, loss = 0.47400928\n",
      "Iteration 6049, loss = 0.47371265\n",
      "Iteration 6050, loss = 0.47310654\n",
      "Iteration 6051, loss = 0.47380876\n",
      "Iteration 6052, loss = 0.47365892\n",
      "Iteration 6053, loss = 0.47371809\n",
      "Iteration 6054, loss = 0.47357778\n",
      "Iteration 6055, loss = 0.47355906\n",
      "Iteration 6056, loss = 0.47339102\n",
      "Iteration 6057, loss = 0.47351995\n",
      "Iteration 6058, loss = 0.47342433\n",
      "Iteration 6059, loss = 0.47341961\n",
      "Iteration 6060, loss = 0.47343617\n",
      "Iteration 6061, loss = 0.47341892\n",
      "Iteration 6062, loss = 0.47337667\n",
      "Iteration 6063, loss = 0.47328166\n",
      "Iteration 6064, loss = 0.47323240\n",
      "Iteration 6065, loss = 0.47328005\n",
      "Iteration 6066, loss = 0.47347671\n",
      "Iteration 6067, loss = 0.47359604\n",
      "Iteration 6068, loss = 0.47545628\n",
      "Iteration 6069, loss = 0.47656714\n",
      "Iteration 6070, loss = 0.47588057\n",
      "Iteration 6071, loss = 0.47493789\n",
      "Iteration 6072, loss = 0.47425527\n",
      "Iteration 6073, loss = 0.47421839\n",
      "Iteration 6074, loss = 0.47426345\n",
      "Iteration 6075, loss = 0.47447818\n",
      "Iteration 6076, loss = 0.47447284\n",
      "Iteration 6077, loss = 0.47398115\n",
      "Iteration 6078, loss = 0.47357895\n",
      "Iteration 6079, loss = 0.47362344\n",
      "Iteration 6080, loss = 0.47364801\n",
      "Iteration 6081, loss = 0.47400573\n",
      "Iteration 6082, loss = 0.47352364\n",
      "Iteration 6083, loss = 0.47290883\n",
      "Iteration 6084, loss = 0.47372579\n",
      "Iteration 6085, loss = 0.47417271\n",
      "Iteration 6086, loss = 0.47569163\n",
      "Iteration 6087, loss = 0.47541760\n",
      "Iteration 6088, loss = 0.47412521\n",
      "Iteration 6089, loss = 0.47425272\n",
      "Iteration 6090, loss = 0.47369988\n",
      "Iteration 6091, loss = 0.47398970\n",
      "Iteration 6092, loss = 0.47584993\n",
      "Iteration 6093, loss = 0.47522944\n",
      "Iteration 6094, loss = 0.47421096\n",
      "Iteration 6095, loss = 0.47351252\n",
      "Iteration 6096, loss = 0.47361592\n",
      "Iteration 6097, loss = 0.47392798\n",
      "Iteration 6098, loss = 0.47440876\n",
      "Iteration 6099, loss = 0.47449611\n",
      "Iteration 6100, loss = 0.47404485\n",
      "Iteration 6101, loss = 0.47386418\n",
      "Iteration 6102, loss = 0.47355901\n",
      "Iteration 6103, loss = 0.47355851\n",
      "Iteration 6104, loss = 0.47339583\n",
      "Iteration 6105, loss = 0.47315923\n",
      "Iteration 6106, loss = 0.47357633\n",
      "Iteration 6107, loss = 0.47339253\n",
      "Iteration 6108, loss = 0.47356159\n",
      "Iteration 6109, loss = 0.47365469\n",
      "Iteration 6110, loss = 0.47391247\n",
      "Iteration 6111, loss = 0.47368927\n",
      "Iteration 6112, loss = 0.47345590\n",
      "Iteration 6113, loss = 0.47334123\n",
      "Iteration 6114, loss = 0.47386075\n",
      "Iteration 6115, loss = 0.47601308\n",
      "Iteration 6116, loss = 0.47702770\n",
      "Iteration 6117, loss = 0.47722782\n",
      "Iteration 6118, loss = 0.47769317\n",
      "Iteration 6119, loss = 0.47715400\n",
      "Iteration 6120, loss = 0.47592088\n",
      "Iteration 6121, loss = 0.47358476\n",
      "Iteration 6122, loss = 0.47343398\n",
      "Iteration 6123, loss = 0.47338523\n",
      "Iteration 6124, loss = 0.47571050\n",
      "Iteration 6125, loss = 0.47612840\n",
      "Iteration 6126, loss = 0.47524519\n",
      "Iteration 6127, loss = 0.47433488\n",
      "Iteration 6128, loss = 0.47332639\n",
      "Iteration 6129, loss = 0.47306889\n",
      "Iteration 6130, loss = 0.47392695\n",
      "Iteration 6131, loss = 0.47449526\n",
      "Iteration 6132, loss = 0.47499237\n",
      "Iteration 6133, loss = 0.47508320\n",
      "Iteration 6134, loss = 0.47419651\n",
      "Iteration 6135, loss = 0.47350654\n",
      "Iteration 6136, loss = 0.47313538\n",
      "Iteration 6137, loss = 0.47377992\n",
      "Iteration 6138, loss = 0.47414347\n",
      "Iteration 6139, loss = 0.47424676\n",
      "Iteration 6140, loss = 0.47417433\n",
      "Iteration 6141, loss = 0.47414390\n",
      "Iteration 6142, loss = 0.47333535\n",
      "Iteration 6143, loss = 0.47371297\n",
      "Iteration 6144, loss = 0.47344535\n",
      "Iteration 6145, loss = 0.47324622\n",
      "Iteration 6146, loss = 0.47371773\n",
      "Iteration 6147, loss = 0.47357594\n",
      "Iteration 6148, loss = 0.47333911\n",
      "Iteration 6149, loss = 0.47334333\n",
      "Iteration 6150, loss = 0.47322873\n",
      "Iteration 6151, loss = 0.47326647\n",
      "Iteration 6152, loss = 0.47332708\n",
      "Iteration 6153, loss = 0.47349355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6154, loss = 0.47348988\n",
      "Iteration 6155, loss = 0.47350017\n",
      "Iteration 6156, loss = 0.47339677\n",
      "Iteration 6157, loss = 0.47326200\n",
      "Iteration 6158, loss = 0.47317022\n",
      "Iteration 6159, loss = 0.47323519\n",
      "Iteration 6160, loss = 0.47374566\n",
      "Iteration 6161, loss = 0.47453911\n",
      "Iteration 6162, loss = 0.47502756\n",
      "Iteration 6163, loss = 0.47425430\n",
      "Iteration 6164, loss = 0.47347847\n",
      "Iteration 6165, loss = 0.47321800\n",
      "Iteration 6166, loss = 0.47333688\n",
      "Iteration 6167, loss = 0.47379620\n",
      "Iteration 6168, loss = 0.47513531\n",
      "Iteration 6169, loss = 0.47604757\n",
      "Iteration 6170, loss = 0.47545876\n",
      "Iteration 6171, loss = 0.47466829\n",
      "Iteration 6172, loss = 0.47386641\n",
      "Iteration 6173, loss = 0.47338973\n",
      "Iteration 6174, loss = 0.47332972\n",
      "Iteration 6175, loss = 0.47333047\n",
      "Iteration 6176, loss = 0.47334479\n",
      "Iteration 6177, loss = 0.47332057\n",
      "Iteration 6178, loss = 0.47332243\n",
      "Iteration 6179, loss = 0.47358493\n",
      "Iteration 6180, loss = 0.47328693\n",
      "Iteration 6181, loss = 0.47311223\n",
      "Iteration 6182, loss = 0.47322618\n",
      "Iteration 6183, loss = 0.47378328\n",
      "Iteration 6184, loss = 0.47381750\n",
      "Iteration 6185, loss = 0.47345405\n",
      "Iteration 6186, loss = 0.47445724\n",
      "Iteration 6187, loss = 0.47359449\n",
      "Iteration 6188, loss = 0.47337055\n",
      "Iteration 6189, loss = 0.47333388\n",
      "Iteration 6190, loss = 0.47345750\n",
      "Iteration 6191, loss = 0.47398061\n",
      "Iteration 6192, loss = 0.47395794\n",
      "Iteration 6193, loss = 0.47372713\n",
      "Iteration 6194, loss = 0.47349536\n",
      "Iteration 6195, loss = 0.47301961\n",
      "Iteration 6196, loss = 0.47403464\n",
      "Iteration 6197, loss = 0.47399018\n",
      "Iteration 6198, loss = 0.47416424\n",
      "Iteration 6199, loss = 0.47354455\n",
      "Iteration 6200, loss = 0.47366187\n",
      "Iteration 6201, loss = 0.47334301\n",
      "Iteration 6202, loss = 0.47337863\n",
      "Iteration 6203, loss = 0.47343933\n",
      "Iteration 6204, loss = 0.47335500\n",
      "Iteration 6205, loss = 0.47350924\n",
      "Iteration 6206, loss = 0.47319192\n",
      "Iteration 6207, loss = 0.47353363\n",
      "Iteration 6208, loss = 0.47346191\n",
      "Iteration 6209, loss = 0.47322508\n",
      "Iteration 6210, loss = 0.47323935\n",
      "Iteration 6211, loss = 0.47332990\n",
      "Iteration 6212, loss = 0.47335774\n",
      "Iteration 6213, loss = 0.47353760\n",
      "Iteration 6214, loss = 0.47365664\n",
      "Iteration 6215, loss = 0.47396608\n",
      "Iteration 6216, loss = 0.47449704\n",
      "Iteration 6217, loss = 0.47452371\n",
      "Iteration 6218, loss = 0.47425696\n",
      "Iteration 6219, loss = 0.47374104\n",
      "Iteration 6220, loss = 0.47335133\n",
      "Iteration 6221, loss = 0.47346808\n",
      "Iteration 6222, loss = 0.47325857\n",
      "Iteration 6223, loss = 0.47325197\n",
      "Iteration 6224, loss = 0.47324665\n",
      "Iteration 6225, loss = 0.47330563\n",
      "Iteration 6226, loss = 0.47326540\n",
      "Iteration 6227, loss = 0.47326665\n",
      "Iteration 6228, loss = 0.47328869\n",
      "Iteration 6229, loss = 0.47361737\n",
      "Iteration 6230, loss = 0.47405404\n",
      "Iteration 6231, loss = 0.47374437\n",
      "Iteration 6232, loss = 0.47336460\n",
      "Iteration 6233, loss = 0.47317848\n",
      "Iteration 6234, loss = 0.47352309\n",
      "Iteration 6235, loss = 0.47328312\n",
      "Iteration 6236, loss = 0.47310936\n",
      "Iteration 6237, loss = 0.47327084\n",
      "Iteration 6238, loss = 0.47412165\n",
      "Iteration 6239, loss = 0.47652956\n",
      "Iteration 6240, loss = 0.47684261\n",
      "Iteration 6241, loss = 0.47580798\n",
      "Iteration 6242, loss = 0.47489445\n",
      "Iteration 6243, loss = 0.47444965\n",
      "Iteration 6244, loss = 0.47371423\n",
      "Iteration 6245, loss = 0.47382203\n",
      "Iteration 6246, loss = 0.47474583\n",
      "Iteration 6247, loss = 0.47638262\n",
      "Iteration 6248, loss = 0.47609239\n",
      "Iteration 6249, loss = 0.47490878\n",
      "Iteration 6250, loss = 0.47418579\n",
      "Iteration 6251, loss = 0.47425060\n",
      "Iteration 6252, loss = 0.47345574\n",
      "Iteration 6253, loss = 0.47338332\n",
      "Iteration 6254, loss = 0.47334067\n",
      "Iteration 6255, loss = 0.47318018\n",
      "Iteration 6256, loss = 0.47382563\n",
      "Iteration 6257, loss = 0.47393960\n",
      "Iteration 6258, loss = 0.47367915\n",
      "Iteration 6259, loss = 0.47333842\n",
      "Iteration 6260, loss = 0.47327220\n",
      "Iteration 6261, loss = 0.47318967\n",
      "Iteration 6262, loss = 0.47324164\n",
      "Iteration 6263, loss = 0.47329086\n",
      "Iteration 6264, loss = 0.47337480\n",
      "Iteration 6265, loss = 0.47337356\n",
      "Iteration 6266, loss = 0.47331093\n",
      "Iteration 6267, loss = 0.47340979\n",
      "Iteration 6268, loss = 0.47348672\n",
      "Iteration 6269, loss = 0.47355737\n",
      "Iteration 6270, loss = 0.47363619\n",
      "Iteration 6271, loss = 0.47350748\n",
      "Iteration 6272, loss = 0.47326825\n",
      "Iteration 6273, loss = 0.47359959\n",
      "Iteration 6274, loss = 0.47422701\n",
      "Iteration 6275, loss = 0.47399833\n",
      "Iteration 6276, loss = 0.47391927\n",
      "Iteration 6277, loss = 0.47348933\n",
      "Iteration 6278, loss = 0.47351393\n",
      "Iteration 6279, loss = 0.47360475\n",
      "Iteration 6280, loss = 0.47343788\n",
      "Iteration 6281, loss = 0.47345990\n",
      "Iteration 6282, loss = 0.47392114\n",
      "Iteration 6283, loss = 0.47442095\n",
      "Iteration 6284, loss = 0.47422927\n",
      "Iteration 6285, loss = 0.47365553\n",
      "Iteration 6286, loss = 0.47361529\n",
      "Iteration 6287, loss = 0.47344905\n",
      "Iteration 6288, loss = 0.47357167\n",
      "Iteration 6289, loss = 0.47362291\n",
      "Iteration 6290, loss = 0.47351179\n",
      "Iteration 6291, loss = 0.47325774\n",
      "Iteration 6292, loss = 0.47326821\n",
      "Iteration 6293, loss = 0.47334931\n",
      "Iteration 6294, loss = 0.47328182\n",
      "Iteration 6295, loss = 0.47326586\n",
      "Iteration 6296, loss = 0.47330891\n",
      "Iteration 6297, loss = 0.47331265\n",
      "Iteration 6298, loss = 0.47334112\n",
      "Iteration 6299, loss = 0.47348594\n",
      "Iteration 6300, loss = 0.47344051\n",
      "Iteration 6301, loss = 0.47337214\n",
      "Iteration 6302, loss = 0.47369987\n",
      "Iteration 6303, loss = 0.47508796\n",
      "Iteration 6304, loss = 0.47653963\n",
      "Iteration 6305, loss = 0.47728288\n",
      "Iteration 6306, loss = 0.47663798\n",
      "Iteration 6307, loss = 0.47520829\n",
      "Iteration 6308, loss = 0.47415790\n",
      "Iteration 6309, loss = 0.47361116\n",
      "Iteration 6310, loss = 0.47343329\n",
      "Iteration 6311, loss = 0.47385599\n",
      "Iteration 6312, loss = 0.47450511\n",
      "Iteration 6313, loss = 0.47502076\n",
      "Iteration 6314, loss = 0.47538569\n",
      "Iteration 6315, loss = 0.47479916\n",
      "Iteration 6316, loss = 0.47364064\n",
      "Iteration 6317, loss = 0.47343843\n",
      "Iteration 6318, loss = 0.47396961\n",
      "Iteration 6319, loss = 0.47398730\n",
      "Iteration 6320, loss = 0.47373521\n",
      "Iteration 6321, loss = 0.47376259\n",
      "Iteration 6322, loss = 0.47342121\n",
      "Iteration 6323, loss = 0.47345519\n",
      "Iteration 6324, loss = 0.47333388\n",
      "Iteration 6325, loss = 0.47336175\n",
      "Iteration 6326, loss = 0.47344442\n",
      "Iteration 6327, loss = 0.47383484\n",
      "Iteration 6328, loss = 0.47395206\n",
      "Iteration 6329, loss = 0.47370082\n",
      "Iteration 6330, loss = 0.47332068\n",
      "Iteration 6331, loss = 0.47371687\n",
      "Iteration 6332, loss = 0.47389439\n",
      "Iteration 6333, loss = 0.47438505\n",
      "Iteration 6334, loss = 0.47434587\n",
      "Iteration 6335, loss = 0.47442960\n",
      "Iteration 6336, loss = 0.47377475\n",
      "Iteration 6337, loss = 0.47408779\n",
      "Iteration 6338, loss = 0.47383323\n",
      "Iteration 6339, loss = 0.47316845\n",
      "Iteration 6340, loss = 0.47307525\n",
      "Iteration 6341, loss = 0.47400173\n",
      "Iteration 6342, loss = 0.47459166\n",
      "Iteration 6343, loss = 0.47456343\n",
      "Iteration 6344, loss = 0.47407094\n",
      "Iteration 6345, loss = 0.47358451\n",
      "Iteration 6346, loss = 0.47325792\n",
      "Iteration 6347, loss = 0.47321414\n",
      "Iteration 6348, loss = 0.47373738\n",
      "Iteration 6349, loss = 0.47468853\n",
      "Iteration 6350, loss = 0.47486539\n",
      "Iteration 6351, loss = 0.47465068\n",
      "Iteration 6352, loss = 0.47433641\n",
      "Iteration 6353, loss = 0.47414206\n",
      "Iteration 6354, loss = 0.47386836\n",
      "Iteration 6355, loss = 0.47464629\n",
      "Iteration 6356, loss = 0.47504574\n",
      "Iteration 6357, loss = 0.47534975\n",
      "Iteration 6358, loss = 0.47507512\n",
      "Iteration 6359, loss = 0.47409984\n",
      "Iteration 6360, loss = 0.47277229\n",
      "Iteration 6361, loss = 0.47357303\n",
      "Iteration 6362, loss = 0.47531801\n",
      "Iteration 6363, loss = 0.47714396\n",
      "Iteration 6364, loss = 0.47629692\n",
      "Iteration 6365, loss = 0.47413589\n",
      "Iteration 6366, loss = 0.47280522\n",
      "Iteration 6367, loss = 0.47300034\n",
      "Iteration 6368, loss = 0.47543241\n",
      "Iteration 6369, loss = 0.47779622\n",
      "Iteration 6370, loss = 0.47962606\n",
      "Iteration 6371, loss = 0.48051924\n",
      "Iteration 6372, loss = 0.47945099\n",
      "Iteration 6373, loss = 0.47701780\n",
      "Iteration 6374, loss = 0.47419137\n",
      "Iteration 6375, loss = 0.47284646\n",
      "Iteration 6376, loss = 0.47483362\n",
      "Iteration 6377, loss = 0.47551074\n",
      "Iteration 6378, loss = 0.47457843\n",
      "Iteration 6379, loss = 0.47309887\n",
      "Iteration 6380, loss = 0.47316147\n",
      "Iteration 6381, loss = 0.47442938\n",
      "Iteration 6382, loss = 0.47498930\n",
      "Iteration 6383, loss = 0.47449801\n",
      "Iteration 6384, loss = 0.47346932\n",
      "Iteration 6385, loss = 0.47365982\n",
      "Iteration 6386, loss = 0.47337562\n",
      "Iteration 6387, loss = 0.47357285\n",
      "Iteration 6388, loss = 0.47359246\n",
      "Iteration 6389, loss = 0.47347449\n",
      "Iteration 6390, loss = 0.47330838\n",
      "Iteration 6391, loss = 0.47318561\n",
      "Iteration 6392, loss = 0.47345153\n",
      "Iteration 6393, loss = 0.47370451\n",
      "Iteration 6394, loss = 0.47370407\n",
      "Iteration 6395, loss = 0.47346359\n",
      "Iteration 6396, loss = 0.47336491\n",
      "Iteration 6397, loss = 0.47354552\n",
      "Iteration 6398, loss = 0.47333464\n",
      "Iteration 6399, loss = 0.47324154\n",
      "Iteration 6400, loss = 0.47334532\n",
      "Iteration 6401, loss = 0.47338311\n",
      "Iteration 6402, loss = 0.47372010\n",
      "Iteration 6403, loss = 0.47330439\n",
      "Iteration 6404, loss = 0.47323260\n",
      "Iteration 6405, loss = 0.47320833\n",
      "Iteration 6406, loss = 0.47317768\n",
      "Iteration 6407, loss = 0.47338361\n",
      "Iteration 6408, loss = 0.47316248\n",
      "Iteration 6409, loss = 0.47309619\n",
      "Iteration 6410, loss = 0.47335117\n",
      "Iteration 6411, loss = 0.47381558\n",
      "Iteration 6412, loss = 0.47477974\n",
      "Iteration 6413, loss = 0.47495664\n",
      "Iteration 6414, loss = 0.47438190\n",
      "Iteration 6415, loss = 0.47419999\n",
      "Iteration 6416, loss = 0.47337458\n",
      "Iteration 6417, loss = 0.47352405\n",
      "Iteration 6418, loss = 0.47340220\n",
      "Iteration 6419, loss = 0.47344049\n",
      "Iteration 6420, loss = 0.47314052\n",
      "Iteration 6421, loss = 0.47335980\n",
      "Iteration 6422, loss = 0.47463766\n",
      "Iteration 6423, loss = 0.47557421\n",
      "Iteration 6424, loss = 0.47489276\n",
      "Iteration 6425, loss = 0.47397617\n",
      "Iteration 6426, loss = 0.47306517\n",
      "Iteration 6427, loss = 0.47349189\n",
      "Iteration 6428, loss = 0.47404342\n",
      "Iteration 6429, loss = 0.47417670\n",
      "Iteration 6430, loss = 0.47384894\n",
      "Iteration 6431, loss = 0.47326140\n",
      "Iteration 6432, loss = 0.47375631\n",
      "Iteration 6433, loss = 0.47350993\n",
      "Iteration 6434, loss = 0.47376305\n",
      "Iteration 6435, loss = 0.47443137\n",
      "Iteration 6436, loss = 0.47431270\n",
      "Iteration 6437, loss = 0.47354057\n",
      "Iteration 6438, loss = 0.47317675\n",
      "Iteration 6439, loss = 0.47377104\n",
      "Iteration 6440, loss = 0.47370537\n",
      "Iteration 6441, loss = 0.47373006\n",
      "Iteration 6442, loss = 0.47386266\n",
      "Iteration 6443, loss = 0.47379711\n",
      "Iteration 6444, loss = 0.47355149\n",
      "Iteration 6445, loss = 0.47329997\n",
      "Iteration 6446, loss = 0.47368040\n",
      "Iteration 6447, loss = 0.47350905\n",
      "Iteration 6448, loss = 0.47358011\n",
      "Iteration 6449, loss = 0.47343358\n",
      "Iteration 6450, loss = 0.47321807\n",
      "Iteration 6451, loss = 0.47311971\n",
      "Iteration 6452, loss = 0.47323291\n",
      "Iteration 6453, loss = 0.47404746\n",
      "Iteration 6454, loss = 0.47453719\n",
      "Iteration 6455, loss = 0.47469605\n",
      "Iteration 6456, loss = 0.47477066\n",
      "Iteration 6457, loss = 0.47444085\n",
      "Iteration 6458, loss = 0.47404218\n",
      "Iteration 6459, loss = 0.47350352\n",
      "Iteration 6460, loss = 0.47334607\n",
      "Iteration 6461, loss = 0.47334287\n",
      "Iteration 6462, loss = 0.47338696\n",
      "Iteration 6463, loss = 0.47337062\n",
      "Iteration 6464, loss = 0.47341295\n",
      "Iteration 6465, loss = 0.47328934\n",
      "Iteration 6466, loss = 0.47331680\n",
      "Iteration 6467, loss = 0.47358930\n",
      "Iteration 6468, loss = 0.47395419\n",
      "Iteration 6469, loss = 0.47458379\n",
      "Iteration 6470, loss = 0.47474675\n",
      "Iteration 6471, loss = 0.47419005\n",
      "Iteration 6472, loss = 0.47345766\n",
      "Iteration 6473, loss = 0.47328255\n",
      "Iteration 6474, loss = 0.47317026\n",
      "Iteration 6475, loss = 0.47344962\n",
      "Iteration 6476, loss = 0.47394696\n",
      "Iteration 6477, loss = 0.47423694\n",
      "Iteration 6478, loss = 0.47414976\n",
      "Iteration 6479, loss = 0.47395107\n",
      "Iteration 6480, loss = 0.47350529\n",
      "Iteration 6481, loss = 0.47307184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6482, loss = 0.47369062\n",
      "Iteration 6483, loss = 0.47364426\n",
      "Iteration 6484, loss = 0.47387506\n",
      "Iteration 6485, loss = 0.47403415\n",
      "Iteration 6486, loss = 0.47381399\n",
      "Iteration 6487, loss = 0.47427993\n",
      "Iteration 6488, loss = 0.47405445\n",
      "Iteration 6489, loss = 0.47344021\n",
      "Iteration 6490, loss = 0.47385958\n",
      "Iteration 6491, loss = 0.47324109\n",
      "Iteration 6492, loss = 0.47318844\n",
      "Iteration 6493, loss = 0.47311238\n",
      "Iteration 6494, loss = 0.47308241\n",
      "Iteration 6495, loss = 0.47384942\n",
      "Iteration 6496, loss = 0.47452775\n",
      "Iteration 6497, loss = 0.47478494\n",
      "Iteration 6498, loss = 0.47460202\n",
      "Iteration 6499, loss = 0.47455845\n",
      "Iteration 6500, loss = 0.47415003\n",
      "Iteration 6501, loss = 0.47311551\n",
      "Iteration 6502, loss = 0.47294051\n",
      "Iteration 6503, loss = 0.47351286\n",
      "Iteration 6504, loss = 0.47505899\n",
      "Iteration 6505, loss = 0.47726862\n",
      "Iteration 6506, loss = 0.47724910\n",
      "Iteration 6507, loss = 0.47574422\n",
      "Iteration 6508, loss = 0.47411271\n",
      "Iteration 6509, loss = 0.47318077\n",
      "Iteration 6510, loss = 0.47419656\n",
      "Iteration 6511, loss = 0.47399120\n",
      "Iteration 6512, loss = 0.47394170\n",
      "Iteration 6513, loss = 0.47366405\n",
      "Iteration 6514, loss = 0.47349565\n",
      "Iteration 6515, loss = 0.47320879\n",
      "Iteration 6516, loss = 0.47313750\n",
      "Iteration 6517, loss = 0.47342527\n",
      "Iteration 6518, loss = 0.47408548\n",
      "Iteration 6519, loss = 0.47474028\n",
      "Iteration 6520, loss = 0.47527668\n",
      "Iteration 6521, loss = 0.47551317\n",
      "Iteration 6522, loss = 0.47478552\n",
      "Iteration 6523, loss = 0.47437781\n",
      "Iteration 6524, loss = 0.47321922\n",
      "Iteration 6525, loss = 0.47326553\n",
      "Iteration 6526, loss = 0.47451455\n",
      "Iteration 6527, loss = 0.47545113\n",
      "Iteration 6528, loss = 0.47563996\n",
      "Iteration 6529, loss = 0.47454032\n",
      "Iteration 6530, loss = 0.47368196\n",
      "Iteration 6531, loss = 0.47318388\n",
      "Iteration 6532, loss = 0.47405534\n",
      "Iteration 6533, loss = 0.47466009\n",
      "Iteration 6534, loss = 0.47481770\n",
      "Iteration 6535, loss = 0.47452889\n",
      "Iteration 6536, loss = 0.47375319\n",
      "Iteration 6537, loss = 0.47326971\n",
      "Iteration 6538, loss = 0.47302074\n",
      "Iteration 6539, loss = 0.47400492\n",
      "Iteration 6540, loss = 0.47432628\n",
      "Iteration 6541, loss = 0.47397563\n",
      "Iteration 6542, loss = 0.47349193\n",
      "Iteration 6543, loss = 0.47330888\n",
      "Iteration 6544, loss = 0.47329551\n",
      "Iteration 6545, loss = 0.47329057\n",
      "Iteration 6546, loss = 0.47326901\n",
      "Iteration 6547, loss = 0.47318394\n",
      "Iteration 6548, loss = 0.47338902\n",
      "Iteration 6549, loss = 0.47331197\n",
      "Iteration 6550, loss = 0.47322972\n",
      "Iteration 6551, loss = 0.47318503\n",
      "Iteration 6552, loss = 0.47326819\n",
      "Iteration 6553, loss = 0.47327418\n",
      "Iteration 6554, loss = 0.47330336\n",
      "Iteration 6555, loss = 0.47325046\n",
      "Iteration 6556, loss = 0.47332809\n",
      "Iteration 6557, loss = 0.47343436\n",
      "Iteration 6558, loss = 0.47396952\n",
      "Iteration 6559, loss = 0.47407723\n",
      "Iteration 6560, loss = 0.47372653\n",
      "Iteration 6561, loss = 0.47359221\n",
      "Iteration 6562, loss = 0.47336230\n",
      "Iteration 6563, loss = 0.47351689\n",
      "Iteration 6564, loss = 0.47358770\n",
      "Iteration 6565, loss = 0.47318939\n",
      "Iteration 6566, loss = 0.47344255\n",
      "Iteration 6567, loss = 0.47348321\n",
      "Iteration 6568, loss = 0.47353512\n",
      "Iteration 6569, loss = 0.47355694\n",
      "Iteration 6570, loss = 0.47331149\n",
      "Iteration 6571, loss = 0.47319268\n",
      "Iteration 6572, loss = 0.47362420\n",
      "Iteration 6573, loss = 0.47347041\n",
      "Iteration 6574, loss = 0.47350443\n",
      "Iteration 6575, loss = 0.47341718\n",
      "Iteration 6576, loss = 0.47340100\n",
      "Iteration 6577, loss = 0.47333479\n",
      "Iteration 6578, loss = 0.47340540\n",
      "Iteration 6579, loss = 0.47356529\n",
      "Iteration 6580, loss = 0.47376821\n",
      "Iteration 6581, loss = 0.47367303\n",
      "Iteration 6582, loss = 0.47382496\n",
      "Iteration 6583, loss = 0.47370501\n",
      "Iteration 6584, loss = 0.47350343\n",
      "Iteration 6585, loss = 0.47324315\n",
      "Iteration 6586, loss = 0.47339011\n",
      "Iteration 6587, loss = 0.47323122\n",
      "Iteration 6588, loss = 0.47337651\n",
      "Iteration 6589, loss = 0.47365246\n",
      "Iteration 6590, loss = 0.47397846\n",
      "Iteration 6591, loss = 0.47421665\n",
      "Iteration 6592, loss = 0.47457993\n",
      "Iteration 6593, loss = 0.47490405\n",
      "Iteration 6594, loss = 0.47416060\n",
      "Iteration 6595, loss = 0.47323382\n",
      "Iteration 6596, loss = 0.47268370\n",
      "Iteration 6597, loss = 0.47441107\n",
      "Iteration 6598, loss = 0.47569717\n",
      "Iteration 6599, loss = 0.47733524\n",
      "Iteration 6600, loss = 0.47780715\n",
      "Iteration 6601, loss = 0.47683475\n",
      "Iteration 6602, loss = 0.47483834\n",
      "Iteration 6603, loss = 0.47328022\n",
      "Iteration 6604, loss = 0.47389434\n",
      "Iteration 6605, loss = 0.47358461\n",
      "Iteration 6606, loss = 0.47424688\n",
      "Iteration 6607, loss = 0.47403577\n",
      "Iteration 6608, loss = 0.47312376\n",
      "Iteration 6609, loss = 0.47323289\n",
      "Iteration 6610, loss = 0.47435042\n",
      "Iteration 6611, loss = 0.47412999\n",
      "Iteration 6612, loss = 0.47347729\n",
      "Iteration 6613, loss = 0.47304681\n",
      "Iteration 6614, loss = 0.47292087\n",
      "Iteration 6615, loss = 0.47484341\n",
      "Iteration 6616, loss = 0.47612787\n",
      "Iteration 6617, loss = 0.47579584\n",
      "Iteration 6618, loss = 0.47423644\n",
      "Iteration 6619, loss = 0.47297029\n",
      "Iteration 6620, loss = 0.47324577\n",
      "Iteration 6621, loss = 0.47440629\n",
      "Iteration 6622, loss = 0.47553259\n",
      "Iteration 6623, loss = 0.47631417\n",
      "Iteration 6624, loss = 0.47504771\n",
      "Iteration 6625, loss = 0.47371972\n",
      "Iteration 6626, loss = 0.47293244\n",
      "Iteration 6627, loss = 0.47359818\n",
      "Iteration 6628, loss = 0.47439666\n",
      "Iteration 6629, loss = 0.47470658\n",
      "Iteration 6630, loss = 0.47452100\n",
      "Iteration 6631, loss = 0.47416179\n",
      "Iteration 6632, loss = 0.47382075\n",
      "Iteration 6633, loss = 0.47325354\n",
      "Iteration 6634, loss = 0.47318002\n",
      "Iteration 6635, loss = 0.47316085\n",
      "Iteration 6636, loss = 0.47321474\n",
      "Iteration 6637, loss = 0.47328507\n",
      "Iteration 6638, loss = 0.47313828\n",
      "Iteration 6639, loss = 0.47327330\n",
      "Iteration 6640, loss = 0.47359207\n",
      "Iteration 6641, loss = 0.47395017\n",
      "Iteration 6642, loss = 0.47400034\n",
      "Iteration 6643, loss = 0.47340552\n",
      "Iteration 6644, loss = 0.47346186\n",
      "Iteration 6645, loss = 0.47336787\n",
      "Iteration 6646, loss = 0.47415683\n",
      "Iteration 6647, loss = 0.47343337\n",
      "Iteration 6648, loss = 0.47290984\n",
      "Iteration 6649, loss = 0.47345323\n",
      "Iteration 6650, loss = 0.47437649\n",
      "Iteration 6651, loss = 0.47505140\n",
      "Iteration 6652, loss = 0.47424219\n",
      "Iteration 6653, loss = 0.47308494\n",
      "Iteration 6654, loss = 0.47368894\n",
      "Iteration 6655, loss = 0.47462256\n",
      "Iteration 6656, loss = 0.47354101\n",
      "Iteration 6657, loss = 0.47320111\n",
      "Iteration 6658, loss = 0.47452741\n",
      "Iteration 6659, loss = 0.47543733\n",
      "Iteration 6660, loss = 0.47486787\n",
      "Iteration 6661, loss = 0.47362570\n",
      "Iteration 6662, loss = 0.47309013\n",
      "Iteration 6663, loss = 0.47386511\n",
      "Iteration 6664, loss = 0.47459657\n",
      "Iteration 6665, loss = 0.47473735\n",
      "Iteration 6666, loss = 0.47429773\n",
      "Iteration 6667, loss = 0.47363662\n",
      "Iteration 6668, loss = 0.47353078\n",
      "Iteration 6669, loss = 0.47408408\n",
      "Iteration 6670, loss = 0.47407900\n",
      "Iteration 6671, loss = 0.47407563\n",
      "Iteration 6672, loss = 0.47417945\n",
      "Iteration 6673, loss = 0.47380525\n",
      "Iteration 6674, loss = 0.47360046\n",
      "Iteration 6675, loss = 0.47346917\n",
      "Iteration 6676, loss = 0.47361924\n",
      "Iteration 6677, loss = 0.47427971\n",
      "Iteration 6678, loss = 0.47440854\n",
      "Iteration 6679, loss = 0.47407173\n",
      "Iteration 6680, loss = 0.47348306\n",
      "Iteration 6681, loss = 0.47309441\n",
      "Iteration 6682, loss = 0.47362119\n",
      "Iteration 6683, loss = 0.47353885\n",
      "Iteration 6684, loss = 0.47364030\n",
      "Iteration 6685, loss = 0.47362713\n",
      "Iteration 6686, loss = 0.47364030\n",
      "Iteration 6687, loss = 0.47359897\n",
      "Iteration 6688, loss = 0.47326834\n",
      "Iteration 6689, loss = 0.47309325\n",
      "Iteration 6690, loss = 0.47315815\n",
      "Iteration 6691, loss = 0.47347529\n",
      "Iteration 6692, loss = 0.47361222\n",
      "Iteration 6693, loss = 0.47380015\n",
      "Iteration 6694, loss = 0.47345333\n",
      "Iteration 6695, loss = 0.47333239\n",
      "Iteration 6696, loss = 0.47319993\n",
      "Iteration 6697, loss = 0.47318057\n",
      "Iteration 6698, loss = 0.47317298\n",
      "Iteration 6699, loss = 0.47309684\n",
      "Iteration 6700, loss = 0.47340453\n",
      "Iteration 6701, loss = 0.47307305\n",
      "Iteration 6702, loss = 0.47308517\n",
      "Iteration 6703, loss = 0.47347653\n",
      "Iteration 6704, loss = 0.47366786\n",
      "Iteration 6705, loss = 0.47329535\n",
      "Iteration 6706, loss = 0.47311598\n",
      "Iteration 6707, loss = 0.47405901\n",
      "Iteration 6708, loss = 0.47401207\n",
      "Iteration 6709, loss = 0.47362582\n",
      "Iteration 6710, loss = 0.47328601\n",
      "Iteration 6711, loss = 0.47317252\n",
      "Iteration 6712, loss = 0.47317660\n",
      "Iteration 6713, loss = 0.47325669\n",
      "Iteration 6714, loss = 0.47326927\n",
      "Iteration 6715, loss = 0.47324257\n",
      "Iteration 6716, loss = 0.47313528\n",
      "Iteration 6717, loss = 0.47300515\n",
      "Iteration 6718, loss = 0.47344946\n",
      "Iteration 6719, loss = 0.47488147\n",
      "Iteration 6720, loss = 0.47503638\n",
      "Iteration 6721, loss = 0.47441840\n",
      "Iteration 6722, loss = 0.47421110\n",
      "Iteration 6723, loss = 0.47326422\n",
      "Iteration 6724, loss = 0.47305826\n",
      "Iteration 6725, loss = 0.47330385\n",
      "Iteration 6726, loss = 0.47425256\n",
      "Iteration 6727, loss = 0.47461027\n",
      "Iteration 6728, loss = 0.47429753\n",
      "Iteration 6729, loss = 0.47395385\n",
      "Iteration 6730, loss = 0.47335299\n",
      "Iteration 6731, loss = 0.47315787\n",
      "Iteration 6732, loss = 0.47312412\n",
      "Iteration 6733, loss = 0.47341495\n",
      "Iteration 6734, loss = 0.47319371\n",
      "Iteration 6735, loss = 0.47331883\n",
      "Iteration 6736, loss = 0.47370001\n",
      "Iteration 6737, loss = 0.47341859\n",
      "Iteration 6738, loss = 0.47326532\n",
      "Iteration 6739, loss = 0.47318447\n",
      "Iteration 6740, loss = 0.47328531\n",
      "Iteration 6741, loss = 0.47309636\n",
      "Iteration 6742, loss = 0.47316727\n",
      "Iteration 6743, loss = 0.47319513\n",
      "Iteration 6744, loss = 0.47350147\n",
      "Iteration 6745, loss = 0.47312221\n",
      "Iteration 6746, loss = 0.47304929\n",
      "Iteration 6747, loss = 0.47296493\n",
      "Iteration 6748, loss = 0.47453150\n",
      "Iteration 6749, loss = 0.47452721\n",
      "Iteration 6750, loss = 0.47412987\n",
      "Iteration 6751, loss = 0.47364054\n",
      "Iteration 6752, loss = 0.47338670\n",
      "Iteration 6753, loss = 0.47317907\n",
      "Iteration 6754, loss = 0.47323850\n",
      "Iteration 6755, loss = 0.47316514\n",
      "Iteration 6756, loss = 0.47314237\n",
      "Iteration 6757, loss = 0.47373783\n",
      "Iteration 6758, loss = 0.47413926\n",
      "Iteration 6759, loss = 0.47408224\n",
      "Iteration 6760, loss = 0.47362663\n",
      "Iteration 6761, loss = 0.47330773\n",
      "Iteration 6762, loss = 0.47307606\n",
      "Iteration 6763, loss = 0.47299320\n",
      "Iteration 6764, loss = 0.47332767\n",
      "Iteration 6765, loss = 0.47412780\n",
      "Iteration 6766, loss = 0.47428151\n",
      "Iteration 6767, loss = 0.47406886\n",
      "Iteration 6768, loss = 0.47365079\n",
      "Iteration 6769, loss = 0.47346745\n",
      "Iteration 6770, loss = 0.47391002\n",
      "Iteration 6771, loss = 0.47403113\n",
      "Iteration 6772, loss = 0.47462693\n",
      "Iteration 6773, loss = 0.47487955\n",
      "Iteration 6774, loss = 0.47421151\n",
      "Iteration 6775, loss = 0.47406083\n",
      "Iteration 6776, loss = 0.47350319\n",
      "Iteration 6777, loss = 0.47362343\n",
      "Iteration 6778, loss = 0.47355631\n",
      "Iteration 6779, loss = 0.47317965\n",
      "Iteration 6780, loss = 0.47305444\n",
      "Iteration 6781, loss = 0.47334932\n",
      "Iteration 6782, loss = 0.47397415\n",
      "Iteration 6783, loss = 0.47460233\n",
      "Iteration 6784, loss = 0.47418635\n",
      "Iteration 6785, loss = 0.47321426\n",
      "Iteration 6786, loss = 0.47367272\n",
      "Iteration 6787, loss = 0.47339338\n",
      "Iteration 6788, loss = 0.47346847\n",
      "Iteration 6789, loss = 0.47343870\n",
      "Iteration 6790, loss = 0.47335457\n",
      "Iteration 6791, loss = 0.47378243\n",
      "Iteration 6792, loss = 0.47381258\n",
      "Iteration 6793, loss = 0.47347847\n",
      "Iteration 6794, loss = 0.47344566\n",
      "Iteration 6795, loss = 0.47325675\n",
      "Iteration 6796, loss = 0.47335120\n",
      "Iteration 6797, loss = 0.47377030\n",
      "Iteration 6798, loss = 0.47415643\n",
      "Iteration 6799, loss = 0.47360730\n",
      "Iteration 6800, loss = 0.47328145\n",
      "Iteration 6801, loss = 0.47347074\n",
      "Iteration 6802, loss = 0.47388934\n",
      "Iteration 6803, loss = 0.47386778\n",
      "Iteration 6804, loss = 0.47345750\n",
      "Iteration 6805, loss = 0.47309657\n",
      "Iteration 6806, loss = 0.47350858\n",
      "Iteration 6807, loss = 0.47357321\n",
      "Iteration 6808, loss = 0.47373348\n",
      "Iteration 6809, loss = 0.47433435\n",
      "Iteration 6810, loss = 0.47452366\n",
      "Iteration 6811, loss = 0.47384048\n",
      "Iteration 6812, loss = 0.47277611\n",
      "Iteration 6813, loss = 0.47347179\n",
      "Iteration 6814, loss = 0.47473015\n",
      "Iteration 6815, loss = 0.47534399\n",
      "Iteration 6816, loss = 0.47472199\n",
      "Iteration 6817, loss = 0.47381703\n",
      "Iteration 6818, loss = 0.47301609\n",
      "Iteration 6819, loss = 0.47309083\n",
      "Iteration 6820, loss = 0.47506021\n",
      "Iteration 6821, loss = 0.47585696\n",
      "Iteration 6822, loss = 0.47533934\n",
      "Iteration 6823, loss = 0.47436386\n",
      "Iteration 6824, loss = 0.47350247\n",
      "Iteration 6825, loss = 0.47315206\n",
      "Iteration 6826, loss = 0.47295879\n",
      "Iteration 6827, loss = 0.47337196\n",
      "Iteration 6828, loss = 0.47374763\n",
      "Iteration 6829, loss = 0.47433321\n",
      "Iteration 6830, loss = 0.47459415\n",
      "Iteration 6831, loss = 0.47437422\n",
      "Iteration 6832, loss = 0.47387812\n",
      "Iteration 6833, loss = 0.47349799\n",
      "Iteration 6834, loss = 0.47394508\n",
      "Iteration 6835, loss = 0.47366796\n",
      "Iteration 6836, loss = 0.47331074\n",
      "Iteration 6837, loss = 0.47314972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6838, loss = 0.47423053\n",
      "Iteration 6839, loss = 0.47452851\n",
      "Iteration 6840, loss = 0.47441720\n",
      "Iteration 6841, loss = 0.47423994\n",
      "Iteration 6842, loss = 0.47364058\n",
      "Iteration 6843, loss = 0.47336216\n",
      "Iteration 6844, loss = 0.47408993\n",
      "Iteration 6845, loss = 0.47376137\n",
      "Iteration 6846, loss = 0.47337225\n",
      "Iteration 6847, loss = 0.47380587\n",
      "Iteration 6848, loss = 0.47424111\n",
      "Iteration 6849, loss = 0.47437164\n",
      "Iteration 6850, loss = 0.47364226\n",
      "Iteration 6851, loss = 0.47338948\n",
      "Iteration 6852, loss = 0.47404170\n",
      "Iteration 6853, loss = 0.47396160\n",
      "Iteration 6854, loss = 0.47348791\n",
      "Iteration 6855, loss = 0.47264161\n",
      "Iteration 6856, loss = 0.47416130\n",
      "Iteration 6857, loss = 0.47551436\n",
      "Iteration 6858, loss = 0.47745933\n",
      "Iteration 6859, loss = 0.48030240\n",
      "Iteration 6860, loss = 0.48155391\n",
      "Iteration 6861, loss = 0.47981207\n",
      "Iteration 6862, loss = 0.47626608\n",
      "Iteration 6863, loss = 0.47413284\n",
      "Iteration 6864, loss = 0.47330500\n",
      "Iteration 6865, loss = 0.47385646\n",
      "Iteration 6866, loss = 0.47502568\n",
      "Iteration 6867, loss = 0.47590208\n",
      "Iteration 6868, loss = 0.47620046\n",
      "Iteration 6869, loss = 0.47597339\n",
      "Iteration 6870, loss = 0.47529566\n",
      "Iteration 6871, loss = 0.47523913\n",
      "Iteration 6872, loss = 0.47386471\n",
      "Iteration 6873, loss = 0.47377575\n",
      "Iteration 6874, loss = 0.47348291\n",
      "Iteration 6875, loss = 0.47348605\n",
      "Iteration 6876, loss = 0.47399988\n",
      "Iteration 6877, loss = 0.47426632\n",
      "Iteration 6878, loss = 0.47443295\n",
      "Iteration 6879, loss = 0.47442601\n",
      "Iteration 6880, loss = 0.47421631\n",
      "Iteration 6881, loss = 0.47436623\n",
      "Iteration 6882, loss = 0.47406549\n",
      "Iteration 6883, loss = 0.47457319\n",
      "Iteration 6884, loss = 0.47436753\n",
      "Iteration 6885, loss = 0.47387139\n",
      "Iteration 6886, loss = 0.47316540\n",
      "Iteration 6887, loss = 0.47360483\n",
      "Iteration 6888, loss = 0.47304595\n",
      "Iteration 6889, loss = 0.47319695\n",
      "Iteration 6890, loss = 0.47326484\n",
      "Iteration 6891, loss = 0.47364902\n",
      "Iteration 6892, loss = 0.47390516\n",
      "Iteration 6893, loss = 0.47398656\n",
      "Iteration 6894, loss = 0.47409669\n",
      "Iteration 6895, loss = 0.47402647\n",
      "Iteration 6896, loss = 0.47369842\n",
      "Iteration 6897, loss = 0.47350533\n",
      "Iteration 6898, loss = 0.47298690\n",
      "Iteration 6899, loss = 0.47382325\n",
      "Iteration 6900, loss = 0.47405810\n",
      "Iteration 6901, loss = 0.47436386\n",
      "Iteration 6902, loss = 0.47504826\n",
      "Iteration 6903, loss = 0.47508800\n",
      "Iteration 6904, loss = 0.47475309\n",
      "Iteration 6905, loss = 0.47442818\n",
      "Iteration 6906, loss = 0.47412302\n",
      "Iteration 6907, loss = 0.47358731\n",
      "Iteration 6908, loss = 0.47315171\n",
      "Iteration 6909, loss = 0.47300440\n",
      "Iteration 6910, loss = 0.47304917\n",
      "Iteration 6911, loss = 0.47329531\n",
      "Iteration 6912, loss = 0.47333125\n",
      "Iteration 6913, loss = 0.47336296\n",
      "Iteration 6914, loss = 0.47381075\n",
      "Iteration 6915, loss = 0.47436466\n",
      "Iteration 6916, loss = 0.47426728\n",
      "Iteration 6917, loss = 0.47369070\n",
      "Iteration 6918, loss = 0.47357534\n",
      "Iteration 6919, loss = 0.47374824\n",
      "Iteration 6920, loss = 0.47364773\n",
      "Iteration 6921, loss = 0.47318609\n",
      "Iteration 6922, loss = 0.47310799\n",
      "Iteration 6923, loss = 0.47332381\n",
      "Iteration 6924, loss = 0.47299966\n",
      "Iteration 6925, loss = 0.47330019\n",
      "Iteration 6926, loss = 0.47345872\n",
      "Iteration 6927, loss = 0.47329447\n",
      "Iteration 6928, loss = 0.47348771\n",
      "Iteration 6929, loss = 0.47310960\n",
      "Iteration 6930, loss = 0.47311100\n",
      "Iteration 6931, loss = 0.47311367\n",
      "Iteration 6932, loss = 0.47311374\n",
      "Iteration 6933, loss = 0.47308379\n",
      "Iteration 6934, loss = 0.47310637\n",
      "Iteration 6935, loss = 0.47395604\n",
      "Iteration 6936, loss = 0.47367513\n",
      "Iteration 6937, loss = 0.47335109\n",
      "Iteration 6938, loss = 0.47317809\n",
      "Iteration 6939, loss = 0.47317952\n",
      "Iteration 6940, loss = 0.47303225\n",
      "Iteration 6941, loss = 0.47299431\n",
      "Iteration 6942, loss = 0.47333125\n",
      "Iteration 6943, loss = 0.47469950\n",
      "Iteration 6944, loss = 0.47519237\n",
      "Iteration 6945, loss = 0.47522031\n",
      "Iteration 6946, loss = 0.47536480\n",
      "Iteration 6947, loss = 0.47405592\n",
      "Iteration 6948, loss = 0.47392980\n",
      "Iteration 6949, loss = 0.47371632\n",
      "Iteration 6950, loss = 0.47376525\n",
      "Iteration 6951, loss = 0.47391710\n",
      "Iteration 6952, loss = 0.47387128\n",
      "Iteration 6953, loss = 0.47412884\n",
      "Iteration 6954, loss = 0.47444126\n",
      "Iteration 6955, loss = 0.47451229\n",
      "Iteration 6956, loss = 0.47414302\n",
      "Iteration 6957, loss = 0.47387437\n",
      "Iteration 6958, loss = 0.47347013\n",
      "Iteration 6959, loss = 0.47330143\n",
      "Iteration 6960, loss = 0.47306429\n",
      "Iteration 6961, loss = 0.47340711\n",
      "Iteration 6962, loss = 0.47334590\n",
      "Iteration 6963, loss = 0.47325567\n",
      "Iteration 6964, loss = 0.47307542\n",
      "Iteration 6965, loss = 0.47350931\n",
      "Iteration 6966, loss = 0.47330734\n",
      "Iteration 6967, loss = 0.47321084\n",
      "Iteration 6968, loss = 0.47318975\n",
      "Iteration 6969, loss = 0.47330738\n",
      "Iteration 6970, loss = 0.47318808\n",
      "Iteration 6971, loss = 0.47346718\n",
      "Iteration 6972, loss = 0.47336535\n",
      "Iteration 6973, loss = 0.47324641\n",
      "Iteration 6974, loss = 0.47325385\n",
      "Iteration 6975, loss = 0.47315738\n",
      "Iteration 6976, loss = 0.47319042\n",
      "Iteration 6977, loss = 0.47319293\n",
      "Iteration 6978, loss = 0.47316767\n",
      "Iteration 6979, loss = 0.47318869\n",
      "Iteration 6980, loss = 0.47318075\n",
      "Iteration 6981, loss = 0.47305649\n",
      "Iteration 6982, loss = 0.47355267\n",
      "Iteration 6983, loss = 0.47345609\n",
      "Iteration 6984, loss = 0.47348280\n",
      "Iteration 6985, loss = 0.47346280\n",
      "Iteration 6986, loss = 0.47376031\n",
      "Iteration 6987, loss = 0.47342964\n",
      "Iteration 6988, loss = 0.47475216\n",
      "Iteration 6989, loss = 0.47679124\n",
      "Iteration 6990, loss = 0.47744665\n",
      "Iteration 6991, loss = 0.47679688\n",
      "Iteration 6992, loss = 0.47489528\n",
      "Iteration 6993, loss = 0.47493534\n",
      "Iteration 6994, loss = 0.47331337\n",
      "Iteration 6995, loss = 0.47355055\n",
      "Iteration 6996, loss = 0.47420922\n",
      "Iteration 6997, loss = 0.47441270\n",
      "Iteration 6998, loss = 0.47459723\n",
      "Iteration 6999, loss = 0.47455615\n",
      "Iteration 7000, loss = 0.47509975\n",
      "Iteration 7001, loss = 0.47563160\n",
      "Iteration 7002, loss = 0.47592214\n",
      "Iteration 7003, loss = 0.47579258\n",
      "Iteration 7004, loss = 0.47529283\n",
      "Iteration 7005, loss = 0.47441977\n",
      "Iteration 7006, loss = 0.47405970\n",
      "Iteration 7007, loss = 0.47338671\n",
      "Iteration 7008, loss = 0.47331472\n",
      "Iteration 7009, loss = 0.47321578\n",
      "Iteration 7010, loss = 0.47327446\n",
      "Iteration 7011, loss = 0.47331005\n",
      "Iteration 7012, loss = 0.47325009\n",
      "Iteration 7013, loss = 0.47315569\n",
      "Iteration 7014, loss = 0.47320176\n",
      "Iteration 7015, loss = 0.47315832\n",
      "Iteration 7016, loss = 0.47349843\n",
      "Iteration 7017, loss = 0.47362452\n",
      "Iteration 7018, loss = 0.47367166\n",
      "Iteration 7019, loss = 0.47353128\n",
      "Iteration 7020, loss = 0.47357070\n",
      "Iteration 7021, loss = 0.47338100\n",
      "Iteration 7022, loss = 0.47338902\n",
      "Iteration 7023, loss = 0.47368851\n",
      "Iteration 7024, loss = 0.47453177\n",
      "Iteration 7025, loss = 0.47548552\n",
      "Iteration 7026, loss = 0.47643417\n",
      "Iteration 7027, loss = 0.47537624\n",
      "Iteration 7028, loss = 0.47400088\n",
      "Iteration 7029, loss = 0.47415358\n",
      "Iteration 7030, loss = 0.47375649\n",
      "Iteration 7031, loss = 0.47387483\n",
      "Iteration 7032, loss = 0.47378828\n",
      "Iteration 7033, loss = 0.47363370\n",
      "Iteration 7034, loss = 0.47354718\n",
      "Iteration 7035, loss = 0.47351636\n",
      "Iteration 7036, loss = 0.47356707\n",
      "Iteration 7037, loss = 0.47340761\n",
      "Iteration 7038, loss = 0.47350238\n",
      "Iteration 7039, loss = 0.47385176\n",
      "Iteration 7040, loss = 0.47429122\n",
      "Iteration 7041, loss = 0.47506188\n",
      "Iteration 7042, loss = 0.47461731\n",
      "Iteration 7043, loss = 0.47386237\n",
      "Iteration 7044, loss = 0.47311508\n",
      "Iteration 7045, loss = 0.47296172\n",
      "Iteration 7046, loss = 0.47336122\n",
      "Iteration 7047, loss = 0.47351723\n",
      "Iteration 7048, loss = 0.47379290\n",
      "Iteration 7049, loss = 0.47357029\n",
      "Iteration 7050, loss = 0.47323523\n",
      "Iteration 7051, loss = 0.47308250\n",
      "Iteration 7052, loss = 0.47303108\n",
      "Iteration 7053, loss = 0.47341804\n",
      "Iteration 7054, loss = 0.47309444\n",
      "Iteration 7055, loss = 0.47291872\n",
      "Iteration 7056, loss = 0.47301859\n",
      "Iteration 7057, loss = 0.47345888\n",
      "Iteration 7058, loss = 0.47392068\n",
      "Iteration 7059, loss = 0.47383146\n",
      "Iteration 7060, loss = 0.47315119\n",
      "Iteration 7061, loss = 0.47291864\n",
      "Iteration 7062, loss = 0.47347356\n",
      "Iteration 7063, loss = 0.47504075\n",
      "Iteration 7064, loss = 0.47492054\n",
      "Iteration 7065, loss = 0.47397573\n",
      "Iteration 7066, loss = 0.47336098\n",
      "Iteration 7067, loss = 0.47326006\n",
      "Iteration 7068, loss = 0.47323433\n",
      "Iteration 7069, loss = 0.47332832\n",
      "Iteration 7070, loss = 0.47325453\n",
      "Iteration 7071, loss = 0.47313996\n",
      "Iteration 7072, loss = 0.47313312\n",
      "Iteration 7073, loss = 0.47322500\n",
      "Iteration 7074, loss = 0.47386374\n",
      "Iteration 7075, loss = 0.47420740\n",
      "Iteration 7076, loss = 0.47407862\n",
      "Iteration 7077, loss = 0.47345636\n",
      "Iteration 7078, loss = 0.47328674\n",
      "Iteration 7079, loss = 0.47359207\n",
      "Iteration 7080, loss = 0.47347223\n",
      "Iteration 7081, loss = 0.47339231\n",
      "Iteration 7082, loss = 0.47319374\n",
      "Iteration 7083, loss = 0.47340993\n",
      "Iteration 7084, loss = 0.47354676\n",
      "Iteration 7085, loss = 0.47361885\n",
      "Iteration 7086, loss = 0.47385726\n",
      "Iteration 7087, loss = 0.47305664\n",
      "Iteration 7088, loss = 0.47304585\n",
      "Iteration 7089, loss = 0.47347467\n",
      "Iteration 7090, loss = 0.47289737\n",
      "Iteration 7091, loss = 0.47310739\n",
      "Iteration 7092, loss = 0.47366964\n",
      "Iteration 7093, loss = 0.47422431\n",
      "Iteration 7094, loss = 0.47442431\n",
      "Iteration 7095, loss = 0.47413389\n",
      "Iteration 7096, loss = 0.47362266\n",
      "Iteration 7097, loss = 0.47338235\n",
      "Iteration 7098, loss = 0.47326690\n",
      "Iteration 7099, loss = 0.47366162\n",
      "Iteration 7100, loss = 0.47408889\n",
      "Iteration 7101, loss = 0.47437201\n",
      "Iteration 7102, loss = 0.47428566\n",
      "Iteration 7103, loss = 0.47393404\n",
      "Iteration 7104, loss = 0.47339731\n",
      "Iteration 7105, loss = 0.47354360\n",
      "Iteration 7106, loss = 0.47401366\n",
      "Iteration 7107, loss = 0.47358811\n",
      "Iteration 7108, loss = 0.47294085\n",
      "Iteration 7109, loss = 0.47277985\n",
      "Iteration 7110, loss = 0.47360062\n",
      "Iteration 7111, loss = 0.47532018\n",
      "Iteration 7112, loss = 0.47500128\n",
      "Iteration 7113, loss = 0.47357671\n",
      "Iteration 7114, loss = 0.47359446\n",
      "Iteration 7115, loss = 0.47373954\n",
      "Iteration 7116, loss = 0.47383145\n",
      "Iteration 7117, loss = 0.47376131\n",
      "Iteration 7118, loss = 0.47355960\n",
      "Iteration 7119, loss = 0.47340119\n",
      "Iteration 7120, loss = 0.47337105\n",
      "Iteration 7121, loss = 0.47351097\n",
      "Iteration 7122, loss = 0.47346561\n",
      "Iteration 7123, loss = 0.47371676\n",
      "Iteration 7124, loss = 0.47400340\n",
      "Iteration 7125, loss = 0.47383799\n",
      "Iteration 7126, loss = 0.47342596\n",
      "Iteration 7127, loss = 0.47339948\n",
      "Iteration 7128, loss = 0.47303192\n",
      "Iteration 7129, loss = 0.47282796\n",
      "Iteration 7130, loss = 0.47370552\n",
      "Iteration 7131, loss = 0.47393832\n",
      "Iteration 7132, loss = 0.47400404\n",
      "Iteration 7133, loss = 0.47391954\n",
      "Iteration 7134, loss = 0.47385492\n",
      "Iteration 7135, loss = 0.47381945\n",
      "Iteration 7136, loss = 0.47363576\n",
      "Iteration 7137, loss = 0.47406627\n",
      "Iteration 7138, loss = 0.47357127\n",
      "Iteration 7139, loss = 0.47353005\n",
      "Iteration 7140, loss = 0.47405712\n",
      "Iteration 7141, loss = 0.47373048\n",
      "Iteration 7142, loss = 0.47370069\n",
      "Iteration 7143, loss = 0.47335763\n",
      "Iteration 7144, loss = 0.47336070\n",
      "Iteration 7145, loss = 0.47333400\n",
      "Iteration 7146, loss = 0.47323851\n",
      "Iteration 7147, loss = 0.47388906\n",
      "Iteration 7148, loss = 0.47333703\n",
      "Iteration 7149, loss = 0.47326046\n",
      "Iteration 7150, loss = 0.47372007\n",
      "Iteration 7151, loss = 0.47372918\n",
      "Iteration 7152, loss = 0.47341800\n",
      "Iteration 7153, loss = 0.47312262\n",
      "Iteration 7154, loss = 0.47307581\n",
      "Iteration 7155, loss = 0.47367389\n",
      "Iteration 7156, loss = 0.47479445\n",
      "Iteration 7157, loss = 0.47545176\n",
      "Iteration 7158, loss = 0.47574968\n",
      "Iteration 7159, loss = 0.47540766\n",
      "Iteration 7160, loss = 0.47409959\n",
      "Iteration 7161, loss = 0.47347838\n",
      "Iteration 7162, loss = 0.47333414\n",
      "Iteration 7163, loss = 0.47353319\n",
      "Iteration 7164, loss = 0.47341529\n",
      "Iteration 7165, loss = 0.47362072\n",
      "Iteration 7166, loss = 0.47323395\n",
      "Iteration 7167, loss = 0.47337823\n",
      "Iteration 7168, loss = 0.47328484\n",
      "Iteration 7169, loss = 0.47332628\n",
      "Iteration 7170, loss = 0.47427343\n",
      "Iteration 7171, loss = 0.47548844\n",
      "Iteration 7172, loss = 0.47589209\n",
      "Iteration 7173, loss = 0.47573678\n",
      "Iteration 7174, loss = 0.47538540\n",
      "Iteration 7175, loss = 0.47553941\n",
      "Iteration 7176, loss = 0.47527384\n",
      "Iteration 7177, loss = 0.47460966\n",
      "Iteration 7178, loss = 0.47361215\n",
      "Iteration 7179, loss = 0.47351757\n",
      "Iteration 7180, loss = 0.47328109\n",
      "Iteration 7181, loss = 0.47326014\n",
      "Iteration 7182, loss = 0.47311247\n",
      "Iteration 7183, loss = 0.47325125\n",
      "Iteration 7184, loss = 0.47332900\n",
      "Iteration 7185, loss = 0.47306238\n",
      "Iteration 7186, loss = 0.47312313\n",
      "Iteration 7187, loss = 0.47352360\n",
      "Iteration 7188, loss = 0.47359135\n",
      "Iteration 7189, loss = 0.47314150\n",
      "Iteration 7190, loss = 0.47349428\n",
      "Iteration 7191, loss = 0.47381200\n",
      "Iteration 7192, loss = 0.47310735\n",
      "Iteration 7193, loss = 0.47268198\n",
      "Iteration 7194, loss = 0.47325535\n",
      "Iteration 7195, loss = 0.47526382\n",
      "Iteration 7196, loss = 0.47636784\n",
      "Iteration 7197, loss = 0.47698189\n",
      "Iteration 7198, loss = 0.47688737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7199, loss = 0.47578342\n",
      "Iteration 7200, loss = 0.47436231\n",
      "Iteration 7201, loss = 0.47375866\n",
      "Iteration 7202, loss = 0.47363036\n",
      "Iteration 7203, loss = 0.47312650\n",
      "Iteration 7204, loss = 0.47311022\n",
      "Iteration 7205, loss = 0.47303319\n",
      "Iteration 7206, loss = 0.47303790\n",
      "Iteration 7207, loss = 0.47309616\n",
      "Iteration 7208, loss = 0.47307479\n",
      "Iteration 7209, loss = 0.47302298\n",
      "Iteration 7210, loss = 0.47302474\n",
      "Iteration 7211, loss = 0.47322684\n",
      "Iteration 7212, loss = 0.47344306\n",
      "Iteration 7213, loss = 0.47399256\n",
      "Iteration 7214, loss = 0.47376926\n",
      "Iteration 7215, loss = 0.47329883\n",
      "Iteration 7216, loss = 0.47335922\n",
      "Iteration 7217, loss = 0.47357318\n",
      "Iteration 7218, loss = 0.47401566\n",
      "Iteration 7219, loss = 0.47408542\n",
      "Iteration 7220, loss = 0.47375661\n",
      "Iteration 7221, loss = 0.47351355\n",
      "Iteration 7222, loss = 0.47278697\n",
      "Iteration 7223, loss = 0.47376422\n",
      "Iteration 7224, loss = 0.47370124\n",
      "Iteration 7225, loss = 0.47428861\n",
      "Iteration 7226, loss = 0.47446672\n",
      "Iteration 7227, loss = 0.47417486\n",
      "Iteration 7228, loss = 0.47403136\n",
      "Iteration 7229, loss = 0.47305979\n",
      "Iteration 7230, loss = 0.47327915\n",
      "Iteration 7231, loss = 0.47312497\n",
      "Iteration 7232, loss = 0.47306590\n",
      "Iteration 7233, loss = 0.47313758\n",
      "Iteration 7234, loss = 0.47303542\n",
      "Iteration 7235, loss = 0.47298707\n",
      "Iteration 7236, loss = 0.47353383\n",
      "Iteration 7237, loss = 0.47343317\n",
      "Iteration 7238, loss = 0.47324744\n",
      "Iteration 7239, loss = 0.47332542\n",
      "Iteration 7240, loss = 0.47357621\n",
      "Iteration 7241, loss = 0.47556039\n",
      "Iteration 7242, loss = 0.47514063\n",
      "Iteration 7243, loss = 0.47402982\n",
      "Iteration 7244, loss = 0.47291895\n",
      "Iteration 7245, loss = 0.47353631\n",
      "Iteration 7246, loss = 0.47457467\n",
      "Iteration 7247, loss = 0.47565160\n",
      "Iteration 7248, loss = 0.47722812\n",
      "Iteration 7249, loss = 0.47723446\n",
      "Iteration 7250, loss = 0.47515369\n",
      "Iteration 7251, loss = 0.47364076\n",
      "Iteration 7252, loss = 0.47330582\n",
      "Iteration 7253, loss = 0.47455977\n",
      "Iteration 7254, loss = 0.47487019\n",
      "Iteration 7255, loss = 0.47452194\n",
      "Iteration 7256, loss = 0.47448498\n",
      "Iteration 7257, loss = 0.47400183\n",
      "Iteration 7258, loss = 0.47361307\n",
      "Iteration 7259, loss = 0.47349540\n",
      "Iteration 7260, loss = 0.47352299\n",
      "Iteration 7261, loss = 0.47367307\n",
      "Iteration 7262, loss = 0.47332127\n",
      "Iteration 7263, loss = 0.47345663\n",
      "Iteration 7264, loss = 0.47459315\n",
      "Iteration 7265, loss = 0.47379611\n",
      "Iteration 7266, loss = 0.47431274\n",
      "Iteration 7267, loss = 0.47394059\n",
      "Iteration 7268, loss = 0.47429936\n",
      "Iteration 7269, loss = 0.47494190\n",
      "Iteration 7270, loss = 0.47507179\n",
      "Iteration 7271, loss = 0.47458880\n",
      "Iteration 7272, loss = 0.47403721\n",
      "Iteration 7273, loss = 0.47347068\n",
      "Iteration 7274, loss = 0.47337306\n",
      "Iteration 7275, loss = 0.47317383\n",
      "Iteration 7276, loss = 0.47321618\n",
      "Iteration 7277, loss = 0.47296191\n",
      "Iteration 7278, loss = 0.47306447\n",
      "Iteration 7279, loss = 0.47335320\n",
      "Iteration 7280, loss = 0.47300817\n",
      "Iteration 7281, loss = 0.47374092\n",
      "Iteration 7282, loss = 0.47300484\n",
      "Iteration 7283, loss = 0.47293868\n",
      "Iteration 7284, loss = 0.47295059\n",
      "Iteration 7285, loss = 0.47305942\n",
      "Iteration 7286, loss = 0.47355737\n",
      "Iteration 7287, loss = 0.47325372\n",
      "Iteration 7288, loss = 0.47306742\n",
      "Iteration 7289, loss = 0.47296603\n",
      "Iteration 7290, loss = 0.47312404\n",
      "Iteration 7291, loss = 0.47321233\n",
      "Iteration 7292, loss = 0.47337169\n",
      "Iteration 7293, loss = 0.47275346\n",
      "Iteration 7294, loss = 0.47271551\n",
      "Iteration 7295, loss = 0.47424171\n",
      "Iteration 7296, loss = 0.47576686\n",
      "Iteration 7297, loss = 0.47637825\n",
      "Iteration 7298, loss = 0.47604706\n",
      "Iteration 7299, loss = 0.47489191\n",
      "Iteration 7300, loss = 0.47423832\n",
      "Iteration 7301, loss = 0.47388502\n",
      "Iteration 7302, loss = 0.47350856\n",
      "Iteration 7303, loss = 0.47351826\n",
      "Iteration 7304, loss = 0.47325003\n",
      "Iteration 7305, loss = 0.47342553\n",
      "Iteration 7306, loss = 0.47325227\n",
      "Iteration 7307, loss = 0.47336207\n",
      "Iteration 7308, loss = 0.47309607\n",
      "Iteration 7309, loss = 0.47316967\n",
      "Iteration 7310, loss = 0.47309656\n",
      "Iteration 7311, loss = 0.47293615\n",
      "Iteration 7312, loss = 0.47288393\n",
      "Iteration 7313, loss = 0.47319750\n",
      "Iteration 7314, loss = 0.47343388\n",
      "Iteration 7315, loss = 0.47367172\n",
      "Iteration 7316, loss = 0.47381793\n",
      "Iteration 7317, loss = 0.47347138\n",
      "Iteration 7318, loss = 0.47321844\n",
      "Iteration 7319, loss = 0.47295857\n",
      "Iteration 7320, loss = 0.47318033\n",
      "Iteration 7321, loss = 0.47361522\n",
      "Iteration 7322, loss = 0.47401880\n",
      "Iteration 7323, loss = 0.47381041\n",
      "Iteration 7324, loss = 0.47332550\n",
      "Iteration 7325, loss = 0.47293462\n",
      "Iteration 7326, loss = 0.47327527\n",
      "Iteration 7327, loss = 0.47293329\n",
      "Iteration 7328, loss = 0.47273851\n",
      "Iteration 7329, loss = 0.47327075\n",
      "Iteration 7330, loss = 0.47417975\n",
      "Iteration 7331, loss = 0.47491850\n",
      "Iteration 7332, loss = 0.47492327\n",
      "Iteration 7333, loss = 0.47409803\n",
      "Iteration 7334, loss = 0.47377401\n",
      "Iteration 7335, loss = 0.47319973\n",
      "Iteration 7336, loss = 0.47290413\n",
      "Iteration 7337, loss = 0.47288230\n",
      "Iteration 7338, loss = 0.47329143\n",
      "Iteration 7339, loss = 0.47338800\n",
      "Iteration 7340, loss = 0.47337785\n",
      "Iteration 7341, loss = 0.47334900\n",
      "Iteration 7342, loss = 0.47331886\n",
      "Iteration 7343, loss = 0.47332835\n",
      "Iteration 7344, loss = 0.47338270\n",
      "Iteration 7345, loss = 0.47351762\n",
      "Iteration 7346, loss = 0.47451237\n",
      "Iteration 7347, loss = 0.47384369\n",
      "Iteration 7348, loss = 0.47308837\n",
      "Iteration 7349, loss = 0.47299427\n",
      "Iteration 7350, loss = 0.47312785\n",
      "Iteration 7351, loss = 0.47395759\n",
      "Iteration 7352, loss = 0.47433513\n",
      "Iteration 7353, loss = 0.47434295\n",
      "Iteration 7354, loss = 0.47441890\n",
      "Iteration 7355, loss = 0.47449423\n",
      "Iteration 7356, loss = 0.47415682\n",
      "Iteration 7357, loss = 0.47351253\n",
      "Iteration 7358, loss = 0.47274885\n",
      "Iteration 7359, loss = 0.47410369\n",
      "Iteration 7360, loss = 0.47444972\n",
      "Iteration 7361, loss = 0.47599054\n",
      "Iteration 7362, loss = 0.47530785\n",
      "Iteration 7363, loss = 0.47366681\n",
      "Iteration 7364, loss = 0.47300535\n",
      "Iteration 7365, loss = 0.47345733\n",
      "Iteration 7366, loss = 0.47371003\n",
      "Iteration 7367, loss = 0.47398651\n",
      "Iteration 7368, loss = 0.47407783\n",
      "Iteration 7369, loss = 0.47388415\n",
      "Iteration 7370, loss = 0.47368551\n",
      "Iteration 7371, loss = 0.47330361\n",
      "Iteration 7372, loss = 0.47305034\n",
      "Iteration 7373, loss = 0.47298631\n",
      "Iteration 7374, loss = 0.47305924\n",
      "Iteration 7375, loss = 0.47306711\n",
      "Iteration 7376, loss = 0.47302563\n",
      "Iteration 7377, loss = 0.47309851\n",
      "Iteration 7378, loss = 0.47297393\n",
      "Iteration 7379, loss = 0.47293287\n",
      "Iteration 7380, loss = 0.47295114\n",
      "Iteration 7381, loss = 0.47292367\n",
      "Iteration 7382, loss = 0.47287990\n",
      "Iteration 7383, loss = 0.47284540\n",
      "Iteration 7384, loss = 0.47283223\n",
      "Iteration 7385, loss = 0.47297259\n",
      "Iteration 7386, loss = 0.47348005\n",
      "Iteration 7387, loss = 0.47413273\n",
      "Iteration 7388, loss = 0.47427840\n",
      "Iteration 7389, loss = 0.47368686\n",
      "Iteration 7390, loss = 0.47323438\n",
      "Iteration 7391, loss = 0.47330670\n",
      "Iteration 7392, loss = 0.47387756\n",
      "Iteration 7393, loss = 0.47391038\n",
      "Iteration 7394, loss = 0.47350301\n",
      "Iteration 7395, loss = 0.47298672\n",
      "Iteration 7396, loss = 0.47366947\n",
      "Iteration 7397, loss = 0.47454639\n",
      "Iteration 7398, loss = 0.47522575\n",
      "Iteration 7399, loss = 0.47540408\n",
      "Iteration 7400, loss = 0.47514723\n",
      "Iteration 7401, loss = 0.47454094\n",
      "Iteration 7402, loss = 0.47388069\n",
      "Iteration 7403, loss = 0.47359255\n",
      "Iteration 7404, loss = 0.47306583\n",
      "Iteration 7405, loss = 0.47287470\n",
      "Iteration 7406, loss = 0.47285424\n",
      "Iteration 7407, loss = 0.47297344\n",
      "Iteration 7408, loss = 0.47320672\n",
      "Iteration 7409, loss = 0.47328315\n",
      "Iteration 7410, loss = 0.47349794\n",
      "Iteration 7411, loss = 0.47315488\n",
      "Iteration 7412, loss = 0.47347301\n",
      "Iteration 7413, loss = 0.47382393\n",
      "Iteration 7414, loss = 0.47385932\n",
      "Iteration 7415, loss = 0.47337209\n",
      "Iteration 7416, loss = 0.47325243\n",
      "Iteration 7417, loss = 0.47321912\n",
      "Iteration 7418, loss = 0.47424217\n",
      "Iteration 7419, loss = 0.47500224\n",
      "Iteration 7420, loss = 0.47492218\n",
      "Iteration 7421, loss = 0.47441660\n",
      "Iteration 7422, loss = 0.47393658\n",
      "Iteration 7423, loss = 0.47364348\n",
      "Iteration 7424, loss = 0.47295826\n",
      "Iteration 7425, loss = 0.47282862\n",
      "Iteration 7426, loss = 0.47295054\n",
      "Iteration 7427, loss = 0.47290516\n",
      "Iteration 7428, loss = 0.47298924\n",
      "Iteration 7429, loss = 0.47299135\n",
      "Iteration 7430, loss = 0.47296480\n",
      "Iteration 7431, loss = 0.47311461\n",
      "Iteration 7432, loss = 0.47353200\n",
      "Iteration 7433, loss = 0.47345518\n",
      "Iteration 7434, loss = 0.47327186\n",
      "Iteration 7435, loss = 0.47317296\n",
      "Iteration 7436, loss = 0.47316307\n",
      "Iteration 7437, loss = 0.47341868\n",
      "Iteration 7438, loss = 0.47381632\n",
      "Iteration 7439, loss = 0.47410787\n",
      "Iteration 7440, loss = 0.47362481\n",
      "Iteration 7441, loss = 0.47311826\n",
      "Iteration 7442, loss = 0.47327956\n",
      "Iteration 7443, loss = 0.47372932\n",
      "Iteration 7444, loss = 0.47438804\n",
      "Iteration 7445, loss = 0.47370727\n",
      "Iteration 7446, loss = 0.47308996\n",
      "Iteration 7447, loss = 0.47337337\n",
      "Iteration 7448, loss = 0.47342243\n",
      "Iteration 7449, loss = 0.47322790\n",
      "Iteration 7450, loss = 0.47265905\n",
      "Iteration 7451, loss = 0.47277476\n",
      "Iteration 7452, loss = 0.47348572\n",
      "Iteration 7453, loss = 0.47531609\n",
      "Iteration 7454, loss = 0.47835196\n",
      "Iteration 7455, loss = 0.48038579\n",
      "Iteration 7456, loss = 0.47946896\n",
      "Iteration 7457, loss = 0.47671941\n",
      "Iteration 7458, loss = 0.47393785\n",
      "Iteration 7459, loss = 0.47283611\n",
      "Iteration 7460, loss = 0.47416663\n",
      "Iteration 7461, loss = 0.47609022\n",
      "Iteration 7462, loss = 0.47636398\n",
      "Iteration 7463, loss = 0.47549150\n",
      "Iteration 7464, loss = 0.47419571\n",
      "Iteration 7465, loss = 0.47399627\n",
      "Iteration 7466, loss = 0.47315473\n",
      "Iteration 7467, loss = 0.47289990\n",
      "Iteration 7468, loss = 0.47327900\n",
      "Iteration 7469, loss = 0.47414328\n",
      "Iteration 7470, loss = 0.47330596\n",
      "Iteration 7471, loss = 0.47279768\n",
      "Iteration 7472, loss = 0.47371813\n",
      "Iteration 7473, loss = 0.47410669\n",
      "Iteration 7474, loss = 0.47417634\n",
      "Iteration 7475, loss = 0.47376373\n",
      "Iteration 7476, loss = 0.47308740\n",
      "Iteration 7477, loss = 0.47311998\n",
      "Iteration 7478, loss = 0.47362356\n",
      "Iteration 7479, loss = 0.47402357\n",
      "Iteration 7480, loss = 0.47429006\n",
      "Iteration 7481, loss = 0.47410048\n",
      "Iteration 7482, loss = 0.47337182\n",
      "Iteration 7483, loss = 0.47314385\n",
      "Iteration 7484, loss = 0.47373138\n",
      "Iteration 7485, loss = 0.47427165\n",
      "Iteration 7486, loss = 0.47501865\n",
      "Iteration 7487, loss = 0.47609175\n",
      "Iteration 7488, loss = 0.47604470\n",
      "Iteration 7489, loss = 0.47496311\n",
      "Iteration 7490, loss = 0.47464715\n",
      "Iteration 7491, loss = 0.47399280\n",
      "Iteration 7492, loss = 0.47380481\n",
      "Iteration 7493, loss = 0.47373459\n",
      "Iteration 7494, loss = 0.47388071\n",
      "Iteration 7495, loss = 0.47443914\n",
      "Iteration 7496, loss = 0.47442789\n",
      "Iteration 7497, loss = 0.47405402\n",
      "Iteration 7498, loss = 0.47333574\n",
      "Iteration 7499, loss = 0.47319791\n",
      "Iteration 7500, loss = 0.47376877\n",
      "Iteration 7501, loss = 0.47350481\n",
      "Iteration 7502, loss = 0.47340842\n",
      "Iteration 7503, loss = 0.47331207\n",
      "Iteration 7504, loss = 0.47325436\n",
      "Iteration 7505, loss = 0.47351910\n",
      "Iteration 7506, loss = 0.47371167\n",
      "Iteration 7507, loss = 0.47399715\n",
      "Iteration 7508, loss = 0.47442224\n",
      "Iteration 7509, loss = 0.47529611\n",
      "Iteration 7510, loss = 0.47491797\n",
      "Iteration 7511, loss = 0.47380912\n",
      "Iteration 7512, loss = 0.47293637\n",
      "Iteration 7513, loss = 0.47382887\n",
      "Iteration 7514, loss = 0.47444126\n",
      "Iteration 7515, loss = 0.47422616\n",
      "Iteration 7516, loss = 0.47324998\n",
      "Iteration 7517, loss = 0.47221892\n",
      "Iteration 7518, loss = 0.47447082\n",
      "Iteration 7519, loss = 0.47714570\n",
      "Iteration 7520, loss = 0.47929090\n",
      "Iteration 7521, loss = 0.48046759\n",
      "Iteration 7522, loss = 0.47904873\n",
      "Iteration 7523, loss = 0.47499614\n",
      "Iteration 7524, loss = 0.47334399\n",
      "Iteration 7525, loss = 0.47392530\n",
      "Iteration 7526, loss = 0.47639905\n",
      "Iteration 7527, loss = 0.47741460\n",
      "Iteration 7528, loss = 0.47709470\n",
      "Iteration 7529, loss = 0.47528032\n",
      "Iteration 7530, loss = 0.47308861\n",
      "Iteration 7531, loss = 0.47359512\n",
      "Iteration 7532, loss = 0.47461157\n",
      "Iteration 7533, loss = 0.47564853\n",
      "Iteration 7534, loss = 0.47581800\n",
      "Iteration 7535, loss = 0.47452978\n",
      "Iteration 7536, loss = 0.47410496\n",
      "Iteration 7537, loss = 0.47305207\n",
      "Iteration 7538, loss = 0.47334334\n",
      "Iteration 7539, loss = 0.47354689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7540, loss = 0.47368810\n",
      "Iteration 7541, loss = 0.47356213\n",
      "Iteration 7542, loss = 0.47326102\n",
      "Iteration 7543, loss = 0.47351872\n",
      "Iteration 7544, loss = 0.47333400\n",
      "Iteration 7545, loss = 0.47326833\n",
      "Iteration 7546, loss = 0.47376332\n",
      "Iteration 7547, loss = 0.47383026\n",
      "Iteration 7548, loss = 0.47368720\n",
      "Iteration 7549, loss = 0.47322233\n",
      "Iteration 7550, loss = 0.47319292\n",
      "Iteration 7551, loss = 0.47375167\n",
      "Iteration 7552, loss = 0.47395269\n",
      "Iteration 7553, loss = 0.47332282\n",
      "Iteration 7554, loss = 0.47266895\n",
      "Iteration 7555, loss = 0.47379152\n",
      "Iteration 7556, loss = 0.47477470\n",
      "Iteration 7557, loss = 0.47347891\n",
      "Iteration 7558, loss = 0.47347725\n",
      "Iteration 7559, loss = 0.47383911\n",
      "Iteration 7560, loss = 0.47382908\n",
      "Iteration 7561, loss = 0.47364431\n",
      "Iteration 7562, loss = 0.47317160\n",
      "Iteration 7563, loss = 0.47302956\n",
      "Iteration 7564, loss = 0.47286287\n",
      "Iteration 7565, loss = 0.47279658\n",
      "Iteration 7566, loss = 0.47297579\n",
      "Iteration 7567, loss = 0.47369820\n",
      "Iteration 7568, loss = 0.47447089\n",
      "Iteration 7569, loss = 0.47366929\n",
      "Iteration 7570, loss = 0.47241541\n",
      "Iteration 7571, loss = 0.47339848\n",
      "Iteration 7572, loss = 0.47531254\n",
      "Iteration 7573, loss = 0.47594461\n",
      "Iteration 7574, loss = 0.47451232\n",
      "Iteration 7575, loss = 0.47332377\n",
      "Iteration 7576, loss = 0.47294053\n",
      "Iteration 7577, loss = 0.47356490\n",
      "Iteration 7578, loss = 0.47393655\n",
      "Iteration 7579, loss = 0.47375350\n",
      "Iteration 7580, loss = 0.47300920\n",
      "Iteration 7581, loss = 0.47254117\n",
      "Iteration 7582, loss = 0.47383933\n",
      "Iteration 7583, loss = 0.47410734\n",
      "Iteration 7584, loss = 0.47409836\n",
      "Iteration 7585, loss = 0.47308736\n",
      "Iteration 7586, loss = 0.47304459\n",
      "Iteration 7587, loss = 0.47352357\n",
      "Iteration 7588, loss = 0.47400780\n",
      "Iteration 7589, loss = 0.47432083\n",
      "Iteration 7590, loss = 0.47448325\n",
      "Iteration 7591, loss = 0.47434090\n",
      "Iteration 7592, loss = 0.47319394\n",
      "Iteration 7593, loss = 0.47312892\n",
      "Iteration 7594, loss = 0.47312667\n",
      "Iteration 7595, loss = 0.47352904\n",
      "Iteration 7596, loss = 0.47323967\n",
      "Iteration 7597, loss = 0.47285294\n",
      "Iteration 7598, loss = 0.47357514\n",
      "Iteration 7599, loss = 0.47423326\n",
      "Iteration 7600, loss = 0.47466842\n",
      "Iteration 7601, loss = 0.47509420\n",
      "Iteration 7602, loss = 0.47404859\n",
      "Iteration 7603, loss = 0.47343955\n",
      "Iteration 7604, loss = 0.47264199\n",
      "Iteration 7605, loss = 0.47274086\n",
      "Iteration 7606, loss = 0.47310064\n",
      "Iteration 7607, loss = 0.47322548\n",
      "Iteration 7608, loss = 0.47328093\n",
      "Iteration 7609, loss = 0.47312440\n",
      "Iteration 7610, loss = 0.47300853\n",
      "Iteration 7611, loss = 0.47325723\n",
      "Iteration 7612, loss = 0.47315519\n",
      "Iteration 7613, loss = 0.47304280\n",
      "Iteration 7614, loss = 0.47283015\n",
      "Iteration 7615, loss = 0.47305415\n",
      "Iteration 7616, loss = 0.47334702\n",
      "Iteration 7617, loss = 0.47355211\n",
      "Iteration 7618, loss = 0.47355909\n",
      "Iteration 7619, loss = 0.47355026\n",
      "Iteration 7620, loss = 0.47326762\n",
      "Iteration 7621, loss = 0.47282156\n",
      "Iteration 7622, loss = 0.47289945\n",
      "Iteration 7623, loss = 0.47385936\n",
      "Iteration 7624, loss = 0.47424764\n",
      "Iteration 7625, loss = 0.47349184\n",
      "Iteration 7626, loss = 0.47272508\n",
      "Iteration 7627, loss = 0.47279821\n",
      "Iteration 7628, loss = 0.47321352\n",
      "Iteration 7629, loss = 0.47386456\n",
      "Iteration 7630, loss = 0.47430711\n",
      "Iteration 7631, loss = 0.47450467\n",
      "Iteration 7632, loss = 0.47438105\n",
      "Iteration 7633, loss = 0.47403202\n",
      "Iteration 7634, loss = 0.47332657\n",
      "Iteration 7635, loss = 0.47311081\n",
      "Iteration 7636, loss = 0.47296770\n",
      "Iteration 7637, loss = 0.47309010\n",
      "Iteration 7638, loss = 0.47309501\n",
      "Iteration 7639, loss = 0.47342854\n",
      "Iteration 7640, loss = 0.47328853\n",
      "Iteration 7641, loss = 0.47315810\n",
      "Iteration 7642, loss = 0.47303628\n",
      "Iteration 7643, loss = 0.47313268\n",
      "Iteration 7644, loss = 0.47438261\n",
      "Iteration 7645, loss = 0.47385974\n",
      "Iteration 7646, loss = 0.47298383\n",
      "Iteration 7647, loss = 0.47287836\n",
      "Iteration 7648, loss = 0.47284601\n",
      "Iteration 7649, loss = 0.47302332\n",
      "Iteration 7650, loss = 0.47313507\n",
      "Iteration 7651, loss = 0.47292972\n",
      "Iteration 7652, loss = 0.47278926\n",
      "Iteration 7653, loss = 0.47298247\n",
      "Iteration 7654, loss = 0.47290874\n",
      "Iteration 7655, loss = 0.47282450\n",
      "Iteration 7656, loss = 0.47274843\n",
      "Iteration 7657, loss = 0.47296807\n",
      "Iteration 7658, loss = 0.47286712\n",
      "Iteration 7659, loss = 0.47254601\n",
      "Iteration 7660, loss = 0.47395670\n",
      "Iteration 7661, loss = 0.47422537\n",
      "Iteration 7662, loss = 0.47412089\n",
      "Iteration 7663, loss = 0.47387045\n",
      "Iteration 7664, loss = 0.47400089\n",
      "Iteration 7665, loss = 0.47400946\n",
      "Iteration 7666, loss = 0.47413578\n",
      "Iteration 7667, loss = 0.47381114\n",
      "Iteration 7668, loss = 0.47358187\n",
      "Iteration 7669, loss = 0.47308123\n",
      "Iteration 7670, loss = 0.47336301\n",
      "Iteration 7671, loss = 0.47317228\n",
      "Iteration 7672, loss = 0.47311835\n",
      "Iteration 7673, loss = 0.47295944\n",
      "Iteration 7674, loss = 0.47277978\n",
      "Iteration 7675, loss = 0.47269205\n",
      "Iteration 7676, loss = 0.47274808\n",
      "Iteration 7677, loss = 0.47316863\n",
      "Iteration 7678, loss = 0.47356712\n",
      "Iteration 7679, loss = 0.47376662\n",
      "Iteration 7680, loss = 0.47398152\n",
      "Iteration 7681, loss = 0.47353230\n",
      "Iteration 7682, loss = 0.47236164\n",
      "Iteration 7683, loss = 0.47310653\n",
      "Iteration 7684, loss = 0.47541919\n",
      "Iteration 7685, loss = 0.47504640\n",
      "Iteration 7686, loss = 0.47392483\n",
      "Iteration 7687, loss = 0.47296828\n",
      "Iteration 7688, loss = 0.47280236\n",
      "Iteration 7689, loss = 0.47304313\n",
      "Iteration 7690, loss = 0.47423373\n",
      "Iteration 7691, loss = 0.47485272\n",
      "Iteration 7692, loss = 0.47467651\n",
      "Iteration 7693, loss = 0.47388714\n",
      "Iteration 7694, loss = 0.47324119\n",
      "Iteration 7695, loss = 0.47286332\n",
      "Iteration 7696, loss = 0.47305050\n",
      "Iteration 7697, loss = 0.47320640\n",
      "Iteration 7698, loss = 0.47328271\n",
      "Iteration 7699, loss = 0.47321672\n",
      "Iteration 7700, loss = 0.47293171\n",
      "Iteration 7701, loss = 0.47264773\n",
      "Iteration 7702, loss = 0.47273498\n",
      "Iteration 7703, loss = 0.47331816\n",
      "Iteration 7704, loss = 0.47448367\n",
      "Iteration 7705, loss = 0.47449085\n",
      "Iteration 7706, loss = 0.47384483\n",
      "Iteration 7707, loss = 0.47338248\n",
      "Iteration 7708, loss = 0.47240604\n",
      "Iteration 7709, loss = 0.47271771\n",
      "Iteration 7710, loss = 0.47448838\n",
      "Iteration 7711, loss = 0.47642875\n",
      "Iteration 7712, loss = 0.47730111\n",
      "Iteration 7713, loss = 0.47686717\n",
      "Iteration 7714, loss = 0.47487404\n",
      "Iteration 7715, loss = 0.47278685\n",
      "Iteration 7716, loss = 0.47310842\n",
      "Iteration 7717, loss = 0.47435235\n",
      "Iteration 7718, loss = 0.47493004\n",
      "Iteration 7719, loss = 0.47440060\n",
      "Iteration 7720, loss = 0.47336658\n",
      "Iteration 7721, loss = 0.47275515\n",
      "Iteration 7722, loss = 0.47274395\n",
      "Iteration 7723, loss = 0.47318746\n",
      "Iteration 7724, loss = 0.47381005\n",
      "Iteration 7725, loss = 0.47380836\n",
      "Iteration 7726, loss = 0.47331774\n",
      "Iteration 7727, loss = 0.47340066\n",
      "Iteration 7728, loss = 0.47284629\n",
      "Iteration 7729, loss = 0.47287446\n",
      "Iteration 7730, loss = 0.47285791\n",
      "Iteration 7731, loss = 0.47287379\n",
      "Iteration 7732, loss = 0.47300772\n",
      "Iteration 7733, loss = 0.47246151\n",
      "Iteration 7734, loss = 0.47416350\n",
      "Iteration 7735, loss = 0.47559660\n",
      "Iteration 7736, loss = 0.47576824\n",
      "Iteration 7737, loss = 0.47412238\n",
      "Iteration 7738, loss = 0.47407451\n",
      "Iteration 7739, loss = 0.47393780\n",
      "Iteration 7740, loss = 0.47354065\n",
      "Iteration 7741, loss = 0.47305979\n",
      "Iteration 7742, loss = 0.47281415\n",
      "Iteration 7743, loss = 0.47257225\n",
      "Iteration 7744, loss = 0.47286937\n",
      "Iteration 7745, loss = 0.47304196\n",
      "Iteration 7746, loss = 0.47328603\n",
      "Iteration 7747, loss = 0.47307710\n",
      "Iteration 7748, loss = 0.47275271\n",
      "Iteration 7749, loss = 0.47276460\n",
      "Iteration 7750, loss = 0.47272500\n",
      "Iteration 7751, loss = 0.47304592\n",
      "Iteration 7752, loss = 0.47318780\n",
      "Iteration 7753, loss = 0.47338736\n",
      "Iteration 7754, loss = 0.47321359\n",
      "Iteration 7755, loss = 0.47281739\n",
      "Iteration 7756, loss = 0.47275052\n",
      "Iteration 7757, loss = 0.47332949\n",
      "Iteration 7758, loss = 0.47322757\n",
      "Iteration 7759, loss = 0.47269291\n",
      "Iteration 7760, loss = 0.47248318\n",
      "Iteration 7761, loss = 0.47350631\n",
      "Iteration 7762, loss = 0.47404218\n",
      "Iteration 7763, loss = 0.47420563\n",
      "Iteration 7764, loss = 0.47388537\n",
      "Iteration 7765, loss = 0.47328119\n",
      "Iteration 7766, loss = 0.47275344\n",
      "Iteration 7767, loss = 0.47290604\n",
      "Iteration 7768, loss = 0.47347491\n",
      "Iteration 7769, loss = 0.47425131\n",
      "Iteration 7770, loss = 0.47413919\n",
      "Iteration 7771, loss = 0.47348187\n",
      "Iteration 7772, loss = 0.47307827\n",
      "Iteration 7773, loss = 0.47318701\n",
      "Iteration 7774, loss = 0.47282236\n",
      "Iteration 7775, loss = 0.47322380\n",
      "Iteration 7776, loss = 0.47295937\n",
      "Iteration 7777, loss = 0.47256525\n",
      "Iteration 7778, loss = 0.47286165\n",
      "Iteration 7779, loss = 0.47329382\n",
      "Iteration 7780, loss = 0.47418420\n",
      "Iteration 7781, loss = 0.47503943\n",
      "Iteration 7782, loss = 0.47410129\n",
      "Iteration 7783, loss = 0.47311167\n",
      "Iteration 7784, loss = 0.47257020\n",
      "Iteration 7785, loss = 0.47278562\n",
      "Iteration 7786, loss = 0.47360955\n",
      "Iteration 7787, loss = 0.47416066\n",
      "Iteration 7788, loss = 0.47413624\n",
      "Iteration 7789, loss = 0.47354103\n",
      "Iteration 7790, loss = 0.47295894\n",
      "Iteration 7791, loss = 0.47305013\n",
      "Iteration 7792, loss = 0.47271898\n",
      "Iteration 7793, loss = 0.47288679\n",
      "Iteration 7794, loss = 0.47286709\n",
      "Iteration 7795, loss = 0.47285584\n",
      "Iteration 7796, loss = 0.47291430\n",
      "Iteration 7797, loss = 0.47268600\n",
      "Iteration 7798, loss = 0.47303173\n",
      "Iteration 7799, loss = 0.47347820\n",
      "Iteration 7800, loss = 0.47382947\n",
      "Iteration 7801, loss = 0.47397459\n",
      "Iteration 7802, loss = 0.47311058\n",
      "Iteration 7803, loss = 0.47234324\n",
      "Iteration 7804, loss = 0.47368162\n",
      "Iteration 7805, loss = 0.47399342\n",
      "Iteration 7806, loss = 0.47407864\n",
      "Iteration 7807, loss = 0.47332335\n",
      "Iteration 7808, loss = 0.47331066\n",
      "Iteration 7809, loss = 0.47335025\n",
      "Iteration 7810, loss = 0.47331074\n",
      "Iteration 7811, loss = 0.47323576\n",
      "Iteration 7812, loss = 0.47307957\n",
      "Iteration 7813, loss = 0.47320362\n",
      "Iteration 7814, loss = 0.47295623\n",
      "Iteration 7815, loss = 0.47296148\n",
      "Iteration 7816, loss = 0.47292017\n",
      "Iteration 7817, loss = 0.47299256\n",
      "Iteration 7818, loss = 0.47286747\n",
      "Iteration 7819, loss = 0.47302234\n",
      "Iteration 7820, loss = 0.47303140\n",
      "Iteration 7821, loss = 0.47314663\n",
      "Iteration 7822, loss = 0.47365059\n",
      "Iteration 7823, loss = 0.47364818\n",
      "Iteration 7824, loss = 0.47332784\n",
      "Iteration 7825, loss = 0.47291116\n",
      "Iteration 7826, loss = 0.47285947\n",
      "Iteration 7827, loss = 0.47335019\n",
      "Iteration 7828, loss = 0.47277519\n",
      "Iteration 7829, loss = 0.47298870\n",
      "Iteration 7830, loss = 0.47319184\n",
      "Iteration 7831, loss = 0.47325737\n",
      "Iteration 7832, loss = 0.47299881\n",
      "Iteration 7833, loss = 0.47277050\n",
      "Iteration 7834, loss = 0.47291242\n",
      "Iteration 7835, loss = 0.47288203\n",
      "Iteration 7836, loss = 0.47290830\n",
      "Iteration 7837, loss = 0.47311236\n",
      "Iteration 7838, loss = 0.47324372\n",
      "Iteration 7839, loss = 0.47309211\n",
      "Iteration 7840, loss = 0.47300854\n",
      "Iteration 7841, loss = 0.47275643\n",
      "Iteration 7842, loss = 0.47245319\n",
      "Iteration 7843, loss = 0.47305682\n",
      "Iteration 7844, loss = 0.47431192\n",
      "Iteration 7845, loss = 0.47466697\n",
      "Iteration 7846, loss = 0.47416739\n",
      "Iteration 7847, loss = 0.47350808\n",
      "Iteration 7848, loss = 0.47304455\n",
      "Iteration 7849, loss = 0.47280771\n",
      "Iteration 7850, loss = 0.47295172\n",
      "Iteration 7851, loss = 0.47286631\n",
      "Iteration 7852, loss = 0.47295349\n",
      "Iteration 7853, loss = 0.47287771\n",
      "Iteration 7854, loss = 0.47303665\n",
      "Iteration 7855, loss = 0.47283035\n",
      "Iteration 7856, loss = 0.47266034\n",
      "Iteration 7857, loss = 0.47280935\n",
      "Iteration 7858, loss = 0.47435371\n",
      "Iteration 7859, loss = 0.47438138\n",
      "Iteration 7860, loss = 0.47288224\n",
      "Iteration 7861, loss = 0.47313934\n",
      "Iteration 7862, loss = 0.47399661\n",
      "Iteration 7863, loss = 0.47488786\n",
      "Iteration 7864, loss = 0.47568583\n",
      "Iteration 7865, loss = 0.47588589\n",
      "Iteration 7866, loss = 0.47531498\n",
      "Iteration 7867, loss = 0.47459849\n",
      "Iteration 7868, loss = 0.47313531\n",
      "Iteration 7869, loss = 0.47279982\n",
      "Iteration 7870, loss = 0.47288888\n",
      "Iteration 7871, loss = 0.47390762\n",
      "Iteration 7872, loss = 0.47407856\n",
      "Iteration 7873, loss = 0.47352684\n",
      "Iteration 7874, loss = 0.47338239\n",
      "Iteration 7875, loss = 0.47289907\n",
      "Iteration 7876, loss = 0.47290505\n",
      "Iteration 7877, loss = 0.47268740\n",
      "Iteration 7878, loss = 0.47280099\n",
      "Iteration 7879, loss = 0.47305500\n",
      "Iteration 7880, loss = 0.47272985\n",
      "Iteration 7881, loss = 0.47281800\n",
      "Iteration 7882, loss = 0.47345087\n",
      "Iteration 7883, loss = 0.47356128\n",
      "Iteration 7884, loss = 0.47346136\n",
      "Iteration 7885, loss = 0.47322540\n",
      "Iteration 7886, loss = 0.47345806\n",
      "Iteration 7887, loss = 0.47288641\n",
      "Iteration 7888, loss = 0.47288007\n",
      "Iteration 7889, loss = 0.47287900\n",
      "Iteration 7890, loss = 0.47301070\n",
      "Iteration 7891, loss = 0.47311459\n",
      "Iteration 7892, loss = 0.47304199\n",
      "Iteration 7893, loss = 0.47294478\n",
      "Iteration 7894, loss = 0.47288124\n",
      "Iteration 7895, loss = 0.47285801\n",
      "Iteration 7896, loss = 0.47300138\n",
      "Iteration 7897, loss = 0.47304678\n",
      "Iteration 7898, loss = 0.47274586\n",
      "Iteration 7899, loss = 0.47287883\n",
      "Iteration 7900, loss = 0.47257392\n",
      "Iteration 7901, loss = 0.47295111\n",
      "Iteration 7902, loss = 0.47315163\n",
      "Iteration 7903, loss = 0.47305211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7904, loss = 0.47288915\n",
      "Iteration 7905, loss = 0.47320165\n",
      "Iteration 7906, loss = 0.47279509\n",
      "Iteration 7907, loss = 0.47293760\n",
      "Iteration 7908, loss = 0.47318804\n",
      "Iteration 7909, loss = 0.47318760\n",
      "Iteration 7910, loss = 0.47301267\n",
      "Iteration 7911, loss = 0.47284774\n",
      "Iteration 7912, loss = 0.47284930\n",
      "Iteration 7913, loss = 0.47261729\n",
      "Iteration 7914, loss = 0.47393357\n",
      "Iteration 7915, loss = 0.47607891\n",
      "Iteration 7916, loss = 0.47506614\n",
      "Iteration 7917, loss = 0.47370140\n",
      "Iteration 7918, loss = 0.47287771\n",
      "Iteration 7919, loss = 0.47340726\n",
      "Iteration 7920, loss = 0.47379384\n",
      "Iteration 7921, loss = 0.47384903\n",
      "Iteration 7922, loss = 0.47380773\n",
      "Iteration 7923, loss = 0.47356516\n",
      "Iteration 7924, loss = 0.47324003\n",
      "Iteration 7925, loss = 0.47312148\n",
      "Iteration 7926, loss = 0.47273787\n",
      "Iteration 7927, loss = 0.47269410\n",
      "Iteration 7928, loss = 0.47293745\n",
      "Iteration 7929, loss = 0.47323687\n",
      "Iteration 7930, loss = 0.47320811\n",
      "Iteration 7931, loss = 0.47295503\n",
      "Iteration 7932, loss = 0.47277928\n",
      "Iteration 7933, loss = 0.47329119\n",
      "Iteration 7934, loss = 0.47240159\n",
      "Iteration 7935, loss = 0.47290880\n",
      "Iteration 7936, loss = 0.47402287\n",
      "Iteration 7937, loss = 0.47471171\n",
      "Iteration 7938, loss = 0.47421883\n",
      "Iteration 7939, loss = 0.47357207\n",
      "Iteration 7940, loss = 0.47275296\n",
      "Iteration 7941, loss = 0.47370874\n",
      "Iteration 7942, loss = 0.47419920\n",
      "Iteration 7943, loss = 0.47417595\n",
      "Iteration 7944, loss = 0.47402708\n",
      "Iteration 7945, loss = 0.47356993\n",
      "Iteration 7946, loss = 0.47281082\n",
      "Iteration 7947, loss = 0.47228651\n",
      "Iteration 7948, loss = 0.47398837\n",
      "Iteration 7949, loss = 0.47395577\n",
      "Iteration 7950, loss = 0.47357517\n",
      "Iteration 7951, loss = 0.47298986\n",
      "Iteration 7952, loss = 0.47277481\n",
      "Iteration 7953, loss = 0.47263569\n",
      "Iteration 7954, loss = 0.47295138\n",
      "Iteration 7955, loss = 0.47270200\n",
      "Iteration 7956, loss = 0.47260329\n",
      "Iteration 7957, loss = 0.47266723\n",
      "Iteration 7958, loss = 0.47279490\n",
      "Iteration 7959, loss = 0.47285927\n",
      "Iteration 7960, loss = 0.47277659\n",
      "Iteration 7961, loss = 0.47318682\n",
      "Iteration 7962, loss = 0.47272706\n",
      "Iteration 7963, loss = 0.47308150\n",
      "Iteration 7964, loss = 0.47312006\n",
      "Iteration 7965, loss = 0.47297485\n",
      "Iteration 7966, loss = 0.47268342\n",
      "Iteration 7967, loss = 0.47271367\n",
      "Iteration 7968, loss = 0.47356216\n",
      "Iteration 7969, loss = 0.47434312\n",
      "Iteration 7970, loss = 0.47457914\n",
      "Iteration 7971, loss = 0.47403844\n",
      "Iteration 7972, loss = 0.47299015\n",
      "Iteration 7973, loss = 0.47268631\n",
      "Iteration 7974, loss = 0.47314971\n",
      "Iteration 7975, loss = 0.47423421\n",
      "Iteration 7976, loss = 0.47544106\n",
      "Iteration 7977, loss = 0.47597447\n",
      "Iteration 7978, loss = 0.47519515\n",
      "Iteration 7979, loss = 0.47402019\n",
      "Iteration 7980, loss = 0.47265639\n",
      "Iteration 7981, loss = 0.47347667\n",
      "Iteration 7982, loss = 0.47376446\n",
      "Iteration 7983, loss = 0.47454048\n",
      "Iteration 7984, loss = 0.47384975\n",
      "Iteration 7985, loss = 0.47283758\n",
      "Iteration 7986, loss = 0.47314123\n",
      "Iteration 7987, loss = 0.47296820\n",
      "Iteration 7988, loss = 0.47299808\n",
      "Iteration 7989, loss = 0.47261309\n",
      "Iteration 7990, loss = 0.47312809\n",
      "Iteration 7991, loss = 0.47277583\n",
      "Iteration 7992, loss = 0.47277423\n",
      "Iteration 7993, loss = 0.47284873\n",
      "Iteration 7994, loss = 0.47273288\n",
      "Iteration 7995, loss = 0.47300587\n",
      "Iteration 7996, loss = 0.47432658\n",
      "Iteration 7997, loss = 0.47518792\n",
      "Iteration 7998, loss = 0.47567599\n",
      "Iteration 7999, loss = 0.47632242\n",
      "Iteration 8000, loss = 0.47631301\n",
      "Iteration 8001, loss = 0.47510733\n",
      "Iteration 8002, loss = 0.47368188\n",
      "Iteration 8003, loss = 0.47252168\n",
      "Iteration 8004, loss = 0.47276447\n",
      "Iteration 8005, loss = 0.47311552\n",
      "Iteration 8006, loss = 0.47403168\n",
      "Iteration 8007, loss = 0.47476152\n",
      "Iteration 8008, loss = 0.47362123\n",
      "Iteration 8009, loss = 0.47218798\n",
      "Iteration 8010, loss = 0.47324867\n",
      "Iteration 8011, loss = 0.47442511\n",
      "Iteration 8012, loss = 0.47519457\n",
      "Iteration 8013, loss = 0.47393872\n",
      "Iteration 8014, loss = 0.47299684\n",
      "Iteration 8015, loss = 0.47277161\n",
      "Iteration 8016, loss = 0.47499219\n",
      "Iteration 8017, loss = 0.47575110\n",
      "Iteration 8018, loss = 0.47471203\n",
      "Iteration 8019, loss = 0.47313090\n",
      "Iteration 8020, loss = 0.47237063\n",
      "Iteration 8021, loss = 0.47250270\n",
      "Iteration 8022, loss = 0.47370281\n",
      "Iteration 8023, loss = 0.47588950\n",
      "Iteration 8024, loss = 0.47682036\n",
      "Iteration 8025, loss = 0.47667543\n",
      "Iteration 8026, loss = 0.47618958\n",
      "Iteration 8027, loss = 0.47464555\n",
      "Iteration 8028, loss = 0.47360803\n",
      "Iteration 8029, loss = 0.47270661\n",
      "Iteration 8030, loss = 0.47326374\n",
      "Iteration 8031, loss = 0.47423058\n",
      "Iteration 8032, loss = 0.47558387\n",
      "Iteration 8033, loss = 0.47717214\n",
      "Iteration 8034, loss = 0.47768820\n",
      "Iteration 8035, loss = 0.47665897\n",
      "Iteration 8036, loss = 0.47477990\n",
      "Iteration 8037, loss = 0.47397927\n",
      "Iteration 8038, loss = 0.47329610\n",
      "Iteration 8039, loss = 0.47279319\n",
      "Iteration 8040, loss = 0.47306836\n",
      "Iteration 8041, loss = 0.47239065\n",
      "Iteration 8042, loss = 0.47385989\n",
      "Iteration 8043, loss = 0.47385922\n",
      "Iteration 8044, loss = 0.47378645\n",
      "Iteration 8045, loss = 0.47330720\n",
      "Iteration 8046, loss = 0.47219480\n",
      "Iteration 8047, loss = 0.47196808\n",
      "Iteration 8048, loss = 0.47629625\n",
      "Iteration 8049, loss = 0.47950356\n",
      "Iteration 8050, loss = 0.47984408\n",
      "Iteration 8051, loss = 0.47723937\n",
      "Iteration 8052, loss = 0.47457016\n",
      "Iteration 8053, loss = 0.47262466\n",
      "Iteration 8054, loss = 0.47258711\n",
      "Iteration 8055, loss = 0.47436903\n",
      "Iteration 8056, loss = 0.47438006\n",
      "Iteration 8057, loss = 0.47372447\n",
      "Iteration 8058, loss = 0.47281298\n",
      "Iteration 8059, loss = 0.47300190\n",
      "Iteration 8060, loss = 0.47293104\n",
      "Iteration 8061, loss = 0.47297706\n",
      "Iteration 8062, loss = 0.47306407\n",
      "Iteration 8063, loss = 0.47274155\n",
      "Iteration 8064, loss = 0.47283180\n",
      "Iteration 8065, loss = 0.47260237\n",
      "Iteration 8066, loss = 0.47252897\n",
      "Iteration 8067, loss = 0.47446597\n",
      "Iteration 8068, loss = 0.47596268\n",
      "Iteration 8069, loss = 0.47687364\n",
      "Iteration 8070, loss = 0.47659555\n",
      "Iteration 8071, loss = 0.47565203\n",
      "Iteration 8072, loss = 0.47431536\n",
      "Iteration 8073, loss = 0.47367494\n",
      "Iteration 8074, loss = 0.47334472\n",
      "Iteration 8075, loss = 0.47296041\n",
      "Iteration 8076, loss = 0.47289198\n",
      "Iteration 8077, loss = 0.47315581\n",
      "Iteration 8078, loss = 0.47311770\n",
      "Iteration 8079, loss = 0.47304701\n",
      "Iteration 8080, loss = 0.47278520\n",
      "Iteration 8081, loss = 0.47270406\n",
      "Iteration 8082, loss = 0.47259096\n",
      "Iteration 8083, loss = 0.47249464\n",
      "Iteration 8084, loss = 0.47305114\n",
      "Iteration 8085, loss = 0.47325191\n",
      "Iteration 8086, loss = 0.47322949\n",
      "Iteration 8087, loss = 0.47302243\n",
      "Iteration 8088, loss = 0.47292042\n",
      "Iteration 8089, loss = 0.47296586\n",
      "Iteration 8090, loss = 0.47290006\n",
      "Iteration 8091, loss = 0.47276332\n",
      "Iteration 8092, loss = 0.47260185\n",
      "Iteration 8093, loss = 0.47292213\n",
      "Iteration 8094, loss = 0.47248977\n",
      "Iteration 8095, loss = 0.47260524\n",
      "Iteration 8096, loss = 0.47339522\n",
      "Iteration 8097, loss = 0.47360700\n",
      "Iteration 8098, loss = 0.47330040\n",
      "Iteration 8099, loss = 0.47337256\n",
      "Iteration 8100, loss = 0.47303181\n",
      "Iteration 8101, loss = 0.47288648\n",
      "Iteration 8102, loss = 0.47279067\n",
      "Iteration 8103, loss = 0.47357426\n",
      "Iteration 8104, loss = 0.47391873\n",
      "Iteration 8105, loss = 0.47419017\n",
      "Iteration 8106, loss = 0.47436729\n",
      "Iteration 8107, loss = 0.47369762\n",
      "Iteration 8108, loss = 0.47329605\n",
      "Iteration 8109, loss = 0.47272515\n",
      "Iteration 8110, loss = 0.47271016\n",
      "Iteration 8111, loss = 0.47284290\n",
      "Iteration 8112, loss = 0.47241728\n",
      "Iteration 8113, loss = 0.47217071\n",
      "Iteration 8114, loss = 0.47417576\n",
      "Iteration 8115, loss = 0.47588845\n",
      "Iteration 8116, loss = 0.47652087\n",
      "Iteration 8117, loss = 0.47497062\n",
      "Iteration 8118, loss = 0.47269702\n",
      "Iteration 8119, loss = 0.47238035\n",
      "Iteration 8120, loss = 0.47504447\n",
      "Iteration 8121, loss = 0.47815720\n",
      "Iteration 8122, loss = 0.47826703\n",
      "Iteration 8123, loss = 0.47650311\n",
      "Iteration 8124, loss = 0.47395238\n",
      "Iteration 8125, loss = 0.47269924\n",
      "Iteration 8126, loss = 0.47247738\n",
      "Iteration 8127, loss = 0.47287187\n",
      "Iteration 8128, loss = 0.47288377\n",
      "Iteration 8129, loss = 0.47293853\n",
      "Iteration 8130, loss = 0.47268238\n",
      "Iteration 8131, loss = 0.47276982\n",
      "Iteration 8132, loss = 0.47299255\n",
      "Iteration 8133, loss = 0.47298974\n",
      "Iteration 8134, loss = 0.47296879\n",
      "Iteration 8135, loss = 0.47287778\n",
      "Iteration 8136, loss = 0.47284444\n",
      "Iteration 8137, loss = 0.47292305\n",
      "Iteration 8138, loss = 0.47276386\n",
      "Iteration 8139, loss = 0.47288483\n",
      "Iteration 8140, loss = 0.47365285\n",
      "Iteration 8141, loss = 0.47390610\n",
      "Iteration 8142, loss = 0.47377226\n",
      "Iteration 8143, loss = 0.47361993\n",
      "Iteration 8144, loss = 0.47316225\n",
      "Iteration 8145, loss = 0.47311488\n",
      "Iteration 8146, loss = 0.47298068\n",
      "Iteration 8147, loss = 0.47294604\n",
      "Iteration 8148, loss = 0.47282349\n",
      "Iteration 8149, loss = 0.47261780\n",
      "Iteration 8150, loss = 0.47308418\n",
      "Iteration 8151, loss = 0.47330411\n",
      "Iteration 8152, loss = 0.47345292\n",
      "Iteration 8153, loss = 0.47327534\n",
      "Iteration 8154, loss = 0.47302521\n",
      "Iteration 8155, loss = 0.47268969\n",
      "Iteration 8156, loss = 0.47269659\n",
      "Iteration 8157, loss = 0.47282891\n",
      "Iteration 8158, loss = 0.47268620\n",
      "Iteration 8159, loss = 0.47307418\n",
      "Iteration 8160, loss = 0.47257387\n",
      "Iteration 8161, loss = 0.47269953\n",
      "Iteration 8162, loss = 0.47246946\n",
      "Iteration 8163, loss = 0.47290142\n",
      "Iteration 8164, loss = 0.47321393\n",
      "Iteration 8165, loss = 0.47378997\n",
      "Iteration 8166, loss = 0.47513669\n",
      "Iteration 8167, loss = 0.47634968\n",
      "Iteration 8168, loss = 0.47542379\n",
      "Iteration 8169, loss = 0.47427312\n",
      "Iteration 8170, loss = 0.47305443\n",
      "Iteration 8171, loss = 0.47335502\n",
      "Iteration 8172, loss = 0.47401341\n",
      "Iteration 8173, loss = 0.47354004\n",
      "Iteration 8174, loss = 0.47263440\n",
      "Iteration 8175, loss = 0.47211423\n",
      "Iteration 8176, loss = 0.47386322\n",
      "Iteration 8177, loss = 0.47475136\n",
      "Iteration 8178, loss = 0.47493699\n",
      "Iteration 8179, loss = 0.47459016\n",
      "Iteration 8180, loss = 0.47392021\n",
      "Iteration 8181, loss = 0.47323926\n",
      "Iteration 8182, loss = 0.47342704\n",
      "Iteration 8183, loss = 0.47419919\n",
      "Iteration 8184, loss = 0.47353376\n",
      "Iteration 8185, loss = 0.47304993\n",
      "Iteration 8186, loss = 0.47304933\n",
      "Iteration 8187, loss = 0.47379242\n",
      "Iteration 8188, loss = 0.47389209\n",
      "Iteration 8189, loss = 0.47375642\n",
      "Iteration 8190, loss = 0.47325663\n",
      "Iteration 8191, loss = 0.47299341\n",
      "Iteration 8192, loss = 0.47283075\n",
      "Iteration 8193, loss = 0.47298084\n",
      "Iteration 8194, loss = 0.47324910\n",
      "Iteration 8195, loss = 0.47346688\n",
      "Iteration 8196, loss = 0.47365044\n",
      "Iteration 8197, loss = 0.47391195\n",
      "Iteration 8198, loss = 0.47361844\n",
      "Iteration 8199, loss = 0.47305266\n",
      "Iteration 8200, loss = 0.47258311\n",
      "Iteration 8201, loss = 0.47216364\n",
      "Iteration 8202, loss = 0.47334516\n",
      "Iteration 8203, loss = 0.47508348\n",
      "Iteration 8204, loss = 0.47605143\n",
      "Iteration 8205, loss = 0.47508954\n",
      "Iteration 8206, loss = 0.47332980\n",
      "Iteration 8207, loss = 0.47306435\n",
      "Iteration 8208, loss = 0.47306000\n",
      "Iteration 8209, loss = 0.47336821\n",
      "Iteration 8210, loss = 0.47337261\n",
      "Iteration 8211, loss = 0.47303829\n",
      "Iteration 8212, loss = 0.47296850\n",
      "Iteration 8213, loss = 0.47313147\n",
      "Iteration 8214, loss = 0.47445273\n",
      "Iteration 8215, loss = 0.47547780\n",
      "Iteration 8216, loss = 0.47562189\n",
      "Iteration 8217, loss = 0.47489513\n",
      "Iteration 8218, loss = 0.47387269\n",
      "Iteration 8219, loss = 0.47287876\n",
      "Iteration 8220, loss = 0.47267988\n",
      "Iteration 8221, loss = 0.47311512\n",
      "Iteration 8222, loss = 0.47472552\n",
      "Iteration 8223, loss = 0.47501768\n",
      "Iteration 8224, loss = 0.47454018\n",
      "Iteration 8225, loss = 0.47395585\n",
      "Iteration 8226, loss = 0.47283228\n",
      "Iteration 8227, loss = 0.47279419\n",
      "Iteration 8228, loss = 0.47338592\n",
      "Iteration 8229, loss = 0.47369565\n",
      "Iteration 8230, loss = 0.47365784\n",
      "Iteration 8231, loss = 0.47361451\n",
      "Iteration 8232, loss = 0.47345240\n",
      "Iteration 8233, loss = 0.47295700\n",
      "Iteration 8234, loss = 0.47280582\n",
      "Iteration 8235, loss = 0.47302795\n",
      "Iteration 8236, loss = 0.47284565\n",
      "Iteration 8237, loss = 0.47242848\n",
      "Iteration 8238, loss = 0.47268710\n",
      "Iteration 8239, loss = 0.47317719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8240, loss = 0.47377371\n",
      "Iteration 8241, loss = 0.47439432\n",
      "Iteration 8242, loss = 0.47475824\n",
      "Iteration 8243, loss = 0.47481319\n",
      "Iteration 8244, loss = 0.47452671\n",
      "Iteration 8245, loss = 0.47446674\n",
      "Iteration 8246, loss = 0.47393004\n",
      "Iteration 8247, loss = 0.47306839\n",
      "Iteration 8248, loss = 0.47267015\n",
      "Iteration 8249, loss = 0.47292294\n",
      "Iteration 8250, loss = 0.47303976\n",
      "Iteration 8251, loss = 0.47333949\n",
      "Iteration 8252, loss = 0.47358504\n",
      "Iteration 8253, loss = 0.47341090\n",
      "Iteration 8254, loss = 0.47350617\n",
      "Iteration 8255, loss = 0.47322396\n",
      "Iteration 8256, loss = 0.47278709\n",
      "Iteration 8257, loss = 0.47248098\n",
      "Iteration 8258, loss = 0.47302971\n",
      "Iteration 8259, loss = 0.47349450\n",
      "Iteration 8260, loss = 0.47411908\n",
      "Iteration 8261, loss = 0.47444591\n",
      "Iteration 8262, loss = 0.47365770\n",
      "Iteration 8263, loss = 0.47237546\n",
      "Iteration 8264, loss = 0.47266146\n",
      "Iteration 8265, loss = 0.47512685\n",
      "Iteration 8266, loss = 0.47512537\n",
      "Iteration 8267, loss = 0.47434326\n",
      "Iteration 8268, loss = 0.47304165\n",
      "Iteration 8269, loss = 0.47270444\n",
      "Iteration 8270, loss = 0.47267086\n",
      "Iteration 8271, loss = 0.47261147\n",
      "Iteration 8272, loss = 0.47290756\n",
      "Iteration 8273, loss = 0.47303364\n",
      "Iteration 8274, loss = 0.47303292\n",
      "Iteration 8275, loss = 0.47290198\n",
      "Iteration 8276, loss = 0.47332897\n",
      "Iteration 8277, loss = 0.47288257\n",
      "Iteration 8278, loss = 0.47255235\n",
      "Iteration 8279, loss = 0.47322187\n",
      "Iteration 8280, loss = 0.47264709\n",
      "Iteration 8281, loss = 0.47250759\n",
      "Iteration 8282, loss = 0.47288491\n",
      "Iteration 8283, loss = 0.47306075\n",
      "Iteration 8284, loss = 0.47379553\n",
      "Iteration 8285, loss = 0.47490161\n",
      "Iteration 8286, loss = 0.47434873\n",
      "Iteration 8287, loss = 0.47286228\n",
      "Iteration 8288, loss = 0.47356585\n",
      "Iteration 8289, loss = 0.47343384\n",
      "Iteration 8290, loss = 0.47426533\n",
      "Iteration 8291, loss = 0.47456429\n",
      "Iteration 8292, loss = 0.47396279\n",
      "Iteration 8293, loss = 0.47286365\n",
      "Iteration 8294, loss = 0.47303382\n",
      "Iteration 8295, loss = 0.47354456\n",
      "Iteration 8296, loss = 0.47459592\n",
      "Iteration 8297, loss = 0.47486393\n",
      "Iteration 8298, loss = 0.47472973\n",
      "Iteration 8299, loss = 0.47380602\n",
      "Iteration 8300, loss = 0.47346788\n",
      "Iteration 8301, loss = 0.47243057\n",
      "Iteration 8302, loss = 0.47292459\n",
      "Iteration 8303, loss = 0.47392229\n",
      "Iteration 8304, loss = 0.47627868\n",
      "Iteration 8305, loss = 0.47745478\n",
      "Iteration 8306, loss = 0.47671698\n",
      "Iteration 8307, loss = 0.47508471\n",
      "Iteration 8308, loss = 0.47262955\n",
      "Iteration 8309, loss = 0.47295542\n",
      "Iteration 8310, loss = 0.47383455\n",
      "Iteration 8311, loss = 0.47451971\n",
      "Iteration 8312, loss = 0.47472158\n",
      "Iteration 8313, loss = 0.47371314\n",
      "Iteration 8314, loss = 0.47209991\n",
      "Iteration 8315, loss = 0.47293424\n",
      "Iteration 8316, loss = 0.47673514\n",
      "Iteration 8317, loss = 0.47796149\n",
      "Iteration 8318, loss = 0.47694457\n",
      "Iteration 8319, loss = 0.47523006\n",
      "Iteration 8320, loss = 0.47347614\n",
      "Iteration 8321, loss = 0.47289447\n",
      "Iteration 8322, loss = 0.47326376\n",
      "Iteration 8323, loss = 0.47392346\n",
      "Iteration 8324, loss = 0.47541593\n",
      "Iteration 8325, loss = 0.47687929\n",
      "Iteration 8326, loss = 0.47610594\n",
      "Iteration 8327, loss = 0.47456270\n",
      "Iteration 8328, loss = 0.47311573\n",
      "Iteration 8329, loss = 0.47231268\n",
      "Iteration 8330, loss = 0.47268578\n",
      "Iteration 8331, loss = 0.47469394\n",
      "Iteration 8332, loss = 0.47687750\n",
      "Iteration 8333, loss = 0.47597082\n",
      "Iteration 8334, loss = 0.47377972\n",
      "Iteration 8335, loss = 0.47243360\n",
      "Iteration 8336, loss = 0.47361195\n",
      "Iteration 8337, loss = 0.47392255\n",
      "Iteration 8338, loss = 0.47392808\n",
      "Iteration 8339, loss = 0.47371019\n",
      "Iteration 8340, loss = 0.47297768\n",
      "Iteration 8341, loss = 0.47261629\n",
      "Iteration 8342, loss = 0.47236952\n",
      "Iteration 8343, loss = 0.47295031\n",
      "Iteration 8344, loss = 0.47404555\n",
      "Iteration 8345, loss = 0.47350618\n",
      "Iteration 8346, loss = 0.47272343\n",
      "Iteration 8347, loss = 0.47227683\n",
      "Iteration 8348, loss = 0.47385812\n",
      "Iteration 8349, loss = 0.47450376\n",
      "Iteration 8350, loss = 0.47432974\n",
      "Iteration 8351, loss = 0.47401054\n",
      "Iteration 8352, loss = 0.47327936\n",
      "Iteration 8353, loss = 0.47317351\n",
      "Iteration 8354, loss = 0.47284645\n",
      "Iteration 8355, loss = 0.47263994\n",
      "Iteration 8356, loss = 0.47241176\n",
      "Iteration 8357, loss = 0.47279310\n",
      "Iteration 8358, loss = 0.47347957\n",
      "Iteration 8359, loss = 0.47418594\n",
      "Iteration 8360, loss = 0.47487756\n",
      "Iteration 8361, loss = 0.47562370\n",
      "Iteration 8362, loss = 0.47558678\n",
      "Iteration 8363, loss = 0.47460887\n",
      "Iteration 8364, loss = 0.47371933\n",
      "Iteration 8365, loss = 0.47284740\n",
      "Iteration 8366, loss = 0.47294569\n",
      "Iteration 8367, loss = 0.47279754\n",
      "Iteration 8368, loss = 0.47277757\n",
      "Iteration 8369, loss = 0.47268342\n",
      "Iteration 8370, loss = 0.47259234\n",
      "Iteration 8371, loss = 0.47244854\n",
      "Iteration 8372, loss = 0.47239335\n",
      "Iteration 8373, loss = 0.47305347\n",
      "Iteration 8374, loss = 0.47318773\n",
      "Iteration 8375, loss = 0.47310196\n",
      "Iteration 8376, loss = 0.47313924\n",
      "Iteration 8377, loss = 0.47323116\n",
      "Iteration 8378, loss = 0.47294211\n",
      "Iteration 8379, loss = 0.47270093\n",
      "Iteration 8380, loss = 0.47258681\n",
      "Iteration 8381, loss = 0.47266655\n",
      "Iteration 8382, loss = 0.47282914\n",
      "Iteration 8383, loss = 0.47311891\n",
      "Iteration 8384, loss = 0.47301885\n",
      "Iteration 8385, loss = 0.47236927\n",
      "Iteration 8386, loss = 0.47254759\n",
      "Iteration 8387, loss = 0.47362469\n",
      "Iteration 8388, loss = 0.47569533\n",
      "Iteration 8389, loss = 0.47673871\n",
      "Iteration 8390, loss = 0.47579911\n",
      "Iteration 8391, loss = 0.47404638\n",
      "Iteration 8392, loss = 0.47336649\n",
      "Iteration 8393, loss = 0.47273969\n",
      "Iteration 8394, loss = 0.47287418\n",
      "Iteration 8395, loss = 0.47283029\n",
      "Iteration 8396, loss = 0.47273692\n",
      "Iteration 8397, loss = 0.47277775\n",
      "Iteration 8398, loss = 0.47263138\n",
      "Iteration 8399, loss = 0.47262924\n",
      "Iteration 8400, loss = 0.47298016\n",
      "Iteration 8401, loss = 0.47273540\n",
      "Iteration 8402, loss = 0.47240681\n",
      "Iteration 8403, loss = 0.47248894\n",
      "Iteration 8404, loss = 0.47313560\n",
      "Iteration 8405, loss = 0.47340684\n",
      "Iteration 8406, loss = 0.47329714\n",
      "Iteration 8407, loss = 0.47276870\n",
      "Iteration 8408, loss = 0.47272872\n",
      "Iteration 8409, loss = 0.47257758\n",
      "Iteration 8410, loss = 0.47234719\n",
      "Iteration 8411, loss = 0.47334108\n",
      "Iteration 8412, loss = 0.47350865\n",
      "Iteration 8413, loss = 0.47342084\n",
      "Iteration 8414, loss = 0.47299955\n",
      "Iteration 8415, loss = 0.47263905\n",
      "Iteration 8416, loss = 0.47297815\n",
      "Iteration 8417, loss = 0.47357393\n",
      "Iteration 8418, loss = 0.47367849\n",
      "Iteration 8419, loss = 0.47358380\n",
      "Iteration 8420, loss = 0.47332304\n",
      "Iteration 8421, loss = 0.47332609\n",
      "Iteration 8422, loss = 0.47285735\n",
      "Iteration 8423, loss = 0.47291629\n",
      "Iteration 8424, loss = 0.47255651\n",
      "Iteration 8425, loss = 0.47341423\n",
      "Iteration 8426, loss = 0.47293036\n",
      "Iteration 8427, loss = 0.47247215\n",
      "Iteration 8428, loss = 0.47237729\n",
      "Iteration 8429, loss = 0.47290911\n",
      "Iteration 8430, loss = 0.47307810\n",
      "Iteration 8431, loss = 0.47281845\n",
      "Iteration 8432, loss = 0.47256482\n",
      "Iteration 8433, loss = 0.47252170\n",
      "Iteration 8434, loss = 0.47259958\n",
      "Iteration 8435, loss = 0.47266171\n",
      "Iteration 8436, loss = 0.47261036\n",
      "Iteration 8437, loss = 0.47272580\n",
      "Iteration 8438, loss = 0.47300304\n",
      "Iteration 8439, loss = 0.47316775\n",
      "Iteration 8440, loss = 0.47308598\n",
      "Iteration 8441, loss = 0.47315337\n",
      "Iteration 8442, loss = 0.47280573\n",
      "Iteration 8443, loss = 0.47243818\n",
      "Iteration 8444, loss = 0.47329958\n",
      "Iteration 8445, loss = 0.47289106\n",
      "Iteration 8446, loss = 0.47268315\n",
      "Iteration 8447, loss = 0.47254894\n",
      "Iteration 8448, loss = 0.47355406\n",
      "Iteration 8449, loss = 0.47347062\n",
      "Iteration 8450, loss = 0.47336012\n",
      "Iteration 8451, loss = 0.47316447\n",
      "Iteration 8452, loss = 0.47324741\n",
      "Iteration 8453, loss = 0.47293006\n",
      "Iteration 8454, loss = 0.47291054\n",
      "Iteration 8455, loss = 0.47282373\n",
      "Iteration 8456, loss = 0.47269364\n",
      "Iteration 8457, loss = 0.47262016\n",
      "Iteration 8458, loss = 0.47252070\n",
      "Iteration 8459, loss = 0.47215485\n",
      "Iteration 8460, loss = 0.47312262\n",
      "Iteration 8461, loss = 0.47400463\n",
      "Iteration 8462, loss = 0.47385357\n",
      "Iteration 8463, loss = 0.47294176\n",
      "Iteration 8464, loss = 0.47274035\n",
      "Iteration 8465, loss = 0.47247907\n",
      "Iteration 8466, loss = 0.47267450\n",
      "Iteration 8467, loss = 0.47257442\n",
      "Iteration 8468, loss = 0.47255782\n",
      "Iteration 8469, loss = 0.47255517\n",
      "Iteration 8470, loss = 0.47246992\n",
      "Iteration 8471, loss = 0.47284223\n",
      "Iteration 8472, loss = 0.47261910\n",
      "Iteration 8473, loss = 0.47256008\n",
      "Iteration 8474, loss = 0.47280425\n",
      "Iteration 8475, loss = 0.47244778\n",
      "Iteration 8476, loss = 0.47231317\n",
      "Iteration 8477, loss = 0.47238156\n",
      "Iteration 8478, loss = 0.47332664\n",
      "Iteration 8479, loss = 0.47430676\n",
      "Iteration 8480, loss = 0.47543164\n",
      "Iteration 8481, loss = 0.47625084\n",
      "Iteration 8482, loss = 0.47596366\n",
      "Iteration 8483, loss = 0.47471775\n",
      "Iteration 8484, loss = 0.47323860\n",
      "Iteration 8485, loss = 0.47355460\n",
      "Iteration 8486, loss = 0.47476742\n",
      "Iteration 8487, loss = 0.47639853\n",
      "Iteration 8488, loss = 0.47644670\n",
      "Iteration 8489, loss = 0.47519843\n",
      "Iteration 8490, loss = 0.47370217\n",
      "Iteration 8491, loss = 0.47262153\n",
      "Iteration 8492, loss = 0.47249251\n",
      "Iteration 8493, loss = 0.47243317\n",
      "Iteration 8494, loss = 0.47273905\n",
      "Iteration 8495, loss = 0.47286391\n",
      "Iteration 8496, loss = 0.47294515\n",
      "Iteration 8497, loss = 0.47298149\n",
      "Iteration 8498, loss = 0.47301782\n",
      "Iteration 8499, loss = 0.47300981\n",
      "Iteration 8500, loss = 0.47319213\n",
      "Iteration 8501, loss = 0.47332036\n",
      "Iteration 8502, loss = 0.47344311\n",
      "Iteration 8503, loss = 0.47360414\n",
      "Iteration 8504, loss = 0.47405486\n",
      "Iteration 8505, loss = 0.47380617\n",
      "Iteration 8506, loss = 0.47366962\n",
      "Iteration 8507, loss = 0.47361026\n",
      "Iteration 8508, loss = 0.47331925\n",
      "Iteration 8509, loss = 0.47316118\n",
      "Iteration 8510, loss = 0.47316059\n",
      "Iteration 8511, loss = 0.47426945\n",
      "Iteration 8512, loss = 0.47380876\n",
      "Iteration 8513, loss = 0.47324538\n",
      "Iteration 8514, loss = 0.47276683\n",
      "Iteration 8515, loss = 0.47225930\n",
      "Iteration 8516, loss = 0.47259057\n",
      "Iteration 8517, loss = 0.47325613\n",
      "Iteration 8518, loss = 0.47478635\n",
      "Iteration 8519, loss = 0.47514142\n",
      "Iteration 8520, loss = 0.47446065\n",
      "Iteration 8521, loss = 0.47346285\n",
      "Iteration 8522, loss = 0.47307270\n",
      "Iteration 8523, loss = 0.47275720\n",
      "Iteration 8524, loss = 0.47318230\n",
      "Iteration 8525, loss = 0.47383083\n",
      "Iteration 8526, loss = 0.47408582\n",
      "Iteration 8527, loss = 0.47344909\n",
      "Iteration 8528, loss = 0.47253162\n",
      "Iteration 8529, loss = 0.47318969\n",
      "Iteration 8530, loss = 0.47283158\n",
      "Iteration 8531, loss = 0.47277284\n",
      "Iteration 8532, loss = 0.47293984\n",
      "Iteration 8533, loss = 0.47298580\n",
      "Iteration 8534, loss = 0.47288396\n",
      "Iteration 8535, loss = 0.47260431\n",
      "Iteration 8536, loss = 0.47235068\n",
      "Iteration 8537, loss = 0.47309754\n",
      "Iteration 8538, loss = 0.47309369\n",
      "Iteration 8539, loss = 0.47311920\n",
      "Iteration 8540, loss = 0.47302511\n",
      "Iteration 8541, loss = 0.47289005\n",
      "Iteration 8542, loss = 0.47330717\n",
      "Iteration 8543, loss = 0.47345881\n",
      "Iteration 8544, loss = 0.47339504\n",
      "Iteration 8545, loss = 0.47298792\n",
      "Iteration 8546, loss = 0.47305285\n",
      "Iteration 8547, loss = 0.47288669\n",
      "Iteration 8548, loss = 0.47289990\n",
      "Iteration 8549, loss = 0.47314531\n",
      "Iteration 8550, loss = 0.47295148\n",
      "Iteration 8551, loss = 0.47277471\n",
      "Iteration 8552, loss = 0.47251666\n",
      "Iteration 8553, loss = 0.47258350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8554, loss = 0.47244698\n",
      "Iteration 8555, loss = 0.47276251\n",
      "Iteration 8556, loss = 0.47275091\n",
      "Iteration 8557, loss = 0.47258032\n",
      "Iteration 8558, loss = 0.47335397\n",
      "Iteration 8559, loss = 0.47254484\n",
      "Iteration 8560, loss = 0.47242787\n",
      "Iteration 8561, loss = 0.47276544\n",
      "Iteration 8562, loss = 0.47302034\n",
      "Iteration 8563, loss = 0.47339465\n",
      "Iteration 8564, loss = 0.47337624\n",
      "Iteration 8565, loss = 0.47283323\n",
      "Iteration 8566, loss = 0.47325494\n",
      "Iteration 8567, loss = 0.47248779\n",
      "Iteration 8568, loss = 0.47244528\n",
      "Iteration 8569, loss = 0.47247629\n",
      "Iteration 8570, loss = 0.47250682\n",
      "Iteration 8571, loss = 0.47256327\n",
      "Iteration 8572, loss = 0.47272585\n",
      "Iteration 8573, loss = 0.47342940\n",
      "Iteration 8574, loss = 0.47408430\n",
      "Iteration 8575, loss = 0.47415211\n",
      "Iteration 8576, loss = 0.47329634\n",
      "Iteration 8577, loss = 0.47236284\n",
      "Iteration 8578, loss = 0.47320790\n",
      "Iteration 8579, loss = 0.47349635\n",
      "Iteration 8580, loss = 0.47350676\n",
      "Iteration 8581, loss = 0.47284030\n",
      "Iteration 8582, loss = 0.47264062\n",
      "Iteration 8583, loss = 0.47257220\n",
      "Iteration 8584, loss = 0.47237289\n",
      "Iteration 8585, loss = 0.47246877\n",
      "Iteration 8586, loss = 0.47264886\n",
      "Iteration 8587, loss = 0.47315682\n",
      "Iteration 8588, loss = 0.47356845\n",
      "Iteration 8589, loss = 0.47352168\n",
      "Iteration 8590, loss = 0.47307100\n",
      "Iteration 8591, loss = 0.47247150\n",
      "Iteration 8592, loss = 0.47235419\n",
      "Iteration 8593, loss = 0.47313073\n",
      "Iteration 8594, loss = 0.47419398\n",
      "Iteration 8595, loss = 0.47513145\n",
      "Iteration 8596, loss = 0.47498718\n",
      "Iteration 8597, loss = 0.47406380\n",
      "Iteration 8598, loss = 0.47350465\n",
      "Iteration 8599, loss = 0.47280223\n",
      "Iteration 8600, loss = 0.47320732\n",
      "Iteration 8601, loss = 0.47290002\n",
      "Iteration 8602, loss = 0.47258850\n",
      "Iteration 8603, loss = 0.47242688\n",
      "Iteration 8604, loss = 0.47262846\n",
      "Iteration 8605, loss = 0.47287081\n",
      "Iteration 8606, loss = 0.47306866\n",
      "Iteration 8607, loss = 0.47254400\n",
      "Iteration 8608, loss = 0.47282488\n",
      "Iteration 8609, loss = 0.47274557\n",
      "Iteration 8610, loss = 0.47369139\n",
      "Iteration 8611, loss = 0.47411718\n",
      "Iteration 8612, loss = 0.47409654\n",
      "Iteration 8613, loss = 0.47344303\n",
      "Iteration 8614, loss = 0.47361695\n",
      "Iteration 8615, loss = 0.47335659\n",
      "Iteration 8616, loss = 0.47280730\n",
      "Iteration 8617, loss = 0.47254376\n",
      "Iteration 8618, loss = 0.47245642\n",
      "Iteration 8619, loss = 0.47255785\n",
      "Iteration 8620, loss = 0.47266395\n",
      "Iteration 8621, loss = 0.47268803\n",
      "Iteration 8622, loss = 0.47247802\n",
      "Iteration 8623, loss = 0.47247887\n",
      "Iteration 8624, loss = 0.47253610\n",
      "Iteration 8625, loss = 0.47259888\n",
      "Iteration 8626, loss = 0.47253551\n",
      "Iteration 8627, loss = 0.47259235\n",
      "Iteration 8628, loss = 0.47274198\n",
      "Iteration 8629, loss = 0.47288282\n",
      "Iteration 8630, loss = 0.47357506\n",
      "Iteration 8631, loss = 0.47335297\n",
      "Iteration 8632, loss = 0.47285776\n",
      "Iteration 8633, loss = 0.47259862\n",
      "Iteration 8634, loss = 0.47240286\n",
      "Iteration 8635, loss = 0.47242886\n",
      "Iteration 8636, loss = 0.47261424\n",
      "Iteration 8637, loss = 0.47264278\n",
      "Iteration 8638, loss = 0.47238052\n",
      "Iteration 8639, loss = 0.47322101\n",
      "Iteration 8640, loss = 0.47251528\n",
      "Iteration 8641, loss = 0.47239452\n",
      "Iteration 8642, loss = 0.47239615\n",
      "Iteration 8643, loss = 0.47291454\n",
      "Iteration 8644, loss = 0.47290663\n",
      "Iteration 8645, loss = 0.47255258\n",
      "Iteration 8646, loss = 0.47235478\n",
      "Iteration 8647, loss = 0.47343929\n",
      "Iteration 8648, loss = 0.47365824\n",
      "Iteration 8649, loss = 0.47381840\n",
      "Iteration 8650, loss = 0.47275478\n",
      "Iteration 8651, loss = 0.47287944\n",
      "Iteration 8652, loss = 0.47260532\n",
      "Iteration 8653, loss = 0.47255879\n",
      "Iteration 8654, loss = 0.47233133\n",
      "Iteration 8655, loss = 0.47263315\n",
      "Iteration 8656, loss = 0.47271761\n",
      "Iteration 8657, loss = 0.47260969\n",
      "Iteration 8658, loss = 0.47237517\n",
      "Iteration 8659, loss = 0.47235852\n",
      "Iteration 8660, loss = 0.47310418\n",
      "Iteration 8661, loss = 0.47285753\n",
      "Iteration 8662, loss = 0.47279976\n",
      "Iteration 8663, loss = 0.47256034\n",
      "Iteration 8664, loss = 0.47248301\n",
      "Iteration 8665, loss = 0.47252380\n",
      "Iteration 8666, loss = 0.47255819\n",
      "Iteration 8667, loss = 0.47242601\n",
      "Iteration 8668, loss = 0.47240438\n",
      "Iteration 8669, loss = 0.47233633\n",
      "Iteration 8670, loss = 0.47286556\n",
      "Iteration 8671, loss = 0.47252952\n",
      "Iteration 8672, loss = 0.47264801\n",
      "Iteration 8673, loss = 0.47257271\n",
      "Iteration 8674, loss = 0.47266768\n",
      "Iteration 8675, loss = 0.47257512\n",
      "Iteration 8676, loss = 0.47257214\n",
      "Iteration 8677, loss = 0.47239896\n",
      "Iteration 8678, loss = 0.47236693\n",
      "Iteration 8679, loss = 0.47290097\n",
      "Iteration 8680, loss = 0.47369166\n",
      "Iteration 8681, loss = 0.47424957\n",
      "Iteration 8682, loss = 0.47383950\n",
      "Iteration 8683, loss = 0.47277741\n",
      "Iteration 8684, loss = 0.47289428\n",
      "Iteration 8685, loss = 0.47271578\n",
      "Iteration 8686, loss = 0.47316348\n",
      "Iteration 8687, loss = 0.47428557\n",
      "Iteration 8688, loss = 0.47586178\n",
      "Iteration 8689, loss = 0.47577746\n",
      "Iteration 8690, loss = 0.47455569\n",
      "Iteration 8691, loss = 0.47324474\n",
      "Iteration 8692, loss = 0.47292290\n",
      "Iteration 8693, loss = 0.47245858\n",
      "Iteration 8694, loss = 0.47259982\n",
      "Iteration 8695, loss = 0.47263780\n",
      "Iteration 8696, loss = 0.47230104\n",
      "Iteration 8697, loss = 0.47237752\n",
      "Iteration 8698, loss = 0.47336300\n",
      "Iteration 8699, loss = 0.47379644\n",
      "Iteration 8700, loss = 0.47400686\n",
      "Iteration 8701, loss = 0.47461390\n",
      "Iteration 8702, loss = 0.47503135\n",
      "Iteration 8703, loss = 0.47463776\n",
      "Iteration 8704, loss = 0.47370891\n",
      "Iteration 8705, loss = 0.47225995\n",
      "Iteration 8706, loss = 0.47256748\n",
      "Iteration 8707, loss = 0.47331152\n",
      "Iteration 8708, loss = 0.47398361\n",
      "Iteration 8709, loss = 0.47422288\n",
      "Iteration 8710, loss = 0.47417564\n",
      "Iteration 8711, loss = 0.47362107\n",
      "Iteration 8712, loss = 0.47286442\n",
      "Iteration 8713, loss = 0.47244896\n",
      "Iteration 8714, loss = 0.47298338\n",
      "Iteration 8715, loss = 0.47434578\n",
      "Iteration 8716, loss = 0.47652915\n",
      "Iteration 8717, loss = 0.47684037\n",
      "Iteration 8718, loss = 0.47528422\n",
      "Iteration 8719, loss = 0.47341007\n",
      "Iteration 8720, loss = 0.47171933\n",
      "Iteration 8721, loss = 0.47353676\n",
      "Iteration 8722, loss = 0.47697728\n",
      "Iteration 8723, loss = 0.47921973\n",
      "Iteration 8724, loss = 0.47861044\n",
      "Iteration 8725, loss = 0.47549554\n",
      "Iteration 8726, loss = 0.47409333\n",
      "Iteration 8727, loss = 0.47347288\n",
      "Iteration 8728, loss = 0.47265453\n",
      "Iteration 8729, loss = 0.47237899\n",
      "Iteration 8730, loss = 0.47237231\n",
      "Iteration 8731, loss = 0.47287604\n",
      "Iteration 8732, loss = 0.47308706\n",
      "Iteration 8733, loss = 0.47192313\n",
      "Iteration 8734, loss = 0.47365058\n",
      "Iteration 8735, loss = 0.47435775\n",
      "Iteration 8736, loss = 0.47578566\n",
      "Iteration 8737, loss = 0.47561288\n",
      "Iteration 8738, loss = 0.47469011\n",
      "Iteration 8739, loss = 0.47276957\n",
      "Iteration 8740, loss = 0.47261907\n",
      "Iteration 8741, loss = 0.47264580\n",
      "Iteration 8742, loss = 0.47290241\n",
      "Iteration 8743, loss = 0.47341547\n",
      "Iteration 8744, loss = 0.47398412\n",
      "Iteration 8745, loss = 0.47340697\n",
      "Iteration 8746, loss = 0.47275404\n",
      "Iteration 8747, loss = 0.47261042\n",
      "Iteration 8748, loss = 0.47361290\n",
      "Iteration 8749, loss = 0.47392765\n",
      "Iteration 8750, loss = 0.47380928\n",
      "Iteration 8751, loss = 0.47283976\n",
      "Iteration 8752, loss = 0.47240393\n",
      "Iteration 8753, loss = 0.47331306\n",
      "Iteration 8754, loss = 0.47370182\n",
      "Iteration 8755, loss = 0.47371418\n",
      "Iteration 8756, loss = 0.47337164\n",
      "Iteration 8757, loss = 0.47311446\n",
      "Iteration 8758, loss = 0.47313916\n",
      "Iteration 8759, loss = 0.47251271\n",
      "Iteration 8760, loss = 0.47255730\n",
      "Iteration 8761, loss = 0.47256002\n",
      "Iteration 8762, loss = 0.47312479\n",
      "Iteration 8763, loss = 0.47284005\n",
      "Iteration 8764, loss = 0.47249298\n",
      "Iteration 8765, loss = 0.47257073\n",
      "Iteration 8766, loss = 0.47244387\n",
      "Iteration 8767, loss = 0.47240129\n",
      "Iteration 8768, loss = 0.47260411\n",
      "Iteration 8769, loss = 0.47241574\n",
      "Iteration 8770, loss = 0.47239224\n",
      "Iteration 8771, loss = 0.47239627\n",
      "Iteration 8772, loss = 0.47242700\n",
      "Iteration 8773, loss = 0.47254055\n",
      "Iteration 8774, loss = 0.47247303\n",
      "Iteration 8775, loss = 0.47242849\n",
      "Iteration 8776, loss = 0.47241663\n",
      "Iteration 8777, loss = 0.47246439\n",
      "Iteration 8778, loss = 0.47251498\n",
      "Iteration 8779, loss = 0.47323770\n",
      "Iteration 8780, loss = 0.47354780\n",
      "Iteration 8781, loss = 0.47335346\n",
      "Iteration 8782, loss = 0.47250324\n",
      "Iteration 8783, loss = 0.47290333\n",
      "Iteration 8784, loss = 0.47297867\n",
      "Iteration 8785, loss = 0.47353855\n",
      "Iteration 8786, loss = 0.47389029\n",
      "Iteration 8787, loss = 0.47374001\n",
      "Iteration 8788, loss = 0.47296506\n",
      "Iteration 8789, loss = 0.47189190\n",
      "Iteration 8790, loss = 0.47265677\n",
      "Iteration 8791, loss = 0.47523685\n",
      "Iteration 8792, loss = 0.47671893\n",
      "Iteration 8793, loss = 0.47684993\n",
      "Iteration 8794, loss = 0.47602310\n",
      "Iteration 8795, loss = 0.47447872\n",
      "Iteration 8796, loss = 0.47331949\n",
      "Iteration 8797, loss = 0.47249990\n",
      "Iteration 8798, loss = 0.47234717\n",
      "Iteration 8799, loss = 0.47284061\n",
      "Iteration 8800, loss = 0.47277729\n",
      "Iteration 8801, loss = 0.47247570\n",
      "Iteration 8802, loss = 0.47212332\n",
      "Iteration 8803, loss = 0.47252662\n",
      "Iteration 8804, loss = 0.47363542\n",
      "Iteration 8805, loss = 0.47623277\n",
      "Iteration 8806, loss = 0.47804502\n",
      "Iteration 8807, loss = 0.47835049\n",
      "Iteration 8808, loss = 0.47637301\n",
      "Iteration 8809, loss = 0.47348613\n",
      "Iteration 8810, loss = 0.47292370\n",
      "Iteration 8811, loss = 0.47286445\n",
      "Iteration 8812, loss = 0.47366101\n",
      "Iteration 8813, loss = 0.47509600\n",
      "Iteration 8814, loss = 0.47669503\n",
      "Iteration 8815, loss = 0.47788633\n",
      "Iteration 8816, loss = 0.47683941\n",
      "Iteration 8817, loss = 0.47414641\n",
      "Iteration 8818, loss = 0.47240144\n",
      "Iteration 8819, loss = 0.47271947\n",
      "Iteration 8820, loss = 0.47329517\n",
      "Iteration 8821, loss = 0.47339788\n",
      "Iteration 8822, loss = 0.47319024\n",
      "Iteration 8823, loss = 0.47284419\n",
      "Iteration 8824, loss = 0.47256205\n",
      "Iteration 8825, loss = 0.47241587\n",
      "Iteration 8826, loss = 0.47304093\n",
      "Iteration 8827, loss = 0.47493443\n",
      "Iteration 8828, loss = 0.47568417\n",
      "Iteration 8829, loss = 0.47536219\n",
      "Iteration 8830, loss = 0.47470782\n",
      "Iteration 8831, loss = 0.47421065\n",
      "Iteration 8832, loss = 0.47338043\n",
      "Iteration 8833, loss = 0.47292113\n",
      "Iteration 8834, loss = 0.47267195\n",
      "Iteration 8835, loss = 0.47264143\n",
      "Iteration 8836, loss = 0.47316070\n",
      "Iteration 8837, loss = 0.47433031\n",
      "Iteration 8838, loss = 0.47440612\n",
      "Iteration 8839, loss = 0.47401384\n",
      "Iteration 8840, loss = 0.47296217\n",
      "Iteration 8841, loss = 0.47260993\n",
      "Iteration 8842, loss = 0.47260101\n",
      "Iteration 8843, loss = 0.47233006\n",
      "Iteration 8844, loss = 0.47232213\n",
      "Iteration 8845, loss = 0.47291505\n",
      "Iteration 8846, loss = 0.47237058\n",
      "Iteration 8847, loss = 0.47274127\n",
      "Iteration 8848, loss = 0.47249277\n",
      "Iteration 8849, loss = 0.47223615\n",
      "Iteration 8850, loss = 0.47313350\n",
      "Iteration 8851, loss = 0.47275622\n",
      "Iteration 8852, loss = 0.47262289\n",
      "Iteration 8853, loss = 0.47228476\n",
      "Iteration 8854, loss = 0.47274924\n",
      "Iteration 8855, loss = 0.47284737\n",
      "Iteration 8856, loss = 0.47272469\n",
      "Iteration 8857, loss = 0.47234046\n",
      "Iteration 8858, loss = 0.47253193\n",
      "Iteration 8859, loss = 0.47240953\n",
      "Iteration 8860, loss = 0.47230269\n",
      "Iteration 8861, loss = 0.47264400\n",
      "Iteration 8862, loss = 0.47254130\n",
      "Iteration 8863, loss = 0.47261881\n",
      "Iteration 8864, loss = 0.47293774\n",
      "Iteration 8865, loss = 0.47264832\n",
      "Iteration 8866, loss = 0.47200719\n",
      "Iteration 8867, loss = 0.47324759\n",
      "Iteration 8868, loss = 0.47368857\n",
      "Iteration 8869, loss = 0.47435484\n",
      "Iteration 8870, loss = 0.47463786\n",
      "Iteration 8871, loss = 0.47431036\n",
      "Iteration 8872, loss = 0.47402147\n",
      "Iteration 8873, loss = 0.47329188\n",
      "Iteration 8874, loss = 0.47299275\n",
      "Iteration 8875, loss = 0.47265538\n",
      "Iteration 8876, loss = 0.47270332\n",
      "Iteration 8877, loss = 0.47242287\n",
      "Iteration 8878, loss = 0.47238238\n",
      "Iteration 8879, loss = 0.47240028\n",
      "Iteration 8880, loss = 0.47238622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8881, loss = 0.47246061\n",
      "Iteration 8882, loss = 0.47261512\n",
      "Iteration 8883, loss = 0.47270760\n",
      "Iteration 8884, loss = 0.47282448\n",
      "Iteration 8885, loss = 0.47279128\n",
      "Iteration 8886, loss = 0.47245350\n",
      "Iteration 8887, loss = 0.47226891\n",
      "Iteration 8888, loss = 0.47231269\n",
      "Iteration 8889, loss = 0.47292535\n",
      "Iteration 8890, loss = 0.47382581\n",
      "Iteration 8891, loss = 0.47451505\n",
      "Iteration 8892, loss = 0.47457014\n",
      "Iteration 8893, loss = 0.47390487\n",
      "Iteration 8894, loss = 0.47279701\n",
      "Iteration 8895, loss = 0.47166141\n",
      "Iteration 8896, loss = 0.47351467\n",
      "Iteration 8897, loss = 0.47701043\n",
      "Iteration 8898, loss = 0.47985663\n",
      "Iteration 8899, loss = 0.48030069\n",
      "Iteration 8900, loss = 0.47804899\n",
      "Iteration 8901, loss = 0.47474364\n",
      "Iteration 8902, loss = 0.47242454\n",
      "Iteration 8903, loss = 0.47269431\n",
      "Iteration 8904, loss = 0.47363132\n",
      "Iteration 8905, loss = 0.47432239\n",
      "Iteration 8906, loss = 0.47415555\n",
      "Iteration 8907, loss = 0.47318140\n",
      "Iteration 8908, loss = 0.47234482\n",
      "Iteration 8909, loss = 0.47228644\n",
      "Iteration 8910, loss = 0.47358110\n",
      "Iteration 8911, loss = 0.47484534\n",
      "Iteration 8912, loss = 0.47527465\n",
      "Iteration 8913, loss = 0.47369902\n",
      "Iteration 8914, loss = 0.47332482\n",
      "Iteration 8915, loss = 0.47232094\n",
      "Iteration 8916, loss = 0.47290611\n",
      "Iteration 8917, loss = 0.47666597\n",
      "Iteration 8918, loss = 0.47665122\n",
      "Iteration 8919, loss = 0.47514178\n",
      "Iteration 8920, loss = 0.47336238\n",
      "Iteration 8921, loss = 0.47311762\n",
      "Iteration 8922, loss = 0.47240926\n",
      "Iteration 8923, loss = 0.47351339\n",
      "Iteration 8924, loss = 0.47356242\n",
      "Iteration 8925, loss = 0.47317882\n",
      "Iteration 8926, loss = 0.47269405\n",
      "Iteration 8927, loss = 0.47260043\n",
      "Iteration 8928, loss = 0.47234557\n",
      "Iteration 8929, loss = 0.47252395\n",
      "Iteration 8930, loss = 0.47276267\n",
      "Iteration 8931, loss = 0.47302262\n",
      "Iteration 8932, loss = 0.47286812\n",
      "Iteration 8933, loss = 0.47249649\n",
      "Iteration 8934, loss = 0.47237962\n",
      "Iteration 8935, loss = 0.47245579\n",
      "Iteration 8936, loss = 0.47256941\n",
      "Iteration 8937, loss = 0.47364386\n",
      "Iteration 8938, loss = 0.47317885\n",
      "Iteration 8939, loss = 0.47263867\n",
      "Iteration 8940, loss = 0.47248773\n",
      "Iteration 8941, loss = 0.47268531\n",
      "Iteration 8942, loss = 0.47247071\n",
      "Iteration 8943, loss = 0.47248506\n",
      "Iteration 8944, loss = 0.47329588\n",
      "Iteration 8945, loss = 0.47286843\n",
      "Iteration 8946, loss = 0.47250125\n",
      "Iteration 8947, loss = 0.47231267\n",
      "Iteration 8948, loss = 0.47238285\n",
      "Iteration 8949, loss = 0.47272591\n",
      "Iteration 8950, loss = 0.47325409\n",
      "Iteration 8951, loss = 0.47326603\n",
      "Iteration 8952, loss = 0.47569876\n",
      "Iteration 8953, loss = 0.47713868\n",
      "Iteration 8954, loss = 0.47532530\n",
      "Iteration 8955, loss = 0.47308651\n",
      "Iteration 8956, loss = 0.47181823\n",
      "Iteration 8957, loss = 0.47338934\n",
      "Iteration 8958, loss = 0.47621714\n",
      "Iteration 8959, loss = 0.47681501\n",
      "Iteration 8960, loss = 0.47567118\n",
      "Iteration 8961, loss = 0.47432651\n",
      "Iteration 8962, loss = 0.47263060\n",
      "Iteration 8963, loss = 0.47262230\n",
      "Iteration 8964, loss = 0.47277353\n",
      "Iteration 8965, loss = 0.47304734\n",
      "Iteration 8966, loss = 0.47332234\n",
      "Iteration 8967, loss = 0.47345527\n",
      "Iteration 8968, loss = 0.47335425\n",
      "Iteration 8969, loss = 0.47302620\n",
      "Iteration 8970, loss = 0.47266453\n",
      "Iteration 8971, loss = 0.47266939\n",
      "Iteration 8972, loss = 0.47266005\n",
      "Iteration 8973, loss = 0.47266856\n",
      "Iteration 8974, loss = 0.47277582\n",
      "Iteration 8975, loss = 0.47243465\n",
      "Iteration 8976, loss = 0.47241221\n",
      "Iteration 8977, loss = 0.47263965\n",
      "Iteration 8978, loss = 0.47320469\n",
      "Iteration 8979, loss = 0.47402185\n",
      "Iteration 8980, loss = 0.47456237\n",
      "Iteration 8981, loss = 0.47465631\n",
      "Iteration 8982, loss = 0.47399794\n",
      "Iteration 8983, loss = 0.47256848\n",
      "Iteration 8984, loss = 0.47284518\n",
      "Iteration 8985, loss = 0.47273331\n",
      "Iteration 8986, loss = 0.47381893\n",
      "Iteration 8987, loss = 0.47467117\n",
      "Iteration 8988, loss = 0.47410120\n",
      "Iteration 8989, loss = 0.47291190\n",
      "Iteration 8990, loss = 0.47242341\n",
      "Iteration 8991, loss = 0.47248467\n",
      "Iteration 8992, loss = 0.47245532\n",
      "Iteration 8993, loss = 0.47250180\n",
      "Iteration 8994, loss = 0.47281205\n",
      "Iteration 8995, loss = 0.47318483\n",
      "Iteration 8996, loss = 0.47334459\n",
      "Iteration 8997, loss = 0.47324693\n",
      "Iteration 8998, loss = 0.47282633\n",
      "Iteration 8999, loss = 0.47246448\n",
      "Iteration 9000, loss = 0.47222070\n",
      "Iteration 9001, loss = 0.47248912\n",
      "Iteration 9002, loss = 0.47320478\n",
      "Iteration 9003, loss = 0.47323355\n",
      "Iteration 9004, loss = 0.47301629\n",
      "Iteration 9005, loss = 0.47288541\n",
      "Iteration 9006, loss = 0.47270704\n",
      "Iteration 9007, loss = 0.47281751\n",
      "Iteration 9008, loss = 0.47273984\n",
      "Iteration 9009, loss = 0.47332084\n",
      "Iteration 9010, loss = 0.47424180\n",
      "Iteration 9011, loss = 0.47438938\n",
      "Iteration 9012, loss = 0.47344342\n",
      "Iteration 9013, loss = 0.47236514\n",
      "Iteration 9014, loss = 0.47325997\n",
      "Iteration 9015, loss = 0.47310010\n",
      "Iteration 9016, loss = 0.47354024\n",
      "Iteration 9017, loss = 0.47389719\n",
      "Iteration 9018, loss = 0.47388637\n",
      "Iteration 9019, loss = 0.47348386\n",
      "Iteration 9020, loss = 0.47292750\n",
      "Iteration 9021, loss = 0.47251867\n",
      "Iteration 9022, loss = 0.47227380\n",
      "Iteration 9023, loss = 0.47298713\n",
      "Iteration 9024, loss = 0.47462059\n",
      "Iteration 9025, loss = 0.47517428\n",
      "Iteration 9026, loss = 0.47461457\n",
      "Iteration 9027, loss = 0.47370389\n",
      "Iteration 9028, loss = 0.47291636\n",
      "Iteration 9029, loss = 0.47293635\n",
      "Iteration 9030, loss = 0.47282369\n",
      "Iteration 9031, loss = 0.47305066\n",
      "Iteration 9032, loss = 0.47309413\n",
      "Iteration 9033, loss = 0.47301449\n",
      "Iteration 9034, loss = 0.47330334\n",
      "Iteration 9035, loss = 0.47343614\n",
      "Iteration 9036, loss = 0.47344154\n",
      "Iteration 9037, loss = 0.47346783\n",
      "Iteration 9038, loss = 0.47328931\n",
      "Iteration 9039, loss = 0.47312085\n",
      "Iteration 9040, loss = 0.47295823\n",
      "Iteration 9041, loss = 0.47290966\n",
      "Iteration 9042, loss = 0.47292946\n",
      "Iteration 9043, loss = 0.47304459\n",
      "Iteration 9044, loss = 0.47324603\n",
      "Iteration 9045, loss = 0.47314386\n",
      "Iteration 9046, loss = 0.47309279\n",
      "Iteration 9047, loss = 0.47301991\n",
      "Iteration 9048, loss = 0.47279047\n",
      "Iteration 9049, loss = 0.47288483\n",
      "Iteration 9050, loss = 0.47247641\n",
      "Iteration 9051, loss = 0.47247742\n",
      "Iteration 9052, loss = 0.47241372\n",
      "Iteration 9053, loss = 0.47289061\n",
      "Iteration 9054, loss = 0.47227947\n",
      "Iteration 9055, loss = 0.47274001\n",
      "Iteration 9056, loss = 0.47300375\n",
      "Iteration 9057, loss = 0.47290155\n",
      "Iteration 9058, loss = 0.47268945\n",
      "Iteration 9059, loss = 0.47251415\n",
      "Iteration 9060, loss = 0.47237571\n",
      "Iteration 9061, loss = 0.47239795\n",
      "Iteration 9062, loss = 0.47243935\n",
      "Iteration 9063, loss = 0.47248126\n",
      "Iteration 9064, loss = 0.47235302\n",
      "Iteration 9065, loss = 0.47228582\n",
      "Iteration 9066, loss = 0.47267955\n",
      "Iteration 9067, loss = 0.47333112\n",
      "Iteration 9068, loss = 0.47373203\n",
      "Iteration 9069, loss = 0.47360344\n",
      "Iteration 9070, loss = 0.47299163\n",
      "Iteration 9071, loss = 0.47290032\n",
      "Iteration 9072, loss = 0.47253988\n",
      "Iteration 9073, loss = 0.47244888\n",
      "Iteration 9074, loss = 0.47255190\n",
      "Iteration 9075, loss = 0.47279874\n",
      "Iteration 9076, loss = 0.47343378\n",
      "Iteration 9077, loss = 0.47393633\n",
      "Iteration 9078, loss = 0.47314861\n",
      "Iteration 9079, loss = 0.47215691\n",
      "Iteration 9080, loss = 0.47254464\n",
      "Iteration 9081, loss = 0.47339611\n",
      "Iteration 9082, loss = 0.47406999\n",
      "Iteration 9083, loss = 0.47305317\n",
      "Iteration 9084, loss = 0.47290833\n",
      "Iteration 9085, loss = 0.47261148\n",
      "Iteration 9086, loss = 0.47274857\n",
      "Iteration 9087, loss = 0.47279015\n",
      "Iteration 9088, loss = 0.47241985\n",
      "Iteration 9089, loss = 0.47267837\n",
      "Iteration 9090, loss = 0.47284357\n",
      "Iteration 9091, loss = 0.47293415\n",
      "Iteration 9092, loss = 0.47265634\n",
      "Iteration 9093, loss = 0.47275429\n",
      "Iteration 9094, loss = 0.47276581\n",
      "Iteration 9095, loss = 0.47263683\n",
      "Iteration 9096, loss = 0.47272500\n",
      "Iteration 9097, loss = 0.47282691\n",
      "Iteration 9098, loss = 0.47245815\n",
      "Iteration 9099, loss = 0.47255722\n",
      "Iteration 9100, loss = 0.47282353\n",
      "Iteration 9101, loss = 0.47329994\n",
      "Iteration 9102, loss = 0.47314233\n",
      "Iteration 9103, loss = 0.47255694\n",
      "Iteration 9104, loss = 0.47251479\n",
      "Iteration 9105, loss = 0.47252712\n",
      "Iteration 9106, loss = 0.47285994\n",
      "Iteration 9107, loss = 0.47252540\n",
      "Iteration 9108, loss = 0.47214533\n",
      "Iteration 9109, loss = 0.47210550\n",
      "Iteration 9110, loss = 0.47468845\n",
      "Iteration 9111, loss = 0.47500772\n",
      "Iteration 9112, loss = 0.47489295\n",
      "Iteration 9113, loss = 0.47506643\n",
      "Iteration 9114, loss = 0.47756640\n",
      "Iteration 9115, loss = 0.47643817\n",
      "Iteration 9116, loss = 0.47448614\n",
      "Iteration 9117, loss = 0.47292250\n",
      "Iteration 9118, loss = 0.47256151\n",
      "Iteration 9119, loss = 0.47312000\n",
      "Iteration 9120, loss = 0.47404304\n",
      "Iteration 9121, loss = 0.47405109\n",
      "Iteration 9122, loss = 0.47356656\n",
      "Iteration 9123, loss = 0.47363829\n",
      "Iteration 9124, loss = 0.47341280\n",
      "Iteration 9125, loss = 0.47409746\n",
      "Iteration 9126, loss = 0.47372844\n",
      "Iteration 9127, loss = 0.47283644\n",
      "Iteration 9128, loss = 0.47247837\n",
      "Iteration 9129, loss = 0.47237882\n",
      "Iteration 9130, loss = 0.47247057\n",
      "Iteration 9131, loss = 0.47367934\n",
      "Iteration 9132, loss = 0.47478579\n",
      "Iteration 9133, loss = 0.47402333\n",
      "Iteration 9134, loss = 0.47229344\n",
      "Iteration 9135, loss = 0.47252624\n",
      "Iteration 9136, loss = 0.47330911\n",
      "Iteration 9137, loss = 0.47512945\n",
      "Iteration 9138, loss = 0.47669966\n",
      "Iteration 9139, loss = 0.47740156\n",
      "Iteration 9140, loss = 0.47755118\n",
      "Iteration 9141, loss = 0.47765031\n",
      "Iteration 9142, loss = 0.47546050\n",
      "Iteration 9143, loss = 0.47397067\n",
      "Iteration 9144, loss = 0.47311700\n",
      "Iteration 9145, loss = 0.47386079\n",
      "Iteration 9146, loss = 0.47557848\n",
      "Iteration 9147, loss = 0.47604610\n",
      "Iteration 9148, loss = 0.47457868\n",
      "Iteration 9149, loss = 0.47276401\n",
      "Iteration 9150, loss = 0.47207222\n",
      "Iteration 9151, loss = 0.47370691\n",
      "Iteration 9152, loss = 0.47510079\n",
      "Iteration 9153, loss = 0.47613911\n",
      "Iteration 9154, loss = 0.47580783\n",
      "Iteration 9155, loss = 0.47448442\n",
      "Iteration 9156, loss = 0.47357038\n",
      "Iteration 9157, loss = 0.47245910\n",
      "Iteration 9158, loss = 0.47217950\n",
      "Iteration 9159, loss = 0.47314299\n",
      "Iteration 9160, loss = 0.47285694\n",
      "Iteration 9161, loss = 0.47269555\n",
      "Iteration 9162, loss = 0.47234709\n",
      "Iteration 9163, loss = 0.47229359\n",
      "Iteration 9164, loss = 0.47259881\n",
      "Iteration 9165, loss = 0.47261315\n",
      "Iteration 9166, loss = 0.47264657\n",
      "Iteration 9167, loss = 0.47271481\n",
      "Iteration 9168, loss = 0.47265880\n",
      "Iteration 9169, loss = 0.47248797\n",
      "Iteration 9170, loss = 0.47258682\n",
      "Iteration 9171, loss = 0.47255800\n",
      "Iteration 9172, loss = 0.47250895\n",
      "Iteration 9173, loss = 0.47245048\n",
      "Iteration 9174, loss = 0.47245706\n",
      "Iteration 9175, loss = 0.47240013\n",
      "Iteration 9176, loss = 0.47238522\n",
      "Iteration 9177, loss = 0.47239181\n",
      "Iteration 9178, loss = 0.47267028\n",
      "Iteration 9179, loss = 0.47308241\n",
      "Iteration 9180, loss = 0.47352843\n",
      "Iteration 9181, loss = 0.47364396\n",
      "Iteration 9182, loss = 0.47333282\n",
      "Iteration 9183, loss = 0.47320572\n",
      "Iteration 9184, loss = 0.47277660\n",
      "Iteration 9185, loss = 0.47315635\n",
      "Iteration 9186, loss = 0.47254916\n",
      "Iteration 9187, loss = 0.47258994\n",
      "Iteration 9188, loss = 0.47272428\n",
      "Iteration 9189, loss = 0.47259408\n",
      "Iteration 9190, loss = 0.47223131\n",
      "Iteration 9191, loss = 0.47252384\n",
      "Iteration 9192, loss = 0.47344128\n",
      "Iteration 9193, loss = 0.47392045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9194, loss = 0.47338118\n",
      "Iteration 9195, loss = 0.47293210\n",
      "Iteration 9196, loss = 0.47222236\n",
      "Iteration 9197, loss = 0.47241354\n",
      "Iteration 9198, loss = 0.47301524\n",
      "Iteration 9199, loss = 0.47357540\n",
      "Iteration 9200, loss = 0.47346765\n",
      "Iteration 9201, loss = 0.47296068\n",
      "Iteration 9202, loss = 0.47220641\n",
      "Iteration 9203, loss = 0.47297999\n",
      "Iteration 9204, loss = 0.47279399\n",
      "Iteration 9205, loss = 0.47284361\n",
      "Iteration 9206, loss = 0.47280341\n",
      "Iteration 9207, loss = 0.47243874\n",
      "Iteration 9208, loss = 0.47275084\n",
      "Iteration 9209, loss = 0.47247583\n",
      "Iteration 9210, loss = 0.47271896\n",
      "Iteration 9211, loss = 0.47294853\n",
      "Iteration 9212, loss = 0.47354873\n",
      "Iteration 9213, loss = 0.47404495\n",
      "Iteration 9214, loss = 0.47409346\n",
      "Iteration 9215, loss = 0.47395729\n",
      "Iteration 9216, loss = 0.47328405\n",
      "Iteration 9217, loss = 0.47306803\n",
      "Iteration 9218, loss = 0.47318286\n",
      "Iteration 9219, loss = 0.47328602\n",
      "Iteration 9220, loss = 0.47282645\n",
      "Iteration 9221, loss = 0.47258811\n",
      "Iteration 9222, loss = 0.47251032\n",
      "Iteration 9223, loss = 0.47254023\n",
      "Iteration 9224, loss = 0.47245976\n",
      "Iteration 9225, loss = 0.47254319\n",
      "Iteration 9226, loss = 0.47250981\n",
      "Iteration 9227, loss = 0.47253213\n",
      "Iteration 9228, loss = 0.47249581\n",
      "Iteration 9229, loss = 0.47247283\n",
      "Iteration 9230, loss = 0.47252349\n",
      "Iteration 9231, loss = 0.47303376\n",
      "Iteration 9232, loss = 0.47325713\n",
      "Iteration 9233, loss = 0.47327295\n",
      "Iteration 9234, loss = 0.47350490\n",
      "Iteration 9235, loss = 0.47325936\n",
      "Iteration 9236, loss = 0.47282599\n",
      "Iteration 9237, loss = 0.47254067\n",
      "Iteration 9238, loss = 0.47301724\n",
      "Iteration 9239, loss = 0.47415370\n",
      "Iteration 9240, loss = 0.47407131\n",
      "Iteration 9241, loss = 0.47375111\n",
      "Iteration 9242, loss = 0.47303932\n",
      "Iteration 9243, loss = 0.47293276\n",
      "Iteration 9244, loss = 0.47263645\n",
      "Iteration 9245, loss = 0.47262866\n",
      "Iteration 9246, loss = 0.47240223\n",
      "Iteration 9247, loss = 0.47266767\n",
      "Iteration 9248, loss = 0.47298207\n",
      "Iteration 9249, loss = 0.47338048\n",
      "Iteration 9250, loss = 0.47377727\n",
      "Iteration 9251, loss = 0.47436504\n",
      "Iteration 9252, loss = 0.47452118\n",
      "Iteration 9253, loss = 0.47431137\n",
      "Iteration 9254, loss = 0.47378607\n",
      "Iteration 9255, loss = 0.47269747\n",
      "Iteration 9256, loss = 0.47272063\n",
      "Iteration 9257, loss = 0.47288103\n",
      "Iteration 9258, loss = 0.47357223\n",
      "Iteration 9259, loss = 0.47415242\n",
      "Iteration 9260, loss = 0.47410787\n",
      "Iteration 9261, loss = 0.47337036\n",
      "Iteration 9262, loss = 0.47321739\n",
      "Iteration 9263, loss = 0.47276262\n",
      "Iteration 9264, loss = 0.47258794\n",
      "Iteration 9265, loss = 0.47272732\n",
      "Iteration 9266, loss = 0.47257210\n",
      "Iteration 9267, loss = 0.47269552\n",
      "Iteration 9268, loss = 0.47300704\n",
      "Iteration 9269, loss = 0.47368668\n",
      "Iteration 9270, loss = 0.47357527\n",
      "Iteration 9271, loss = 0.47297517\n",
      "Iteration 9272, loss = 0.47259703\n",
      "Iteration 9273, loss = 0.47252781\n",
      "Iteration 9274, loss = 0.47262480\n",
      "Iteration 9275, loss = 0.47324190\n",
      "Iteration 9276, loss = 0.47489054\n",
      "Iteration 9277, loss = 0.47548947\n",
      "Iteration 9278, loss = 0.47501470\n",
      "Iteration 9279, loss = 0.47422902\n",
      "Iteration 9280, loss = 0.47287908\n",
      "Iteration 9281, loss = 0.47215046\n",
      "Iteration 9282, loss = 0.47256910\n",
      "Iteration 9283, loss = 0.47375805\n",
      "Iteration 9284, loss = 0.47303129\n",
      "Iteration 9285, loss = 0.47281225\n",
      "Iteration 9286, loss = 0.47231649\n",
      "Iteration 9287, loss = 0.47256007\n",
      "Iteration 9288, loss = 0.47319405\n",
      "Iteration 9289, loss = 0.47322552\n",
      "Iteration 9290, loss = 0.47290285\n",
      "Iteration 9291, loss = 0.47277332\n",
      "Iteration 9292, loss = 0.47243764\n",
      "Iteration 9293, loss = 0.47237869\n",
      "Iteration 9294, loss = 0.47256970\n",
      "Iteration 9295, loss = 0.47247634\n",
      "Iteration 9296, loss = 0.47231738\n",
      "Iteration 9297, loss = 0.47247734\n",
      "Iteration 9298, loss = 0.47264432\n",
      "Iteration 9299, loss = 0.47275538\n",
      "Iteration 9300, loss = 0.47293828\n",
      "Iteration 9301, loss = 0.47290336\n",
      "Iteration 9302, loss = 0.47255278\n",
      "Iteration 9303, loss = 0.47218354\n",
      "Iteration 9304, loss = 0.47205931\n",
      "Iteration 9305, loss = 0.47359936\n",
      "Iteration 9306, loss = 0.47393005\n",
      "Iteration 9307, loss = 0.47368015\n",
      "Iteration 9308, loss = 0.47376497\n",
      "Iteration 9309, loss = 0.47314361\n",
      "Iteration 9310, loss = 0.47386728\n",
      "Iteration 9311, loss = 0.47410029\n",
      "Iteration 9312, loss = 0.47351926\n",
      "Iteration 9313, loss = 0.47349838\n",
      "Iteration 9314, loss = 0.47347468\n",
      "Iteration 9315, loss = 0.47310132\n",
      "Iteration 9316, loss = 0.47308630\n",
      "Iteration 9317, loss = 0.47273527\n",
      "Iteration 9318, loss = 0.47247599\n",
      "Iteration 9319, loss = 0.47214351\n",
      "Iteration 9320, loss = 0.47236821\n",
      "Iteration 9321, loss = 0.47256664\n",
      "Iteration 9322, loss = 0.47334627\n",
      "Iteration 9323, loss = 0.47341921\n",
      "Iteration 9324, loss = 0.47300272\n",
      "Iteration 9325, loss = 0.47232431\n",
      "Iteration 9326, loss = 0.47298003\n",
      "Iteration 9327, loss = 0.47411946\n",
      "Iteration 9328, loss = 0.47349703\n",
      "Iteration 9329, loss = 0.47232099\n",
      "Iteration 9330, loss = 0.47201696\n",
      "Iteration 9331, loss = 0.47333105\n",
      "Iteration 9332, loss = 0.47579368\n",
      "Iteration 9333, loss = 0.47531011\n",
      "Iteration 9334, loss = 0.47356726\n",
      "Iteration 9335, loss = 0.47306522\n",
      "Iteration 9336, loss = 0.47220875\n",
      "Iteration 9337, loss = 0.47308739\n",
      "Iteration 9338, loss = 0.47253133\n",
      "Iteration 9339, loss = 0.47265030\n",
      "Iteration 9340, loss = 0.47297100\n",
      "Iteration 9341, loss = 0.47229063\n",
      "Iteration 9342, loss = 0.47235566\n",
      "Iteration 9343, loss = 0.47284035\n",
      "Iteration 9344, loss = 0.47362316\n",
      "Iteration 9345, loss = 0.47325414\n",
      "Iteration 9346, loss = 0.47240934\n",
      "Iteration 9347, loss = 0.47256627\n",
      "Iteration 9348, loss = 0.47234886\n",
      "Iteration 9349, loss = 0.47255711\n",
      "Iteration 9350, loss = 0.47255137\n",
      "Iteration 9351, loss = 0.47244592\n",
      "Iteration 9352, loss = 0.47248386\n",
      "Iteration 9353, loss = 0.47254243\n",
      "Iteration 9354, loss = 0.47223235\n",
      "Iteration 9355, loss = 0.47228867\n",
      "Iteration 9356, loss = 0.47286399\n",
      "Iteration 9357, loss = 0.47263734\n",
      "Iteration 9358, loss = 0.47239112\n",
      "Iteration 9359, loss = 0.47232134\n",
      "Iteration 9360, loss = 0.47301844\n",
      "Iteration 9361, loss = 0.47263265\n",
      "Iteration 9362, loss = 0.47245110\n",
      "Iteration 9363, loss = 0.47242662\n",
      "Iteration 9364, loss = 0.47254343\n",
      "Iteration 9365, loss = 0.47265585\n",
      "Iteration 9366, loss = 0.47268729\n",
      "Iteration 9367, loss = 0.47277729\n",
      "Iteration 9368, loss = 0.47273168\n",
      "Iteration 9369, loss = 0.47245966\n",
      "Iteration 9370, loss = 0.47238600\n",
      "Iteration 9371, loss = 0.47220955\n",
      "Iteration 9372, loss = 0.47231505\n",
      "Iteration 9373, loss = 0.47315455\n",
      "Iteration 9374, loss = 0.47331264\n",
      "Iteration 9375, loss = 0.47322845\n",
      "Iteration 9376, loss = 0.47294491\n",
      "Iteration 9377, loss = 0.47289298\n",
      "Iteration 9378, loss = 0.47273478\n",
      "Iteration 9379, loss = 0.47261511\n",
      "Iteration 9380, loss = 0.47264770\n",
      "Iteration 9381, loss = 0.47247509\n",
      "Iteration 9382, loss = 0.47247915\n",
      "Iteration 9383, loss = 0.47247404\n",
      "Iteration 9384, loss = 0.47233051\n",
      "Iteration 9385, loss = 0.47215115\n",
      "Iteration 9386, loss = 0.47248366\n",
      "Iteration 9387, loss = 0.47347607\n",
      "Iteration 9388, loss = 0.47401612\n",
      "Iteration 9389, loss = 0.47414824\n",
      "Iteration 9390, loss = 0.47402432\n",
      "Iteration 9391, loss = 0.47344955\n",
      "Iteration 9392, loss = 0.47244120\n",
      "Iteration 9393, loss = 0.47177124\n",
      "Iteration 9394, loss = 0.47296314\n",
      "Iteration 9395, loss = 0.47548494\n",
      "Iteration 9396, loss = 0.47613985\n",
      "Iteration 9397, loss = 0.47514514\n",
      "Iteration 9398, loss = 0.47371429\n",
      "Iteration 9399, loss = 0.47237529\n",
      "Iteration 9400, loss = 0.47322552\n",
      "Iteration 9401, loss = 0.47258323\n",
      "Iteration 9402, loss = 0.47263970\n",
      "Iteration 9403, loss = 0.47270027\n",
      "Iteration 9404, loss = 0.47282258\n",
      "Iteration 9405, loss = 0.47238774\n",
      "Iteration 9406, loss = 0.47238238\n",
      "Iteration 9407, loss = 0.47235152\n",
      "Iteration 9408, loss = 0.47243787\n",
      "Iteration 9409, loss = 0.47226869\n",
      "Iteration 9410, loss = 0.47229782\n",
      "Iteration 9411, loss = 0.47232779\n",
      "Iteration 9412, loss = 0.47244686\n",
      "Iteration 9413, loss = 0.47250786\n",
      "Iteration 9414, loss = 0.47234492\n",
      "Iteration 9415, loss = 0.47257608\n",
      "Iteration 9416, loss = 0.47267838\n",
      "Iteration 9417, loss = 0.47264302\n",
      "Iteration 9418, loss = 0.47264285\n",
      "Iteration 9419, loss = 0.47248909\n",
      "Iteration 9420, loss = 0.47267691\n",
      "Iteration 9421, loss = 0.47312414\n",
      "Iteration 9422, loss = 0.47280035\n",
      "Iteration 9423, loss = 0.47239503\n",
      "Iteration 9424, loss = 0.47225404\n",
      "Iteration 9425, loss = 0.47230265\n",
      "Iteration 9426, loss = 0.47383799\n",
      "Iteration 9427, loss = 0.47467865\n",
      "Iteration 9428, loss = 0.47420529\n",
      "Iteration 9429, loss = 0.47286018\n",
      "Iteration 9430, loss = 0.47230714\n",
      "Iteration 9431, loss = 0.47278920\n",
      "Iteration 9432, loss = 0.47360971\n",
      "Iteration 9433, loss = 0.47429958\n",
      "Iteration 9434, loss = 0.47434783\n",
      "Iteration 9435, loss = 0.47377206\n",
      "Iteration 9436, loss = 0.47306245\n",
      "Iteration 9437, loss = 0.47217398\n",
      "Iteration 9438, loss = 0.47254855\n",
      "Iteration 9439, loss = 0.47420973\n",
      "Iteration 9440, loss = 0.47497656\n",
      "Iteration 9441, loss = 0.47474963\n",
      "Iteration 9442, loss = 0.47392104\n",
      "Iteration 9443, loss = 0.47320650\n",
      "Iteration 9444, loss = 0.47272674\n",
      "Iteration 9445, loss = 0.47249421\n",
      "Iteration 9446, loss = 0.47290518\n",
      "Iteration 9447, loss = 0.47314107\n",
      "Iteration 9448, loss = 0.47336795\n",
      "Iteration 9449, loss = 0.47311584\n",
      "Iteration 9450, loss = 0.47315383\n",
      "Iteration 9451, loss = 0.47273419\n",
      "Iteration 9452, loss = 0.47267492\n",
      "Iteration 9453, loss = 0.47267345\n",
      "Iteration 9454, loss = 0.47266167\n",
      "Iteration 9455, loss = 0.47257093\n",
      "Iteration 9456, loss = 0.47320176\n",
      "Iteration 9457, loss = 0.47424026\n",
      "Iteration 9458, loss = 0.47457723\n",
      "Iteration 9459, loss = 0.47416612\n",
      "Iteration 9460, loss = 0.47331394\n",
      "Iteration 9461, loss = 0.47289477\n",
      "Iteration 9462, loss = 0.47257346\n",
      "Iteration 9463, loss = 0.47267812\n",
      "Iteration 9464, loss = 0.47236867\n",
      "Iteration 9465, loss = 0.47254515\n",
      "Iteration 9466, loss = 0.47244884\n",
      "Iteration 9467, loss = 0.47251347\n",
      "Iteration 9468, loss = 0.47269299\n",
      "Iteration 9469, loss = 0.47256590\n",
      "Iteration 9470, loss = 0.47248708\n",
      "Iteration 9471, loss = 0.47234688\n",
      "Iteration 9472, loss = 0.47231835\n",
      "Iteration 9473, loss = 0.47235705\n",
      "Iteration 9474, loss = 0.47238743\n",
      "Iteration 9475, loss = 0.47235730\n",
      "Iteration 9476, loss = 0.47280735\n",
      "Iteration 9477, loss = 0.47262846\n",
      "Iteration 9478, loss = 0.47259429\n",
      "Iteration 9479, loss = 0.47246020\n",
      "Iteration 9480, loss = 0.47273914\n",
      "Iteration 9481, loss = 0.47374642\n",
      "Iteration 9482, loss = 0.47409446\n",
      "Iteration 9483, loss = 0.47384175\n",
      "Iteration 9484, loss = 0.47346002\n",
      "Iteration 9485, loss = 0.47309008\n",
      "Iteration 9486, loss = 0.47323597\n",
      "Iteration 9487, loss = 0.47302499\n",
      "Iteration 9488, loss = 0.47326194\n",
      "Iteration 9489, loss = 0.47353402\n",
      "Iteration 9490, loss = 0.47381228\n",
      "Iteration 9491, loss = 0.47381428\n",
      "Iteration 9492, loss = 0.47351153\n",
      "Iteration 9493, loss = 0.47303611\n",
      "Iteration 9494, loss = 0.47347027\n",
      "Iteration 9495, loss = 0.47302831\n",
      "Iteration 9496, loss = 0.47268503\n",
      "Iteration 9497, loss = 0.47307494\n",
      "Iteration 9498, loss = 0.47309051\n",
      "Iteration 9499, loss = 0.47327008\n",
      "Iteration 9500, loss = 0.47338706\n",
      "Iteration 9501, loss = 0.47295581\n",
      "Iteration 9502, loss = 0.47226046\n",
      "Iteration 9503, loss = 0.47287752\n",
      "Iteration 9504, loss = 0.47406724\n",
      "Iteration 9505, loss = 0.47513361\n",
      "Iteration 9506, loss = 0.47563368\n",
      "Iteration 9507, loss = 0.47447254\n",
      "Iteration 9508, loss = 0.47378827\n",
      "Iteration 9509, loss = 0.47278163\n",
      "Iteration 9510, loss = 0.47276491\n",
      "Iteration 9511, loss = 0.47274151\n",
      "Iteration 9512, loss = 0.47290486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9513, loss = 0.47318544\n",
      "Iteration 9514, loss = 0.47420698\n",
      "Iteration 9515, loss = 0.47461267\n",
      "Iteration 9516, loss = 0.47401453\n",
      "Iteration 9517, loss = 0.47264765\n",
      "Iteration 9518, loss = 0.47286499\n",
      "Iteration 9519, loss = 0.47460380\n",
      "Iteration 9520, loss = 0.47652579\n",
      "Iteration 9521, loss = 0.47653116\n",
      "Iteration 9522, loss = 0.47484410\n",
      "Iteration 9523, loss = 0.47347888\n",
      "Iteration 9524, loss = 0.47273188\n",
      "Iteration 9525, loss = 0.47279834\n",
      "Iteration 9526, loss = 0.47308085\n",
      "Iteration 9527, loss = 0.47358990\n",
      "Iteration 9528, loss = 0.47302162\n",
      "Iteration 9529, loss = 0.47254903\n",
      "Iteration 9530, loss = 0.47219131\n",
      "Iteration 9531, loss = 0.47296189\n",
      "Iteration 9532, loss = 0.47384728\n",
      "Iteration 9533, loss = 0.47490449\n",
      "Iteration 9534, loss = 0.47488839\n",
      "Iteration 9535, loss = 0.47336994\n",
      "Iteration 9536, loss = 0.47206459\n",
      "Iteration 9537, loss = 0.47286778\n",
      "Iteration 9538, loss = 0.47344734\n",
      "Iteration 9539, loss = 0.47392856\n",
      "Iteration 9540, loss = 0.47416837\n",
      "Iteration 9541, loss = 0.47364556\n",
      "Iteration 9542, loss = 0.47314563\n",
      "Iteration 9543, loss = 0.47210270\n",
      "Iteration 9544, loss = 0.47233134\n",
      "Iteration 9545, loss = 0.47272401\n",
      "Iteration 9546, loss = 0.47405107\n",
      "Iteration 9547, loss = 0.47493695\n",
      "Iteration 9548, loss = 0.47504123\n",
      "Iteration 9549, loss = 0.47378364\n",
      "Iteration 9550, loss = 0.47238621\n",
      "Iteration 9551, loss = 0.47214857\n",
      "Iteration 9552, loss = 0.47284297\n",
      "Iteration 9553, loss = 0.47471510\n",
      "Iteration 9554, loss = 0.47685519\n",
      "Iteration 9555, loss = 0.47808064\n",
      "Iteration 9556, loss = 0.47828575\n",
      "Iteration 9557, loss = 0.47755405\n",
      "Iteration 9558, loss = 0.47568765\n",
      "Iteration 9559, loss = 0.47438109\n",
      "Iteration 9560, loss = 0.47295795\n",
      "Iteration 9561, loss = 0.47234994\n",
      "Iteration 9562, loss = 0.47262056\n",
      "Iteration 9563, loss = 0.47232323\n",
      "Iteration 9564, loss = 0.47212950\n",
      "Iteration 9565, loss = 0.47253301\n",
      "Iteration 9566, loss = 0.47301383\n",
      "Iteration 9567, loss = 0.47279269\n",
      "Iteration 9568, loss = 0.47222812\n",
      "Iteration 9569, loss = 0.47261158\n",
      "Iteration 9570, loss = 0.47311547\n",
      "Iteration 9571, loss = 0.47416301\n",
      "Iteration 9572, loss = 0.47462550\n",
      "Iteration 9573, loss = 0.47434743\n",
      "Iteration 9574, loss = 0.47426787\n",
      "Iteration 9575, loss = 0.47375502\n",
      "Iteration 9576, loss = 0.47261152\n",
      "Iteration 9577, loss = 0.47218231\n",
      "Iteration 9578, loss = 0.47252344\n",
      "Iteration 9579, loss = 0.47333842\n",
      "Iteration 9580, loss = 0.47360959\n",
      "Iteration 9581, loss = 0.47307796\n",
      "Iteration 9582, loss = 0.47229163\n",
      "Iteration 9583, loss = 0.47310768\n",
      "Iteration 9584, loss = 0.47308340\n",
      "Iteration 9585, loss = 0.47278503\n",
      "Iteration 9586, loss = 0.47230794\n",
      "Iteration 9587, loss = 0.47220940\n",
      "Iteration 9588, loss = 0.47254010\n",
      "Iteration 9589, loss = 0.47254682\n",
      "Iteration 9590, loss = 0.47252287\n",
      "Iteration 9591, loss = 0.47246611\n",
      "Iteration 9592, loss = 0.47248961\n",
      "Iteration 9593, loss = 0.47247056\n",
      "Iteration 9594, loss = 0.47250326\n",
      "Iteration 9595, loss = 0.47245380\n",
      "Iteration 9596, loss = 0.47251789\n",
      "Iteration 9597, loss = 0.47255674\n",
      "Iteration 9598, loss = 0.47279240\n",
      "Iteration 9599, loss = 0.47270675\n",
      "Iteration 9600, loss = 0.47268374\n",
      "Iteration 9601, loss = 0.47267255\n",
      "Iteration 9602, loss = 0.47281291\n",
      "Iteration 9603, loss = 0.47275006\n",
      "Iteration 9604, loss = 0.47250011\n",
      "Iteration 9605, loss = 0.47226341\n",
      "Iteration 9606, loss = 0.47274651\n",
      "Iteration 9607, loss = 0.47274785\n",
      "Iteration 9608, loss = 0.47270223\n",
      "Iteration 9609, loss = 0.47254366\n",
      "Iteration 9610, loss = 0.47252739\n",
      "Iteration 9611, loss = 0.47245455\n",
      "Iteration 9612, loss = 0.47220379\n",
      "Iteration 9613, loss = 0.47212671\n",
      "Iteration 9614, loss = 0.47337767\n",
      "Iteration 9615, loss = 0.47340234\n",
      "Iteration 9616, loss = 0.47336481\n",
      "Iteration 9617, loss = 0.47308545\n",
      "Iteration 9618, loss = 0.47370237\n",
      "Iteration 9619, loss = 0.47324504\n",
      "Iteration 9620, loss = 0.47322794\n",
      "Iteration 9621, loss = 0.47343185\n",
      "Iteration 9622, loss = 0.47388310\n",
      "Iteration 9623, loss = 0.47407823\n",
      "Iteration 9624, loss = 0.47399522\n",
      "Iteration 9625, loss = 0.47402087\n",
      "Iteration 9626, loss = 0.47394215\n",
      "Iteration 9627, loss = 0.47387648\n",
      "Iteration 9628, loss = 0.47389732\n",
      "Iteration 9629, loss = 0.47364445\n",
      "Iteration 9630, loss = 0.47357813\n",
      "Iteration 9631, loss = 0.47381069\n",
      "Iteration 9632, loss = 0.47478907\n",
      "Iteration 9633, loss = 0.47548881\n",
      "Iteration 9634, loss = 0.47631128\n",
      "Iteration 9635, loss = 0.47773571\n",
      "Iteration 9636, loss = 0.47740943\n",
      "Iteration 9637, loss = 0.47591060\n",
      "Iteration 9638, loss = 0.47410575\n",
      "Iteration 9639, loss = 0.47342828\n",
      "Iteration 9640, loss = 0.47277353\n",
      "Iteration 9641, loss = 0.47314916\n",
      "Iteration 9642, loss = 0.47358525\n",
      "Iteration 9643, loss = 0.47399250\n",
      "Iteration 9644, loss = 0.47376733\n",
      "Iteration 9645, loss = 0.47289883\n",
      "Iteration 9646, loss = 0.47286155\n",
      "Iteration 9647, loss = 0.47255670\n",
      "Iteration 9648, loss = 0.47254172\n",
      "Iteration 9649, loss = 0.47254032\n",
      "Iteration 9650, loss = 0.47283923\n",
      "Iteration 9651, loss = 0.47270869\n",
      "Iteration 9652, loss = 0.47251414\n",
      "Iteration 9653, loss = 0.47224920\n",
      "Iteration 9654, loss = 0.47230525\n",
      "Iteration 9655, loss = 0.47266425\n",
      "Iteration 9656, loss = 0.47293632\n",
      "Iteration 9657, loss = 0.47322376\n",
      "Iteration 9658, loss = 0.47347823\n",
      "Iteration 9659, loss = 0.47327823\n",
      "Iteration 9660, loss = 0.47281023\n",
      "Iteration 9661, loss = 0.47289888\n",
      "Iteration 9662, loss = 0.47239772\n",
      "Iteration 9663, loss = 0.47246831\n",
      "Iteration 9664, loss = 0.47222396\n",
      "Iteration 9665, loss = 0.47257581\n",
      "Iteration 9666, loss = 0.47242881\n",
      "Iteration 9667, loss = 0.47345498\n",
      "Iteration 9668, loss = 0.47417869\n",
      "Iteration 9669, loss = 0.47342200\n",
      "Iteration 9670, loss = 0.47212857\n",
      "Iteration 9671, loss = 0.47269122\n",
      "Iteration 9672, loss = 0.47308648\n",
      "Iteration 9673, loss = 0.47334817\n",
      "Iteration 9674, loss = 0.47330373\n",
      "Iteration 9675, loss = 0.47307994\n",
      "Iteration 9676, loss = 0.47262793\n",
      "Iteration 9677, loss = 0.47299349\n",
      "Iteration 9678, loss = 0.47208804\n",
      "Iteration 9679, loss = 0.47211912\n",
      "Iteration 9680, loss = 0.47278989\n",
      "Iteration 9681, loss = 0.47421760\n",
      "Iteration 9682, loss = 0.47576140\n",
      "Iteration 9683, loss = 0.47630278\n",
      "Iteration 9684, loss = 0.47571356\n",
      "Iteration 9685, loss = 0.47386176\n",
      "Iteration 9686, loss = 0.47309174\n",
      "Iteration 9687, loss = 0.47221816\n",
      "Iteration 9688, loss = 0.47251040\n",
      "Iteration 9689, loss = 0.47282522\n",
      "Iteration 9690, loss = 0.47326428\n",
      "Iteration 9691, loss = 0.47304881\n",
      "Iteration 9692, loss = 0.47196053\n",
      "Iteration 9693, loss = 0.47216333\n",
      "Iteration 9694, loss = 0.47455028\n",
      "Iteration 9695, loss = 0.47515050\n",
      "Iteration 9696, loss = 0.47498437\n",
      "Iteration 9697, loss = 0.47411827\n",
      "Iteration 9698, loss = 0.47319914\n",
      "Iteration 9699, loss = 0.47233027\n",
      "Iteration 9700, loss = 0.47271827\n",
      "Iteration 9701, loss = 0.47263019\n",
      "Iteration 9702, loss = 0.47237108\n",
      "Iteration 9703, loss = 0.47262365\n",
      "Iteration 9704, loss = 0.47224064\n",
      "Iteration 9705, loss = 0.47252827\n",
      "Iteration 9706, loss = 0.47227182\n",
      "Iteration 9707, loss = 0.47236939\n",
      "Iteration 9708, loss = 0.47237045\n",
      "Iteration 9709, loss = 0.47219012\n",
      "Iteration 9710, loss = 0.47220527\n",
      "Iteration 9711, loss = 0.47242549\n",
      "Iteration 9712, loss = 0.47246401\n",
      "Iteration 9713, loss = 0.47254285\n",
      "Iteration 9714, loss = 0.47240042\n",
      "Iteration 9715, loss = 0.47219401\n",
      "Iteration 9716, loss = 0.47215673\n",
      "Iteration 9717, loss = 0.47272006\n",
      "Iteration 9718, loss = 0.47291318\n",
      "Iteration 9719, loss = 0.47306965\n",
      "Iteration 9720, loss = 0.47261208\n",
      "Iteration 9721, loss = 0.47167664\n",
      "Iteration 9722, loss = 0.47285597\n",
      "Iteration 9723, loss = 0.47442468\n",
      "Iteration 9724, loss = 0.47537260\n",
      "Iteration 9725, loss = 0.47508971\n",
      "Iteration 9726, loss = 0.47365562\n",
      "Iteration 9727, loss = 0.47317369\n",
      "Iteration 9728, loss = 0.47267734\n",
      "Iteration 9729, loss = 0.47242460\n",
      "Iteration 9730, loss = 0.47235787\n",
      "Iteration 9731, loss = 0.47222468\n",
      "Iteration 9732, loss = 0.47221704\n",
      "Iteration 9733, loss = 0.47218737\n",
      "Iteration 9734, loss = 0.47216535\n",
      "Iteration 9735, loss = 0.47221854\n",
      "Iteration 9736, loss = 0.47221740\n",
      "Iteration 9737, loss = 0.47223506\n",
      "Iteration 9738, loss = 0.47223454\n",
      "Iteration 9739, loss = 0.47230701\n",
      "Iteration 9740, loss = 0.47262788\n",
      "Iteration 9741, loss = 0.47318172\n",
      "Iteration 9742, loss = 0.47373195\n",
      "Iteration 9743, loss = 0.47349620\n",
      "Iteration 9744, loss = 0.47299247\n",
      "Iteration 9745, loss = 0.47270250\n",
      "Iteration 9746, loss = 0.47238017\n",
      "Iteration 9747, loss = 0.47243733\n",
      "Iteration 9748, loss = 0.47236828\n",
      "Iteration 9749, loss = 0.47233420\n",
      "Iteration 9750, loss = 0.47228728\n",
      "Iteration 9751, loss = 0.47245732\n",
      "Iteration 9752, loss = 0.47250582\n",
      "Iteration 9753, loss = 0.47255199\n",
      "Iteration 9754, loss = 0.47250512\n",
      "Iteration 9755, loss = 0.47247740\n",
      "Iteration 9756, loss = 0.47236337\n",
      "Iteration 9757, loss = 0.47235661\n",
      "Iteration 9758, loss = 0.47241420\n",
      "Iteration 9759, loss = 0.47239548\n",
      "Iteration 9760, loss = 0.47253605\n",
      "Iteration 9761, loss = 0.47257459\n",
      "Iteration 9762, loss = 0.47259394\n",
      "Iteration 9763, loss = 0.47251836\n",
      "Iteration 9764, loss = 0.47227584\n",
      "Iteration 9765, loss = 0.47310903\n",
      "Iteration 9766, loss = 0.47280226\n",
      "Iteration 9767, loss = 0.47287824\n",
      "Iteration 9768, loss = 0.47398587\n",
      "Iteration 9769, loss = 0.47499622\n",
      "Iteration 9770, loss = 0.47454437\n",
      "Iteration 9771, loss = 0.47400998\n",
      "Iteration 9772, loss = 0.47291608\n",
      "Iteration 9773, loss = 0.47286198\n",
      "Iteration 9774, loss = 0.47269692\n",
      "Iteration 9775, loss = 0.47259416\n",
      "Iteration 9776, loss = 0.47274592\n",
      "Iteration 9777, loss = 0.47289291\n",
      "Iteration 9778, loss = 0.47279423\n",
      "Iteration 9779, loss = 0.47259314\n",
      "Iteration 9780, loss = 0.47256589\n",
      "Iteration 9781, loss = 0.47273113\n",
      "Iteration 9782, loss = 0.47244942\n",
      "Iteration 9783, loss = 0.47276631\n",
      "Iteration 9784, loss = 0.47313605\n",
      "Iteration 9785, loss = 0.47345213\n",
      "Iteration 9786, loss = 0.47413263\n",
      "Iteration 9787, loss = 0.47386081\n",
      "Iteration 9788, loss = 0.47330828\n",
      "Iteration 9789, loss = 0.47210166\n",
      "Iteration 9790, loss = 0.47173532\n",
      "Iteration 9791, loss = 0.47357893\n",
      "Iteration 9792, loss = 0.47588956\n",
      "Iteration 9793, loss = 0.47781971\n",
      "Iteration 9794, loss = 0.47654205\n",
      "Iteration 9795, loss = 0.47337653\n",
      "Iteration 9796, loss = 0.47276694\n",
      "Iteration 9797, loss = 0.47280057\n",
      "Iteration 9798, loss = 0.47430753\n",
      "Iteration 9799, loss = 0.47507627\n",
      "Iteration 9800, loss = 0.47438935\n",
      "Iteration 9801, loss = 0.47296255\n",
      "Iteration 9802, loss = 0.47245491\n",
      "Iteration 9803, loss = 0.47220513\n",
      "Iteration 9804, loss = 0.47298513\n",
      "Iteration 9805, loss = 0.47424343\n",
      "Iteration 9806, loss = 0.47455822\n",
      "Iteration 9807, loss = 0.47401074\n",
      "Iteration 9808, loss = 0.47381888\n",
      "Iteration 9809, loss = 0.47349347\n",
      "Iteration 9810, loss = 0.47342767\n",
      "Iteration 9811, loss = 0.47329914\n",
      "Iteration 9812, loss = 0.47288472\n",
      "Iteration 9813, loss = 0.47264976\n",
      "Iteration 9814, loss = 0.47287529\n",
      "Iteration 9815, loss = 0.47238156\n",
      "Iteration 9816, loss = 0.47276793\n",
      "Iteration 9817, loss = 0.47267345\n",
      "Iteration 9818, loss = 0.47249951\n",
      "Iteration 9819, loss = 0.47247206\n",
      "Iteration 9820, loss = 0.47252767\n",
      "Iteration 9821, loss = 0.47239755\n",
      "Iteration 9822, loss = 0.47269209\n",
      "Iteration 9823, loss = 0.47243064\n",
      "Iteration 9824, loss = 0.47244558\n",
      "Iteration 9825, loss = 0.47221839\n",
      "Iteration 9826, loss = 0.47232294\n",
      "Iteration 9827, loss = 0.47238579\n",
      "Iteration 9828, loss = 0.47239853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9829, loss = 0.47274098\n",
      "Iteration 9830, loss = 0.47257603\n",
      "Iteration 9831, loss = 0.47250578\n",
      "Iteration 9832, loss = 0.47224947\n",
      "Iteration 9833, loss = 0.47226074\n",
      "Iteration 9834, loss = 0.47250424\n",
      "Iteration 9835, loss = 0.47250545\n",
      "Iteration 9836, loss = 0.47220497\n",
      "Iteration 9837, loss = 0.47219693\n",
      "Iteration 9838, loss = 0.47219201\n",
      "Iteration 9839, loss = 0.47227102\n",
      "Iteration 9840, loss = 0.47228705\n",
      "Iteration 9841, loss = 0.47245301\n",
      "Iteration 9842, loss = 0.47224867\n",
      "Iteration 9843, loss = 0.47220559\n",
      "Iteration 9844, loss = 0.47214528\n",
      "Iteration 9845, loss = 0.47256070\n",
      "Iteration 9846, loss = 0.47228333\n",
      "Iteration 9847, loss = 0.47231798\n",
      "Iteration 9848, loss = 0.47220392\n",
      "Iteration 9849, loss = 0.47226282\n",
      "Iteration 9850, loss = 0.47262090\n",
      "Iteration 9851, loss = 0.47246098\n",
      "Iteration 9852, loss = 0.47213736\n",
      "Iteration 9853, loss = 0.47207885\n",
      "Iteration 9854, loss = 0.47325200\n",
      "Iteration 9855, loss = 0.47342749\n",
      "Iteration 9856, loss = 0.47342562\n",
      "Iteration 9857, loss = 0.47326379\n",
      "Iteration 9858, loss = 0.47308616\n",
      "Iteration 9859, loss = 0.47235105\n",
      "Iteration 9860, loss = 0.47214334\n",
      "Iteration 9861, loss = 0.47249386\n",
      "Iteration 9862, loss = 0.47259810\n",
      "Iteration 9863, loss = 0.47295857\n",
      "Iteration 9864, loss = 0.47325832\n",
      "Iteration 9865, loss = 0.47308327\n",
      "Iteration 9866, loss = 0.47239391\n",
      "Iteration 9867, loss = 0.47179673\n",
      "Iteration 9868, loss = 0.47290423\n",
      "Iteration 9869, loss = 0.47377541\n",
      "Iteration 9870, loss = 0.47524124\n",
      "Iteration 9871, loss = 0.47476845\n",
      "Iteration 9872, loss = 0.47340872\n",
      "Iteration 9873, loss = 0.47283935\n",
      "Iteration 9874, loss = 0.47266080\n",
      "Iteration 9875, loss = 0.47246047\n",
      "Iteration 9876, loss = 0.47270700\n",
      "Iteration 9877, loss = 0.47223659\n",
      "Iteration 9878, loss = 0.47284822\n",
      "Iteration 9879, loss = 0.47245455\n",
      "Iteration 9880, loss = 0.47224923\n",
      "Iteration 9881, loss = 0.47233315\n",
      "Iteration 9882, loss = 0.47218099\n",
      "Iteration 9883, loss = 0.47218269\n",
      "Iteration 9884, loss = 0.47220281\n",
      "Iteration 9885, loss = 0.47220532\n",
      "Iteration 9886, loss = 0.47215108\n",
      "Iteration 9887, loss = 0.47210259\n",
      "Iteration 9888, loss = 0.47233016\n",
      "Iteration 9889, loss = 0.47262158\n",
      "Iteration 9890, loss = 0.47268641\n",
      "Iteration 9891, loss = 0.47276956\n",
      "Iteration 9892, loss = 0.47233095\n",
      "Iteration 9893, loss = 0.47224502\n",
      "Iteration 9894, loss = 0.47256936\n",
      "Iteration 9895, loss = 0.47314314\n",
      "Iteration 9896, loss = 0.47366456\n",
      "Iteration 9897, loss = 0.47316080\n",
      "Iteration 9898, loss = 0.47224128\n",
      "Iteration 9899, loss = 0.47217174\n",
      "Iteration 9900, loss = 0.47251947\n",
      "Iteration 9901, loss = 0.47294011\n",
      "Iteration 9902, loss = 0.47308532\n",
      "Iteration 9903, loss = 0.47290940\n",
      "Iteration 9904, loss = 0.47276823\n",
      "Iteration 9905, loss = 0.47243065\n",
      "Iteration 9906, loss = 0.47268725\n",
      "Iteration 9907, loss = 0.47239876\n",
      "Iteration 9908, loss = 0.47238701\n",
      "Iteration 9909, loss = 0.47240739\n",
      "Iteration 9910, loss = 0.47263580\n",
      "Iteration 9911, loss = 0.47262915\n",
      "Iteration 9912, loss = 0.47252621\n",
      "Iteration 9913, loss = 0.47238505\n",
      "Iteration 9914, loss = 0.47273422\n",
      "Iteration 9915, loss = 0.47247027\n",
      "Iteration 9916, loss = 0.47242200\n",
      "Iteration 9917, loss = 0.47251297\n",
      "Iteration 9918, loss = 0.47248740\n",
      "Iteration 9919, loss = 0.47218564\n",
      "Iteration 9920, loss = 0.47211331\n",
      "Iteration 9921, loss = 0.47279737\n",
      "Iteration 9922, loss = 0.47229636\n",
      "Iteration 9923, loss = 0.47220393\n",
      "Iteration 9924, loss = 0.47216609\n",
      "Iteration 9925, loss = 0.47227745\n",
      "Iteration 9926, loss = 0.47228332\n",
      "Iteration 9927, loss = 0.47222657\n",
      "Iteration 9928, loss = 0.47212164\n",
      "Iteration 9929, loss = 0.47229808\n",
      "Iteration 9930, loss = 0.47295240\n",
      "Iteration 9931, loss = 0.47303713\n",
      "Iteration 9932, loss = 0.47273907\n",
      "Iteration 9933, loss = 0.47231792\n",
      "Iteration 9934, loss = 0.47214191\n",
      "Iteration 9935, loss = 0.47232087\n",
      "Iteration 9936, loss = 0.47313090\n",
      "Iteration 9937, loss = 0.47370516\n",
      "Iteration 9938, loss = 0.47358827\n",
      "Iteration 9939, loss = 0.47298293\n",
      "Iteration 9940, loss = 0.47230146\n",
      "Iteration 9941, loss = 0.47209774\n",
      "Iteration 9942, loss = 0.47268749\n",
      "Iteration 9943, loss = 0.47384853\n",
      "Iteration 9944, loss = 0.47321017\n",
      "Iteration 9945, loss = 0.47184726\n",
      "Iteration 9946, loss = 0.47305021\n",
      "Iteration 9947, loss = 0.47424885\n",
      "Iteration 9948, loss = 0.47400539\n",
      "Iteration 9949, loss = 0.47298826\n",
      "Iteration 9950, loss = 0.47212506\n",
      "Iteration 9951, loss = 0.47253824\n",
      "Iteration 9952, loss = 0.47247854\n",
      "Iteration 9953, loss = 0.47267357\n",
      "Iteration 9954, loss = 0.47255927\n",
      "Iteration 9955, loss = 0.47215425\n",
      "Iteration 9956, loss = 0.47205158\n",
      "Iteration 9957, loss = 0.47292245\n",
      "Iteration 9958, loss = 0.47397646\n",
      "Iteration 9959, loss = 0.47471225\n",
      "Iteration 9960, loss = 0.47431865\n",
      "Iteration 9961, loss = 0.47306928\n",
      "Iteration 9962, loss = 0.47212453\n",
      "Iteration 9963, loss = 0.47270162\n",
      "Iteration 9964, loss = 0.47342627\n",
      "Iteration 9965, loss = 0.47327817\n",
      "Iteration 9966, loss = 0.47260989\n",
      "Iteration 9967, loss = 0.47237265\n",
      "Iteration 9968, loss = 0.47281573\n",
      "Iteration 9969, loss = 0.47310141\n",
      "Iteration 9970, loss = 0.47365099\n",
      "Iteration 9971, loss = 0.47622495\n",
      "Iteration 9972, loss = 0.47673643\n",
      "Iteration 9973, loss = 0.47537211\n",
      "Iteration 9974, loss = 0.47391723\n",
      "Iteration 9975, loss = 0.47262876\n",
      "Iteration 9976, loss = 0.47222148\n",
      "Iteration 9977, loss = 0.47240316\n",
      "Iteration 9978, loss = 0.47300861\n",
      "Iteration 9979, loss = 0.47355077\n",
      "Iteration 9980, loss = 0.47371435\n",
      "Iteration 9981, loss = 0.47338827\n",
      "Iteration 9982, loss = 0.47348606\n",
      "Iteration 9983, loss = 0.47366049\n",
      "Iteration 9984, loss = 0.47318595\n",
      "Iteration 9985, loss = 0.47242798\n",
      "Iteration 9986, loss = 0.47217433\n",
      "Iteration 9987, loss = 0.47356085\n",
      "Iteration 9988, loss = 0.47361997\n",
      "Iteration 9989, loss = 0.47348123\n",
      "Iteration 9990, loss = 0.47353669\n",
      "Iteration 9991, loss = 0.47305443\n",
      "Iteration 9992, loss = 0.47271863\n",
      "Iteration 9993, loss = 0.47246585\n",
      "Iteration 9994, loss = 0.47220829\n",
      "Iteration 9995, loss = 0.47218610\n",
      "Iteration 9996, loss = 0.47250311\n",
      "Iteration 9997, loss = 0.47263901\n",
      "Iteration 9998, loss = 0.47254406\n",
      "Iteration 9999, loss = 0.47225529\n",
      "Iteration 10000, loss = 0.47224359\n",
      "Iteration 10001, loss = 0.47224327\n",
      "Iteration 10002, loss = 0.47347533\n",
      "Iteration 10003, loss = 0.47334682\n",
      "Iteration 10004, loss = 0.47280726\n",
      "Iteration 10005, loss = 0.47249717\n",
      "Iteration 10006, loss = 0.47229747\n",
      "Iteration 10007, loss = 0.47218987\n",
      "Iteration 10008, loss = 0.47222740\n",
      "Iteration 10009, loss = 0.47264616\n",
      "Iteration 10010, loss = 0.47233117\n",
      "Iteration 10011, loss = 0.47238768\n",
      "Iteration 10012, loss = 0.47231809\n",
      "Iteration 10013, loss = 0.47233802\n",
      "Iteration 10014, loss = 0.47227355\n",
      "Iteration 10015, loss = 0.47228132\n",
      "Iteration 10016, loss = 0.47247060\n",
      "Iteration 10017, loss = 0.47315801\n",
      "Iteration 10018, loss = 0.47315539\n",
      "Iteration 10019, loss = 0.47290067\n",
      "Iteration 10020, loss = 0.47250692\n",
      "Iteration 10021, loss = 0.47213969\n",
      "Iteration 10022, loss = 0.47187367\n",
      "Iteration 10023, loss = 0.47283180\n",
      "Iteration 10024, loss = 0.47516652\n",
      "Iteration 10025, loss = 0.47609534\n",
      "Iteration 10026, loss = 0.47562846\n",
      "Iteration 10027, loss = 0.47444569\n",
      "Iteration 10028, loss = 0.47295729\n",
      "Iteration 10029, loss = 0.47224571\n",
      "Iteration 10030, loss = 0.47220638\n",
      "Iteration 10031, loss = 0.47255796\n",
      "Iteration 10032, loss = 0.47323721\n",
      "Iteration 10033, loss = 0.47379394\n",
      "Iteration 10034, loss = 0.47389454\n",
      "Iteration 10035, loss = 0.47298954\n",
      "Iteration 10036, loss = 0.47246748\n",
      "Iteration 10037, loss = 0.47250387\n",
      "Iteration 10038, loss = 0.47251989\n",
      "Iteration 10039, loss = 0.47241096\n",
      "Iteration 10040, loss = 0.47218565\n",
      "Iteration 10041, loss = 0.47216844\n",
      "Iteration 10042, loss = 0.47221130\n",
      "Iteration 10043, loss = 0.47225309\n",
      "Iteration 10044, loss = 0.47219553\n",
      "Iteration 10045, loss = 0.47219227\n",
      "Iteration 10046, loss = 0.47220354\n",
      "Iteration 10047, loss = 0.47342996\n",
      "Iteration 10048, loss = 0.47352620\n",
      "Iteration 10049, loss = 0.47318131\n",
      "Iteration 10050, loss = 0.47281566\n",
      "Iteration 10051, loss = 0.47255396\n",
      "Iteration 10052, loss = 0.47244537\n",
      "Iteration 10053, loss = 0.47227679\n",
      "Iteration 10054, loss = 0.47239041\n",
      "Iteration 10055, loss = 0.47227187\n",
      "Iteration 10056, loss = 0.47253676\n",
      "Iteration 10057, loss = 0.47336678\n",
      "Iteration 10058, loss = 0.47425163\n",
      "Iteration 10059, loss = 0.47389723\n",
      "Iteration 10060, loss = 0.47288160\n",
      "Iteration 10061, loss = 0.47214670\n",
      "Iteration 10062, loss = 0.47249682\n",
      "Iteration 10063, loss = 0.47496741\n",
      "Iteration 10064, loss = 0.47697537\n",
      "Iteration 10065, loss = 0.47724491\n",
      "Iteration 10066, loss = 0.47587027\n",
      "Iteration 10067, loss = 0.47408884\n",
      "Iteration 10068, loss = 0.47229211\n",
      "Iteration 10069, loss = 0.47252183\n",
      "Iteration 10070, loss = 0.47363586\n",
      "Iteration 10071, loss = 0.47421016\n",
      "Iteration 10072, loss = 0.47335838\n",
      "Iteration 10073, loss = 0.47270385\n",
      "Iteration 10074, loss = 0.47233669\n",
      "Iteration 10075, loss = 0.47340791\n",
      "Iteration 10076, loss = 0.47457588\n",
      "Iteration 10077, loss = 0.47455569\n",
      "Iteration 10078, loss = 0.47406830\n",
      "Iteration 10079, loss = 0.47321606\n",
      "Iteration 10080, loss = 0.47310396\n",
      "Iteration 10081, loss = 0.47269939\n",
      "Iteration 10082, loss = 0.47228030\n",
      "Iteration 10083, loss = 0.47222551\n",
      "Iteration 10084, loss = 0.47261902\n",
      "Iteration 10085, loss = 0.47278451\n",
      "Iteration 10086, loss = 0.47260435\n",
      "Iteration 10087, loss = 0.47233549\n",
      "Iteration 10088, loss = 0.47242739\n",
      "Iteration 10089, loss = 0.47246545\n",
      "Iteration 10090, loss = 0.47268249\n",
      "Iteration 10091, loss = 0.47270065\n",
      "Iteration 10092, loss = 0.47252560\n",
      "Iteration 10093, loss = 0.47239221\n",
      "Iteration 10094, loss = 0.47223510\n",
      "Iteration 10095, loss = 0.47307096\n",
      "Iteration 10096, loss = 0.47224192\n",
      "Iteration 10097, loss = 0.47234603\n",
      "Iteration 10098, loss = 0.47333056\n",
      "Iteration 10099, loss = 0.47520609\n",
      "Iteration 10100, loss = 0.47525347\n",
      "Iteration 10101, loss = 0.47407832\n",
      "Iteration 10102, loss = 0.47361923\n",
      "Iteration 10103, loss = 0.47270831\n",
      "Iteration 10104, loss = 0.47259977\n",
      "Iteration 10105, loss = 0.47248069\n",
      "Iteration 10106, loss = 0.47279363\n",
      "Iteration 10107, loss = 0.47339419\n",
      "Iteration 10108, loss = 0.47312676\n",
      "Iteration 10109, loss = 0.47269854\n",
      "Iteration 10110, loss = 0.47255047\n",
      "Iteration 10111, loss = 0.47267269\n",
      "Iteration 10112, loss = 0.47309565\n",
      "Iteration 10113, loss = 0.47301612\n",
      "Iteration 10114, loss = 0.47293324\n",
      "Iteration 10115, loss = 0.47288757\n",
      "Iteration 10116, loss = 0.47349561\n",
      "Iteration 10117, loss = 0.47329204\n",
      "Iteration 10118, loss = 0.47269352\n",
      "Iteration 10119, loss = 0.47251417\n",
      "Iteration 10120, loss = 0.47251408\n",
      "Iteration 10121, loss = 0.47259772\n",
      "Iteration 10122, loss = 0.47295229\n",
      "Iteration 10123, loss = 0.47316745\n",
      "Iteration 10124, loss = 0.47259359\n",
      "Iteration 10125, loss = 0.47226365\n",
      "Iteration 10126, loss = 0.47277434\n",
      "Iteration 10127, loss = 0.47423671\n",
      "Iteration 10128, loss = 0.47439593\n",
      "Iteration 10129, loss = 0.47360553\n",
      "Iteration 10130, loss = 0.47269366\n",
      "Iteration 10131, loss = 0.47225778\n",
      "Iteration 10132, loss = 0.47252980\n",
      "Iteration 10133, loss = 0.47251185\n",
      "Iteration 10134, loss = 0.47249570\n",
      "Iteration 10135, loss = 0.47249596\n",
      "Iteration 10136, loss = 0.47242108\n",
      "Iteration 10137, loss = 0.47243056\n",
      "Iteration 10138, loss = 0.47251298\n",
      "Iteration 10139, loss = 0.47342083\n",
      "Iteration 10140, loss = 0.47305680\n",
      "Iteration 10141, loss = 0.47239538\n",
      "Iteration 10142, loss = 0.47234477\n",
      "Iteration 10143, loss = 0.47222470\n",
      "Iteration 10144, loss = 0.47299511\n",
      "Iteration 10145, loss = 0.47388846\n",
      "Iteration 10146, loss = 0.47380033\n",
      "Iteration 10147, loss = 0.47304867\n",
      "Iteration 10148, loss = 0.47244136\n",
      "Iteration 10149, loss = 0.47216817\n",
      "Iteration 10150, loss = 0.47209507\n",
      "Iteration 10151, loss = 0.47241238\n",
      "Iteration 10152, loss = 0.47348758\n",
      "Iteration 10153, loss = 0.47479403\n",
      "Iteration 10154, loss = 0.47575658\n",
      "Iteration 10155, loss = 0.47526273\n",
      "Iteration 10156, loss = 0.47386732\n",
      "Iteration 10157, loss = 0.47362866\n",
      "Iteration 10158, loss = 0.47247937\n",
      "Iteration 10159, loss = 0.47244337\n",
      "Iteration 10160, loss = 0.47249919\n",
      "Iteration 10161, loss = 0.47246569\n",
      "Iteration 10162, loss = 0.47314562\n",
      "Iteration 10163, loss = 0.47374173\n",
      "Iteration 10164, loss = 0.47352302\n",
      "Iteration 10165, loss = 0.47298144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10166, loss = 0.47267578\n",
      "Iteration 10167, loss = 0.47244982\n",
      "Iteration 10168, loss = 0.47245067\n",
      "Iteration 10169, loss = 0.47232550\n",
      "Iteration 10170, loss = 0.47235913\n",
      "Iteration 10171, loss = 0.47267228\n",
      "Iteration 10172, loss = 0.47338624\n",
      "Iteration 10173, loss = 0.47292031\n",
      "Iteration 10174, loss = 0.47210223\n",
      "Iteration 10175, loss = 0.47265634\n",
      "Iteration 10176, loss = 0.47291312\n",
      "Iteration 10177, loss = 0.47341855\n",
      "Iteration 10178, loss = 0.47377644\n",
      "Iteration 10179, loss = 0.47377308\n",
      "Iteration 10180, loss = 0.47350197\n",
      "Iteration 10181, loss = 0.47318709\n",
      "Iteration 10182, loss = 0.47259871\n",
      "Iteration 10183, loss = 0.47237200\n",
      "Iteration 10184, loss = 0.47209145\n",
      "Iteration 10185, loss = 0.47213699\n",
      "Iteration 10186, loss = 0.47230195\n",
      "Iteration 10187, loss = 0.47366443\n",
      "Iteration 10188, loss = 0.47452898\n",
      "Iteration 10189, loss = 0.47412375\n",
      "Iteration 10190, loss = 0.47279911\n",
      "Iteration 10191, loss = 0.47218910\n",
      "Iteration 10192, loss = 0.47289598\n",
      "Iteration 10193, loss = 0.47377903\n",
      "Iteration 10194, loss = 0.47425484\n",
      "Iteration 10195, loss = 0.47416507\n",
      "Iteration 10196, loss = 0.47395076\n",
      "Iteration 10197, loss = 0.47330789\n",
      "Iteration 10198, loss = 0.47299311\n",
      "Iteration 10199, loss = 0.47217329\n",
      "Iteration 10200, loss = 0.47199747\n",
      "Iteration 10201, loss = 0.47369003\n",
      "Iteration 10202, loss = 0.47507073\n",
      "Iteration 10203, loss = 0.47490472\n",
      "Iteration 10204, loss = 0.47371912\n",
      "Iteration 10205, loss = 0.47310768\n",
      "Iteration 10206, loss = 0.47267338\n",
      "Iteration 10207, loss = 0.47242822\n",
      "Iteration 10208, loss = 0.47279539\n",
      "Iteration 10209, loss = 0.47293555\n",
      "Iteration 10210, loss = 0.47277821\n",
      "Iteration 10211, loss = 0.47238451\n",
      "Iteration 10212, loss = 0.47226802\n",
      "Iteration 10213, loss = 0.47213280\n",
      "Iteration 10214, loss = 0.47235511\n",
      "Iteration 10215, loss = 0.47243739\n",
      "Iteration 10216, loss = 0.47247995\n",
      "Iteration 10217, loss = 0.47235994\n",
      "Iteration 10218, loss = 0.47239555\n",
      "Iteration 10219, loss = 0.47217129\n",
      "Iteration 10220, loss = 0.47235466\n",
      "Iteration 10221, loss = 0.47251306\n",
      "Iteration 10222, loss = 0.47269452\n",
      "Iteration 10223, loss = 0.47248091\n",
      "Iteration 10224, loss = 0.47243289\n",
      "Iteration 10225, loss = 0.47228290\n",
      "Iteration 10226, loss = 0.47215789\n",
      "Iteration 10227, loss = 0.47235548\n",
      "Iteration 10228, loss = 0.47331067\n",
      "Iteration 10229, loss = 0.47260091\n",
      "Iteration 10230, loss = 0.47249507\n",
      "Iteration 10231, loss = 0.47257682\n",
      "Iteration 10232, loss = 0.47269879\n",
      "Iteration 10233, loss = 0.47286583\n",
      "Iteration 10234, loss = 0.47310047\n",
      "Iteration 10235, loss = 0.47419488\n",
      "Iteration 10236, loss = 0.47424543\n",
      "Iteration 10237, loss = 0.47396335\n",
      "Iteration 10238, loss = 0.47309807\n",
      "Iteration 10239, loss = 0.47281098\n",
      "Iteration 10240, loss = 0.47262409\n",
      "Iteration 10241, loss = 0.47228815\n",
      "Iteration 10242, loss = 0.47210586\n",
      "Iteration 10243, loss = 0.47218393\n",
      "Iteration 10244, loss = 0.47278600\n",
      "Iteration 10245, loss = 0.47284355\n",
      "Iteration 10246, loss = 0.47266464\n",
      "Iteration 10247, loss = 0.47248932\n",
      "Iteration 10248, loss = 0.47223774\n",
      "Iteration 10249, loss = 0.47220576\n",
      "Iteration 10250, loss = 0.47228567\n",
      "Iteration 10251, loss = 0.47249478\n",
      "Iteration 10252, loss = 0.47261835\n",
      "Iteration 10253, loss = 0.47262122\n",
      "Iteration 10254, loss = 0.47292395\n",
      "Iteration 10255, loss = 0.47324105\n",
      "Iteration 10256, loss = 0.47274942\n",
      "Iteration 10257, loss = 0.47224761\n",
      "Iteration 10258, loss = 0.47233647\n",
      "Iteration 10259, loss = 0.47255651\n",
      "Iteration 10260, loss = 0.47286166\n",
      "Iteration 10261, loss = 0.47336113\n",
      "Iteration 10262, loss = 0.47334735\n",
      "Iteration 10263, loss = 0.47323006\n",
      "Iteration 10264, loss = 0.47271897\n",
      "Iteration 10265, loss = 0.47284745\n",
      "Iteration 10266, loss = 0.47233955\n",
      "Iteration 10267, loss = 0.47230875\n",
      "Iteration 10268, loss = 0.47269091\n",
      "Iteration 10269, loss = 0.47286456\n",
      "Iteration 10270, loss = 0.47264990\n",
      "Iteration 10271, loss = 0.47268855\n",
      "Iteration 10272, loss = 0.47211694\n",
      "Iteration 10273, loss = 0.47211736\n",
      "Iteration 10274, loss = 0.47228146\n",
      "Iteration 10275, loss = 0.47298876\n",
      "Iteration 10276, loss = 0.47396726\n",
      "Iteration 10277, loss = 0.47337971\n",
      "Iteration 10278, loss = 0.47246719\n",
      "Iteration 10279, loss = 0.47217474\n",
      "Iteration 10280, loss = 0.47255458\n",
      "Iteration 10281, loss = 0.47270625\n",
      "Iteration 10282, loss = 0.47287449\n",
      "Iteration 10283, loss = 0.47254195\n",
      "Iteration 10284, loss = 0.47265824\n",
      "Iteration 10285, loss = 0.47259745\n",
      "Iteration 10286, loss = 0.47247213\n",
      "Iteration 10287, loss = 0.47217126\n",
      "Iteration 10288, loss = 0.47229006\n",
      "Iteration 10289, loss = 0.47217066\n",
      "Iteration 10290, loss = 0.47228568\n",
      "Iteration 10291, loss = 0.47218825\n",
      "Iteration 10292, loss = 0.47234515\n",
      "Iteration 10293, loss = 0.47239193\n",
      "Iteration 10294, loss = 0.47190098\n",
      "Iteration 10295, loss = 0.47206275\n",
      "Iteration 10296, loss = 0.47306478\n",
      "Iteration 10297, loss = 0.47390075\n",
      "Iteration 10298, loss = 0.47433916\n",
      "Iteration 10299, loss = 0.47445335\n",
      "Iteration 10300, loss = 0.47350577\n",
      "Iteration 10301, loss = 0.47276285\n",
      "Iteration 10302, loss = 0.47257036\n",
      "Iteration 10303, loss = 0.47237136\n",
      "Iteration 10304, loss = 0.47238645\n",
      "Iteration 10305, loss = 0.47245995\n",
      "Iteration 10306, loss = 0.47229634\n",
      "Iteration 10307, loss = 0.47247704\n",
      "Iteration 10308, loss = 0.47293520\n",
      "Iteration 10309, loss = 0.47326056\n",
      "Iteration 10310, loss = 0.47322538\n",
      "Iteration 10311, loss = 0.47319945\n",
      "Iteration 10312, loss = 0.47298150\n",
      "Iteration 10313, loss = 0.47373432\n",
      "Iteration 10314, loss = 0.47326862\n",
      "Iteration 10315, loss = 0.47194803\n",
      "Iteration 10316, loss = 0.47214233\n",
      "Iteration 10317, loss = 0.47374637\n",
      "Iteration 10318, loss = 0.47504741\n",
      "Iteration 10319, loss = 0.47533537\n",
      "Iteration 10320, loss = 0.47432781\n",
      "Iteration 10321, loss = 0.47330404\n",
      "Iteration 10322, loss = 0.47242682\n",
      "Iteration 10323, loss = 0.47225324\n",
      "Iteration 10324, loss = 0.47236237\n",
      "Iteration 10325, loss = 0.47244348\n",
      "Iteration 10326, loss = 0.47254349\n",
      "Iteration 10327, loss = 0.47222063\n",
      "Iteration 10328, loss = 0.47220339\n",
      "Iteration 10329, loss = 0.47228335\n",
      "Iteration 10330, loss = 0.47238839\n",
      "Iteration 10331, loss = 0.47221737\n",
      "Iteration 10332, loss = 0.47197629\n",
      "Iteration 10333, loss = 0.47229526\n",
      "Iteration 10334, loss = 0.47255026\n",
      "Iteration 10335, loss = 0.47280047\n",
      "Iteration 10336, loss = 0.47302207\n",
      "Iteration 10337, loss = 0.47251676\n",
      "Iteration 10338, loss = 0.47194848\n",
      "Iteration 10339, loss = 0.47292119\n",
      "Iteration 10340, loss = 0.47347386\n",
      "Iteration 10341, loss = 0.47461708\n",
      "Iteration 10342, loss = 0.47597497\n",
      "Iteration 10343, loss = 0.47630091\n",
      "Iteration 10344, loss = 0.47568332\n",
      "Iteration 10345, loss = 0.47399908\n",
      "Iteration 10346, loss = 0.47321979\n",
      "Iteration 10347, loss = 0.47209353\n",
      "Iteration 10348, loss = 0.47202113\n",
      "Iteration 10349, loss = 0.47348363\n",
      "Iteration 10350, loss = 0.47312516\n",
      "Iteration 10351, loss = 0.47250039\n",
      "Iteration 10352, loss = 0.47209385\n",
      "Iteration 10353, loss = 0.47214046\n",
      "Iteration 10354, loss = 0.47310641\n",
      "Iteration 10355, loss = 0.47348677\n",
      "Iteration 10356, loss = 0.47322882\n",
      "Iteration 10357, loss = 0.47278943\n",
      "Iteration 10358, loss = 0.47228026\n",
      "Iteration 10359, loss = 0.47202283\n",
      "Iteration 10360, loss = 0.47194787\n",
      "Iteration 10361, loss = 0.47257166\n",
      "Iteration 10362, loss = 0.47346062\n",
      "Iteration 10363, loss = 0.47312484\n",
      "Iteration 10364, loss = 0.47219911\n",
      "Iteration 10365, loss = 0.47197793\n",
      "Iteration 10366, loss = 0.47280342\n",
      "Iteration 10367, loss = 0.47349789\n",
      "Iteration 10368, loss = 0.47435899\n",
      "Iteration 10369, loss = 0.47545509\n",
      "Iteration 10370, loss = 0.47478492\n",
      "Iteration 10371, loss = 0.47340596\n",
      "Iteration 10372, loss = 0.47216046\n",
      "Iteration 10373, loss = 0.47223817\n",
      "Iteration 10374, loss = 0.47309689\n",
      "Iteration 10375, loss = 0.47403198\n",
      "Iteration 10376, loss = 0.47433491\n",
      "Iteration 10377, loss = 0.47397447\n",
      "Iteration 10378, loss = 0.47328553\n",
      "Iteration 10379, loss = 0.47303888\n",
      "Iteration 10380, loss = 0.47289447\n",
      "Iteration 10381, loss = 0.47268349\n",
      "Iteration 10382, loss = 0.47249881\n",
      "Iteration 10383, loss = 0.47238964\n",
      "Iteration 10384, loss = 0.47224777\n",
      "Iteration 10385, loss = 0.47207685\n",
      "Iteration 10386, loss = 0.47235304\n",
      "Iteration 10387, loss = 0.47243723\n",
      "Iteration 10388, loss = 0.47216171\n",
      "Iteration 10389, loss = 0.47259951\n",
      "Iteration 10390, loss = 0.47249106\n",
      "Iteration 10391, loss = 0.47258515\n",
      "Iteration 10392, loss = 0.47304166\n",
      "Iteration 10393, loss = 0.47289854\n",
      "Iteration 10394, loss = 0.47232641\n",
      "Iteration 10395, loss = 0.47189040\n",
      "Iteration 10396, loss = 0.47268690\n",
      "Iteration 10397, loss = 0.47540313\n",
      "Iteration 10398, loss = 0.47709374\n",
      "Iteration 10399, loss = 0.47763858\n",
      "Iteration 10400, loss = 0.47733543\n",
      "Iteration 10401, loss = 0.47556483\n",
      "Iteration 10402, loss = 0.47359763\n",
      "Iteration 10403, loss = 0.47224771\n",
      "Iteration 10404, loss = 0.47221900\n",
      "Iteration 10405, loss = 0.47335798\n",
      "Iteration 10406, loss = 0.47360218\n",
      "Iteration 10407, loss = 0.47334369\n",
      "Iteration 10408, loss = 0.47358326\n",
      "Iteration 10409, loss = 0.47371426\n",
      "Iteration 10410, loss = 0.47358374\n",
      "Iteration 10411, loss = 0.47322059\n",
      "Iteration 10412, loss = 0.47323853\n",
      "Iteration 10413, loss = 0.47295386\n",
      "Iteration 10414, loss = 0.47315376\n",
      "Iteration 10415, loss = 0.47326455\n",
      "Iteration 10416, loss = 0.47285518\n",
      "Iteration 10417, loss = 0.47305550\n",
      "Iteration 10418, loss = 0.47312002\n",
      "Iteration 10419, loss = 0.47303898\n",
      "Iteration 10420, loss = 0.47258438\n",
      "Iteration 10421, loss = 0.47230832\n",
      "Iteration 10422, loss = 0.47229650\n",
      "Iteration 10423, loss = 0.47261789\n",
      "Iteration 10424, loss = 0.47264486\n",
      "Iteration 10425, loss = 0.47249704\n",
      "Iteration 10426, loss = 0.47234442\n",
      "Iteration 10427, loss = 0.47267828\n",
      "Iteration 10428, loss = 0.47259730\n",
      "Iteration 10429, loss = 0.47241470\n",
      "Iteration 10430, loss = 0.47220314\n",
      "Iteration 10431, loss = 0.47204362\n",
      "Iteration 10432, loss = 0.47281446\n",
      "Iteration 10433, loss = 0.47222842\n",
      "Iteration 10434, loss = 0.47213775\n",
      "Iteration 10435, loss = 0.47211360\n",
      "Iteration 10436, loss = 0.47222054\n",
      "Iteration 10437, loss = 0.47251083\n",
      "Iteration 10438, loss = 0.47310000\n",
      "Iteration 10439, loss = 0.47405632\n",
      "Iteration 10440, loss = 0.47515483\n",
      "Iteration 10441, loss = 0.47524527\n",
      "Iteration 10442, loss = 0.47444659\n",
      "Iteration 10443, loss = 0.47340898\n",
      "Iteration 10444, loss = 0.47264146\n",
      "Iteration 10445, loss = 0.47211681\n",
      "Iteration 10446, loss = 0.47208890\n",
      "Iteration 10447, loss = 0.47213987\n",
      "Iteration 10448, loss = 0.47235407\n",
      "Iteration 10449, loss = 0.47253149\n",
      "Iteration 10450, loss = 0.47275399\n",
      "Iteration 10451, loss = 0.47319614\n",
      "Iteration 10452, loss = 0.47391887\n",
      "Iteration 10453, loss = 0.47445291\n",
      "Iteration 10454, loss = 0.47449916\n",
      "Iteration 10455, loss = 0.47378263\n",
      "Iteration 10456, loss = 0.47301883\n",
      "Iteration 10457, loss = 0.47280742\n",
      "Iteration 10458, loss = 0.47214328\n",
      "Iteration 10459, loss = 0.47295013\n",
      "Iteration 10460, loss = 0.47345332\n",
      "Iteration 10461, loss = 0.47478847\n",
      "Iteration 10462, loss = 0.47500803\n",
      "Iteration 10463, loss = 0.47419413\n",
      "Iteration 10464, loss = 0.47297624\n",
      "Iteration 10465, loss = 0.47284058\n",
      "Iteration 10466, loss = 0.47237064\n",
      "Iteration 10467, loss = 0.47226024\n",
      "Iteration 10468, loss = 0.47245531\n",
      "Iteration 10469, loss = 0.47191845\n",
      "Iteration 10470, loss = 0.47191672\n",
      "Iteration 10471, loss = 0.47284787\n",
      "Iteration 10472, loss = 0.47375258\n",
      "Iteration 10473, loss = 0.47427006\n",
      "Iteration 10474, loss = 0.47439767\n",
      "Iteration 10475, loss = 0.47402240\n",
      "Iteration 10476, loss = 0.47281349\n",
      "Iteration 10477, loss = 0.47279680\n",
      "Iteration 10478, loss = 0.47192257\n",
      "Iteration 10479, loss = 0.47243442\n",
      "Iteration 10480, loss = 0.47305236\n",
      "Iteration 10481, loss = 0.47345012\n",
      "Iteration 10482, loss = 0.47359409\n",
      "Iteration 10483, loss = 0.47305912\n",
      "Iteration 10484, loss = 0.47320277\n",
      "Iteration 10485, loss = 0.47337954\n",
      "Iteration 10486, loss = 0.47263806\n",
      "Iteration 10487, loss = 0.47231795\n",
      "Iteration 10488, loss = 0.47230091\n",
      "Iteration 10489, loss = 0.47248637\n",
      "Iteration 10490, loss = 0.47284942\n",
      "Iteration 10491, loss = 0.47303760\n",
      "Iteration 10492, loss = 0.47244788\n",
      "Iteration 10493, loss = 0.47171640\n",
      "Iteration 10494, loss = 0.47255257\n",
      "Iteration 10495, loss = 0.47625418\n",
      "Iteration 10496, loss = 0.47610250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10497, loss = 0.47401813\n",
      "Iteration 10498, loss = 0.47156310\n",
      "Iteration 10499, loss = 0.47333050\n",
      "Iteration 10500, loss = 0.47398903\n",
      "Iteration 10501, loss = 0.47517708\n",
      "Iteration 10502, loss = 0.47467349\n",
      "Iteration 10503, loss = 0.47309026\n",
      "Iteration 10504, loss = 0.47224874\n",
      "Iteration 10505, loss = 0.47282268\n",
      "Iteration 10506, loss = 0.47306326\n",
      "Iteration 10507, loss = 0.47305002\n",
      "Iteration 10508, loss = 0.47288598\n",
      "Iteration 10509, loss = 0.47215980\n",
      "Iteration 10510, loss = 0.47279251\n",
      "Iteration 10511, loss = 0.47188388\n",
      "Iteration 10512, loss = 0.47244943\n",
      "Iteration 10513, loss = 0.47304005\n",
      "Iteration 10514, loss = 0.47209072\n",
      "Iteration 10515, loss = 0.47203175\n",
      "Iteration 10516, loss = 0.47296367\n",
      "Iteration 10517, loss = 0.47329320\n",
      "Iteration 10518, loss = 0.47293825\n",
      "Iteration 10519, loss = 0.47221042\n",
      "Iteration 10520, loss = 0.47220505\n",
      "Iteration 10521, loss = 0.47263332\n",
      "Iteration 10522, loss = 0.47312055\n",
      "Iteration 10523, loss = 0.47291378\n",
      "Iteration 10524, loss = 0.47212861\n",
      "Iteration 10525, loss = 0.47203348\n",
      "Iteration 10526, loss = 0.47265354\n",
      "Iteration 10527, loss = 0.47393741\n",
      "Iteration 10528, loss = 0.47430748\n",
      "Iteration 10529, loss = 0.47407901\n",
      "Iteration 10530, loss = 0.47332805\n",
      "Iteration 10531, loss = 0.47327258\n",
      "Iteration 10532, loss = 0.47237472\n",
      "Iteration 10533, loss = 0.47216153\n",
      "Iteration 10534, loss = 0.47231430\n",
      "Iteration 10535, loss = 0.47226756\n",
      "Iteration 10536, loss = 0.47233769\n",
      "Iteration 10537, loss = 0.47231145\n",
      "Iteration 10538, loss = 0.47223895\n",
      "Iteration 10539, loss = 0.47223857\n",
      "Iteration 10540, loss = 0.47213295\n",
      "Iteration 10541, loss = 0.47206325\n",
      "Iteration 10542, loss = 0.47205421\n",
      "Iteration 10543, loss = 0.47219207\n",
      "Iteration 10544, loss = 0.47227039\n",
      "Iteration 10545, loss = 0.47218359\n",
      "Iteration 10546, loss = 0.47213374\n",
      "Iteration 10547, loss = 0.47239311\n",
      "Iteration 10548, loss = 0.47242153\n",
      "Iteration 10549, loss = 0.47247606\n",
      "Iteration 10550, loss = 0.47213055\n",
      "Iteration 10551, loss = 0.47212816\n",
      "Iteration 10552, loss = 0.47266732\n",
      "Iteration 10553, loss = 0.47218528\n",
      "Iteration 10554, loss = 0.47217367\n",
      "Iteration 10555, loss = 0.47204095\n",
      "Iteration 10556, loss = 0.47223189\n",
      "Iteration 10557, loss = 0.47318988\n",
      "Iteration 10558, loss = 0.47390850\n",
      "Iteration 10559, loss = 0.47417612\n",
      "Iteration 10560, loss = 0.47405812\n",
      "Iteration 10561, loss = 0.47445039\n",
      "Iteration 10562, loss = 0.47389469\n",
      "Iteration 10563, loss = 0.47299126\n",
      "Iteration 10564, loss = 0.47273050\n",
      "Iteration 10565, loss = 0.47235672\n",
      "Iteration 10566, loss = 0.47234151\n",
      "Iteration 10567, loss = 0.47232036\n",
      "Iteration 10568, loss = 0.47246602\n",
      "Iteration 10569, loss = 0.47295026\n",
      "Iteration 10570, loss = 0.47252851\n",
      "Iteration 10571, loss = 0.47286772\n",
      "Iteration 10572, loss = 0.47222115\n",
      "Iteration 10573, loss = 0.47245494\n",
      "Iteration 10574, loss = 0.47226899\n",
      "Iteration 10575, loss = 0.47254819\n",
      "Iteration 10576, loss = 0.47228126\n",
      "Iteration 10577, loss = 0.47231048\n",
      "Iteration 10578, loss = 0.47230425\n",
      "Iteration 10579, loss = 0.47223559\n",
      "Iteration 10580, loss = 0.47214886\n",
      "Iteration 10581, loss = 0.47234210\n",
      "Iteration 10582, loss = 0.47369774\n",
      "Iteration 10583, loss = 0.47317750\n",
      "Iteration 10584, loss = 0.47249656\n",
      "Iteration 10585, loss = 0.47214533\n",
      "Iteration 10586, loss = 0.47233412\n",
      "Iteration 10587, loss = 0.47252238\n",
      "Iteration 10588, loss = 0.47268947\n",
      "Iteration 10589, loss = 0.47246951\n",
      "Iteration 10590, loss = 0.47222455\n",
      "Iteration 10591, loss = 0.47202908\n",
      "Iteration 10592, loss = 0.47200684\n",
      "Iteration 10593, loss = 0.47236665\n",
      "Iteration 10594, loss = 0.47237168\n",
      "Iteration 10595, loss = 0.47235634\n",
      "Iteration 10596, loss = 0.47232285\n",
      "Iteration 10597, loss = 0.47207322\n",
      "Iteration 10598, loss = 0.47200635\n",
      "Iteration 10599, loss = 0.47221858\n",
      "Iteration 10600, loss = 0.47224646\n",
      "Iteration 10601, loss = 0.47218572\n",
      "Iteration 10602, loss = 0.47206474\n",
      "Iteration 10603, loss = 0.47208503\n",
      "Iteration 10604, loss = 0.47194743\n",
      "Iteration 10605, loss = 0.47194041\n",
      "Iteration 10606, loss = 0.47262767\n",
      "Iteration 10607, loss = 0.47333358\n",
      "Iteration 10608, loss = 0.47375999\n",
      "Iteration 10609, loss = 0.47382862\n",
      "Iteration 10610, loss = 0.47351416\n",
      "Iteration 10611, loss = 0.47339464\n",
      "Iteration 10612, loss = 0.47303286\n",
      "Iteration 10613, loss = 0.47242665\n",
      "Iteration 10614, loss = 0.47201261\n",
      "Iteration 10615, loss = 0.47199143\n",
      "Iteration 10616, loss = 0.47226078\n",
      "Iteration 10617, loss = 0.47299866\n",
      "Iteration 10618, loss = 0.47345505\n",
      "Iteration 10619, loss = 0.47302746\n",
      "Iteration 10620, loss = 0.47305032\n",
      "Iteration 10621, loss = 0.47181961\n",
      "Iteration 10622, loss = 0.47240387\n",
      "Iteration 10623, loss = 0.47418131\n",
      "Iteration 10624, loss = 0.47431381\n",
      "Iteration 10625, loss = 0.47332513\n",
      "Iteration 10626, loss = 0.47249267\n",
      "Iteration 10627, loss = 0.47197951\n",
      "Iteration 10628, loss = 0.47299350\n",
      "Iteration 10629, loss = 0.47466954\n",
      "Iteration 10630, loss = 0.47489047\n",
      "Iteration 10631, loss = 0.47389880\n",
      "Iteration 10632, loss = 0.47329994\n",
      "Iteration 10633, loss = 0.47233385\n",
      "Iteration 10634, loss = 0.47211701\n",
      "Iteration 10635, loss = 0.47215103\n",
      "Iteration 10636, loss = 0.47221519\n",
      "Iteration 10637, loss = 0.47267753\n",
      "Iteration 10638, loss = 0.47216550\n",
      "Iteration 10639, loss = 0.47222499\n",
      "Iteration 10640, loss = 0.47228327\n",
      "Iteration 10641, loss = 0.47279018\n",
      "Iteration 10642, loss = 0.47297612\n",
      "Iteration 10643, loss = 0.47286477\n",
      "Iteration 10644, loss = 0.47248478\n",
      "Iteration 10645, loss = 0.47197174\n",
      "Iteration 10646, loss = 0.47288312\n",
      "Iteration 10647, loss = 0.47246678\n",
      "Iteration 10648, loss = 0.47219647\n",
      "Iteration 10649, loss = 0.47224091\n",
      "Iteration 10650, loss = 0.47278278\n",
      "Iteration 10651, loss = 0.47281221\n",
      "Iteration 10652, loss = 0.47262496\n",
      "Iteration 10653, loss = 0.47249136\n",
      "Iteration 10654, loss = 0.47244273\n",
      "Iteration 10655, loss = 0.47241556\n",
      "Iteration 10656, loss = 0.47241851\n",
      "Iteration 10657, loss = 0.47257358\n",
      "Iteration 10658, loss = 0.47231448\n",
      "Iteration 10659, loss = 0.47213077\n",
      "Iteration 10660, loss = 0.47213716\n",
      "Iteration 10661, loss = 0.47222262\n",
      "Iteration 10662, loss = 0.47261192\n",
      "Iteration 10663, loss = 0.47274462\n",
      "Iteration 10664, loss = 0.47238189\n",
      "Iteration 10665, loss = 0.47226867\n",
      "Iteration 10666, loss = 0.47203712\n",
      "Iteration 10667, loss = 0.47223336\n",
      "Iteration 10668, loss = 0.47296622\n",
      "Iteration 10669, loss = 0.47290766\n",
      "Iteration 10670, loss = 0.47273285\n",
      "Iteration 10671, loss = 0.47249650\n",
      "Iteration 10672, loss = 0.47234670\n",
      "Iteration 10673, loss = 0.47299653\n",
      "Iteration 10674, loss = 0.47370195\n",
      "Iteration 10675, loss = 0.47419672\n",
      "Iteration 10676, loss = 0.47385175\n",
      "Iteration 10677, loss = 0.47229190\n",
      "Iteration 10678, loss = 0.47308868\n",
      "Iteration 10679, loss = 0.47293237\n",
      "Iteration 10680, loss = 0.47440002\n",
      "Iteration 10681, loss = 0.47613343\n",
      "Iteration 10682, loss = 0.47703716\n",
      "Iteration 10683, loss = 0.47629356\n",
      "Iteration 10684, loss = 0.47425291\n",
      "Iteration 10685, loss = 0.47285301\n",
      "Iteration 10686, loss = 0.47303213\n",
      "Iteration 10687, loss = 0.47243182\n",
      "Iteration 10688, loss = 0.47268290\n",
      "Iteration 10689, loss = 0.47275383\n",
      "Iteration 10690, loss = 0.47287179\n",
      "Iteration 10691, loss = 0.47297065\n",
      "Iteration 10692, loss = 0.47294930\n",
      "Iteration 10693, loss = 0.47260831\n",
      "Iteration 10694, loss = 0.47260527\n",
      "Iteration 10695, loss = 0.47194409\n",
      "Iteration 10696, loss = 0.47255128\n",
      "Iteration 10697, loss = 0.47212888\n",
      "Iteration 10698, loss = 0.47192640\n",
      "Iteration 10699, loss = 0.47193291\n",
      "Iteration 10700, loss = 0.47283015\n",
      "Iteration 10701, loss = 0.47330999\n",
      "Iteration 10702, loss = 0.47351156\n",
      "Iteration 10703, loss = 0.47330518\n",
      "Iteration 10704, loss = 0.47379594\n",
      "Iteration 10705, loss = 0.47400372\n",
      "Iteration 10706, loss = 0.47344765\n",
      "Iteration 10707, loss = 0.47239132\n",
      "Iteration 10708, loss = 0.47216824\n",
      "Iteration 10709, loss = 0.47198963\n",
      "Iteration 10710, loss = 0.47213104\n",
      "Iteration 10711, loss = 0.47228834\n",
      "Iteration 10712, loss = 0.47219760\n",
      "Iteration 10713, loss = 0.47216103\n",
      "Iteration 10714, loss = 0.47211182\n",
      "Iteration 10715, loss = 0.47218134\n",
      "Iteration 10716, loss = 0.47217333\n",
      "Iteration 10717, loss = 0.47223597\n",
      "Iteration 10718, loss = 0.47232728\n",
      "Iteration 10719, loss = 0.47245661\n",
      "Iteration 10720, loss = 0.47371170\n",
      "Iteration 10721, loss = 0.47460671\n",
      "Training loss did not improve more than tol=0.000100 for 2000 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.63413703\n",
      "Iteration 2, loss = 0.63379359\n",
      "Iteration 3, loss = 0.63355872\n",
      "Iteration 4, loss = 0.63331564\n",
      "Iteration 5, loss = 0.63307080\n",
      "Iteration 6, loss = 0.63280864\n",
      "Iteration 7, loss = 0.63254637\n",
      "Iteration 8, loss = 0.63226992\n",
      "Iteration 9, loss = 0.63201919\n",
      "Iteration 10, loss = 0.63178802\n",
      "Iteration 11, loss = 0.63156821\n",
      "Iteration 12, loss = 0.63136446\n",
      "Iteration 13, loss = 0.63115054\n",
      "Iteration 14, loss = 0.63092319\n",
      "Iteration 15, loss = 0.63070283\n",
      "Iteration 16, loss = 0.63048019\n",
      "Iteration 17, loss = 0.63024009\n",
      "Iteration 18, loss = 0.63006359\n",
      "Iteration 19, loss = 0.62992007\n",
      "Iteration 20, loss = 0.62973073\n",
      "Iteration 21, loss = 0.62955314\n",
      "Iteration 22, loss = 0.62938516\n",
      "Iteration 23, loss = 0.62920167\n",
      "Iteration 24, loss = 0.62902585\n",
      "Iteration 25, loss = 0.62885469\n",
      "Iteration 26, loss = 0.62868819\n",
      "Iteration 27, loss = 0.62852528\n",
      "Iteration 28, loss = 0.62834577\n",
      "Iteration 29, loss = 0.62817186\n",
      "Iteration 30, loss = 0.62796990\n",
      "Iteration 31, loss = 0.62781321\n",
      "Iteration 32, loss = 0.62766467\n",
      "Iteration 33, loss = 0.62752373\n",
      "Iteration 34, loss = 0.62733417\n",
      "Iteration 35, loss = 0.62715872\n",
      "Iteration 36, loss = 0.62704879\n",
      "Iteration 37, loss = 0.62695419\n",
      "Iteration 38, loss = 0.62680226\n",
      "Iteration 39, loss = 0.62667268\n",
      "Iteration 40, loss = 0.62653175\n",
      "Iteration 41, loss = 0.62639317\n",
      "Iteration 42, loss = 0.62624738\n",
      "Iteration 43, loss = 0.62612266\n",
      "Iteration 44, loss = 0.62599909\n",
      "Iteration 45, loss = 0.62585124\n",
      "Iteration 46, loss = 0.62572473\n",
      "Iteration 47, loss = 0.62562143\n",
      "Iteration 48, loss = 0.62549461\n",
      "Iteration 49, loss = 0.62532880\n",
      "Iteration 50, loss = 0.62521205\n",
      "Iteration 51, loss = 0.62508900\n",
      "Iteration 52, loss = 0.62498591\n",
      "Iteration 53, loss = 0.62486107\n",
      "Iteration 54, loss = 0.62473842\n",
      "Iteration 55, loss = 0.62460020\n",
      "Iteration 56, loss = 0.62447949\n",
      "Iteration 57, loss = 0.62436767\n",
      "Iteration 58, loss = 0.62425567\n",
      "Iteration 59, loss = 0.62414904\n",
      "Iteration 60, loss = 0.62404359\n",
      "Iteration 61, loss = 0.62393376\n",
      "Iteration 62, loss = 0.62382882\n",
      "Iteration 63, loss = 0.62373791\n",
      "Iteration 64, loss = 0.62362830\n",
      "Iteration 65, loss = 0.62353594\n",
      "Iteration 66, loss = 0.62343718\n",
      "Iteration 67, loss = 0.62334269\n",
      "Iteration 68, loss = 0.62325304\n",
      "Iteration 69, loss = 0.62316664\n",
      "Iteration 70, loss = 0.62307017\n",
      "Iteration 71, loss = 0.62297051\n",
      "Iteration 72, loss = 0.62288153\n",
      "Iteration 73, loss = 0.62277845\n",
      "Iteration 74, loss = 0.62270051\n",
      "Iteration 75, loss = 0.62259754\n",
      "Iteration 76, loss = 0.62251134\n",
      "Iteration 77, loss = 0.62243305\n",
      "Iteration 78, loss = 0.62234417\n",
      "Iteration 79, loss = 0.62225886\n",
      "Iteration 80, loss = 0.62217828\n",
      "Iteration 81, loss = 0.62209026\n",
      "Iteration 82, loss = 0.62200881\n",
      "Iteration 83, loss = 0.62192093\n",
      "Iteration 84, loss = 0.62186557\n",
      "Iteration 85, loss = 0.62176185\n",
      "Iteration 86, loss = 0.62168343\n",
      "Iteration 87, loss = 0.62161235\n",
      "Iteration 88, loss = 0.62155091\n",
      "Iteration 89, loss = 0.62147449\n",
      "Iteration 90, loss = 0.62140162\n",
      "Iteration 91, loss = 0.62134031\n",
      "Iteration 92, loss = 0.62126613\n",
      "Iteration 93, loss = 0.62122132\n",
      "Iteration 94, loss = 0.62114719\n",
      "Iteration 95, loss = 0.62111651\n",
      "Iteration 96, loss = 0.62103505\n",
      "Iteration 97, loss = 0.62097663\n",
      "Iteration 98, loss = 0.62090677\n",
      "Iteration 99, loss = 0.62085029\n",
      "Iteration 100, loss = 0.62077620\n",
      "Iteration 101, loss = 0.62070773\n",
      "Iteration 102, loss = 0.62064025\n",
      "Iteration 103, loss = 0.62058120\n",
      "Iteration 104, loss = 0.62051062\n",
      "Iteration 105, loss = 0.62043750\n",
      "Iteration 106, loss = 0.62038737\n",
      "Iteration 107, loss = 0.62030456\n",
      "Iteration 108, loss = 0.62024328\n",
      "Iteration 109, loss = 0.62017269\n",
      "Iteration 110, loss = 0.62012651\n",
      "Iteration 111, loss = 0.62004798\n",
      "Iteration 112, loss = 0.61998494\n",
      "Iteration 113, loss = 0.61994865\n",
      "Iteration 114, loss = 0.61987318\n",
      "Iteration 115, loss = 0.61983952\n",
      "Iteration 116, loss = 0.61977483\n",
      "Iteration 117, loss = 0.61972272\n",
      "Iteration 118, loss = 0.61967611\n",
      "Iteration 119, loss = 0.61961736\n",
      "Iteration 120, loss = 0.61956689\n",
      "Iteration 121, loss = 0.61952414\n",
      "Iteration 122, loss = 0.61947501\n",
      "Iteration 123, loss = 0.61944078\n",
      "Iteration 124, loss = 0.61939315\n",
      "Iteration 125, loss = 0.61933029\n",
      "Iteration 126, loss = 0.61928477\n",
      "Iteration 127, loss = 0.61923747\n",
      "Iteration 128, loss = 0.61920567\n",
      "Iteration 129, loss = 0.61914839\n",
      "Iteration 130, loss = 0.61910713\n",
      "Iteration 131, loss = 0.61906840\n",
      "Iteration 132, loss = 0.61901406\n",
      "Iteration 133, loss = 0.61896700\n",
      "Iteration 134, loss = 0.61894747\n",
      "Iteration 135, loss = 0.61889259\n",
      "Iteration 136, loss = 0.61887922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 137, loss = 0.61885559\n",
      "Iteration 138, loss = 0.61878468\n",
      "Iteration 139, loss = 0.61874823\n",
      "Iteration 140, loss = 0.61870265\n",
      "Iteration 141, loss = 0.61866451\n",
      "Iteration 142, loss = 0.61864575\n",
      "Iteration 143, loss = 0.61860227\n",
      "Iteration 144, loss = 0.61856191\n",
      "Iteration 145, loss = 0.61853066\n",
      "Iteration 146, loss = 0.61849940\n",
      "Iteration 147, loss = 0.61848470\n",
      "Iteration 148, loss = 0.61844181\n",
      "Iteration 149, loss = 0.61840544\n",
      "Iteration 150, loss = 0.61836031\n",
      "Iteration 151, loss = 0.61830617\n",
      "Iteration 152, loss = 0.61827773\n",
      "Iteration 153, loss = 0.61823357\n",
      "Iteration 154, loss = 0.61821692\n",
      "Iteration 155, loss = 0.61819266\n",
      "Iteration 156, loss = 0.61817009\n",
      "Iteration 157, loss = 0.61811348\n",
      "Iteration 158, loss = 0.61809535\n",
      "Iteration 159, loss = 0.61806016\n",
      "Iteration 160, loss = 0.61801914\n",
      "Iteration 161, loss = 0.61799472\n",
      "Iteration 162, loss = 0.61796205\n",
      "Iteration 163, loss = 0.61792175\n",
      "Iteration 164, loss = 0.61791255\n",
      "Iteration 165, loss = 0.61787012\n",
      "Iteration 166, loss = 0.61786163\n",
      "Iteration 167, loss = 0.61785061\n",
      "Iteration 168, loss = 0.61784389\n",
      "Iteration 169, loss = 0.61781372\n",
      "Iteration 170, loss = 0.61779195\n",
      "Iteration 171, loss = 0.61776570\n",
      "Iteration 172, loss = 0.61773911\n",
      "Iteration 173, loss = 0.61771167\n",
      "Iteration 174, loss = 0.61767175\n",
      "Iteration 175, loss = 0.61766982\n",
      "Iteration 176, loss = 0.61761685\n",
      "Iteration 177, loss = 0.61760300\n",
      "Iteration 178, loss = 0.61756331\n",
      "Iteration 179, loss = 0.61754310\n",
      "Iteration 180, loss = 0.61751288\n",
      "Iteration 181, loss = 0.61750070\n",
      "Iteration 182, loss = 0.61746655\n",
      "Iteration 183, loss = 0.61744381\n",
      "Iteration 184, loss = 0.61742061\n",
      "Iteration 185, loss = 0.61740110\n",
      "Iteration 186, loss = 0.61738008\n",
      "Iteration 187, loss = 0.61735873\n",
      "Iteration 188, loss = 0.61733402\n",
      "Iteration 189, loss = 0.61731805\n",
      "Iteration 190, loss = 0.61730492\n",
      "Iteration 191, loss = 0.61731412\n",
      "Iteration 192, loss = 0.61729355\n",
      "Iteration 193, loss = 0.61727452\n",
      "Iteration 194, loss = 0.61725591\n",
      "Iteration 195, loss = 0.61723778\n",
      "Iteration 196, loss = 0.61721773\n",
      "Iteration 197, loss = 0.61720053\n",
      "Iteration 198, loss = 0.61718050\n",
      "Iteration 199, loss = 0.61718837\n",
      "Iteration 200, loss = 0.61716866\n",
      "Iteration 201, loss = 0.61716142\n",
      "Iteration 202, loss = 0.61713160\n",
      "Iteration 203, loss = 0.61710245\n",
      "Iteration 204, loss = 0.61707264\n",
      "Iteration 205, loss = 0.61703770\n",
      "Iteration 206, loss = 0.61703744\n",
      "Iteration 207, loss = 0.61698007\n",
      "Iteration 208, loss = 0.61697295\n",
      "Iteration 209, loss = 0.61696878\n",
      "Iteration 210, loss = 0.61692569\n",
      "Iteration 211, loss = 0.61691314\n",
      "Iteration 212, loss = 0.61689946\n",
      "Iteration 213, loss = 0.61687860\n",
      "Iteration 214, loss = 0.61688420\n",
      "Iteration 215, loss = 0.61684880\n",
      "Iteration 216, loss = 0.61683594\n",
      "Iteration 217, loss = 0.61683621\n",
      "Iteration 218, loss = 0.61680941\n",
      "Iteration 219, loss = 0.61679403\n",
      "Iteration 220, loss = 0.61678028\n",
      "Iteration 221, loss = 0.61676422\n",
      "Iteration 222, loss = 0.61676565\n",
      "Iteration 223, loss = 0.61673711\n",
      "Iteration 224, loss = 0.61672998\n",
      "Iteration 225, loss = 0.61671407\n",
      "Iteration 226, loss = 0.61669962\n",
      "Iteration 227, loss = 0.61668445\n",
      "Iteration 228, loss = 0.61667015\n",
      "Iteration 229, loss = 0.61666173\n",
      "Iteration 230, loss = 0.61664565\n",
      "Iteration 231, loss = 0.61663710\n",
      "Iteration 232, loss = 0.61662799\n",
      "Iteration 233, loss = 0.61663298\n",
      "Iteration 234, loss = 0.61661680\n",
      "Iteration 235, loss = 0.61660147\n",
      "Iteration 236, loss = 0.61658714\n",
      "Iteration 237, loss = 0.61657548\n",
      "Iteration 238, loss = 0.61655378\n",
      "Iteration 239, loss = 0.61654815\n",
      "Iteration 240, loss = 0.61653832\n",
      "Iteration 241, loss = 0.61651702\n",
      "Iteration 242, loss = 0.61651479\n",
      "Iteration 243, loss = 0.61649812\n",
      "Iteration 244, loss = 0.61648196\n",
      "Iteration 245, loss = 0.61649035\n",
      "Iteration 246, loss = 0.61646541\n",
      "Iteration 247, loss = 0.61647566\n",
      "Iteration 248, loss = 0.61645709\n",
      "Iteration 249, loss = 0.61644438\n",
      "Iteration 250, loss = 0.61643331\n",
      "Iteration 251, loss = 0.61642092\n",
      "Iteration 252, loss = 0.61640797\n",
      "Iteration 253, loss = 0.61640493\n",
      "Iteration 254, loss = 0.61639625\n",
      "Iteration 255, loss = 0.61638703\n",
      "Iteration 256, loss = 0.61636867\n",
      "Iteration 257, loss = 0.61637336\n",
      "Iteration 258, loss = 0.61635355\n",
      "Iteration 259, loss = 0.61634689\n",
      "Iteration 260, loss = 0.61635130\n",
      "Iteration 261, loss = 0.61633642\n",
      "Iteration 262, loss = 0.61633410\n",
      "Iteration 263, loss = 0.61631584\n",
      "Iteration 264, loss = 0.61630875\n",
      "Iteration 265, loss = 0.61630127\n",
      "Iteration 266, loss = 0.61629346\n",
      "Iteration 267, loss = 0.61628583\n",
      "Iteration 268, loss = 0.61627716\n",
      "Iteration 269, loss = 0.61626849\n",
      "Iteration 270, loss = 0.61624985\n",
      "Iteration 271, loss = 0.61624872\n",
      "Iteration 272, loss = 0.61623928\n",
      "Iteration 273, loss = 0.61622512\n",
      "Iteration 274, loss = 0.61623571\n",
      "Iteration 275, loss = 0.61621347\n",
      "Iteration 276, loss = 0.61620954\n",
      "Iteration 277, loss = 0.61622584\n",
      "Iteration 278, loss = 0.61620101\n",
      "Iteration 279, loss = 0.61619045\n",
      "Iteration 280, loss = 0.61618188\n",
      "Iteration 281, loss = 0.61617689\n",
      "Iteration 282, loss = 0.61617034\n",
      "Iteration 283, loss = 0.61616333\n",
      "Iteration 284, loss = 0.61615451\n",
      "Iteration 285, loss = 0.61615653\n",
      "Iteration 286, loss = 0.61614136\n",
      "Iteration 287, loss = 0.61614028\n",
      "Iteration 288, loss = 0.61612761\n",
      "Iteration 289, loss = 0.61613123\n",
      "Iteration 290, loss = 0.61611648\n",
      "Iteration 291, loss = 0.61611001\n",
      "Iteration 292, loss = 0.61610198\n",
      "Iteration 293, loss = 0.61610804\n",
      "Iteration 294, loss = 0.61609374\n",
      "Iteration 295, loss = 0.61608653\n",
      "Iteration 296, loss = 0.61608821\n",
      "Iteration 297, loss = 0.61607381\n",
      "Iteration 298, loss = 0.61606794\n",
      "Iteration 299, loss = 0.61606372\n",
      "Iteration 300, loss = 0.61605713\n",
      "Iteration 301, loss = 0.61606161\n",
      "Iteration 302, loss = 0.61604611\n",
      "Iteration 303, loss = 0.61604251\n",
      "Iteration 304, loss = 0.61603621\n",
      "Iteration 305, loss = 0.61603012\n",
      "Iteration 306, loss = 0.61602491\n",
      "Iteration 307, loss = 0.61602064\n",
      "Iteration 308, loss = 0.61601698\n",
      "Iteration 309, loss = 0.61600937\n",
      "Iteration 310, loss = 0.61600695\n",
      "Iteration 311, loss = 0.61600142\n",
      "Iteration 312, loss = 0.61599319\n",
      "Iteration 313, loss = 0.61599138\n",
      "Iteration 314, loss = 0.61598480\n",
      "Iteration 315, loss = 0.61598229\n",
      "Iteration 316, loss = 0.61597541\n",
      "Iteration 317, loss = 0.61598016\n",
      "Iteration 318, loss = 0.61598577\n",
      "Iteration 319, loss = 0.61597015\n",
      "Iteration 320, loss = 0.61595578\n",
      "Iteration 321, loss = 0.61594878\n",
      "Iteration 322, loss = 0.61594418\n",
      "Iteration 323, loss = 0.61594694\n",
      "Iteration 324, loss = 0.61594478\n",
      "Iteration 325, loss = 0.61595331\n",
      "Iteration 326, loss = 0.61594048\n",
      "Iteration 327, loss = 0.61593755\n",
      "Iteration 328, loss = 0.61593409\n",
      "Iteration 329, loss = 0.61593216\n",
      "Iteration 330, loss = 0.61592378\n",
      "Iteration 331, loss = 0.61591792\n",
      "Iteration 332, loss = 0.61592181\n",
      "Iteration 333, loss = 0.61591509\n",
      "Iteration 334, loss = 0.61590384\n",
      "Iteration 335, loss = 0.61590605\n",
      "Iteration 336, loss = 0.61589402\n",
      "Iteration 337, loss = 0.61589017\n",
      "Iteration 338, loss = 0.61588852\n",
      "Iteration 339, loss = 0.61588242\n",
      "Iteration 340, loss = 0.61587802\n",
      "Iteration 341, loss = 0.61587028\n",
      "Iteration 342, loss = 0.61587318\n",
      "Iteration 343, loss = 0.61586854\n",
      "Iteration 344, loss = 0.61586205\n",
      "Iteration 345, loss = 0.61585088\n",
      "Iteration 346, loss = 0.61586258\n",
      "Iteration 347, loss = 0.61585094\n",
      "Iteration 348, loss = 0.61584644\n",
      "Iteration 349, loss = 0.61584742\n",
      "Iteration 350, loss = 0.61585582\n",
      "Iteration 351, loss = 0.61585395\n",
      "Iteration 352, loss = 0.61585521\n",
      "Iteration 353, loss = 0.61585431\n",
      "Iteration 354, loss = 0.61585783\n",
      "Iteration 355, loss = 0.61586012\n",
      "Iteration 356, loss = 0.61586712\n",
      "Iteration 357, loss = 0.61586793\n",
      "Iteration 358, loss = 0.61586401\n",
      "Iteration 359, loss = 0.61585961\n",
      "Iteration 360, loss = 0.61585724\n",
      "Iteration 361, loss = 0.61585851\n",
      "Iteration 362, loss = 0.61585223\n",
      "Iteration 363, loss = 0.61584934\n",
      "Iteration 364, loss = 0.61584753\n",
      "Iteration 365, loss = 0.61584452\n",
      "Iteration 366, loss = 0.61584035\n",
      "Iteration 367, loss = 0.61584354\n",
      "Iteration 368, loss = 0.61583478\n",
      "Iteration 369, loss = 0.61582795\n",
      "Iteration 370, loss = 0.61582384\n",
      "Iteration 371, loss = 0.61581671\n",
      "Iteration 372, loss = 0.61582038\n",
      "Iteration 373, loss = 0.61581373\n",
      "Iteration 374, loss = 0.61580924\n",
      "Iteration 375, loss = 0.61580702\n",
      "Iteration 376, loss = 0.61579417\n",
      "Iteration 377, loss = 0.61581606\n",
      "Iteration 378, loss = 0.61580656\n",
      "Iteration 379, loss = 0.61580897\n",
      "Iteration 380, loss = 0.61579172\n",
      "Iteration 381, loss = 0.61578909\n",
      "Iteration 382, loss = 0.61578625\n",
      "Iteration 383, loss = 0.61577681\n",
      "Iteration 384, loss = 0.61578103\n",
      "Iteration 385, loss = 0.61576991\n",
      "Iteration 386, loss = 0.61576631\n",
      "Iteration 387, loss = 0.61576298\n",
      "Iteration 388, loss = 0.61576759\n",
      "Iteration 389, loss = 0.61575658\n",
      "Iteration 390, loss = 0.61575338\n",
      "Iteration 391, loss = 0.61574934\n",
      "Iteration 392, loss = 0.61575221\n",
      "Iteration 393, loss = 0.61574315\n",
      "Iteration 394, loss = 0.61573799\n",
      "Iteration 395, loss = 0.61575025\n",
      "Iteration 396, loss = 0.61575110\n",
      "Iteration 397, loss = 0.61574738\n",
      "Iteration 398, loss = 0.61575906\n",
      "Iteration 399, loss = 0.61575130\n",
      "Iteration 400, loss = 0.61574817\n",
      "Iteration 401, loss = 0.61573981\n",
      "Iteration 402, loss = 0.61574545\n",
      "Iteration 403, loss = 0.61573843\n",
      "Iteration 404, loss = 0.61572521\n",
      "Iteration 405, loss = 0.61573033\n",
      "Iteration 406, loss = 0.61571835\n",
      "Iteration 407, loss = 0.61571458\n",
      "Iteration 408, loss = 0.61571205\n",
      "Iteration 409, loss = 0.61570946\n",
      "Iteration 410, loss = 0.61571122\n",
      "Iteration 411, loss = 0.61570548\n",
      "Iteration 412, loss = 0.61570565\n",
      "Iteration 413, loss = 0.61570446\n",
      "Iteration 414, loss = 0.61569697\n",
      "Iteration 415, loss = 0.61569104\n",
      "Iteration 416, loss = 0.61570578\n",
      "Iteration 417, loss = 0.61568763\n",
      "Iteration 418, loss = 0.61568512\n",
      "Iteration 419, loss = 0.61568483\n",
      "Iteration 420, loss = 0.61568081\n",
      "Iteration 421, loss = 0.61567911\n",
      "Iteration 422, loss = 0.61567864\n",
      "Iteration 423, loss = 0.61567128\n",
      "Iteration 424, loss = 0.61566503\n",
      "Iteration 425, loss = 0.61566849\n",
      "Iteration 426, loss = 0.61567205\n",
      "Iteration 427, loss = 0.61566958\n",
      "Iteration 428, loss = 0.61566650\n",
      "Iteration 429, loss = 0.61565325\n",
      "Iteration 430, loss = 0.61566831\n",
      "Iteration 431, loss = 0.61566203\n",
      "Iteration 432, loss = 0.61565797\n",
      "Iteration 433, loss = 0.61565606\n",
      "Iteration 434, loss = 0.61565758\n",
      "Iteration 435, loss = 0.61565855\n",
      "Iteration 436, loss = 0.61565688\n",
      "Iteration 437, loss = 0.61565504\n",
      "Iteration 438, loss = 0.61565376\n",
      "Iteration 439, loss = 0.61565182\n",
      "Iteration 440, loss = 0.61565056\n",
      "Iteration 441, loss = 0.61564858\n",
      "Iteration 442, loss = 0.61564615\n",
      "Iteration 443, loss = 0.61565344\n",
      "Iteration 444, loss = 0.61564598\n",
      "Iteration 445, loss = 0.61564252\n",
      "Iteration 446, loss = 0.61563951\n",
      "Iteration 447, loss = 0.61563555\n",
      "Iteration 448, loss = 0.61563564\n",
      "Iteration 449, loss = 0.61563213\n",
      "Iteration 450, loss = 0.61563253\n",
      "Iteration 451, loss = 0.61563145\n",
      "Iteration 452, loss = 0.61563318\n",
      "Iteration 453, loss = 0.61562645\n",
      "Iteration 454, loss = 0.61562706\n",
      "Iteration 455, loss = 0.61562265\n",
      "Iteration 456, loss = 0.61562589\n",
      "Iteration 457, loss = 0.61562157\n",
      "Iteration 458, loss = 0.61562016\n",
      "Iteration 459, loss = 0.61561824\n",
      "Iteration 460, loss = 0.61562543\n",
      "Iteration 461, loss = 0.61562086\n",
      "Iteration 462, loss = 0.61561322\n",
      "Iteration 463, loss = 0.61563236\n",
      "Iteration 464, loss = 0.61561551\n",
      "Iteration 465, loss = 0.61561410\n",
      "Iteration 466, loss = 0.61560844\n",
      "Iteration 467, loss = 0.61560751\n",
      "Iteration 468, loss = 0.61560857\n",
      "Iteration 469, loss = 0.61560715\n",
      "Iteration 470, loss = 0.61560436\n",
      "Iteration 471, loss = 0.61561671\n",
      "Iteration 472, loss = 0.61560667\n",
      "Iteration 473, loss = 0.61560201\n",
      "Iteration 474, loss = 0.61560205\n",
      "Iteration 475, loss = 0.61560230\n",
      "Iteration 476, loss = 0.61560031\n",
      "Iteration 477, loss = 0.61559962\n",
      "Iteration 478, loss = 0.61560154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 479, loss = 0.61559841\n",
      "Iteration 480, loss = 0.61559469\n",
      "Iteration 481, loss = 0.61560949\n",
      "Iteration 482, loss = 0.61559367\n",
      "Iteration 483, loss = 0.61559301\n",
      "Iteration 484, loss = 0.61558768\n",
      "Iteration 485, loss = 0.61560747\n",
      "Iteration 486, loss = 0.61559149\n",
      "Iteration 487, loss = 0.61558980\n",
      "Iteration 488, loss = 0.61559095\n",
      "Iteration 489, loss = 0.61558779\n",
      "Iteration 490, loss = 0.61558788\n",
      "Iteration 491, loss = 0.61558732\n",
      "Iteration 492, loss = 0.61557700\n",
      "Iteration 493, loss = 0.61559826\n",
      "Iteration 494, loss = 0.61558587\n",
      "Iteration 495, loss = 0.61560109\n",
      "Iteration 496, loss = 0.61558465\n",
      "Iteration 497, loss = 0.61557743\n",
      "Iteration 498, loss = 0.61559704\n",
      "Iteration 499, loss = 0.61558206\n",
      "Iteration 500, loss = 0.61557750\n",
      "Iteration 501, loss = 0.61557691\n",
      "Iteration 502, loss = 0.61558401\n",
      "Iteration 503, loss = 0.61559473\n",
      "Iteration 504, loss = 0.61557455\n",
      "Iteration 505, loss = 0.61556966\n",
      "Iteration 506, loss = 0.61558057\n",
      "Iteration 507, loss = 0.61557257\n",
      "Iteration 508, loss = 0.61557784\n",
      "Iteration 509, loss = 0.61558587\n",
      "Iteration 510, loss = 0.61558349\n",
      "Iteration 511, loss = 0.61558268\n",
      "Iteration 512, loss = 0.61558262\n",
      "Iteration 513, loss = 0.61558661\n",
      "Iteration 514, loss = 0.61559343\n",
      "Iteration 515, loss = 0.61559825\n",
      "Iteration 516, loss = 0.61561588\n",
      "Iteration 517, loss = 0.61560735\n",
      "Iteration 518, loss = 0.61561125\n",
      "Iteration 519, loss = 0.61560171\n",
      "Iteration 520, loss = 0.61560170\n",
      "Iteration 521, loss = 0.61560204\n",
      "Iteration 522, loss = 0.61559965\n",
      "Iteration 523, loss = 0.61560268\n",
      "Iteration 524, loss = 0.61561152\n",
      "Iteration 525, loss = 0.61560896\n",
      "Iteration 526, loss = 0.61561186\n",
      "Iteration 527, loss = 0.61561060\n",
      "Iteration 528, loss = 0.61560840\n",
      "Iteration 529, loss = 0.61560992\n",
      "Iteration 530, loss = 0.61561093\n",
      "Iteration 531, loss = 0.61561163\n",
      "Iteration 532, loss = 0.61561392\n",
      "Iteration 533, loss = 0.61561417\n",
      "Iteration 534, loss = 0.61560812\n",
      "Iteration 535, loss = 0.61561220\n",
      "Iteration 536, loss = 0.61560226\n",
      "Iteration 537, loss = 0.61559983\n",
      "Iteration 538, loss = 0.61560143\n",
      "Iteration 539, loss = 0.61559059\n",
      "Iteration 540, loss = 0.61558567\n",
      "Iteration 541, loss = 0.61558465\n",
      "Iteration 542, loss = 0.61558258\n",
      "Iteration 543, loss = 0.61558092\n",
      "Iteration 544, loss = 0.61558066\n",
      "Iteration 545, loss = 0.61557936\n",
      "Iteration 546, loss = 0.61557193\n",
      "Iteration 547, loss = 0.61556912\n",
      "Iteration 548, loss = 0.61556269\n",
      "Iteration 549, loss = 0.61556282\n",
      "Iteration 550, loss = 0.61556262\n",
      "Iteration 551, loss = 0.61555634\n",
      "Iteration 552, loss = 0.61555049\n",
      "Iteration 553, loss = 0.61554684\n",
      "Iteration 554, loss = 0.61555080\n",
      "Iteration 555, loss = 0.61555442\n",
      "Iteration 556, loss = 0.61554389\n",
      "Iteration 557, loss = 0.61554274\n",
      "Iteration 558, loss = 0.61554238\n",
      "Iteration 559, loss = 0.61554115\n",
      "Iteration 560, loss = 0.61554747\n",
      "Iteration 561, loss = 0.61554347\n",
      "Iteration 562, loss = 0.61554277\n",
      "Iteration 563, loss = 0.61554301\n",
      "Iteration 564, loss = 0.61553991\n",
      "Iteration 565, loss = 0.61554448\n",
      "Iteration 566, loss = 0.61554576\n",
      "Iteration 567, loss = 0.61554411\n",
      "Iteration 568, loss = 0.61554531\n",
      "Iteration 569, loss = 0.61554127\n",
      "Iteration 570, loss = 0.61554138\n",
      "Iteration 571, loss = 0.61554019\n",
      "Iteration 572, loss = 0.61553775\n",
      "Iteration 573, loss = 0.61553674\n",
      "Iteration 574, loss = 0.61553643\n",
      "Iteration 575, loss = 0.61553622\n",
      "Iteration 576, loss = 0.61553478\n",
      "Iteration 577, loss = 0.61553959\n",
      "Iteration 578, loss = 0.61555335\n",
      "Iteration 579, loss = 0.61554707\n",
      "Iteration 580, loss = 0.61554705\n",
      "Iteration 581, loss = 0.61554803\n",
      "Iteration 582, loss = 0.61554719\n",
      "Iteration 583, loss = 0.61554448\n",
      "Iteration 584, loss = 0.61555219\n",
      "Iteration 585, loss = 0.61554573\n",
      "Iteration 586, loss = 0.61554189\n",
      "Iteration 587, loss = 0.61553755\n",
      "Iteration 588, loss = 0.61553495\n",
      "Iteration 589, loss = 0.61553347\n",
      "Iteration 590, loss = 0.61553077\n",
      "Iteration 591, loss = 0.61552609\n",
      "Iteration 592, loss = 0.61553256\n",
      "Iteration 593, loss = 0.61552798\n",
      "Iteration 594, loss = 0.61552721\n",
      "Iteration 595, loss = 0.61552334\n",
      "Iteration 596, loss = 0.61552227\n",
      "Iteration 597, loss = 0.61552137\n",
      "Iteration 598, loss = 0.61551629\n",
      "Iteration 599, loss = 0.61552167\n",
      "Iteration 600, loss = 0.61551271\n",
      "Iteration 601, loss = 0.61551258\n",
      "Iteration 602, loss = 0.61551274\n",
      "Iteration 603, loss = 0.61551120\n",
      "Iteration 604, loss = 0.61551071\n",
      "Iteration 605, loss = 0.61550911\n",
      "Iteration 606, loss = 0.61550900\n",
      "Iteration 607, loss = 0.61550676\n",
      "Iteration 608, loss = 0.61551057\n",
      "Iteration 609, loss = 0.61550833\n",
      "Iteration 610, loss = 0.61550832\n",
      "Iteration 611, loss = 0.61551186\n",
      "Iteration 612, loss = 0.61550928\n",
      "Iteration 613, loss = 0.61550893\n",
      "Iteration 614, loss = 0.61550889\n",
      "Iteration 615, loss = 0.61550808\n",
      "Iteration 616, loss = 0.61550738\n",
      "Iteration 617, loss = 0.61550633\n",
      "Iteration 618, loss = 0.61550251\n",
      "Iteration 619, loss = 0.61550690\n",
      "Iteration 620, loss = 0.61550438\n",
      "Iteration 621, loss = 0.61550106\n",
      "Iteration 622, loss = 0.61550671\n",
      "Iteration 623, loss = 0.61549964\n",
      "Iteration 624, loss = 0.61549949\n",
      "Iteration 625, loss = 0.61549979\n",
      "Iteration 626, loss = 0.61550938\n",
      "Iteration 627, loss = 0.61550119\n",
      "Iteration 628, loss = 0.61549835\n",
      "Iteration 629, loss = 0.61550467\n",
      "Iteration 630, loss = 0.61549969\n",
      "Iteration 631, loss = 0.61549542\n",
      "Iteration 632, loss = 0.61549626\n",
      "Iteration 633, loss = 0.61550084\n",
      "Iteration 634, loss = 0.61549473\n",
      "Iteration 635, loss = 0.61549439\n",
      "Iteration 636, loss = 0.61549316\n",
      "Iteration 637, loss = 0.61549773\n",
      "Iteration 638, loss = 0.61549304\n",
      "Iteration 639, loss = 0.61549358\n",
      "Iteration 640, loss = 0.61548817\n",
      "Iteration 641, loss = 0.61550149\n",
      "Iteration 642, loss = 0.61549448\n",
      "Iteration 643, loss = 0.61549140\n",
      "Iteration 644, loss = 0.61549053\n",
      "Iteration 645, loss = 0.61549554\n",
      "Iteration 646, loss = 0.61548983\n",
      "Iteration 647, loss = 0.61548982\n",
      "Iteration 648, loss = 0.61549084\n",
      "Iteration 649, loss = 0.61548924\n",
      "Iteration 650, loss = 0.61548859\n",
      "Iteration 651, loss = 0.61549083\n",
      "Iteration 652, loss = 0.61549279\n",
      "Iteration 653, loss = 0.61548907\n",
      "Iteration 654, loss = 0.61549286\n",
      "Iteration 655, loss = 0.61548738\n",
      "Iteration 656, loss = 0.61548605\n",
      "Iteration 657, loss = 0.61548498\n",
      "Iteration 658, loss = 0.61548718\n",
      "Iteration 659, loss = 0.61548879\n",
      "Iteration 660, loss = 0.61548552\n",
      "Iteration 661, loss = 0.61548487\n",
      "Iteration 662, loss = 0.61548624\n",
      "Iteration 663, loss = 0.61548442\n",
      "Iteration 664, loss = 0.61548302\n",
      "Iteration 665, loss = 0.61548078\n",
      "Iteration 666, loss = 0.61548598\n",
      "Iteration 667, loss = 0.61549077\n",
      "Iteration 668, loss = 0.61548456\n",
      "Iteration 669, loss = 0.61548380\n",
      "Iteration 670, loss = 0.61548152\n",
      "Iteration 671, loss = 0.61547695\n",
      "Iteration 672, loss = 0.61547703\n",
      "Iteration 673, loss = 0.61547168\n",
      "Iteration 674, loss = 0.61547420\n",
      "Iteration 675, loss = 0.61547416\n",
      "Iteration 676, loss = 0.61548350\n",
      "Iteration 677, loss = 0.61548091\n",
      "Iteration 678, loss = 0.61548937\n",
      "Iteration 679, loss = 0.61548215\n",
      "Iteration 680, loss = 0.61548102\n",
      "Iteration 681, loss = 0.61548043\n",
      "Iteration 682, loss = 0.61548376\n",
      "Iteration 683, loss = 0.61547746\n",
      "Iteration 684, loss = 0.61547879\n",
      "Iteration 685, loss = 0.61547955\n",
      "Iteration 686, loss = 0.61547711\n",
      "Iteration 687, loss = 0.61547223\n",
      "Iteration 688, loss = 0.61548892\n",
      "Iteration 689, loss = 0.61548816\n",
      "Iteration 690, loss = 0.61549441\n",
      "Iteration 691, loss = 0.61549370\n",
      "Iteration 692, loss = 0.61549196\n",
      "Iteration 693, loss = 0.61549234\n",
      "Iteration 694, loss = 0.61549082\n",
      "Iteration 695, loss = 0.61549256\n",
      "Iteration 696, loss = 0.61549380\n",
      "Iteration 697, loss = 0.61548744\n",
      "Iteration 698, loss = 0.61550118\n",
      "Iteration 699, loss = 0.61549563\n",
      "Iteration 700, loss = 0.61549710\n",
      "Iteration 701, loss = 0.61549676\n",
      "Iteration 702, loss = 0.61549603\n",
      "Iteration 703, loss = 0.61549479\n",
      "Iteration 704, loss = 0.61549238\n",
      "Iteration 705, loss = 0.61548670\n",
      "Iteration 706, loss = 0.61549968\n",
      "Iteration 707, loss = 0.61548696\n",
      "Iteration 708, loss = 0.61548366\n",
      "Iteration 709, loss = 0.61548316\n",
      "Iteration 710, loss = 0.61548062\n",
      "Iteration 711, loss = 0.61547790\n",
      "Iteration 712, loss = 0.61547665\n",
      "Iteration 713, loss = 0.61547805\n",
      "Iteration 714, loss = 0.61547891\n",
      "Iteration 715, loss = 0.61547518\n",
      "Iteration 716, loss = 0.61547346\n",
      "Iteration 717, loss = 0.61547611\n",
      "Iteration 718, loss = 0.61547429\n",
      "Iteration 719, loss = 0.61547445\n",
      "Iteration 720, loss = 0.61547332\n",
      "Iteration 721, loss = 0.61548173\n",
      "Iteration 722, loss = 0.61547755\n",
      "Iteration 723, loss = 0.61547311\n",
      "Iteration 724, loss = 0.61547024\n",
      "Iteration 725, loss = 0.61546928\n",
      "Iteration 726, loss = 0.61547261\n",
      "Iteration 727, loss = 0.61546832\n",
      "Iteration 728, loss = 0.61546835\n",
      "Iteration 729, loss = 0.61546801\n",
      "Iteration 730, loss = 0.61546666\n",
      "Iteration 731, loss = 0.61547095\n",
      "Iteration 732, loss = 0.61546659\n",
      "Iteration 733, loss = 0.61546599\n",
      "Iteration 734, loss = 0.61546544\n",
      "Iteration 735, loss = 0.61546423\n",
      "Iteration 736, loss = 0.61546418\n",
      "Iteration 737, loss = 0.61546886\n",
      "Iteration 738, loss = 0.61546310\n",
      "Iteration 739, loss = 0.61546734\n",
      "Iteration 740, loss = 0.61546410\n",
      "Iteration 741, loss = 0.61546281\n",
      "Iteration 742, loss = 0.61545878\n",
      "Iteration 743, loss = 0.61546038\n",
      "Iteration 744, loss = 0.61546176\n",
      "Iteration 745, loss = 0.61546719\n",
      "Iteration 746, loss = 0.61546168\n",
      "Iteration 747, loss = 0.61546065\n",
      "Iteration 748, loss = 0.61545923\n",
      "Iteration 749, loss = 0.61545898\n",
      "Iteration 750, loss = 0.61545873\n",
      "Iteration 751, loss = 0.61545854\n",
      "Iteration 752, loss = 0.61545802\n",
      "Iteration 753, loss = 0.61545739\n",
      "Iteration 754, loss = 0.61545985\n",
      "Iteration 755, loss = 0.61545761\n",
      "Iteration 756, loss = 0.61545685\n",
      "Iteration 757, loss = 0.61545602\n",
      "Iteration 758, loss = 0.61545610\n",
      "Iteration 759, loss = 0.61546476\n",
      "Iteration 760, loss = 0.61545738\n",
      "Iteration 761, loss = 0.61545678\n",
      "Iteration 762, loss = 0.61545640\n",
      "Iteration 763, loss = 0.61545778\n",
      "Iteration 764, loss = 0.61545829\n",
      "Iteration 765, loss = 0.61545607\n",
      "Iteration 766, loss = 0.61545446\n",
      "Iteration 767, loss = 0.61545930\n",
      "Iteration 768, loss = 0.61546058\n",
      "Iteration 769, loss = 0.61545412\n",
      "Iteration 770, loss = 0.61545841\n",
      "Iteration 771, loss = 0.61545412\n",
      "Iteration 772, loss = 0.61545285\n",
      "Iteration 773, loss = 0.61545513\n",
      "Iteration 774, loss = 0.61545270\n",
      "Iteration 775, loss = 0.61545965\n",
      "Iteration 776, loss = 0.61545191\n",
      "Iteration 777, loss = 0.61545578\n",
      "Iteration 778, loss = 0.61545331\n",
      "Iteration 779, loss = 0.61545260\n",
      "Iteration 780, loss = 0.61545239\n",
      "Iteration 781, loss = 0.61545385\n",
      "Iteration 782, loss = 0.61544806\n",
      "Iteration 783, loss = 0.61544753\n",
      "Iteration 784, loss = 0.61545104\n",
      "Iteration 785, loss = 0.61545295\n",
      "Iteration 786, loss = 0.61545123\n",
      "Iteration 787, loss = 0.61545357\n",
      "Iteration 788, loss = 0.61545121\n",
      "Iteration 789, loss = 0.61545185\n",
      "Iteration 790, loss = 0.61545311\n",
      "Iteration 791, loss = 0.61545708\n",
      "Iteration 792, loss = 0.61545116\n",
      "Iteration 793, loss = 0.61545589\n",
      "Iteration 794, loss = 0.61545097\n",
      "Iteration 795, loss = 0.61544742\n",
      "Iteration 796, loss = 0.61544661\n",
      "Iteration 797, loss = 0.61545413\n",
      "Iteration 798, loss = 0.61545481\n",
      "Iteration 799, loss = 0.61544838\n",
      "Iteration 800, loss = 0.61544777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 801, loss = 0.61544668\n",
      "Iteration 802, loss = 0.61544697\n",
      "Iteration 803, loss = 0.61544972\n",
      "Iteration 804, loss = 0.61545429\n",
      "Iteration 805, loss = 0.61545391\n",
      "Iteration 806, loss = 0.61544908\n",
      "Iteration 807, loss = 0.61544803\n",
      "Iteration 808, loss = 0.61545169\n",
      "Iteration 809, loss = 0.61545187\n",
      "Iteration 810, loss = 0.61544857\n",
      "Iteration 811, loss = 0.61544863\n",
      "Iteration 812, loss = 0.61544803\n",
      "Iteration 813, loss = 0.61543944\n",
      "Iteration 814, loss = 0.61544641\n",
      "Iteration 815, loss = 0.61544126\n",
      "Iteration 816, loss = 0.61544296\n",
      "Iteration 817, loss = 0.61544109\n",
      "Iteration 818, loss = 0.61544782\n",
      "Iteration 819, loss = 0.61544064\n",
      "Iteration 820, loss = 0.61545127\n",
      "Iteration 821, loss = 0.61544373\n",
      "Iteration 822, loss = 0.61544287\n",
      "Iteration 823, loss = 0.61544464\n",
      "Iteration 824, loss = 0.61544606\n",
      "Iteration 825, loss = 0.61546902\n",
      "Iteration 826, loss = 0.61545308\n",
      "Iteration 827, loss = 0.61545815\n",
      "Iteration 828, loss = 0.61545281\n",
      "Iteration 829, loss = 0.61545375\n",
      "Iteration 830, loss = 0.61545090\n",
      "Iteration 831, loss = 0.61545500\n",
      "Iteration 832, loss = 0.61545153\n",
      "Iteration 833, loss = 0.61545136\n",
      "Iteration 834, loss = 0.61545086\n",
      "Iteration 835, loss = 0.61545162\n",
      "Iteration 836, loss = 0.61545072\n",
      "Iteration 837, loss = 0.61545158\n",
      "Iteration 838, loss = 0.61545138\n",
      "Iteration 839, loss = 0.61544997\n",
      "Iteration 840, loss = 0.61545055\n",
      "Iteration 841, loss = 0.61544799\n",
      "Iteration 842, loss = 0.61545236\n",
      "Iteration 843, loss = 0.61544937\n",
      "Iteration 844, loss = 0.61544584\n",
      "Iteration 845, loss = 0.61544903\n",
      "Iteration 846, loss = 0.61544755\n",
      "Iteration 847, loss = 0.61544895\n",
      "Iteration 848, loss = 0.61544895\n",
      "Iteration 849, loss = 0.61544946\n",
      "Iteration 850, loss = 0.61544905\n",
      "Iteration 851, loss = 0.61544836\n",
      "Iteration 852, loss = 0.61545375\n",
      "Iteration 853, loss = 0.61546065\n",
      "Iteration 854, loss = 0.61545635\n",
      "Iteration 855, loss = 0.61545767\n",
      "Iteration 856, loss = 0.61545749\n",
      "Iteration 857, loss = 0.61545733\n",
      "Iteration 858, loss = 0.61545682\n",
      "Iteration 859, loss = 0.61545865\n",
      "Iteration 860, loss = 0.61545567\n",
      "Iteration 861, loss = 0.61545500\n",
      "Iteration 862, loss = 0.61545517\n",
      "Iteration 863, loss = 0.61545321\n",
      "Iteration 864, loss = 0.61545196\n",
      "Iteration 865, loss = 0.61544944\n",
      "Iteration 866, loss = 0.61545360\n",
      "Iteration 867, loss = 0.61545024\n",
      "Iteration 868, loss = 0.61544678\n",
      "Iteration 869, loss = 0.61544898\n",
      "Iteration 870, loss = 0.61544592\n",
      "Iteration 871, loss = 0.61544902\n",
      "Iteration 872, loss = 0.61544427\n",
      "Iteration 873, loss = 0.61544446\n",
      "Iteration 874, loss = 0.61543966\n",
      "Iteration 875, loss = 0.61545127\n",
      "Iteration 876, loss = 0.61543987\n",
      "Iteration 877, loss = 0.61543827\n",
      "Iteration 878, loss = 0.61544735\n",
      "Iteration 879, loss = 0.61543931\n",
      "Iteration 880, loss = 0.61544014\n",
      "Iteration 881, loss = 0.61543753\n",
      "Iteration 882, loss = 0.61543754\n",
      "Iteration 883, loss = 0.61543814\n",
      "Iteration 884, loss = 0.61544559\n",
      "Iteration 885, loss = 0.61544026\n",
      "Iteration 886, loss = 0.61543972\n",
      "Iteration 887, loss = 0.61543955\n",
      "Iteration 888, loss = 0.61544034\n",
      "Iteration 889, loss = 0.61543887\n",
      "Iteration 890, loss = 0.61543990\n",
      "Iteration 891, loss = 0.61543947\n",
      "Iteration 892, loss = 0.61543866\n",
      "Iteration 893, loss = 0.61544706\n",
      "Iteration 894, loss = 0.61544222\n",
      "Iteration 895, loss = 0.61544201\n",
      "Iteration 896, loss = 0.61544344\n",
      "Iteration 897, loss = 0.61544432\n",
      "Iteration 898, loss = 0.61544444\n",
      "Iteration 899, loss = 0.61544944\n",
      "Iteration 900, loss = 0.61544469\n",
      "Iteration 901, loss = 0.61544422\n",
      "Iteration 902, loss = 0.61544176\n",
      "Iteration 903, loss = 0.61544058\n",
      "Iteration 904, loss = 0.61544934\n",
      "Iteration 905, loss = 0.61544000\n",
      "Iteration 906, loss = 0.61544029\n",
      "Iteration 907, loss = 0.61543952\n",
      "Iteration 908, loss = 0.61543954\n",
      "Iteration 909, loss = 0.61544145\n",
      "Iteration 910, loss = 0.61543997\n",
      "Iteration 911, loss = 0.61544224\n",
      "Iteration 912, loss = 0.61543925\n",
      "Iteration 913, loss = 0.61543867\n",
      "Iteration 914, loss = 0.61544412\n",
      "Iteration 915, loss = 0.61543674\n",
      "Iteration 916, loss = 0.61543362\n",
      "Iteration 917, loss = 0.61543923\n",
      "Iteration 918, loss = 0.61545155\n",
      "Iteration 919, loss = 0.61544788\n",
      "Iteration 920, loss = 0.61544623\n",
      "Iteration 921, loss = 0.61544608\n",
      "Iteration 922, loss = 0.61544526\n",
      "Iteration 923, loss = 0.61544212\n",
      "Iteration 924, loss = 0.61544562\n",
      "Iteration 925, loss = 0.61543993\n",
      "Iteration 926, loss = 0.61543629\n",
      "Iteration 927, loss = 0.61543535\n",
      "Iteration 928, loss = 0.61543029\n",
      "Iteration 929, loss = 0.61543609\n",
      "Iteration 930, loss = 0.61542746\n",
      "Iteration 931, loss = 0.61543843\n",
      "Iteration 932, loss = 0.61543559\n",
      "Iteration 933, loss = 0.61543257\n",
      "Iteration 934, loss = 0.61542627\n",
      "Iteration 935, loss = 0.61543285\n",
      "Iteration 936, loss = 0.61542842\n",
      "Iteration 937, loss = 0.61543017\n",
      "Iteration 938, loss = 0.61542795\n",
      "Iteration 939, loss = 0.61542751\n",
      "Iteration 940, loss = 0.61542845\n",
      "Iteration 941, loss = 0.61542740\n",
      "Iteration 942, loss = 0.61542808\n",
      "Iteration 943, loss = 0.61542765\n",
      "Iteration 944, loss = 0.61542805\n",
      "Iteration 945, loss = 0.61542450\n",
      "Iteration 946, loss = 0.61542418\n",
      "Iteration 947, loss = 0.61542954\n",
      "Iteration 948, loss = 0.61542287\n",
      "Iteration 949, loss = 0.61543110\n",
      "Iteration 950, loss = 0.61543308\n",
      "Iteration 951, loss = 0.61543170\n",
      "Iteration 952, loss = 0.61542979\n",
      "Iteration 953, loss = 0.61543039\n",
      "Iteration 954, loss = 0.61543256\n",
      "Iteration 955, loss = 0.61542918\n",
      "Iteration 956, loss = 0.61542831\n",
      "Iteration 957, loss = 0.61542949\n",
      "Iteration 958, loss = 0.61543159\n",
      "Iteration 959, loss = 0.61543125\n",
      "Iteration 960, loss = 0.61542783\n",
      "Iteration 961, loss = 0.61543112\n",
      "Iteration 962, loss = 0.61543339\n",
      "Iteration 963, loss = 0.61543131\n",
      "Iteration 964, loss = 0.61542964\n",
      "Iteration 965, loss = 0.61545025\n",
      "Iteration 966, loss = 0.61543662\n",
      "Iteration 967, loss = 0.61543796\n",
      "Iteration 968, loss = 0.61543653\n",
      "Iteration 969, loss = 0.61543862\n",
      "Iteration 970, loss = 0.61543684\n",
      "Iteration 971, loss = 0.61543596\n",
      "Iteration 972, loss = 0.61543595\n",
      "Iteration 973, loss = 0.61543849\n",
      "Iteration 974, loss = 0.61545281\n",
      "Iteration 975, loss = 0.61544319\n",
      "Iteration 976, loss = 0.61544066\n",
      "Iteration 977, loss = 0.61544136\n",
      "Iteration 978, loss = 0.61544153\n",
      "Iteration 979, loss = 0.61544167\n",
      "Iteration 980, loss = 0.61544628\n",
      "Iteration 981, loss = 0.61544553\n",
      "Iteration 982, loss = 0.61544452\n",
      "Iteration 983, loss = 0.61544258\n",
      "Iteration 984, loss = 0.61544548\n",
      "Iteration 985, loss = 0.61543849\n",
      "Iteration 986, loss = 0.61543800\n",
      "Iteration 987, loss = 0.61543778\n",
      "Iteration 988, loss = 0.61543626\n",
      "Iteration 989, loss = 0.61543681\n",
      "Iteration 990, loss = 0.61543551\n",
      "Iteration 991, loss = 0.61543828\n",
      "Iteration 992, loss = 0.61543538\n",
      "Iteration 993, loss = 0.61543781\n",
      "Iteration 994, loss = 0.61543461\n",
      "Iteration 995, loss = 0.61543752\n",
      "Iteration 996, loss = 0.61543182\n",
      "Iteration 997, loss = 0.61542991\n",
      "Iteration 998, loss = 0.61542661\n",
      "Iteration 999, loss = 0.61542812\n",
      "Iteration 1000, loss = 0.61542844\n",
      "Iteration 1001, loss = 0.61542983\n",
      "Iteration 1002, loss = 0.61542524\n",
      "Iteration 1003, loss = 0.61542573\n",
      "Iteration 1004, loss = 0.61542543\n",
      "Iteration 1005, loss = 0.61542500\n",
      "Iteration 1006, loss = 0.61542846\n",
      "Iteration 1007, loss = 0.61542865\n",
      "Iteration 1008, loss = 0.61542590\n",
      "Iteration 1009, loss = 0.61542928\n",
      "Iteration 1010, loss = 0.61542654\n",
      "Iteration 1011, loss = 0.61542489\n",
      "Iteration 1012, loss = 0.61542480\n",
      "Iteration 1013, loss = 0.61542513\n",
      "Iteration 1014, loss = 0.61542442\n",
      "Iteration 1015, loss = 0.61542385\n",
      "Iteration 1016, loss = 0.61542990\n",
      "Iteration 1017, loss = 0.61542790\n",
      "Iteration 1018, loss = 0.61542765\n",
      "Iteration 1019, loss = 0.61542763\n",
      "Iteration 1020, loss = 0.61542409\n",
      "Iteration 1021, loss = 0.61542699\n",
      "Iteration 1022, loss = 0.61542835\n",
      "Iteration 1023, loss = 0.61542646\n",
      "Iteration 1024, loss = 0.61542835\n",
      "Iteration 1025, loss = 0.61542438\n",
      "Iteration 1026, loss = 0.61542432\n",
      "Iteration 1027, loss = 0.61542423\n",
      "Iteration 1028, loss = 0.61543018\n",
      "Iteration 1029, loss = 0.61542444\n",
      "Iteration 1030, loss = 0.61542798\n",
      "Iteration 1031, loss = 0.61542546\n",
      "Iteration 1032, loss = 0.61542559\n",
      "Iteration 1033, loss = 0.61542512\n",
      "Iteration 1034, loss = 0.61543254\n",
      "Iteration 1035, loss = 0.61542963\n",
      "Iteration 1036, loss = 0.61542923\n",
      "Iteration 1037, loss = 0.61542734\n",
      "Iteration 1038, loss = 0.61542737\n",
      "Iteration 1039, loss = 0.61542710\n",
      "Iteration 1040, loss = 0.61542728\n",
      "Iteration 1041, loss = 0.61542731\n",
      "Iteration 1042, loss = 0.61542732\n",
      "Iteration 1043, loss = 0.61542791\n",
      "Iteration 1044, loss = 0.61542820\n",
      "Iteration 1045, loss = 0.61542830\n",
      "Iteration 1046, loss = 0.61542816\n",
      "Iteration 1047, loss = 0.61542769\n",
      "Iteration 1048, loss = 0.61542641\n",
      "Iteration 1049, loss = 0.61543262\n",
      "Iteration 1050, loss = 0.61544223\n",
      "Iteration 1051, loss = 0.61543562\n",
      "Iteration 1052, loss = 0.61543553\n",
      "Iteration 1053, loss = 0.61543589\n",
      "Iteration 1054, loss = 0.61543639\n",
      "Iteration 1055, loss = 0.61543518\n",
      "Iteration 1056, loss = 0.61543189\n",
      "Iteration 1057, loss = 0.61542793\n",
      "Iteration 1058, loss = 0.61542016\n",
      "Iteration 1059, loss = 0.61542303\n",
      "Iteration 1060, loss = 0.61542347\n",
      "Iteration 1061, loss = 0.61542947\n",
      "Iteration 1062, loss = 0.61541841\n",
      "Iteration 1063, loss = 0.61542504\n",
      "Iteration 1064, loss = 0.61541885\n",
      "Iteration 1065, loss = 0.61541973\n",
      "Iteration 1066, loss = 0.61542126\n",
      "Iteration 1067, loss = 0.61542065\n",
      "Iteration 1068, loss = 0.61541895\n",
      "Iteration 1069, loss = 0.61541957\n",
      "Iteration 1070, loss = 0.61541902\n",
      "Iteration 1071, loss = 0.61541862\n",
      "Iteration 1072, loss = 0.61541846\n",
      "Iteration 1073, loss = 0.61541881\n",
      "Iteration 1074, loss = 0.61541816\n",
      "Iteration 1075, loss = 0.61541745\n",
      "Iteration 1076, loss = 0.61541554\n",
      "Iteration 1077, loss = 0.61541881\n",
      "Iteration 1078, loss = 0.61541366\n",
      "Iteration 1079, loss = 0.61541659\n",
      "Iteration 1080, loss = 0.61542045\n",
      "Iteration 1081, loss = 0.61541883\n",
      "Iteration 1082, loss = 0.61541667\n",
      "Iteration 1083, loss = 0.61542041\n",
      "Iteration 1084, loss = 0.61541708\n",
      "Iteration 1085, loss = 0.61541824\n",
      "Iteration 1086, loss = 0.61541774\n",
      "Iteration 1087, loss = 0.61541573\n",
      "Iteration 1088, loss = 0.61541448\n",
      "Iteration 1089, loss = 0.61541160\n",
      "Iteration 1090, loss = 0.61542118\n",
      "Iteration 1091, loss = 0.61541513\n",
      "Iteration 1092, loss = 0.61541330\n",
      "Iteration 1093, loss = 0.61542072\n",
      "Iteration 1094, loss = 0.61542035\n",
      "Iteration 1095, loss = 0.61541701\n",
      "Iteration 1096, loss = 0.61541613\n",
      "Iteration 1097, loss = 0.61541979\n",
      "Iteration 1098, loss = 0.61541683\n",
      "Iteration 1099, loss = 0.61542102\n",
      "Iteration 1100, loss = 0.61541693\n",
      "Iteration 1101, loss = 0.61541798\n",
      "Iteration 1102, loss = 0.61541994\n",
      "Iteration 1103, loss = 0.61541796\n",
      "Iteration 1104, loss = 0.61541571\n",
      "Iteration 1105, loss = 0.61541928\n",
      "Iteration 1106, loss = 0.61541701\n",
      "Iteration 1107, loss = 0.61541838\n",
      "Iteration 1108, loss = 0.61541811\n",
      "Iteration 1109, loss = 0.61541813\n",
      "Iteration 1110, loss = 0.61541606\n",
      "Iteration 1111, loss = 0.61541482\n",
      "Iteration 1112, loss = 0.61541952\n",
      "Iteration 1113, loss = 0.61543235\n",
      "Iteration 1114, loss = 0.61541993\n",
      "Iteration 1115, loss = 0.61542290\n",
      "Iteration 1116, loss = 0.61541998\n",
      "Iteration 1117, loss = 0.61541981\n",
      "Iteration 1118, loss = 0.61541904\n",
      "Iteration 1119, loss = 0.61542102\n",
      "Iteration 1120, loss = 0.61541979\n",
      "Iteration 1121, loss = 0.61542064\n",
      "Iteration 1122, loss = 0.61541791\n",
      "Iteration 1123, loss = 0.61541850\n",
      "Iteration 1124, loss = 0.61542079\n",
      "Iteration 1125, loss = 0.61541941\n",
      "Iteration 1126, loss = 0.61541826\n",
      "Iteration 1127, loss = 0.61541639\n",
      "Iteration 1128, loss = 0.61541740\n",
      "Iteration 1129, loss = 0.61541952\n",
      "Iteration 1130, loss = 0.61541510\n",
      "Iteration 1131, loss = 0.61541858\n",
      "Iteration 1132, loss = 0.61541528\n",
      "Iteration 1133, loss = 0.61541564\n",
      "Iteration 1134, loss = 0.61541557\n",
      "Iteration 1135, loss = 0.61541452\n",
      "Iteration 1136, loss = 0.61541604\n",
      "Iteration 1137, loss = 0.61541527\n",
      "Iteration 1138, loss = 0.61541532\n",
      "Iteration 1139, loss = 0.61541592\n",
      "Iteration 1140, loss = 0.61541573\n",
      "Iteration 1141, loss = 0.61541495\n",
      "Iteration 1142, loss = 0.61541686\n",
      "Iteration 1143, loss = 0.61541602\n",
      "Iteration 1144, loss = 0.61541513\n",
      "Iteration 1145, loss = 0.61541479\n",
      "Iteration 1146, loss = 0.61541459\n",
      "Iteration 1147, loss = 0.61541450\n",
      "Iteration 1148, loss = 0.61541392\n",
      "Iteration 1149, loss = 0.61541154\n",
      "Iteration 1150, loss = 0.61541068\n",
      "Iteration 1151, loss = 0.61541298\n",
      "Iteration 1152, loss = 0.61541696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1153, loss = 0.61542876\n",
      "Iteration 1154, loss = 0.61542602\n",
      "Iteration 1155, loss = 0.61542571\n",
      "Iteration 1156, loss = 0.61542231\n",
      "Iteration 1157, loss = 0.61542869\n",
      "Iteration 1158, loss = 0.61542571\n",
      "Iteration 1159, loss = 0.61542226\n",
      "Iteration 1160, loss = 0.61542130\n",
      "Iteration 1161, loss = 0.61541668\n",
      "Iteration 1162, loss = 0.61542691\n",
      "Iteration 1163, loss = 0.61542360\n",
      "Iteration 1164, loss = 0.61541768\n",
      "Iteration 1165, loss = 0.61541893\n",
      "Iteration 1166, loss = 0.61541748\n",
      "Iteration 1167, loss = 0.61541843\n",
      "Iteration 1168, loss = 0.61541735\n",
      "Iteration 1169, loss = 0.61541660\n",
      "Iteration 1170, loss = 0.61541157\n",
      "Iteration 1171, loss = 0.61540643\n",
      "Iteration 1172, loss = 0.61541935\n",
      "Iteration 1173, loss = 0.61541085\n",
      "Iteration 1174, loss = 0.61541631\n",
      "Iteration 1175, loss = 0.61541456\n",
      "Iteration 1176, loss = 0.61541542\n",
      "Iteration 1177, loss = 0.61541371\n",
      "Iteration 1178, loss = 0.61541105\n",
      "Iteration 1179, loss = 0.61540969\n",
      "Iteration 1180, loss = 0.61540947\n",
      "Iteration 1181, loss = 0.61541319\n",
      "Iteration 1182, loss = 0.61540861\n",
      "Iteration 1183, loss = 0.61540823\n",
      "Iteration 1184, loss = 0.61542148\n",
      "Iteration 1185, loss = 0.61541871\n",
      "Iteration 1186, loss = 0.61541707\n",
      "Iteration 1187, loss = 0.61541642\n",
      "Iteration 1188, loss = 0.61542128\n",
      "Iteration 1189, loss = 0.61541919\n",
      "Iteration 1190, loss = 0.61541804\n",
      "Iteration 1191, loss = 0.61541799\n",
      "Iteration 1192, loss = 0.61542356\n",
      "Iteration 1193, loss = 0.61542658\n",
      "Iteration 1194, loss = 0.61544017\n",
      "Iteration 1195, loss = 0.61543252\n",
      "Iteration 1196, loss = 0.61543440\n",
      "Iteration 1197, loss = 0.61543418\n",
      "Iteration 1198, loss = 0.61543402\n",
      "Iteration 1199, loss = 0.61544146\n",
      "Iteration 1200, loss = 0.61543575\n",
      "Iteration 1201, loss = 0.61543504\n",
      "Iteration 1202, loss = 0.61543676\n",
      "Iteration 1203, loss = 0.61543997\n",
      "Iteration 1204, loss = 0.61543875\n",
      "Iteration 1205, loss = 0.61543782\n",
      "Iteration 1206, loss = 0.61543737\n",
      "Iteration 1207, loss = 0.61543676\n",
      "Iteration 1208, loss = 0.61543538\n",
      "Iteration 1209, loss = 0.61543668\n",
      "Iteration 1210, loss = 0.61543731\n",
      "Iteration 1211, loss = 0.61543729\n",
      "Iteration 1212, loss = 0.61543776\n",
      "Iteration 1213, loss = 0.61543843\n",
      "Iteration 1214, loss = 0.61543738\n",
      "Iteration 1215, loss = 0.61544024\n",
      "Iteration 1216, loss = 0.61544111\n",
      "Iteration 1217, loss = 0.61544313\n",
      "Iteration 1218, loss = 0.61544342\n",
      "Iteration 1219, loss = 0.61544471\n",
      "Iteration 1220, loss = 0.61545032\n",
      "Iteration 1221, loss = 0.61544917\n",
      "Iteration 1222, loss = 0.61545200\n",
      "Iteration 1223, loss = 0.61545106\n",
      "Iteration 1224, loss = 0.61545056\n",
      "Iteration 1225, loss = 0.61545415\n",
      "Iteration 1226, loss = 0.61545306\n",
      "Iteration 1227, loss = 0.61545326\n",
      "Iteration 1228, loss = 0.61545075\n",
      "Iteration 1229, loss = 0.61545313\n",
      "Iteration 1230, loss = 0.61544736\n",
      "Iteration 1231, loss = 0.61544664\n",
      "Iteration 1232, loss = 0.61544385\n",
      "Iteration 1233, loss = 0.61544414\n",
      "Iteration 1234, loss = 0.61544011\n",
      "Iteration 1235, loss = 0.61543941\n",
      "Iteration 1236, loss = 0.61545400\n",
      "Iteration 1237, loss = 0.61544105\n",
      "Iteration 1238, loss = 0.61543858\n",
      "Iteration 1239, loss = 0.61543824\n",
      "Iteration 1240, loss = 0.61543975\n",
      "Iteration 1241, loss = 0.61544137\n",
      "Iteration 1242, loss = 0.61544808\n",
      "Iteration 1243, loss = 0.61545451\n",
      "Iteration 1244, loss = 0.61545811\n",
      "Iteration 1245, loss = 0.61545907\n",
      "Iteration 1246, loss = 0.61546467\n",
      "Iteration 1247, loss = 0.61546482\n",
      "Iteration 1248, loss = 0.61546661\n",
      "Iteration 1249, loss = 0.61546739\n",
      "Iteration 1250, loss = 0.61547636\n",
      "Iteration 1251, loss = 0.61548693\n",
      "Iteration 1252, loss = 0.61548816\n",
      "Iteration 1253, loss = 0.61549368\n",
      "Iteration 1254, loss = 0.61549456\n",
      "Iteration 1255, loss = 0.61549539\n",
      "Iteration 1256, loss = 0.61549517\n",
      "Iteration 1257, loss = 0.61549447\n",
      "Iteration 1258, loss = 0.61549372\n",
      "Iteration 1259, loss = 0.61549520\n",
      "Iteration 1260, loss = 0.61549643\n",
      "Iteration 1261, loss = 0.61550892\n",
      "Iteration 1262, loss = 0.61550463\n",
      "Iteration 1263, loss = 0.61550327\n",
      "Iteration 1264, loss = 0.61550005\n",
      "Iteration 1265, loss = 0.61550037\n",
      "Iteration 1266, loss = 0.61549569\n",
      "Iteration 1267, loss = 0.61549329\n",
      "Iteration 1268, loss = 0.61548917\n",
      "Iteration 1269, loss = 0.61548693\n",
      "Iteration 1270, loss = 0.61547789\n",
      "Iteration 1271, loss = 0.61547432\n",
      "Iteration 1272, loss = 0.61547045\n",
      "Iteration 1273, loss = 0.61546944\n",
      "Iteration 1274, loss = 0.61545976\n",
      "Iteration 1275, loss = 0.61546293\n",
      "Iteration 1276, loss = 0.61545692\n",
      "Iteration 1277, loss = 0.61545518\n",
      "Iteration 1278, loss = 0.61545504\n",
      "Iteration 1279, loss = 0.61545443\n",
      "Iteration 1280, loss = 0.61545242\n",
      "Iteration 1281, loss = 0.61544989\n",
      "Iteration 1282, loss = 0.61544398\n",
      "Iteration 1283, loss = 0.61543997\n",
      "Iteration 1284, loss = 0.61543979\n",
      "Iteration 1285, loss = 0.61543792\n",
      "Iteration 1286, loss = 0.61543813\n",
      "Iteration 1287, loss = 0.61542989\n",
      "Iteration 1288, loss = 0.61542739\n",
      "Iteration 1289, loss = 0.61543294\n",
      "Iteration 1290, loss = 0.61542554\n",
      "Iteration 1291, loss = 0.61542432\n",
      "Iteration 1292, loss = 0.61542625\n",
      "Iteration 1293, loss = 0.61542347\n",
      "Iteration 1294, loss = 0.61542607\n",
      "Iteration 1295, loss = 0.61542222\n",
      "Iteration 1296, loss = 0.61542862\n",
      "Iteration 1297, loss = 0.61542581\n",
      "Iteration 1298, loss = 0.61542774\n",
      "Iteration 1299, loss = 0.61542695\n",
      "Iteration 1300, loss = 0.61542650\n",
      "Iteration 1301, loss = 0.61543185\n",
      "Iteration 1302, loss = 0.61544126\n",
      "Iteration 1303, loss = 0.61543756\n",
      "Iteration 1304, loss = 0.61544144\n",
      "Iteration 1305, loss = 0.61544077\n",
      "Iteration 1306, loss = 0.61544302\n",
      "Iteration 1307, loss = 0.61544661\n",
      "Iteration 1308, loss = 0.61546359\n",
      "Iteration 1309, loss = 0.61545707\n",
      "Iteration 1310, loss = 0.61545744\n",
      "Iteration 1311, loss = 0.61545683\n",
      "Iteration 1312, loss = 0.61545733\n",
      "Iteration 1313, loss = 0.61545778\n",
      "Iteration 1314, loss = 0.61545885\n",
      "Iteration 1315, loss = 0.61545878\n",
      "Iteration 1316, loss = 0.61546233\n",
      "Iteration 1317, loss = 0.61545537\n",
      "Iteration 1318, loss = 0.61545445\n",
      "Iteration 1319, loss = 0.61547176\n",
      "Iteration 1320, loss = 0.61548479\n",
      "Iteration 1321, loss = 0.61548452\n",
      "Iteration 1322, loss = 0.61548561\n",
      "Iteration 1323, loss = 0.61549558\n",
      "Iteration 1324, loss = 0.61549267\n",
      "Iteration 1325, loss = 0.61549521\n",
      "Iteration 1326, loss = 0.61549653\n",
      "Iteration 1327, loss = 0.61549567\n",
      "Iteration 1328, loss = 0.61549827\n",
      "Iteration 1329, loss = 0.61549161\n",
      "Iteration 1330, loss = 0.61549181\n",
      "Iteration 1331, loss = 0.61548615\n",
      "Iteration 1332, loss = 0.61547872\n",
      "Iteration 1333, loss = 0.61548487\n",
      "Iteration 1334, loss = 0.61547413\n",
      "Iteration 1335, loss = 0.61547614\n",
      "Iteration 1336, loss = 0.61547381\n",
      "Iteration 1337, loss = 0.61547413\n",
      "Iteration 1338, loss = 0.61547734\n",
      "Iteration 1339, loss = 0.61547462\n",
      "Iteration 1340, loss = 0.61547029\n",
      "Iteration 1341, loss = 0.61546754\n",
      "Iteration 1342, loss = 0.61546860\n",
      "Iteration 1343, loss = 0.61546075\n",
      "Iteration 1344, loss = 0.61546746\n",
      "Iteration 1345, loss = 0.61545770\n",
      "Iteration 1346, loss = 0.61545667\n",
      "Iteration 1347, loss = 0.61545615\n",
      "Iteration 1348, loss = 0.61545540\n",
      "Iteration 1349, loss = 0.61545862\n",
      "Iteration 1350, loss = 0.61545221\n",
      "Iteration 1351, loss = 0.61546112\n",
      "Iteration 1352, loss = 0.61546041\n",
      "Iteration 1353, loss = 0.61546148\n",
      "Iteration 1354, loss = 0.61546922\n",
      "Iteration 1355, loss = 0.61547686\n",
      "Iteration 1356, loss = 0.61547692\n",
      "Iteration 1357, loss = 0.61548847\n",
      "Iteration 1358, loss = 0.61548635\n",
      "Iteration 1359, loss = 0.61549931\n",
      "Iteration 1360, loss = 0.61549784\n",
      "Iteration 1361, loss = 0.61551920\n",
      "Iteration 1362, loss = 0.61551431\n",
      "Iteration 1363, loss = 0.61551261\n",
      "Iteration 1364, loss = 0.61550993\n",
      "Iteration 1365, loss = 0.61550308\n",
      "Iteration 1366, loss = 0.61550380\n",
      "Iteration 1367, loss = 0.61550338\n",
      "Iteration 1368, loss = 0.61548932\n",
      "Iteration 1369, loss = 0.61548978\n",
      "Iteration 1370, loss = 0.61548468\n",
      "Iteration 1371, loss = 0.61549020\n",
      "Iteration 1372, loss = 0.61548450\n",
      "Iteration 1373, loss = 0.61548179\n",
      "Iteration 1374, loss = 0.61548236\n",
      "Iteration 1375, loss = 0.61548421\n",
      "Iteration 1376, loss = 0.61548523\n",
      "Iteration 1377, loss = 0.61548619\n",
      "Iteration 1378, loss = 0.61548500\n",
      "Iteration 1379, loss = 0.61549131\n",
      "Iteration 1380, loss = 0.61549139\n",
      "Iteration 1381, loss = 0.61549599\n",
      "Iteration 1382, loss = 0.61549839\n",
      "Iteration 1383, loss = 0.61550344\n",
      "Iteration 1384, loss = 0.61550433\n",
      "Iteration 1385, loss = 0.61550217\n",
      "Iteration 1386, loss = 0.61550646\n",
      "Iteration 1387, loss = 0.61551068\n",
      "Iteration 1388, loss = 0.61550938\n",
      "Iteration 1389, loss = 0.61550967\n",
      "Iteration 1390, loss = 0.61550674\n",
      "Iteration 1391, loss = 0.61550200\n",
      "Iteration 1392, loss = 0.61549666\n",
      "Iteration 1393, loss = 0.61549479\n",
      "Iteration 1394, loss = 0.61548534\n",
      "Iteration 1395, loss = 0.61548124\n",
      "Iteration 1396, loss = 0.61548972\n",
      "Iteration 1397, loss = 0.61547453\n",
      "Iteration 1398, loss = 0.61547393\n",
      "Iteration 1399, loss = 0.61547239\n",
      "Iteration 1400, loss = 0.61547114\n",
      "Iteration 1401, loss = 0.61547440\n",
      "Iteration 1402, loss = 0.61546978\n",
      "Iteration 1403, loss = 0.61546718\n",
      "Iteration 1404, loss = 0.61546599\n",
      "Iteration 1405, loss = 0.61546551\n",
      "Iteration 1406, loss = 0.61546631\n",
      "Iteration 1407, loss = 0.61546358\n",
      "Iteration 1408, loss = 0.61547094\n",
      "Iteration 1409, loss = 0.61546798\n",
      "Iteration 1410, loss = 0.61546590\n",
      "Iteration 1411, loss = 0.61546794\n",
      "Iteration 1412, loss = 0.61546076\n",
      "Iteration 1413, loss = 0.61546205\n",
      "Iteration 1414, loss = 0.61545818\n",
      "Iteration 1415, loss = 0.61545688\n",
      "Iteration 1416, loss = 0.61545958\n",
      "Iteration 1417, loss = 0.61546374\n",
      "Iteration 1418, loss = 0.61546032\n",
      "Iteration 1419, loss = 0.61546038\n",
      "Iteration 1420, loss = 0.61545510\n",
      "Iteration 1421, loss = 0.61545123\n",
      "Iteration 1422, loss = 0.61545021\n",
      "Iteration 1423, loss = 0.61544682\n",
      "Iteration 1424, loss = 0.61544700\n",
      "Iteration 1425, loss = 0.61544471\n",
      "Iteration 1426, loss = 0.61544143\n",
      "Iteration 1427, loss = 0.61544171\n",
      "Iteration 1428, loss = 0.61544361\n",
      "Iteration 1429, loss = 0.61544211\n",
      "Iteration 1430, loss = 0.61543902\n",
      "Iteration 1431, loss = 0.61543450\n",
      "Iteration 1432, loss = 0.61543872\n",
      "Iteration 1433, loss = 0.61543303\n",
      "Iteration 1434, loss = 0.61543021\n",
      "Iteration 1435, loss = 0.61542780\n",
      "Iteration 1436, loss = 0.61543033\n",
      "Iteration 1437, loss = 0.61542570\n",
      "Iteration 1438, loss = 0.61542331\n",
      "Iteration 1439, loss = 0.61542703\n",
      "Iteration 1440, loss = 0.61542011\n",
      "Iteration 1441, loss = 0.61541892\n",
      "Iteration 1442, loss = 0.61541311\n",
      "Iteration 1443, loss = 0.61541640\n",
      "Iteration 1444, loss = 0.61540698\n",
      "Iteration 1445, loss = 0.61540951\n",
      "Iteration 1446, loss = 0.61542576\n",
      "Iteration 1447, loss = 0.61540742\n",
      "Iteration 1448, loss = 0.61540457\n",
      "Iteration 1449, loss = 0.61540350\n",
      "Iteration 1450, loss = 0.61541611\n",
      "Iteration 1451, loss = 0.61540903\n",
      "Iteration 1452, loss = 0.61540769\n",
      "Iteration 1453, loss = 0.61540618\n",
      "Iteration 1454, loss = 0.61541333\n",
      "Iteration 1455, loss = 0.61540863\n",
      "Iteration 1456, loss = 0.61540806\n",
      "Iteration 1457, loss = 0.61540816\n",
      "Iteration 1458, loss = 0.61540833\n",
      "Iteration 1459, loss = 0.61540901\n",
      "Iteration 1460, loss = 0.61540865\n",
      "Iteration 1461, loss = 0.61540937\n",
      "Iteration 1462, loss = 0.61540884\n",
      "Iteration 1463, loss = 0.61540892\n",
      "Iteration 1464, loss = 0.61541014\n",
      "Iteration 1465, loss = 0.61541719\n",
      "Iteration 1466, loss = 0.61541952\n",
      "Iteration 1467, loss = 0.61540983\n",
      "Iteration 1468, loss = 0.61540924\n",
      "Iteration 1469, loss = 0.61540736\n",
      "Iteration 1470, loss = 0.61540996\n",
      "Iteration 1471, loss = 0.61540827\n",
      "Iteration 1472, loss = 0.61540755\n",
      "Iteration 1473, loss = 0.61540674\n",
      "Iteration 1474, loss = 0.61540507\n",
      "Iteration 1475, loss = 0.61541148\n",
      "Iteration 1476, loss = 0.61540627\n",
      "Iteration 1477, loss = 0.61540477\n",
      "Iteration 1478, loss = 0.61541844\n",
      "Iteration 1479, loss = 0.61540809\n",
      "Iteration 1480, loss = 0.61540795\n",
      "Iteration 1481, loss = 0.61540770\n",
      "Iteration 1482, loss = 0.61541046\n",
      "Iteration 1483, loss = 0.61540751\n",
      "Iteration 1484, loss = 0.61541365\n",
      "Iteration 1485, loss = 0.61540830\n",
      "Iteration 1486, loss = 0.61540983\n",
      "Iteration 1487, loss = 0.61540824\n",
      "Iteration 1488, loss = 0.61540777\n",
      "Iteration 1489, loss = 0.61540800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1490, loss = 0.61540831\n",
      "Iteration 1491, loss = 0.61540792\n",
      "Iteration 1492, loss = 0.61540800\n",
      "Iteration 1493, loss = 0.61540675\n",
      "Iteration 1494, loss = 0.61540648\n",
      "Iteration 1495, loss = 0.61540853\n",
      "Iteration 1496, loss = 0.61540897\n",
      "Iteration 1497, loss = 0.61540752\n",
      "Iteration 1498, loss = 0.61540746\n",
      "Iteration 1499, loss = 0.61541113\n",
      "Iteration 1500, loss = 0.61540732\n",
      "Iteration 1501, loss = 0.61540442\n",
      "Iteration 1502, loss = 0.61540769\n",
      "Iteration 1503, loss = 0.61540673\n",
      "Iteration 1504, loss = 0.61540865\n",
      "Iteration 1505, loss = 0.61540923\n",
      "Iteration 1506, loss = 0.61541198\n",
      "Iteration 1507, loss = 0.61541150\n",
      "Iteration 1508, loss = 0.61540877\n",
      "Iteration 1509, loss = 0.61540620\n",
      "Iteration 1510, loss = 0.61540503\n",
      "Iteration 1511, loss = 0.61540054\n",
      "Iteration 1512, loss = 0.61540235\n",
      "Iteration 1513, loss = 0.61540762\n",
      "Iteration 1514, loss = 0.61541263\n",
      "Iteration 1515, loss = 0.61540803\n",
      "Iteration 1516, loss = 0.61541193\n",
      "Iteration 1517, loss = 0.61540836\n",
      "Iteration 1518, loss = 0.61540795\n",
      "Iteration 1519, loss = 0.61540753\n",
      "Iteration 1520, loss = 0.61540577\n",
      "Iteration 1521, loss = 0.61541576\n",
      "Iteration 1522, loss = 0.61540706\n",
      "Iteration 1523, loss = 0.61541037\n",
      "Iteration 1524, loss = 0.61540979\n",
      "Iteration 1525, loss = 0.61540704\n",
      "Iteration 1526, loss = 0.61540564\n",
      "Iteration 1527, loss = 0.61540299\n",
      "Iteration 1528, loss = 0.61540120\n",
      "Iteration 1529, loss = 0.61541493\n",
      "Iteration 1530, loss = 0.61541166\n",
      "Iteration 1531, loss = 0.61541169\n",
      "Iteration 1532, loss = 0.61541006\n",
      "Iteration 1533, loss = 0.61540499\n",
      "Iteration 1534, loss = 0.61541344\n",
      "Iteration 1535, loss = 0.61540726\n",
      "Iteration 1536, loss = 0.61543138\n",
      "Iteration 1537, loss = 0.61542052\n",
      "Iteration 1538, loss = 0.61541933\n",
      "Iteration 1539, loss = 0.61542720\n",
      "Iteration 1540, loss = 0.61542499\n",
      "Iteration 1541, loss = 0.61543301\n",
      "Iteration 1542, loss = 0.61542489\n",
      "Iteration 1543, loss = 0.61541662\n",
      "Iteration 1544, loss = 0.61542319\n",
      "Iteration 1545, loss = 0.61541479\n",
      "Iteration 1546, loss = 0.61541268\n",
      "Iteration 1547, loss = 0.61540898\n",
      "Iteration 1548, loss = 0.61540906\n",
      "Iteration 1549, loss = 0.61541223\n",
      "Iteration 1550, loss = 0.61540903\n",
      "Iteration 1551, loss = 0.61541304\n",
      "Iteration 1552, loss = 0.61541058\n",
      "Iteration 1553, loss = 0.61540623\n",
      "Iteration 1554, loss = 0.61540110\n",
      "Iteration 1555, loss = 0.61541023\n",
      "Iteration 1556, loss = 0.61540746\n",
      "Iteration 1557, loss = 0.61541576\n",
      "Iteration 1558, loss = 0.61541491\n",
      "Iteration 1559, loss = 0.61541255\n",
      "Iteration 1560, loss = 0.61542674\n",
      "Iteration 1561, loss = 0.61541831\n",
      "Iteration 1562, loss = 0.61541780\n",
      "Iteration 1563, loss = 0.61542087\n",
      "Iteration 1564, loss = 0.61542340\n",
      "Iteration 1565, loss = 0.61542245\n",
      "Iteration 1566, loss = 0.61542751\n",
      "Iteration 1567, loss = 0.61542868\n",
      "Iteration 1568, loss = 0.61542528\n",
      "Iteration 1569, loss = 0.61542712\n",
      "Iteration 1570, loss = 0.61542385\n",
      "Iteration 1571, loss = 0.61542390\n",
      "Iteration 1572, loss = 0.61542429\n",
      "Iteration 1573, loss = 0.61542356\n",
      "Iteration 1574, loss = 0.61542480\n",
      "Iteration 1575, loss = 0.61542564\n",
      "Iteration 1576, loss = 0.61542588\n",
      "Iteration 1577, loss = 0.61542547\n",
      "Iteration 1578, loss = 0.61543012\n",
      "Iteration 1579, loss = 0.61542892\n",
      "Iteration 1580, loss = 0.61543164\n",
      "Iteration 1581, loss = 0.61543093\n",
      "Iteration 1582, loss = 0.61543390\n",
      "Iteration 1583, loss = 0.61543836\n",
      "Iteration 1584, loss = 0.61544197\n",
      "Iteration 1585, loss = 0.61544181\n",
      "Iteration 1586, loss = 0.61544325\n",
      "Iteration 1587, loss = 0.61544383\n",
      "Iteration 1588, loss = 0.61544426\n",
      "Iteration 1589, loss = 0.61544596\n",
      "Iteration 1590, loss = 0.61544843\n",
      "Iteration 1591, loss = 0.61544702\n",
      "Iteration 1592, loss = 0.61544697\n",
      "Iteration 1593, loss = 0.61544482\n",
      "Iteration 1594, loss = 0.61544480\n",
      "Iteration 1595, loss = 0.61544485\n",
      "Iteration 1596, loss = 0.61544237\n",
      "Iteration 1597, loss = 0.61544268\n",
      "Iteration 1598, loss = 0.61544662\n",
      "Iteration 1599, loss = 0.61544635\n",
      "Iteration 1600, loss = 0.61544661\n",
      "Iteration 1601, loss = 0.61545080\n",
      "Iteration 1602, loss = 0.61545402\n",
      "Iteration 1603, loss = 0.61545401\n",
      "Iteration 1604, loss = 0.61545691\n",
      "Iteration 1605, loss = 0.61545582\n",
      "Iteration 1606, loss = 0.61545831\n",
      "Iteration 1607, loss = 0.61545341\n",
      "Iteration 1608, loss = 0.61545265\n",
      "Iteration 1609, loss = 0.61545313\n",
      "Iteration 1610, loss = 0.61545308\n",
      "Iteration 1611, loss = 0.61545367\n",
      "Iteration 1612, loss = 0.61545890\n",
      "Iteration 1613, loss = 0.61545840\n",
      "Iteration 1614, loss = 0.61545963\n",
      "Iteration 1615, loss = 0.61545716\n",
      "Iteration 1616, loss = 0.61545671\n",
      "Iteration 1617, loss = 0.61546300\n",
      "Iteration 1618, loss = 0.61545412\n",
      "Iteration 1619, loss = 0.61545535\n",
      "Iteration 1620, loss = 0.61545829\n",
      "Iteration 1621, loss = 0.61545713\n",
      "Iteration 1622, loss = 0.61545889\n",
      "Iteration 1623, loss = 0.61545229\n",
      "Iteration 1624, loss = 0.61544925\n",
      "Iteration 1625, loss = 0.61544141\n",
      "Iteration 1626, loss = 0.61543964\n",
      "Iteration 1627, loss = 0.61543041\n",
      "Iteration 1628, loss = 0.61542168\n",
      "Iteration 1629, loss = 0.61545198\n",
      "Iteration 1630, loss = 0.61542705\n",
      "Iteration 1631, loss = 0.61542651\n",
      "Iteration 1632, loss = 0.61542199\n",
      "Iteration 1633, loss = 0.61542111\n",
      "Iteration 1634, loss = 0.61542358\n",
      "Iteration 1635, loss = 0.61542485\n",
      "Iteration 1636, loss = 0.61542587\n",
      "Iteration 1637, loss = 0.61545359\n",
      "Iteration 1638, loss = 0.61543555\n",
      "Iteration 1639, loss = 0.61543375\n",
      "Iteration 1640, loss = 0.61544091\n",
      "Iteration 1641, loss = 0.61543135\n",
      "Iteration 1642, loss = 0.61543119\n",
      "Iteration 1643, loss = 0.61543048\n",
      "Iteration 1644, loss = 0.61543208\n",
      "Iteration 1645, loss = 0.61543211\n",
      "Iteration 1646, loss = 0.61542665\n",
      "Iteration 1647, loss = 0.61542710\n",
      "Iteration 1648, loss = 0.61542404\n",
      "Iteration 1649, loss = 0.61542495\n",
      "Iteration 1650, loss = 0.61542253\n",
      "Iteration 1651, loss = 0.61542330\n",
      "Iteration 1652, loss = 0.61542117\n",
      "Iteration 1653, loss = 0.61542146\n",
      "Iteration 1654, loss = 0.61541764\n",
      "Iteration 1655, loss = 0.61541825\n",
      "Iteration 1656, loss = 0.61541521\n",
      "Iteration 1657, loss = 0.61542037\n",
      "Iteration 1658, loss = 0.61541430\n",
      "Iteration 1659, loss = 0.61541689\n",
      "Iteration 1660, loss = 0.61541346\n",
      "Iteration 1661, loss = 0.61541309\n",
      "Iteration 1662, loss = 0.61541225\n",
      "Iteration 1663, loss = 0.61541719\n",
      "Iteration 1664, loss = 0.61542372\n",
      "Iteration 1665, loss = 0.61542100\n",
      "Iteration 1666, loss = 0.61542009\n",
      "Iteration 1667, loss = 0.61542052\n",
      "Iteration 1668, loss = 0.61541819\n",
      "Iteration 1669, loss = 0.61542135\n",
      "Iteration 1670, loss = 0.61542093\n",
      "Iteration 1671, loss = 0.61541956\n",
      "Iteration 1672, loss = 0.61541999\n",
      "Iteration 1673, loss = 0.61542456\n",
      "Iteration 1674, loss = 0.61542443\n",
      "Iteration 1675, loss = 0.61542634\n",
      "Iteration 1676, loss = 0.61542555\n",
      "Iteration 1677, loss = 0.61542508\n",
      "Iteration 1678, loss = 0.61542500\n",
      "Iteration 1679, loss = 0.61542571\n",
      "Iteration 1680, loss = 0.61542564\n",
      "Iteration 1681, loss = 0.61542705\n",
      "Iteration 1682, loss = 0.61542738\n",
      "Iteration 1683, loss = 0.61542740\n",
      "Iteration 1684, loss = 0.61542829\n",
      "Iteration 1685, loss = 0.61543126\n",
      "Iteration 1686, loss = 0.61542734\n",
      "Iteration 1687, loss = 0.61542458\n",
      "Iteration 1688, loss = 0.61543014\n",
      "Iteration 1689, loss = 0.61542469\n",
      "Iteration 1690, loss = 0.61542324\n",
      "Iteration 1691, loss = 0.61542188\n",
      "Iteration 1692, loss = 0.61542480\n",
      "Iteration 1693, loss = 0.61542158\n",
      "Iteration 1694, loss = 0.61542151\n",
      "Iteration 1695, loss = 0.61542227\n",
      "Iteration 1696, loss = 0.61542319\n",
      "Iteration 1697, loss = 0.61542363\n",
      "Iteration 1698, loss = 0.61542189\n",
      "Iteration 1699, loss = 0.61541934\n",
      "Iteration 1700, loss = 0.61542184\n",
      "Iteration 1701, loss = 0.61541750\n",
      "Iteration 1702, loss = 0.61541542\n",
      "Iteration 1703, loss = 0.61541595\n",
      "Iteration 1704, loss = 0.61541118\n",
      "Iteration 1705, loss = 0.61541515\n",
      "Iteration 1706, loss = 0.61540737\n",
      "Iteration 1707, loss = 0.61541048\n",
      "Iteration 1708, loss = 0.61540280\n",
      "Iteration 1709, loss = 0.61540307\n",
      "Iteration 1710, loss = 0.61539735\n",
      "Iteration 1711, loss = 0.61541362\n",
      "Iteration 1712, loss = 0.61541387\n",
      "Iteration 1713, loss = 0.61540690\n",
      "Iteration 1714, loss = 0.61541285\n",
      "Iteration 1715, loss = 0.61540719\n",
      "Iteration 1716, loss = 0.61540935\n",
      "Iteration 1717, loss = 0.61540647\n",
      "Iteration 1718, loss = 0.61540597\n",
      "Iteration 1719, loss = 0.61540517\n",
      "Iteration 1720, loss = 0.61540372\n",
      "Iteration 1721, loss = 0.61541033\n",
      "Iteration 1722, loss = 0.61541383\n",
      "Iteration 1723, loss = 0.61541195\n",
      "Iteration 1724, loss = 0.61540718\n",
      "Iteration 1725, loss = 0.61541331\n",
      "Iteration 1726, loss = 0.61540635\n",
      "Iteration 1727, loss = 0.61540651\n",
      "Iteration 1728, loss = 0.61541236\n",
      "Iteration 1729, loss = 0.61540732\n",
      "Iteration 1730, loss = 0.61540189\n",
      "Iteration 1731, loss = 0.61539705\n",
      "Iteration 1732, loss = 0.61540404\n",
      "Iteration 1733, loss = 0.61542025\n",
      "Iteration 1734, loss = 0.61540705\n",
      "Iteration 1735, loss = 0.61540989\n",
      "Iteration 1736, loss = 0.61540797\n",
      "Iteration 1737, loss = 0.61540941\n",
      "Iteration 1738, loss = 0.61540794\n",
      "Iteration 1739, loss = 0.61541620\n",
      "Iteration 1740, loss = 0.61540776\n",
      "Iteration 1741, loss = 0.61540963\n",
      "Iteration 1742, loss = 0.61543468\n",
      "Iteration 1743, loss = 0.61541663\n",
      "Iteration 1744, loss = 0.61541633\n",
      "Iteration 1745, loss = 0.61541720\n",
      "Iteration 1746, loss = 0.61541628\n",
      "Iteration 1747, loss = 0.61541521\n",
      "Iteration 1748, loss = 0.61541280\n",
      "Iteration 1749, loss = 0.61540947\n",
      "Iteration 1750, loss = 0.61541209\n",
      "Iteration 1751, loss = 0.61541463\n",
      "Iteration 1752, loss = 0.61541095\n",
      "Iteration 1753, loss = 0.61541188\n",
      "Iteration 1754, loss = 0.61540925\n",
      "Iteration 1755, loss = 0.61541085\n",
      "Iteration 1756, loss = 0.61541099\n",
      "Iteration 1757, loss = 0.61540931\n",
      "Iteration 1758, loss = 0.61540957\n",
      "Iteration 1759, loss = 0.61541062\n",
      "Iteration 1760, loss = 0.61541103\n",
      "Iteration 1761, loss = 0.61541100\n",
      "Iteration 1762, loss = 0.61541302\n",
      "Iteration 1763, loss = 0.61541196\n",
      "Iteration 1764, loss = 0.61541184\n",
      "Iteration 1765, loss = 0.61541163\n",
      "Iteration 1766, loss = 0.61541395\n",
      "Iteration 1767, loss = 0.61541103\n",
      "Iteration 1768, loss = 0.61541159\n",
      "Iteration 1769, loss = 0.61540944\n",
      "Iteration 1770, loss = 0.61541068\n",
      "Iteration 1771, loss = 0.61541193\n",
      "Iteration 1772, loss = 0.61541128\n",
      "Iteration 1773, loss = 0.61541026\n",
      "Iteration 1774, loss = 0.61541119\n",
      "Iteration 1775, loss = 0.61542541\n",
      "Iteration 1776, loss = 0.61541474\n",
      "Iteration 1777, loss = 0.61541371\n",
      "Iteration 1778, loss = 0.61541423\n",
      "Iteration 1779, loss = 0.61541483\n",
      "Iteration 1780, loss = 0.61541247\n",
      "Iteration 1781, loss = 0.61541245\n",
      "Iteration 1782, loss = 0.61542319\n",
      "Iteration 1783, loss = 0.61541462\n",
      "Iteration 1784, loss = 0.61541434\n",
      "Iteration 1785, loss = 0.61541575\n",
      "Iteration 1786, loss = 0.61541254\n",
      "Iteration 1787, loss = 0.61541312\n",
      "Iteration 1788, loss = 0.61541297\n",
      "Iteration 1789, loss = 0.61541202\n",
      "Iteration 1790, loss = 0.61540994\n",
      "Iteration 1791, loss = 0.61541977\n",
      "Iteration 1792, loss = 0.61541382\n",
      "Iteration 1793, loss = 0.61541751\n",
      "Iteration 1794, loss = 0.61541683\n",
      "Iteration 1795, loss = 0.61541539\n",
      "Iteration 1796, loss = 0.61541748\n",
      "Iteration 1797, loss = 0.61541496\n",
      "Iteration 1798, loss = 0.61541409\n",
      "Iteration 1799, loss = 0.61541520\n",
      "Iteration 1800, loss = 0.61541741\n",
      "Iteration 1801, loss = 0.61542036\n",
      "Iteration 1802, loss = 0.61541882\n",
      "Iteration 1803, loss = 0.61541927\n",
      "Iteration 1804, loss = 0.61542649\n",
      "Iteration 1805, loss = 0.61541688\n",
      "Iteration 1806, loss = 0.61541732\n",
      "Iteration 1807, loss = 0.61542546\n",
      "Iteration 1808, loss = 0.61542430\n",
      "Iteration 1809, loss = 0.61541744\n",
      "Iteration 1810, loss = 0.61542079\n",
      "Iteration 1811, loss = 0.61541695\n",
      "Iteration 1812, loss = 0.61541194\n",
      "Iteration 1813, loss = 0.61541000\n",
      "Iteration 1814, loss = 0.61540219\n",
      "Iteration 1815, loss = 0.61541347\n",
      "Iteration 1816, loss = 0.61541111\n",
      "Iteration 1817, loss = 0.61540777\n",
      "Iteration 1818, loss = 0.61540351\n",
      "Iteration 1819, loss = 0.61540305\n",
      "Iteration 1820, loss = 0.61541122\n",
      "Iteration 1821, loss = 0.61540536\n",
      "Iteration 1822, loss = 0.61540954\n",
      "Iteration 1823, loss = 0.61540680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1824, loss = 0.61540649\n",
      "Iteration 1825, loss = 0.61540638\n",
      "Iteration 1826, loss = 0.61540532\n",
      "Iteration 1827, loss = 0.61541235\n",
      "Iteration 1828, loss = 0.61540730\n",
      "Iteration 1829, loss = 0.61540664\n",
      "Iteration 1830, loss = 0.61540412\n",
      "Iteration 1831, loss = 0.61541160\n",
      "Iteration 1832, loss = 0.61540783\n",
      "Iteration 1833, loss = 0.61541339\n",
      "Iteration 1834, loss = 0.61541006\n",
      "Iteration 1835, loss = 0.61541387\n",
      "Iteration 1836, loss = 0.61542362\n",
      "Iteration 1837, loss = 0.61541679\n",
      "Iteration 1838, loss = 0.61541593\n",
      "Iteration 1839, loss = 0.61541659\n",
      "Iteration 1840, loss = 0.61541689\n",
      "Iteration 1841, loss = 0.61541889\n",
      "Iteration 1842, loss = 0.61541997\n",
      "Iteration 1843, loss = 0.61542030\n",
      "Iteration 1844, loss = 0.61542051\n",
      "Iteration 1845, loss = 0.61542174\n",
      "Iteration 1846, loss = 0.61543493\n",
      "Iteration 1847, loss = 0.61542957\n",
      "Iteration 1848, loss = 0.61542395\n",
      "Iteration 1849, loss = 0.61542555\n",
      "Iteration 1850, loss = 0.61542414\n",
      "Iteration 1851, loss = 0.61542886\n",
      "Iteration 1852, loss = 0.61542430\n",
      "Iteration 1853, loss = 0.61542557\n",
      "Iteration 1854, loss = 0.61542138\n",
      "Iteration 1855, loss = 0.61541765\n",
      "Iteration 1856, loss = 0.61541573\n",
      "Iteration 1857, loss = 0.61541305\n",
      "Iteration 1858, loss = 0.61541503\n",
      "Iteration 1859, loss = 0.61541123\n",
      "Iteration 1860, loss = 0.61541361\n",
      "Iteration 1861, loss = 0.61540831\n",
      "Iteration 1862, loss = 0.61540976\n",
      "Iteration 1863, loss = 0.61541183\n",
      "Iteration 1864, loss = 0.61540808\n",
      "Iteration 1865, loss = 0.61540992\n",
      "Iteration 1866, loss = 0.61540817\n",
      "Iteration 1867, loss = 0.61540812\n",
      "Iteration 1868, loss = 0.61540948\n",
      "Iteration 1869, loss = 0.61540765\n",
      "Iteration 1870, loss = 0.61540796\n",
      "Iteration 1871, loss = 0.61540771\n",
      "Iteration 1872, loss = 0.61540680\n",
      "Iteration 1873, loss = 0.61540518\n",
      "Iteration 1874, loss = 0.61541053\n",
      "Iteration 1875, loss = 0.61540686\n",
      "Iteration 1876, loss = 0.61540817\n",
      "Iteration 1877, loss = 0.61540668\n",
      "Iteration 1878, loss = 0.61540718\n",
      "Iteration 1879, loss = 0.61540790\n",
      "Iteration 1880, loss = 0.61540669\n",
      "Iteration 1881, loss = 0.61540431\n",
      "Iteration 1882, loss = 0.61540255\n",
      "Iteration 1883, loss = 0.61541174\n",
      "Iteration 1884, loss = 0.61540671\n",
      "Iteration 1885, loss = 0.61541107\n",
      "Iteration 1886, loss = 0.61540840\n",
      "Iteration 1887, loss = 0.61541160\n",
      "Iteration 1888, loss = 0.61540924\n",
      "Iteration 1889, loss = 0.61540892\n",
      "Iteration 1890, loss = 0.61540816\n",
      "Iteration 1891, loss = 0.61540863\n",
      "Iteration 1892, loss = 0.61540726\n",
      "Iteration 1893, loss = 0.61540892\n",
      "Iteration 1894, loss = 0.61540987\n",
      "Iteration 1895, loss = 0.61541332\n",
      "Iteration 1896, loss = 0.61541281\n",
      "Iteration 1897, loss = 0.61540664\n",
      "Iteration 1898, loss = 0.61540638\n",
      "Iteration 1899, loss = 0.61540614\n",
      "Iteration 1900, loss = 0.61540400\n",
      "Iteration 1901, loss = 0.61540918\n",
      "Iteration 1902, loss = 0.61540266\n",
      "Iteration 1903, loss = 0.61541390\n",
      "Iteration 1904, loss = 0.61540587\n",
      "Iteration 1905, loss = 0.61540879\n",
      "Iteration 1906, loss = 0.61541035\n",
      "Iteration 1907, loss = 0.61540941\n",
      "Iteration 1908, loss = 0.61540841\n",
      "Iteration 1909, loss = 0.61540807\n",
      "Iteration 1910, loss = 0.61540784\n",
      "Iteration 1911, loss = 0.61540794\n",
      "Iteration 1912, loss = 0.61541384\n",
      "Iteration 1913, loss = 0.61540763\n",
      "Iteration 1914, loss = 0.61540867\n",
      "Iteration 1915, loss = 0.61540730\n",
      "Iteration 1916, loss = 0.61540654\n",
      "Iteration 1917, loss = 0.61540907\n",
      "Iteration 1918, loss = 0.61540764\n",
      "Iteration 1919, loss = 0.61540671\n",
      "Iteration 1920, loss = 0.61540632\n",
      "Iteration 1921, loss = 0.61540427\n",
      "Iteration 1922, loss = 0.61541006\n",
      "Iteration 1923, loss = 0.61540996\n",
      "Iteration 1924, loss = 0.61540845\n",
      "Iteration 1925, loss = 0.61540873\n",
      "Iteration 1926, loss = 0.61540911\n",
      "Iteration 1927, loss = 0.61541253\n",
      "Iteration 1928, loss = 0.61541087\n",
      "Iteration 1929, loss = 0.61541136\n",
      "Iteration 1930, loss = 0.61541104\n",
      "Iteration 1931, loss = 0.61541080\n",
      "Iteration 1932, loss = 0.61540977\n",
      "Iteration 1933, loss = 0.61540987\n",
      "Iteration 1934, loss = 0.61540870\n",
      "Iteration 1935, loss = 0.61540892\n",
      "Iteration 1936, loss = 0.61541295\n",
      "Iteration 1937, loss = 0.61540800\n",
      "Iteration 1938, loss = 0.61540806\n",
      "Iteration 1939, loss = 0.61541398\n",
      "Iteration 1940, loss = 0.61540974\n",
      "Iteration 1941, loss = 0.61540914\n",
      "Iteration 1942, loss = 0.61541162\n",
      "Iteration 1943, loss = 0.61541006\n",
      "Iteration 1944, loss = 0.61540710\n",
      "Iteration 1945, loss = 0.61540803\n",
      "Iteration 1946, loss = 0.61540454\n",
      "Iteration 1947, loss = 0.61540524\n",
      "Iteration 1948, loss = 0.61540330\n",
      "Iteration 1949, loss = 0.61541075\n",
      "Iteration 1950, loss = 0.61540446\n",
      "Iteration 1951, loss = 0.61540169\n",
      "Iteration 1952, loss = 0.61540441\n",
      "Iteration 1953, loss = 0.61541691\n",
      "Iteration 1954, loss = 0.61541040\n",
      "Iteration 1955, loss = 0.61540882\n",
      "Iteration 1956, loss = 0.61540950\n",
      "Iteration 1957, loss = 0.61540936\n",
      "Iteration 1958, loss = 0.61540779\n",
      "Iteration 1959, loss = 0.61540787\n",
      "Iteration 1960, loss = 0.61540836\n",
      "Iteration 1961, loss = 0.61540861\n",
      "Iteration 1962, loss = 0.61540722\n",
      "Iteration 1963, loss = 0.61540872\n",
      "Iteration 1964, loss = 0.61540657\n",
      "Iteration 1965, loss = 0.61540880\n",
      "Iteration 1966, loss = 0.61540608\n",
      "Iteration 1967, loss = 0.61540390\n",
      "Iteration 1968, loss = 0.61540672\n",
      "Iteration 1969, loss = 0.61540521\n",
      "Iteration 1970, loss = 0.61540407\n",
      "Iteration 1971, loss = 0.61540127\n",
      "Iteration 1972, loss = 0.61541277\n",
      "Iteration 1973, loss = 0.61540811\n",
      "Iteration 1974, loss = 0.61541777\n",
      "Iteration 1975, loss = 0.61540773\n",
      "Iteration 1976, loss = 0.61540636\n",
      "Iteration 1977, loss = 0.61540623\n",
      "Iteration 1978, loss = 0.61541030\n",
      "Iteration 1979, loss = 0.61541202\n",
      "Iteration 1980, loss = 0.61540573\n",
      "Iteration 1981, loss = 0.61540381\n",
      "Iteration 1982, loss = 0.61541220\n",
      "Iteration 1983, loss = 0.61540951\n",
      "Iteration 1984, loss = 0.61540782\n",
      "Iteration 1985, loss = 0.61540708\n",
      "Iteration 1986, loss = 0.61541316\n",
      "Iteration 1987, loss = 0.61541047\n",
      "Iteration 1988, loss = 0.61541265\n",
      "Iteration 1989, loss = 0.61541013\n",
      "Iteration 1990, loss = 0.61540816\n",
      "Iteration 1991, loss = 0.61541320\n",
      "Iteration 1992, loss = 0.61541086\n",
      "Iteration 1993, loss = 0.61540817\n",
      "Iteration 1994, loss = 0.61540813\n",
      "Iteration 1995, loss = 0.61540860\n",
      "Iteration 1996, loss = 0.61540766\n",
      "Iteration 1997, loss = 0.61540754\n",
      "Iteration 1998, loss = 0.61540713\n",
      "Iteration 1999, loss = 0.61541172\n",
      "Iteration 2000, loss = 0.61541356\n",
      "Iteration 2001, loss = 0.61541048\n",
      "Iteration 2002, loss = 0.61540460\n",
      "Iteration 2003, loss = 0.61540472\n",
      "Iteration 2004, loss = 0.61540174\n",
      "Iteration 2005, loss = 0.61540213\n",
      "Iteration 2006, loss = 0.61540666\n",
      "Iteration 2007, loss = 0.61540927\n",
      "Iteration 2008, loss = 0.61540928\n",
      "Iteration 2009, loss = 0.61540714\n",
      "Iteration 2010, loss = 0.61540440\n",
      "Iteration 2011, loss = 0.61540976\n",
      "Iteration 2012, loss = 0.61542066\n",
      "Iteration 2013, loss = 0.61541197\n",
      "Iteration 2014, loss = 0.61542103\n",
      "Iteration 2015, loss = 0.61541338\n",
      "Iteration 2016, loss = 0.61541773\n",
      "Iteration 2017, loss = 0.61541934\n",
      "Iteration 2018, loss = 0.61541805\n",
      "Iteration 2019, loss = 0.61541996\n",
      "Iteration 2020, loss = 0.61542052\n",
      "Iteration 2021, loss = 0.61541596\n",
      "Iteration 2022, loss = 0.61541583\n",
      "Iteration 2023, loss = 0.61541514\n",
      "Iteration 2024, loss = 0.61541409\n",
      "Iteration 2025, loss = 0.61541241\n",
      "Iteration 2026, loss = 0.61541059\n",
      "Iteration 2027, loss = 0.61541033\n",
      "Iteration 2028, loss = 0.61541645\n",
      "Iteration 2029, loss = 0.61540914\n",
      "Iteration 2030, loss = 0.61541065\n",
      "Iteration 2031, loss = 0.61540830\n",
      "Iteration 2032, loss = 0.61540786\n",
      "Iteration 2033, loss = 0.61541219\n",
      "Iteration 2034, loss = 0.61540882\n",
      "Iteration 2035, loss = 0.61540890\n",
      "Iteration 2036, loss = 0.61540877\n",
      "Iteration 2037, loss = 0.61540853\n",
      "Iteration 2038, loss = 0.61540852\n",
      "Iteration 2039, loss = 0.61541066\n",
      "Iteration 2040, loss = 0.61540706\n",
      "Iteration 2041, loss = 0.61540816\n",
      "Iteration 2042, loss = 0.61541145\n",
      "Iteration 2043, loss = 0.61540865\n",
      "Iteration 2044, loss = 0.61540994\n",
      "Iteration 2045, loss = 0.61540879\n",
      "Iteration 2046, loss = 0.61540938\n",
      "Iteration 2047, loss = 0.61540738\n",
      "Iteration 2048, loss = 0.61540749\n",
      "Iteration 2049, loss = 0.61541180\n",
      "Iteration 2050, loss = 0.61540878\n",
      "Iteration 2051, loss = 0.61540974\n",
      "Iteration 2052, loss = 0.61541166\n",
      "Iteration 2053, loss = 0.61540575\n",
      "Iteration 2054, loss = 0.61541392\n",
      "Iteration 2055, loss = 0.61540659\n",
      "Iteration 2056, loss = 0.61540487\n",
      "Iteration 2057, loss = 0.61540703\n",
      "Iteration 2058, loss = 0.61540344\n",
      "Iteration 2059, loss = 0.61541287\n",
      "Iteration 2060, loss = 0.61540543\n",
      "Iteration 2061, loss = 0.61540768\n",
      "Iteration 2062, loss = 0.61540797\n",
      "Iteration 2063, loss = 0.61540651\n",
      "Iteration 2064, loss = 0.61540670\n",
      "Iteration 2065, loss = 0.61540652\n",
      "Iteration 2066, loss = 0.61540653\n",
      "Iteration 2067, loss = 0.61540673\n",
      "Iteration 2068, loss = 0.61540724\n",
      "Iteration 2069, loss = 0.61540683\n",
      "Iteration 2070, loss = 0.61541031\n",
      "Iteration 2071, loss = 0.61540129\n",
      "Iteration 2072, loss = 0.61541269\n",
      "Iteration 2073, loss = 0.61540941\n",
      "Iteration 2074, loss = 0.61540829\n",
      "Iteration 2075, loss = 0.61540933\n",
      "Iteration 2076, loss = 0.61540877\n",
      "Iteration 2077, loss = 0.61540901\n",
      "Iteration 2078, loss = 0.61540858\n",
      "Iteration 2079, loss = 0.61541586\n",
      "Iteration 2080, loss = 0.61540775\n",
      "Iteration 2081, loss = 0.61541021\n",
      "Iteration 2082, loss = 0.61540880\n",
      "Iteration 2083, loss = 0.61541200\n",
      "Iteration 2084, loss = 0.61541046\n",
      "Iteration 2085, loss = 0.61540907\n",
      "Iteration 2086, loss = 0.61541250\n",
      "Training loss did not improve more than tol=0.000100 for 2000 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.63414402\n",
      "Iteration 2, loss = 0.63380121\n",
      "Iteration 3, loss = 0.63355665\n",
      "Iteration 4, loss = 0.63328395\n",
      "Iteration 5, loss = 0.63302089\n",
      "Iteration 6, loss = 0.63273163\n",
      "Iteration 7, loss = 0.63246151\n",
      "Iteration 8, loss = 0.63220223\n",
      "Iteration 9, loss = 0.63198407\n",
      "Iteration 10, loss = 0.63176326\n",
      "Iteration 11, loss = 0.63153425\n",
      "Iteration 12, loss = 0.63130966\n",
      "Iteration 13, loss = 0.63108621\n",
      "Iteration 14, loss = 0.63086834\n",
      "Iteration 15, loss = 0.63068500\n",
      "Iteration 16, loss = 0.63049087\n",
      "Iteration 17, loss = 0.63030034\n",
      "Iteration 18, loss = 0.63012734\n",
      "Iteration 19, loss = 0.62997379\n",
      "Iteration 20, loss = 0.62978141\n",
      "Iteration 21, loss = 0.62960355\n",
      "Iteration 22, loss = 0.62942557\n",
      "Iteration 23, loss = 0.62923864\n",
      "Iteration 24, loss = 0.62907630\n",
      "Iteration 25, loss = 0.62888651\n",
      "Iteration 26, loss = 0.62871878\n",
      "Iteration 27, loss = 0.62858761\n",
      "Iteration 28, loss = 0.62840181\n",
      "Iteration 29, loss = 0.62823455\n",
      "Iteration 30, loss = 0.62806949\n",
      "Iteration 31, loss = 0.62792935\n",
      "Iteration 32, loss = 0.62774914\n",
      "Iteration 33, loss = 0.62755620\n",
      "Iteration 34, loss = 0.62738862\n",
      "Iteration 35, loss = 0.62723442\n",
      "Iteration 36, loss = 0.62709810\n",
      "Iteration 37, loss = 0.62695255\n",
      "Iteration 38, loss = 0.62680581\n",
      "Iteration 39, loss = 0.62666962\n",
      "Iteration 40, loss = 0.62655175\n",
      "Iteration 41, loss = 0.62641159\n",
      "Iteration 42, loss = 0.62627846\n",
      "Iteration 43, loss = 0.62612553\n",
      "Iteration 44, loss = 0.62603771\n",
      "Iteration 45, loss = 0.62595475\n",
      "Iteration 46, loss = 0.62576144\n",
      "Iteration 47, loss = 0.62563732\n",
      "Iteration 48, loss = 0.62551001\n",
      "Iteration 49, loss = 0.62539820\n",
      "Iteration 50, loss = 0.62530632\n",
      "Iteration 51, loss = 0.62520885\n",
      "Iteration 52, loss = 0.62508920\n",
      "Iteration 53, loss = 0.62496788\n",
      "Iteration 54, loss = 0.62484240\n",
      "Iteration 55, loss = 0.62474629\n",
      "Iteration 56, loss = 0.62460878\n",
      "Iteration 57, loss = 0.62449014\n",
      "Iteration 58, loss = 0.62436938\n",
      "Iteration 59, loss = 0.62425941\n",
      "Iteration 60, loss = 0.62416266\n",
      "Iteration 61, loss = 0.62409747\n",
      "Iteration 62, loss = 0.62397525\n",
      "Iteration 63, loss = 0.62385762\n",
      "Iteration 64, loss = 0.62375515\n",
      "Iteration 65, loss = 0.62366141\n",
      "Iteration 66, loss = 0.62356187\n",
      "Iteration 67, loss = 0.62346806\n",
      "Iteration 68, loss = 0.62337751\n",
      "Iteration 69, loss = 0.62327322\n",
      "Iteration 70, loss = 0.62316827\n",
      "Iteration 71, loss = 0.62309858\n",
      "Iteration 72, loss = 0.62299115\n",
      "Iteration 73, loss = 0.62290478\n",
      "Iteration 74, loss = 0.62283153\n",
      "Iteration 75, loss = 0.62271658\n",
      "Iteration 76, loss = 0.62263567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 77, loss = 0.62255816\n",
      "Iteration 78, loss = 0.62248343\n",
      "Iteration 79, loss = 0.62239607\n",
      "Iteration 80, loss = 0.62234969\n",
      "Iteration 81, loss = 0.62224636\n",
      "Iteration 82, loss = 0.62215712\n",
      "Iteration 83, loss = 0.62207235\n",
      "Iteration 84, loss = 0.62198968\n",
      "Iteration 85, loss = 0.62190863\n",
      "Iteration 86, loss = 0.62184879\n",
      "Iteration 87, loss = 0.62175218\n",
      "Iteration 88, loss = 0.62167066\n",
      "Iteration 89, loss = 0.62161099\n",
      "Iteration 90, loss = 0.62153342\n",
      "Iteration 91, loss = 0.62145802\n",
      "Iteration 92, loss = 0.62140183\n",
      "Iteration 93, loss = 0.62140545\n",
      "Iteration 94, loss = 0.62134973\n",
      "Iteration 95, loss = 0.62133880\n",
      "Iteration 96, loss = 0.62128938\n",
      "Iteration 97, loss = 0.62130541\n",
      "Iteration 98, loss = 0.62123731\n",
      "Iteration 99, loss = 0.62118522\n",
      "Iteration 100, loss = 0.62112596\n",
      "Iteration 101, loss = 0.62103805\n",
      "Iteration 102, loss = 0.62094194\n",
      "Iteration 103, loss = 0.62087152\n",
      "Iteration 104, loss = 0.62074615\n",
      "Iteration 105, loss = 0.62068646\n",
      "Iteration 106, loss = 0.62061395\n",
      "Iteration 107, loss = 0.62050637\n",
      "Iteration 108, loss = 0.62044370\n",
      "Iteration 109, loss = 0.62037738\n",
      "Iteration 110, loss = 0.62031311\n",
      "Iteration 111, loss = 0.62024462\n",
      "Iteration 112, loss = 0.62018363\n",
      "Iteration 113, loss = 0.62011833\n",
      "Iteration 114, loss = 0.62005358\n",
      "Iteration 115, loss = 0.62000875\n",
      "Iteration 116, loss = 0.61993482\n",
      "Iteration 117, loss = 0.61987819\n",
      "Iteration 118, loss = 0.61983423\n",
      "Iteration 119, loss = 0.61977399\n",
      "Iteration 120, loss = 0.61973021\n",
      "Iteration 121, loss = 0.61972932\n",
      "Iteration 122, loss = 0.61967121\n",
      "Iteration 123, loss = 0.61960781\n",
      "Iteration 124, loss = 0.61957505\n",
      "Iteration 125, loss = 0.61947623\n",
      "Iteration 126, loss = 0.61942414\n",
      "Iteration 127, loss = 0.61936021\n",
      "Iteration 128, loss = 0.61933683\n",
      "Iteration 129, loss = 0.61925956\n",
      "Iteration 130, loss = 0.61927693\n",
      "Iteration 131, loss = 0.61918982\n",
      "Iteration 132, loss = 0.61912791\n",
      "Iteration 133, loss = 0.61908145\n",
      "Iteration 134, loss = 0.61902851\n",
      "Iteration 135, loss = 0.61897665\n",
      "Iteration 136, loss = 0.61894398\n",
      "Iteration 137, loss = 0.61894527\n",
      "Iteration 138, loss = 0.61887948\n",
      "Iteration 139, loss = 0.61882352\n",
      "Iteration 140, loss = 0.61877103\n",
      "Iteration 141, loss = 0.61872968\n",
      "Iteration 142, loss = 0.61871861\n",
      "Iteration 143, loss = 0.61864514\n",
      "Iteration 144, loss = 0.61860140\n",
      "Iteration 145, loss = 0.61857165\n",
      "Iteration 146, loss = 0.61853075\n",
      "Iteration 147, loss = 0.61849318\n",
      "Iteration 148, loss = 0.61845511\n",
      "Iteration 149, loss = 0.61841484\n",
      "Iteration 150, loss = 0.61837679\n",
      "Iteration 151, loss = 0.61832279\n",
      "Iteration 152, loss = 0.61831998\n",
      "Iteration 153, loss = 0.61829409\n",
      "Iteration 154, loss = 0.61826524\n",
      "Iteration 155, loss = 0.61823483\n",
      "Iteration 156, loss = 0.61817819\n",
      "Iteration 157, loss = 0.61813138\n",
      "Iteration 158, loss = 0.61811577\n",
      "Iteration 159, loss = 0.61810811\n",
      "Iteration 160, loss = 0.61803133\n",
      "Iteration 161, loss = 0.61799916\n",
      "Iteration 162, loss = 0.61796492\n",
      "Iteration 163, loss = 0.61792769\n",
      "Iteration 164, loss = 0.61793263\n",
      "Iteration 165, loss = 0.61788117\n",
      "Iteration 166, loss = 0.61788073\n",
      "Iteration 167, loss = 0.61784396\n",
      "Iteration 168, loss = 0.61784510\n",
      "Iteration 169, loss = 0.61780598\n",
      "Iteration 170, loss = 0.61776806\n",
      "Iteration 171, loss = 0.61772592\n",
      "Iteration 172, loss = 0.61770408\n",
      "Iteration 173, loss = 0.61765348\n",
      "Iteration 174, loss = 0.61760732\n",
      "Iteration 175, loss = 0.61766291\n",
      "Iteration 176, loss = 0.61756046\n",
      "Iteration 177, loss = 0.61752837\n",
      "Iteration 178, loss = 0.61748676\n",
      "Iteration 179, loss = 0.61749228\n",
      "Iteration 180, loss = 0.61749034\n",
      "Iteration 181, loss = 0.61745670\n",
      "Iteration 182, loss = 0.61745121\n",
      "Iteration 183, loss = 0.61745115\n",
      "Iteration 184, loss = 0.61744975\n",
      "Iteration 185, loss = 0.61745339\n",
      "Iteration 186, loss = 0.61744591\n",
      "Iteration 187, loss = 0.61742914\n",
      "Iteration 188, loss = 0.61741259\n",
      "Iteration 189, loss = 0.61741243\n",
      "Iteration 190, loss = 0.61739275\n",
      "Iteration 191, loss = 0.61737659\n",
      "Iteration 192, loss = 0.61733050\n",
      "Iteration 193, loss = 0.61729941\n",
      "Iteration 194, loss = 0.61723287\n",
      "Iteration 195, loss = 0.61719895\n",
      "Iteration 196, loss = 0.61715976\n",
      "Iteration 197, loss = 0.61713322\n",
      "Iteration 198, loss = 0.61708639\n",
      "Iteration 199, loss = 0.61709037\n",
      "Iteration 200, loss = 0.61709865\n",
      "Iteration 201, loss = 0.61705552\n",
      "Iteration 202, loss = 0.61703330\n",
      "Iteration 203, loss = 0.61696844\n",
      "Iteration 204, loss = 0.61693123\n",
      "Iteration 205, loss = 0.61687958\n",
      "Iteration 206, loss = 0.61684206\n",
      "Iteration 207, loss = 0.61678675\n",
      "Iteration 208, loss = 0.61676836\n",
      "Iteration 209, loss = 0.61673849\n",
      "Iteration 210, loss = 0.61673967\n",
      "Iteration 211, loss = 0.61673968\n",
      "Iteration 212, loss = 0.61668836\n",
      "Iteration 213, loss = 0.61666963\n",
      "Iteration 214, loss = 0.61665575\n",
      "Iteration 215, loss = 0.61659444\n",
      "Iteration 216, loss = 0.61656718\n",
      "Iteration 217, loss = 0.61660912\n",
      "Iteration 218, loss = 0.61656571\n",
      "Iteration 219, loss = 0.61657629\n",
      "Iteration 220, loss = 0.61656983\n",
      "Iteration 221, loss = 0.61657163\n",
      "Iteration 222, loss = 0.61659083\n",
      "Iteration 223, loss = 0.61654033\n",
      "Iteration 224, loss = 0.61650714\n",
      "Iteration 225, loss = 0.61645382\n",
      "Iteration 226, loss = 0.61642960\n",
      "Iteration 227, loss = 0.61640067\n",
      "Iteration 228, loss = 0.61637119\n",
      "Iteration 229, loss = 0.61636425\n",
      "Iteration 230, loss = 0.61629426\n",
      "Iteration 231, loss = 0.61626172\n",
      "Iteration 232, loss = 0.61623063\n",
      "Iteration 233, loss = 0.61621568\n",
      "Iteration 234, loss = 0.61618265\n",
      "Iteration 235, loss = 0.61614404\n",
      "Iteration 236, loss = 0.61611896\n",
      "Iteration 237, loss = 0.61611670\n",
      "Iteration 238, loss = 0.61609581\n",
      "Iteration 239, loss = 0.61608268\n",
      "Iteration 240, loss = 0.61604201\n",
      "Iteration 241, loss = 0.61601562\n",
      "Iteration 242, loss = 0.61597235\n",
      "Iteration 243, loss = 0.61594122\n",
      "Iteration 244, loss = 0.61593713\n",
      "Iteration 245, loss = 0.61589411\n",
      "Iteration 246, loss = 0.61586565\n",
      "Iteration 247, loss = 0.61585912\n",
      "Iteration 248, loss = 0.61576030\n",
      "Iteration 249, loss = 0.61579725\n",
      "Iteration 250, loss = 0.61607548\n",
      "Iteration 251, loss = 0.61593592\n",
      "Iteration 252, loss = 0.61567897\n",
      "Iteration 253, loss = 0.61566943\n",
      "Iteration 254, loss = 0.61565128\n",
      "Iteration 255, loss = 0.61563854\n",
      "Iteration 256, loss = 0.61561563\n",
      "Iteration 257, loss = 0.61567445\n",
      "Iteration 258, loss = 0.61563875\n",
      "Iteration 259, loss = 0.61563915\n",
      "Iteration 260, loss = 0.61561994\n",
      "Iteration 261, loss = 0.61561135\n",
      "Iteration 262, loss = 0.61558595\n",
      "Iteration 263, loss = 0.61554714\n",
      "Iteration 264, loss = 0.61551501\n",
      "Iteration 265, loss = 0.61546167\n",
      "Iteration 266, loss = 0.61541495\n",
      "Iteration 267, loss = 0.61531587\n",
      "Iteration 268, loss = 0.61520952\n",
      "Iteration 269, loss = 0.61507855\n",
      "Iteration 270, loss = 0.61494036\n",
      "Iteration 271, loss = 0.61509630\n",
      "Iteration 272, loss = 0.61544244\n",
      "Iteration 273, loss = 0.61533276\n",
      "Iteration 274, loss = 0.61486892\n",
      "Iteration 275, loss = 0.61483730\n",
      "Iteration 276, loss = 0.61480640\n",
      "Iteration 277, loss = 0.61477996\n",
      "Iteration 278, loss = 0.61476593\n",
      "Iteration 279, loss = 0.61474628\n",
      "Iteration 280, loss = 0.61469656\n",
      "Iteration 281, loss = 0.61464216\n",
      "Iteration 282, loss = 0.61456100\n",
      "Iteration 283, loss = 0.61449096\n",
      "Iteration 284, loss = 0.61441662\n",
      "Iteration 285, loss = 0.61431714\n",
      "Iteration 286, loss = 0.61427245\n",
      "Iteration 287, loss = 0.61416582\n",
      "Iteration 288, loss = 0.61414036\n",
      "Iteration 289, loss = 0.61411367\n",
      "Iteration 290, loss = 0.61392632\n",
      "Iteration 291, loss = 0.61391767\n",
      "Iteration 292, loss = 0.61397623\n",
      "Iteration 293, loss = 0.61374815\n",
      "Iteration 294, loss = 0.61348191\n",
      "Iteration 295, loss = 0.61358598\n",
      "Iteration 296, loss = 0.61360347\n",
      "Iteration 297, loss = 0.61353620\n",
      "Iteration 298, loss = 0.61346672\n",
      "Iteration 299, loss = 0.61336135\n",
      "Iteration 300, loss = 0.61323133\n",
      "Iteration 301, loss = 0.61299860\n",
      "Iteration 302, loss = 0.61276838\n",
      "Iteration 303, loss = 0.61278359\n",
      "Iteration 304, loss = 0.61330873\n",
      "Iteration 305, loss = 0.61321876\n",
      "Iteration 306, loss = 0.61263958\n",
      "Iteration 307, loss = 0.61232216\n",
      "Iteration 308, loss = 0.61219786\n",
      "Iteration 309, loss = 0.61217884\n",
      "Iteration 310, loss = 0.61205563\n",
      "Iteration 311, loss = 0.61192185\n",
      "Iteration 312, loss = 0.61175091\n",
      "Iteration 313, loss = 0.61144794\n",
      "Iteration 314, loss = 0.61119164\n",
      "Iteration 315, loss = 0.61096654\n",
      "Iteration 316, loss = 0.61085449\n",
      "Iteration 317, loss = 0.61087826\n",
      "Iteration 318, loss = 0.61074042\n",
      "Iteration 319, loss = 0.61047566\n",
      "Iteration 320, loss = 0.61003153\n",
      "Iteration 321, loss = 0.60980800\n",
      "Iteration 322, loss = 0.60960686\n",
      "Iteration 323, loss = 0.60938102\n",
      "Iteration 324, loss = 0.60914946\n",
      "Iteration 325, loss = 0.60885841\n",
      "Iteration 326, loss = 0.60858872\n",
      "Iteration 327, loss = 0.60830061\n",
      "Iteration 328, loss = 0.60805656\n",
      "Iteration 329, loss = 0.60777061\n",
      "Iteration 330, loss = 0.60746661\n",
      "Iteration 331, loss = 0.60671291\n",
      "Iteration 332, loss = 0.60696910\n",
      "Iteration 333, loss = 0.60685949\n",
      "Iteration 334, loss = 0.60643338\n",
      "Iteration 335, loss = 0.60567039\n",
      "Iteration 336, loss = 0.60506469\n",
      "Iteration 337, loss = 0.60452929\n",
      "Iteration 338, loss = 0.60396724\n",
      "Iteration 339, loss = 0.60338712\n",
      "Iteration 340, loss = 0.60315506\n",
      "Iteration 341, loss = 0.60284704\n",
      "Iteration 342, loss = 0.60247096\n",
      "Iteration 343, loss = 0.60218310\n",
      "Iteration 344, loss = 0.60168057\n",
      "Iteration 345, loss = 0.60066672\n",
      "Iteration 346, loss = 0.59957386\n",
      "Iteration 347, loss = 0.59886460\n",
      "Iteration 348, loss = 0.59845066\n",
      "Iteration 349, loss = 0.59764149\n",
      "Iteration 350, loss = 0.59703218\n",
      "Iteration 351, loss = 0.59669532\n",
      "Iteration 352, loss = 0.59584731\n",
      "Iteration 353, loss = 0.59533791\n",
      "Iteration 354, loss = 0.59470333\n",
      "Iteration 355, loss = 0.59391058\n",
      "Iteration 356, loss = 0.59405608\n",
      "Iteration 357, loss = 0.59334423\n",
      "Iteration 358, loss = 0.59241836\n",
      "Iteration 359, loss = 0.59205867\n",
      "Iteration 360, loss = 0.59157763\n",
      "Iteration 361, loss = 0.59085106\n",
      "Iteration 362, loss = 0.59015899\n",
      "Iteration 363, loss = 0.59003914\n",
      "Iteration 364, loss = 0.58933277\n",
      "Iteration 365, loss = 0.58889973\n",
      "Iteration 366, loss = 0.58837866\n",
      "Iteration 367, loss = 0.58781482\n",
      "Iteration 368, loss = 0.58744265\n",
      "Iteration 369, loss = 0.58699197\n",
      "Iteration 370, loss = 0.58669704\n",
      "Iteration 371, loss = 0.58593800\n",
      "Iteration 372, loss = 0.58561835\n",
      "Iteration 373, loss = 0.58515382\n",
      "Iteration 374, loss = 0.58453673\n",
      "Iteration 375, loss = 0.58408378\n",
      "Iteration 376, loss = 0.58358113\n",
      "Iteration 377, loss = 0.58345109\n",
      "Iteration 378, loss = 0.58250988\n",
      "Iteration 379, loss = 0.58322275\n",
      "Iteration 380, loss = 0.58282466\n",
      "Iteration 381, loss = 0.58208039\n",
      "Iteration 382, loss = 0.58136547\n",
      "Iteration 383, loss = 0.58061906\n",
      "Iteration 384, loss = 0.58056584\n",
      "Iteration 385, loss = 0.58149357\n",
      "Iteration 386, loss = 0.58027718\n",
      "Iteration 387, loss = 0.57893727\n",
      "Iteration 388, loss = 0.58124555\n",
      "Iteration 389, loss = 0.58007681\n",
      "Iteration 390, loss = 0.57836711\n",
      "Iteration 391, loss = 0.57788216\n",
      "Iteration 392, loss = 0.57828833\n",
      "Iteration 393, loss = 0.57818229\n",
      "Iteration 394, loss = 0.57732680\n",
      "Iteration 395, loss = 0.57606830\n",
      "Iteration 396, loss = 0.57597135\n",
      "Iteration 397, loss = 0.57622003\n",
      "Iteration 398, loss = 0.57629673\n",
      "Iteration 399, loss = 0.57541096\n",
      "Iteration 400, loss = 0.57397846\n",
      "Iteration 401, loss = 0.57537335\n",
      "Iteration 402, loss = 0.57874985\n",
      "Iteration 403, loss = 0.58062873\n",
      "Iteration 404, loss = 0.57897746\n",
      "Iteration 405, loss = 0.57519273\n",
      "Iteration 406, loss = 0.57258825\n",
      "Iteration 407, loss = 0.57396570\n",
      "Iteration 408, loss = 0.57385882\n",
      "Iteration 409, loss = 0.57214913\n",
      "Iteration 410, loss = 0.57120954\n",
      "Iteration 411, loss = 0.57105701\n",
      "Iteration 412, loss = 0.57058203\n",
      "Iteration 413, loss = 0.56988158\n",
      "Iteration 414, loss = 0.57029125\n",
      "Iteration 415, loss = 0.56954671\n",
      "Iteration 416, loss = 0.56930479\n",
      "Iteration 417, loss = 0.56848091\n",
      "Iteration 418, loss = 0.56843488\n",
      "Iteration 419, loss = 0.56942258\n",
      "Iteration 420, loss = 0.56983298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 421, loss = 0.56926588\n",
      "Iteration 422, loss = 0.56754909\n",
      "Iteration 423, loss = 0.56604588\n",
      "Iteration 424, loss = 0.56677926\n",
      "Iteration 425, loss = 0.56884536\n",
      "Iteration 426, loss = 0.56905578\n",
      "Iteration 427, loss = 0.56755867\n",
      "Iteration 428, loss = 0.56540100\n",
      "Iteration 429, loss = 0.56449399\n",
      "Iteration 430, loss = 0.56544757\n",
      "Iteration 431, loss = 0.56582131\n",
      "Iteration 432, loss = 0.56460546\n",
      "Iteration 433, loss = 0.56325194\n",
      "Iteration 434, loss = 0.56265780\n",
      "Iteration 435, loss = 0.56247692\n",
      "Iteration 436, loss = 0.56232862\n",
      "Iteration 437, loss = 0.56199198\n",
      "Iteration 438, loss = 0.56145341\n",
      "Iteration 439, loss = 0.56085862\n",
      "Iteration 440, loss = 0.56054565\n",
      "Iteration 441, loss = 0.56003785\n",
      "Iteration 442, loss = 0.55982667\n",
      "Iteration 443, loss = 0.55967750\n",
      "Iteration 444, loss = 0.55904033\n",
      "Iteration 445, loss = 0.55876322\n",
      "Iteration 446, loss = 0.55838143\n",
      "Iteration 447, loss = 0.55796964\n",
      "Iteration 448, loss = 0.55759821\n",
      "Iteration 449, loss = 0.55717250\n",
      "Iteration 450, loss = 0.55716000\n",
      "Iteration 451, loss = 0.55675935\n",
      "Iteration 452, loss = 0.55620934\n",
      "Iteration 453, loss = 0.55634899\n",
      "Iteration 454, loss = 0.55581236\n",
      "Iteration 455, loss = 0.55534617\n",
      "Iteration 456, loss = 0.55498905\n",
      "Iteration 457, loss = 0.55473408\n",
      "Iteration 458, loss = 0.55457723\n",
      "Iteration 459, loss = 0.55419050\n",
      "Iteration 460, loss = 0.55354389\n",
      "Iteration 461, loss = 0.55261438\n",
      "Iteration 462, loss = 0.55240680\n",
      "Iteration 463, loss = 0.55238347\n",
      "Iteration 464, loss = 0.55135232\n",
      "Iteration 465, loss = 0.55220542\n",
      "Iteration 466, loss = 0.55277147\n",
      "Iteration 467, loss = 0.55210953\n",
      "Iteration 468, loss = 0.55117480\n",
      "Iteration 469, loss = 0.54995476\n",
      "Iteration 470, loss = 0.54962366\n",
      "Iteration 471, loss = 0.54906968\n",
      "Iteration 472, loss = 0.54845191\n",
      "Iteration 473, loss = 0.54799134\n",
      "Iteration 474, loss = 0.54790571\n",
      "Iteration 475, loss = 0.54888418\n",
      "Iteration 476, loss = 0.54849832\n",
      "Iteration 477, loss = 0.54689056\n",
      "Iteration 478, loss = 0.54610451\n",
      "Iteration 479, loss = 0.54567187\n",
      "Iteration 480, loss = 0.54532664\n",
      "Iteration 481, loss = 0.54556646\n",
      "Iteration 482, loss = 0.54523677\n",
      "Iteration 483, loss = 0.54447981\n",
      "Iteration 484, loss = 0.54370163\n",
      "Iteration 485, loss = 0.54363973\n",
      "Iteration 486, loss = 0.54253991\n",
      "Iteration 487, loss = 0.54213793\n",
      "Iteration 488, loss = 0.54188942\n",
      "Iteration 489, loss = 0.54155885\n",
      "Iteration 490, loss = 0.54068955\n",
      "Iteration 491, loss = 0.54023238\n",
      "Iteration 492, loss = 0.54148982\n",
      "Iteration 493, loss = 0.54397244\n",
      "Iteration 494, loss = 0.54568556\n",
      "Iteration 495, loss = 0.54394954\n",
      "Iteration 496, loss = 0.53884802\n",
      "Iteration 497, loss = 0.53696657\n",
      "Iteration 498, loss = 0.54275775\n",
      "Iteration 499, loss = 0.54717764\n",
      "Iteration 500, loss = 0.54599728\n",
      "Iteration 501, loss = 0.54026500\n",
      "Iteration 502, loss = 0.53435708\n",
      "Iteration 503, loss = 0.53808081\n",
      "Iteration 504, loss = 0.54688720\n",
      "Iteration 505, loss = 0.54813509\n",
      "Iteration 506, loss = 0.54097241\n",
      "Iteration 507, loss = 0.53542301\n",
      "Iteration 508, loss = 0.53437478\n",
      "Iteration 509, loss = 0.53427146\n",
      "Iteration 510, loss = 0.53385697\n",
      "Iteration 511, loss = 0.53308788\n",
      "Iteration 512, loss = 0.53179069\n",
      "Iteration 513, loss = 0.53175091\n",
      "Iteration 514, loss = 0.53285296\n",
      "Iteration 515, loss = 0.53377834\n",
      "Iteration 516, loss = 0.53343293\n",
      "Iteration 517, loss = 0.53113046\n",
      "Iteration 518, loss = 0.53070543\n",
      "Iteration 519, loss = 0.52904863\n",
      "Iteration 520, loss = 0.52869948\n",
      "Iteration 521, loss = 0.52817106\n",
      "Iteration 522, loss = 0.52785852\n",
      "Iteration 523, loss = 0.52723578\n",
      "Iteration 524, loss = 0.52686395\n",
      "Iteration 525, loss = 0.52679041\n",
      "Iteration 526, loss = 0.52621105\n",
      "Iteration 527, loss = 0.52536495\n",
      "Iteration 528, loss = 0.52520466\n",
      "Iteration 529, loss = 0.52476219\n",
      "Iteration 530, loss = 0.52428120\n",
      "Iteration 531, loss = 0.52355248\n",
      "Iteration 532, loss = 0.52312780\n",
      "Iteration 533, loss = 0.52268354\n",
      "Iteration 534, loss = 0.52225684\n",
      "Iteration 535, loss = 0.52198719\n",
      "Iteration 536, loss = 0.52146083\n",
      "Iteration 537, loss = 0.52082743\n",
      "Iteration 538, loss = 0.52084005\n",
      "Iteration 539, loss = 0.52051023\n",
      "Iteration 540, loss = 0.51967980\n",
      "Iteration 541, loss = 0.51914476\n",
      "Iteration 542, loss = 0.51904781\n",
      "Iteration 543, loss = 0.51885500\n",
      "Iteration 544, loss = 0.51798742\n",
      "Iteration 545, loss = 0.51771716\n",
      "Iteration 546, loss = 0.51746517\n",
      "Iteration 547, loss = 0.51705733\n",
      "Iteration 548, loss = 0.51692774\n",
      "Iteration 549, loss = 0.51633257\n",
      "Iteration 550, loss = 0.51625246\n",
      "Iteration 551, loss = 0.51614891\n",
      "Iteration 552, loss = 0.51516081\n",
      "Iteration 553, loss = 0.51489625\n",
      "Iteration 554, loss = 0.51614687\n",
      "Iteration 555, loss = 0.51581815\n",
      "Iteration 556, loss = 0.51459694\n",
      "Iteration 557, loss = 0.51374756\n",
      "Iteration 558, loss = 0.51347383\n",
      "Iteration 559, loss = 0.51278302\n",
      "Iteration 560, loss = 0.51235481\n",
      "Iteration 561, loss = 0.51202846\n",
      "Iteration 562, loss = 0.51168387\n",
      "Iteration 563, loss = 0.51141684\n",
      "Iteration 564, loss = 0.51109198\n",
      "Iteration 565, loss = 0.51160025\n",
      "Iteration 566, loss = 0.51123180\n",
      "Iteration 567, loss = 0.51029588\n",
      "Iteration 568, loss = 0.50955439\n",
      "Iteration 569, loss = 0.50936175\n",
      "Iteration 570, loss = 0.50906853\n",
      "Iteration 571, loss = 0.50836475\n",
      "Iteration 572, loss = 0.50798979\n",
      "Iteration 573, loss = 0.51051200\n",
      "Iteration 574, loss = 0.51275834\n",
      "Iteration 575, loss = 0.51173699\n",
      "Iteration 576, loss = 0.51051096\n",
      "Iteration 577, loss = 0.50669095\n",
      "Iteration 578, loss = 0.50625425\n",
      "Iteration 579, loss = 0.50672213\n",
      "Iteration 580, loss = 0.50642080\n",
      "Iteration 581, loss = 0.50578719\n",
      "Iteration 582, loss = 0.50545876\n",
      "Iteration 583, loss = 0.50574291\n",
      "Iteration 584, loss = 0.50527487\n",
      "Iteration 585, loss = 0.50448113\n",
      "Iteration 586, loss = 0.50413030\n",
      "Iteration 587, loss = 0.50389120\n",
      "Iteration 588, loss = 0.50358503\n",
      "Iteration 589, loss = 0.50331016\n",
      "Iteration 590, loss = 0.50292876\n",
      "Iteration 591, loss = 0.50288182\n",
      "Iteration 592, loss = 0.50325632\n",
      "Iteration 593, loss = 0.50342909\n",
      "Iteration 594, loss = 0.50247693\n",
      "Iteration 595, loss = 0.50312128\n",
      "Iteration 596, loss = 0.50319510\n",
      "Iteration 597, loss = 0.50109329\n",
      "Iteration 598, loss = 0.50204723\n",
      "Iteration 599, loss = 0.50791684\n",
      "Iteration 600, loss = 0.50928248\n",
      "Iteration 601, loss = 0.50496650\n",
      "Iteration 602, loss = 0.50193016\n",
      "Iteration 603, loss = 0.50127761\n",
      "Iteration 604, loss = 0.50131870\n",
      "Iteration 605, loss = 0.50113738\n",
      "Iteration 606, loss = 0.50136407\n",
      "Iteration 607, loss = 0.50201894\n",
      "Iteration 608, loss = 0.50117739\n",
      "Iteration 609, loss = 0.50014928\n",
      "Iteration 610, loss = 0.49924947\n",
      "Iteration 611, loss = 0.49916809\n",
      "Iteration 612, loss = 0.49902916\n",
      "Iteration 613, loss = 0.49947123\n",
      "Iteration 614, loss = 0.49840935\n",
      "Iteration 615, loss = 0.49852173\n",
      "Iteration 616, loss = 0.49769814\n",
      "Iteration 617, loss = 0.49779334\n",
      "Iteration 618, loss = 0.49930152\n",
      "Iteration 619, loss = 0.49861269\n",
      "Iteration 620, loss = 0.49704806\n",
      "Iteration 621, loss = 0.49680158\n",
      "Iteration 622, loss = 0.49667063\n",
      "Iteration 623, loss = 0.49651434\n",
      "Iteration 624, loss = 0.49640282\n",
      "Iteration 625, loss = 0.49617528\n",
      "Iteration 626, loss = 0.49884898\n",
      "Iteration 627, loss = 0.49737275\n",
      "Iteration 628, loss = 0.49545609\n",
      "Iteration 629, loss = 0.49634914\n",
      "Iteration 630, loss = 0.49523366\n",
      "Iteration 631, loss = 0.49554247\n",
      "Iteration 632, loss = 0.49525402\n",
      "Iteration 633, loss = 0.49476276\n",
      "Iteration 634, loss = 0.49410602\n",
      "Iteration 635, loss = 0.49414057\n",
      "Iteration 636, loss = 0.49448919\n",
      "Iteration 637, loss = 0.49503598\n",
      "Iteration 638, loss = 0.49465967\n",
      "Iteration 639, loss = 0.49386712\n",
      "Iteration 640, loss = 0.49344136\n",
      "Iteration 641, loss = 0.49336216\n",
      "Iteration 642, loss = 0.49324638\n",
      "Iteration 643, loss = 0.49290488\n",
      "Iteration 644, loss = 0.49296140\n",
      "Iteration 645, loss = 0.49309567\n",
      "Iteration 646, loss = 0.49304221\n",
      "Iteration 647, loss = 0.49461409\n",
      "Iteration 648, loss = 0.49578709\n",
      "Iteration 649, loss = 0.49311383\n",
      "Iteration 650, loss = 0.49272602\n",
      "Iteration 651, loss = 0.49480024\n",
      "Iteration 652, loss = 0.49514218\n",
      "Iteration 653, loss = 0.49304422\n",
      "Iteration 654, loss = 0.49266479\n",
      "Iteration 655, loss = 0.49208990\n",
      "Iteration 656, loss = 0.49146942\n",
      "Iteration 657, loss = 0.49144200\n",
      "Iteration 658, loss = 0.49466659\n",
      "Iteration 659, loss = 0.49564844\n",
      "Iteration 660, loss = 0.49405295\n",
      "Iteration 661, loss = 0.49126474\n",
      "Iteration 662, loss = 0.49080727\n",
      "Iteration 663, loss = 0.49151890\n",
      "Iteration 664, loss = 0.49159391\n",
      "Iteration 665, loss = 0.49098548\n",
      "Iteration 666, loss = 0.49039369\n",
      "Iteration 667, loss = 0.49098656\n",
      "Iteration 668, loss = 0.49153468\n",
      "Iteration 669, loss = 0.49090104\n",
      "Iteration 670, loss = 0.48994708\n",
      "Iteration 671, loss = 0.49013474\n",
      "Iteration 672, loss = 0.49119693\n",
      "Iteration 673, loss = 0.49317960\n",
      "Iteration 674, loss = 0.49368564\n",
      "Iteration 675, loss = 0.49140820\n",
      "Iteration 676, loss = 0.48944538\n",
      "Iteration 677, loss = 0.49129420\n",
      "Iteration 678, loss = 0.49237577\n",
      "Iteration 679, loss = 0.49182534\n",
      "Iteration 680, loss = 0.49096664\n",
      "Iteration 681, loss = 0.48988175\n",
      "Iteration 682, loss = 0.48927428\n",
      "Iteration 683, loss = 0.48954312\n",
      "Iteration 684, loss = 0.49038735\n",
      "Iteration 685, loss = 0.49083372\n",
      "Iteration 686, loss = 0.49039071\n",
      "Iteration 687, loss = 0.49007954\n",
      "Iteration 688, loss = 0.48986190\n",
      "Iteration 689, loss = 0.48908711\n",
      "Iteration 690, loss = 0.48913652\n",
      "Iteration 691, loss = 0.48939518\n",
      "Iteration 692, loss = 0.48872859\n",
      "Iteration 693, loss = 0.48850183\n",
      "Iteration 694, loss = 0.48942158\n",
      "Iteration 695, loss = 0.49140904\n",
      "Iteration 696, loss = 0.49142905\n",
      "Iteration 697, loss = 0.48980205\n",
      "Iteration 698, loss = 0.48852552\n",
      "Iteration 699, loss = 0.48828004\n",
      "Iteration 700, loss = 0.48833787\n",
      "Iteration 701, loss = 0.48831441\n",
      "Iteration 702, loss = 0.48820903\n",
      "Iteration 703, loss = 0.48775624\n",
      "Iteration 704, loss = 0.48850702\n",
      "Iteration 705, loss = 0.49060878\n",
      "Iteration 706, loss = 0.48995493\n",
      "Iteration 707, loss = 0.48798858\n",
      "Iteration 708, loss = 0.48781875\n",
      "Iteration 709, loss = 0.48758173\n",
      "Iteration 710, loss = 0.48868211\n",
      "Iteration 711, loss = 0.48798694\n",
      "Iteration 712, loss = 0.48868935\n",
      "Iteration 713, loss = 0.48796935\n",
      "Iteration 714, loss = 0.48775034\n",
      "Iteration 715, loss = 0.48762032\n",
      "Iteration 716, loss = 0.48764579\n",
      "Iteration 717, loss = 0.48757457\n",
      "Iteration 718, loss = 0.48741697\n",
      "Iteration 719, loss = 0.48717597\n",
      "Iteration 720, loss = 0.48869693\n",
      "Iteration 721, loss = 0.48908965\n",
      "Iteration 722, loss = 0.48830603\n",
      "Iteration 723, loss = 0.48756808\n",
      "Iteration 724, loss = 0.48788714\n",
      "Iteration 725, loss = 0.48733422\n",
      "Iteration 726, loss = 0.48696652\n",
      "Iteration 727, loss = 0.49021944\n",
      "Iteration 728, loss = 0.49129848\n",
      "Iteration 729, loss = 0.48952666\n",
      "Iteration 730, loss = 0.48728908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 731, loss = 0.48721858\n",
      "Iteration 732, loss = 0.48846768\n",
      "Iteration 733, loss = 0.48904058\n",
      "Iteration 734, loss = 0.48849044\n",
      "Iteration 735, loss = 0.48773367\n",
      "Iteration 736, loss = 0.48696648\n",
      "Iteration 737, loss = 0.48695447\n",
      "Iteration 738, loss = 0.48891719\n",
      "Iteration 739, loss = 0.49138038\n",
      "Iteration 740, loss = 0.49357338\n",
      "Iteration 741, loss = 0.49515077\n",
      "Iteration 742, loss = 0.49213762\n",
      "Iteration 743, loss = 0.48722084\n",
      "Iteration 744, loss = 0.48674749\n",
      "Iteration 745, loss = 0.48730253\n",
      "Iteration 746, loss = 0.48830997\n",
      "Iteration 747, loss = 0.48793182\n",
      "Iteration 748, loss = 0.48692660\n",
      "Iteration 749, loss = 0.48663653\n",
      "Iteration 750, loss = 0.48684044\n",
      "Iteration 751, loss = 0.48706471\n",
      "Iteration 752, loss = 0.48711595\n",
      "Iteration 753, loss = 0.48746791\n",
      "Iteration 754, loss = 0.48745085\n",
      "Iteration 755, loss = 0.48688147\n",
      "Iteration 756, loss = 0.48662449\n",
      "Iteration 757, loss = 0.48627725\n",
      "Iteration 758, loss = 0.48678946\n",
      "Iteration 759, loss = 0.48736044\n",
      "Iteration 760, loss = 0.48763048\n",
      "Iteration 761, loss = 0.48677733\n",
      "Iteration 762, loss = 0.48655408\n",
      "Iteration 763, loss = 0.48656620\n",
      "Iteration 764, loss = 0.48665236\n",
      "Iteration 765, loss = 0.48633012\n",
      "Iteration 766, loss = 0.48617930\n",
      "Iteration 767, loss = 0.48642746\n",
      "Iteration 768, loss = 0.48594777\n",
      "Iteration 769, loss = 0.48581545\n",
      "Iteration 770, loss = 0.48805661\n",
      "Iteration 771, loss = 0.48892576\n",
      "Iteration 772, loss = 0.48829846\n",
      "Iteration 773, loss = 0.48639588\n",
      "Iteration 774, loss = 0.48621332\n",
      "Iteration 775, loss = 0.48597992\n",
      "Iteration 776, loss = 0.48587430\n",
      "Iteration 777, loss = 0.48616591\n",
      "Iteration 778, loss = 0.48708334\n",
      "Iteration 779, loss = 0.48841652\n",
      "Iteration 780, loss = 0.48723501\n",
      "Iteration 781, loss = 0.48599171\n",
      "Iteration 782, loss = 0.48623955\n",
      "Iteration 783, loss = 0.48733389\n",
      "Iteration 784, loss = 0.48733563\n",
      "Iteration 785, loss = 0.48589901\n",
      "Iteration 786, loss = 0.48606033\n",
      "Iteration 787, loss = 0.48798408\n",
      "Iteration 788, loss = 0.48943315\n",
      "Iteration 789, loss = 0.48880381\n",
      "Iteration 790, loss = 0.48687608\n",
      "Iteration 791, loss = 0.48610619\n",
      "Iteration 792, loss = 0.48572739\n",
      "Iteration 793, loss = 0.48697220\n",
      "Iteration 794, loss = 0.48769583\n",
      "Iteration 795, loss = 0.48506585\n",
      "Iteration 796, loss = 0.48671730\n",
      "Iteration 797, loss = 0.49209019\n",
      "Iteration 798, loss = 0.49101498\n",
      "Iteration 799, loss = 0.48795290\n",
      "Iteration 800, loss = 0.48547802\n",
      "Iteration 801, loss = 0.48790259\n",
      "Iteration 802, loss = 0.49067673\n",
      "Iteration 803, loss = 0.49158117\n",
      "Iteration 804, loss = 0.48922440\n",
      "Iteration 805, loss = 0.48601041\n",
      "Iteration 806, loss = 0.48562597\n",
      "Iteration 807, loss = 0.48835753\n",
      "Iteration 808, loss = 0.48933269\n",
      "Iteration 809, loss = 0.48818045\n",
      "Iteration 810, loss = 0.48702389\n",
      "Iteration 811, loss = 0.48635075\n",
      "Iteration 812, loss = 0.48571595\n",
      "Iteration 813, loss = 0.48551669\n",
      "Iteration 814, loss = 0.48592937\n",
      "Iteration 815, loss = 0.48594001\n",
      "Iteration 816, loss = 0.48595427\n",
      "Iteration 817, loss = 0.48587364\n",
      "Iteration 818, loss = 0.48626524\n",
      "Iteration 819, loss = 0.48632140\n",
      "Iteration 820, loss = 0.48592049\n",
      "Iteration 821, loss = 0.48562250\n",
      "Iteration 822, loss = 0.48605846\n",
      "Iteration 823, loss = 0.48568012\n",
      "Iteration 824, loss = 0.48534615\n",
      "Iteration 825, loss = 0.48597341\n",
      "Iteration 826, loss = 0.48692094\n",
      "Iteration 827, loss = 0.48733665\n",
      "Iteration 828, loss = 0.48610004\n",
      "Iteration 829, loss = 0.48613530\n",
      "Iteration 830, loss = 0.48637326\n",
      "Iteration 831, loss = 0.48623985\n",
      "Iteration 832, loss = 0.48609837\n",
      "Iteration 833, loss = 0.48550003\n",
      "Iteration 834, loss = 0.48569560\n",
      "Iteration 835, loss = 0.48611915\n",
      "Iteration 836, loss = 0.48676485\n",
      "Iteration 837, loss = 0.48728746\n",
      "Iteration 838, loss = 0.48724753\n",
      "Iteration 839, loss = 0.48651917\n",
      "Iteration 840, loss = 0.48578927\n",
      "Iteration 841, loss = 0.48558651\n",
      "Iteration 842, loss = 0.48600968\n",
      "Iteration 843, loss = 0.48683904\n",
      "Iteration 844, loss = 0.48567509\n",
      "Iteration 845, loss = 0.48514937\n",
      "Iteration 846, loss = 0.48696833\n",
      "Iteration 847, loss = 0.48935931\n",
      "Iteration 848, loss = 0.48879389\n",
      "Iteration 849, loss = 0.48677432\n",
      "Iteration 850, loss = 0.48564216\n",
      "Iteration 851, loss = 0.48553040\n",
      "Iteration 852, loss = 0.48551764\n",
      "Iteration 853, loss = 0.48530700\n",
      "Iteration 854, loss = 0.48498971\n",
      "Iteration 855, loss = 0.48670286\n",
      "Iteration 856, loss = 0.48850788\n",
      "Iteration 857, loss = 0.48745686\n",
      "Iteration 858, loss = 0.48551384\n",
      "Iteration 859, loss = 0.48577449\n",
      "Iteration 860, loss = 0.48592329\n",
      "Iteration 861, loss = 0.48603454\n",
      "Iteration 862, loss = 0.48583367\n",
      "Iteration 863, loss = 0.48537705\n",
      "Iteration 864, loss = 0.48590142\n",
      "Iteration 865, loss = 0.48544962\n",
      "Iteration 866, loss = 0.48546892\n",
      "Iteration 867, loss = 0.48631634\n",
      "Iteration 868, loss = 0.48625958\n",
      "Iteration 869, loss = 0.48621896\n",
      "Iteration 870, loss = 0.48615962\n",
      "Iteration 871, loss = 0.48646375\n",
      "Iteration 872, loss = 0.48559778\n",
      "Iteration 873, loss = 0.48544700\n",
      "Iteration 874, loss = 0.48617938\n",
      "Iteration 875, loss = 0.48851470\n",
      "Iteration 876, loss = 0.48787480\n",
      "Iteration 877, loss = 0.48592638\n",
      "Iteration 878, loss = 0.48513862\n",
      "Iteration 879, loss = 0.48657024\n",
      "Iteration 880, loss = 0.48590966\n",
      "Iteration 881, loss = 0.48580533\n",
      "Iteration 882, loss = 0.48534770\n",
      "Iteration 883, loss = 0.48522614\n",
      "Iteration 884, loss = 0.48601679\n",
      "Iteration 885, loss = 0.48563030\n",
      "Iteration 886, loss = 0.48539289\n",
      "Iteration 887, loss = 0.48527552\n",
      "Iteration 888, loss = 0.48537834\n",
      "Iteration 889, loss = 0.48564070\n",
      "Iteration 890, loss = 0.48595154\n",
      "Iteration 891, loss = 0.48566624\n",
      "Iteration 892, loss = 0.48508506\n",
      "Iteration 893, loss = 0.48513478\n",
      "Iteration 894, loss = 0.48622370\n",
      "Iteration 895, loss = 0.48727916\n",
      "Iteration 896, loss = 0.48712473\n",
      "Iteration 897, loss = 0.48566823\n",
      "Iteration 898, loss = 0.48491689\n",
      "Iteration 899, loss = 0.48687198\n",
      "Iteration 900, loss = 0.48714530\n",
      "Iteration 901, loss = 0.48622053\n",
      "Iteration 902, loss = 0.48487457\n",
      "Iteration 903, loss = 0.48548276\n",
      "Iteration 904, loss = 0.48731893\n",
      "Iteration 905, loss = 0.48754146\n",
      "Iteration 906, loss = 0.48612359\n",
      "Iteration 907, loss = 0.48509412\n",
      "Iteration 908, loss = 0.48446340\n",
      "Iteration 909, loss = 0.48733696\n",
      "Iteration 910, loss = 0.49049350\n",
      "Iteration 911, loss = 0.48863625\n",
      "Iteration 912, loss = 0.48656921\n",
      "Iteration 913, loss = 0.48531487\n",
      "Iteration 914, loss = 0.48537845\n",
      "Iteration 915, loss = 0.48506702\n",
      "Iteration 916, loss = 0.48482013\n",
      "Iteration 917, loss = 0.48546896\n",
      "Iteration 918, loss = 0.48751419\n",
      "Iteration 919, loss = 0.48738558\n",
      "Iteration 920, loss = 0.48631575\n",
      "Iteration 921, loss = 0.48495451\n",
      "Iteration 922, loss = 0.48542802\n",
      "Iteration 923, loss = 0.48608077\n",
      "Iteration 924, loss = 0.48674018\n",
      "Iteration 925, loss = 0.48791949\n",
      "Iteration 926, loss = 0.48744080\n",
      "Iteration 927, loss = 0.48935274\n",
      "Iteration 928, loss = 0.48956981\n",
      "Iteration 929, loss = 0.48669194\n",
      "Iteration 930, loss = 0.48513407\n",
      "Iteration 931, loss = 0.48695173\n",
      "Iteration 932, loss = 0.48934547\n",
      "Iteration 933, loss = 0.49148456\n",
      "Iteration 934, loss = 0.48901446\n",
      "Iteration 935, loss = 0.48642219\n",
      "Iteration 936, loss = 0.48497273\n",
      "Iteration 937, loss = 0.48648596\n",
      "Iteration 938, loss = 0.48751108\n",
      "Iteration 939, loss = 0.48722065\n",
      "Iteration 940, loss = 0.48630017\n",
      "Iteration 941, loss = 0.48568649\n",
      "Iteration 942, loss = 0.48500145\n",
      "Iteration 943, loss = 0.48507403\n",
      "Iteration 944, loss = 0.48523497\n",
      "Iteration 945, loss = 0.48487374\n",
      "Iteration 946, loss = 0.48514858\n",
      "Iteration 947, loss = 0.48581025\n",
      "Iteration 948, loss = 0.48643026\n",
      "Iteration 949, loss = 0.48679772\n",
      "Iteration 950, loss = 0.48616147\n",
      "Iteration 951, loss = 0.48439144\n",
      "Iteration 952, loss = 0.48485458\n",
      "Iteration 953, loss = 0.49191928\n",
      "Iteration 954, loss = 0.49736138\n",
      "Iteration 955, loss = 0.49158435\n",
      "Iteration 956, loss = 0.48559911\n",
      "Iteration 957, loss = 0.48699265\n",
      "Iteration 958, loss = 0.49138585\n",
      "Iteration 959, loss = 0.49275195\n",
      "Iteration 960, loss = 0.49031147\n",
      "Iteration 961, loss = 0.48714046\n",
      "Iteration 962, loss = 0.48512062\n",
      "Iteration 963, loss = 0.48547623\n",
      "Iteration 964, loss = 0.48568863\n",
      "Iteration 965, loss = 0.48607518\n",
      "Iteration 966, loss = 0.48653679\n",
      "Iteration 967, loss = 0.48622096\n",
      "Iteration 968, loss = 0.48541753\n",
      "Iteration 969, loss = 0.48499219\n",
      "Iteration 970, loss = 0.48616628\n",
      "Iteration 971, loss = 0.48600177\n",
      "Iteration 972, loss = 0.48630289\n",
      "Iteration 973, loss = 0.48537549\n",
      "Iteration 974, loss = 0.48620323\n",
      "Iteration 975, loss = 0.48666671\n",
      "Iteration 976, loss = 0.48646497\n",
      "Iteration 977, loss = 0.48586629\n",
      "Iteration 978, loss = 0.48524063\n",
      "Iteration 979, loss = 0.48537675\n",
      "Iteration 980, loss = 0.48545314\n",
      "Iteration 981, loss = 0.48620187\n",
      "Iteration 982, loss = 0.48653853\n",
      "Iteration 983, loss = 0.48621786\n",
      "Iteration 984, loss = 0.48567908\n",
      "Iteration 985, loss = 0.48493755\n",
      "Iteration 986, loss = 0.48549025\n",
      "Iteration 987, loss = 0.48567029\n",
      "Iteration 988, loss = 0.48568409\n",
      "Iteration 989, loss = 0.48562367\n",
      "Iteration 990, loss = 0.48515450\n",
      "Iteration 991, loss = 0.48503383\n",
      "Iteration 992, loss = 0.48510982\n",
      "Iteration 993, loss = 0.48493233\n",
      "Iteration 994, loss = 0.48504560\n",
      "Iteration 995, loss = 0.48586318\n",
      "Iteration 996, loss = 0.48587178\n",
      "Iteration 997, loss = 0.48499130\n",
      "Iteration 998, loss = 0.48499419\n",
      "Iteration 999, loss = 0.48507062\n",
      "Iteration 1000, loss = 0.48512158\n",
      "Iteration 1001, loss = 0.48544202\n",
      "Iteration 1002, loss = 0.48573193\n",
      "Iteration 1003, loss = 0.48774789\n",
      "Iteration 1004, loss = 0.48907864\n",
      "Iteration 1005, loss = 0.48886692\n",
      "Iteration 1006, loss = 0.48777396\n",
      "Iteration 1007, loss = 0.48609864\n",
      "Iteration 1008, loss = 0.48459684\n",
      "Iteration 1009, loss = 0.48518830\n",
      "Iteration 1010, loss = 0.49015651\n",
      "Iteration 1011, loss = 0.49186381\n",
      "Iteration 1012, loss = 0.48963652\n",
      "Iteration 1013, loss = 0.48680456\n",
      "Iteration 1014, loss = 0.48457727\n",
      "Iteration 1015, loss = 0.48593133\n",
      "Iteration 1016, loss = 0.48804326\n",
      "Iteration 1017, loss = 0.48898977\n",
      "Iteration 1018, loss = 0.48740803\n",
      "Iteration 1019, loss = 0.48461929\n",
      "Iteration 1020, loss = 0.48673808\n",
      "Iteration 1021, loss = 0.48738657\n",
      "Iteration 1022, loss = 0.48877754\n",
      "Iteration 1023, loss = 0.48838816\n",
      "Iteration 1024, loss = 0.48598359\n",
      "Iteration 1025, loss = 0.48503736\n",
      "Iteration 1026, loss = 0.48534052\n",
      "Iteration 1027, loss = 0.48621688\n",
      "Iteration 1028, loss = 0.48658239\n",
      "Iteration 1029, loss = 0.48635869\n",
      "Iteration 1030, loss = 0.48554568\n",
      "Iteration 1031, loss = 0.48585592\n",
      "Iteration 1032, loss = 0.48541860\n",
      "Iteration 1033, loss = 0.48566100\n",
      "Iteration 1034, loss = 0.48528122\n",
      "Iteration 1035, loss = 0.48540194\n",
      "Iteration 1036, loss = 0.48498305\n",
      "Iteration 1037, loss = 0.48496709\n",
      "Iteration 1038, loss = 0.48520955\n",
      "Iteration 1039, loss = 0.48514256\n",
      "Iteration 1040, loss = 0.48499258\n",
      "Iteration 1041, loss = 0.48485322\n",
      "Iteration 1042, loss = 0.48483343\n",
      "Iteration 1043, loss = 0.48604632\n",
      "Iteration 1044, loss = 0.48572518\n",
      "Iteration 1045, loss = 0.48570249\n",
      "Iteration 1046, loss = 0.48493511\n",
      "Iteration 1047, loss = 0.48495220\n",
      "Iteration 1048, loss = 0.48501068\n",
      "Iteration 1049, loss = 0.48515929\n",
      "Iteration 1050, loss = 0.48596920\n",
      "Iteration 1051, loss = 0.48543906\n",
      "Iteration 1052, loss = 0.48478591\n",
      "Iteration 1053, loss = 0.48525228\n",
      "Iteration 1054, loss = 0.48609741\n",
      "Iteration 1055, loss = 0.48734656\n",
      "Iteration 1056, loss = 0.48822491\n",
      "Iteration 1057, loss = 0.48748990\n",
      "Iteration 1058, loss = 0.48644696\n",
      "Iteration 1059, loss = 0.48530360\n",
      "Iteration 1060, loss = 0.48500383\n",
      "Iteration 1061, loss = 0.48526451\n",
      "Iteration 1062, loss = 0.48544292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1063, loss = 0.48509616\n",
      "Iteration 1064, loss = 0.48536764\n",
      "Iteration 1065, loss = 0.48574821\n",
      "Iteration 1066, loss = 0.48696591\n",
      "Iteration 1067, loss = 0.48788919\n",
      "Iteration 1068, loss = 0.48626558\n",
      "Iteration 1069, loss = 0.48567214\n",
      "Iteration 1070, loss = 0.48529420\n",
      "Iteration 1071, loss = 0.48578941\n",
      "Iteration 1072, loss = 0.48592162\n",
      "Iteration 1073, loss = 0.48585306\n",
      "Iteration 1074, loss = 0.48551190\n",
      "Iteration 1075, loss = 0.48489503\n",
      "Iteration 1076, loss = 0.48470779\n",
      "Iteration 1077, loss = 0.48517786\n",
      "Iteration 1078, loss = 0.48545502\n",
      "Iteration 1079, loss = 0.48536201\n",
      "Iteration 1080, loss = 0.48530226\n",
      "Iteration 1081, loss = 0.48542528\n",
      "Iteration 1082, loss = 0.48511086\n",
      "Iteration 1083, loss = 0.48475023\n",
      "Iteration 1084, loss = 0.48461449\n",
      "Iteration 1085, loss = 0.48587139\n",
      "Iteration 1086, loss = 0.48594903\n",
      "Iteration 1087, loss = 0.48554488\n",
      "Iteration 1088, loss = 0.48535548\n",
      "Iteration 1089, loss = 0.48498191\n",
      "Iteration 1090, loss = 0.48569942\n",
      "Iteration 1091, loss = 0.48541406\n",
      "Iteration 1092, loss = 0.48501820\n",
      "Iteration 1093, loss = 0.48468780\n",
      "Iteration 1094, loss = 0.48501442\n",
      "Iteration 1095, loss = 0.48498940\n",
      "Iteration 1096, loss = 0.48492073\n",
      "Iteration 1097, loss = 0.48533102\n",
      "Iteration 1098, loss = 0.48583465\n",
      "Iteration 1099, loss = 0.48602268\n",
      "Iteration 1100, loss = 0.48568926\n",
      "Iteration 1101, loss = 0.48530965\n",
      "Iteration 1102, loss = 0.48463588\n",
      "Iteration 1103, loss = 0.48521709\n",
      "Iteration 1104, loss = 0.48547100\n",
      "Iteration 1105, loss = 0.48572246\n",
      "Iteration 1106, loss = 0.48636234\n",
      "Iteration 1107, loss = 0.48634751\n",
      "Iteration 1108, loss = 0.48550233\n",
      "Iteration 1109, loss = 0.48501897\n",
      "Iteration 1110, loss = 0.48479934\n",
      "Iteration 1111, loss = 0.48485590\n",
      "Iteration 1112, loss = 0.48480229\n",
      "Iteration 1113, loss = 0.48476065\n",
      "Iteration 1114, loss = 0.48508383\n",
      "Iteration 1115, loss = 0.48548019\n",
      "Iteration 1116, loss = 0.48561049\n",
      "Iteration 1117, loss = 0.48548873\n",
      "Iteration 1118, loss = 0.48471909\n",
      "Iteration 1119, loss = 0.48541152\n",
      "Iteration 1120, loss = 0.48515930\n",
      "Iteration 1121, loss = 0.48515920\n",
      "Iteration 1122, loss = 0.48485524\n",
      "Iteration 1123, loss = 0.48515281\n",
      "Iteration 1124, loss = 0.48477525\n",
      "Iteration 1125, loss = 0.48466978\n",
      "Iteration 1126, loss = 0.48473308\n",
      "Iteration 1127, loss = 0.48554083\n",
      "Iteration 1128, loss = 0.48566293\n",
      "Iteration 1129, loss = 0.48499885\n",
      "Iteration 1130, loss = 0.48453932\n",
      "Iteration 1131, loss = 0.48518214\n",
      "Iteration 1132, loss = 0.48567350\n",
      "Iteration 1133, loss = 0.48570345\n",
      "Iteration 1134, loss = 0.48545027\n",
      "Iteration 1135, loss = 0.48511580\n",
      "Iteration 1136, loss = 0.48508447\n",
      "Iteration 1137, loss = 0.48478051\n",
      "Iteration 1138, loss = 0.48476807\n",
      "Iteration 1139, loss = 0.48477930\n",
      "Iteration 1140, loss = 0.48472450\n",
      "Iteration 1141, loss = 0.48711619\n",
      "Iteration 1142, loss = 0.48776244\n",
      "Iteration 1143, loss = 0.48601771\n",
      "Iteration 1144, loss = 0.48462814\n",
      "Iteration 1145, loss = 0.48480011\n",
      "Iteration 1146, loss = 0.48717215\n",
      "Iteration 1147, loss = 0.48724330\n",
      "Iteration 1148, loss = 0.48618626\n",
      "Iteration 1149, loss = 0.48508025\n",
      "Iteration 1150, loss = 0.48544050\n",
      "Iteration 1151, loss = 0.48461317\n",
      "Iteration 1152, loss = 0.48522497\n",
      "Iteration 1153, loss = 0.48468683\n",
      "Iteration 1154, loss = 0.48478053\n",
      "Iteration 1155, loss = 0.48504685\n",
      "Iteration 1156, loss = 0.48461770\n",
      "Iteration 1157, loss = 0.48465352\n",
      "Iteration 1158, loss = 0.48555006\n",
      "Iteration 1159, loss = 0.48528834\n",
      "Iteration 1160, loss = 0.48504237\n",
      "Iteration 1161, loss = 0.48513126\n",
      "Iteration 1162, loss = 0.48807810\n",
      "Iteration 1163, loss = 0.48874004\n",
      "Iteration 1164, loss = 0.48580892\n",
      "Iteration 1165, loss = 0.48386393\n",
      "Iteration 1166, loss = 0.48667818\n",
      "Iteration 1167, loss = 0.49054072\n",
      "Iteration 1168, loss = 0.49219350\n",
      "Iteration 1169, loss = 0.48980560\n",
      "Iteration 1170, loss = 0.48600222\n",
      "Iteration 1171, loss = 0.48452020\n",
      "Iteration 1172, loss = 0.48796178\n",
      "Iteration 1173, loss = 0.48917460\n",
      "Iteration 1174, loss = 0.48782788\n",
      "Iteration 1175, loss = 0.48590438\n",
      "Iteration 1176, loss = 0.48497551\n",
      "Iteration 1177, loss = 0.48521433\n",
      "Iteration 1178, loss = 0.48537536\n",
      "Iteration 1179, loss = 0.48529686\n",
      "Iteration 1180, loss = 0.48501588\n",
      "Iteration 1181, loss = 0.48473062\n",
      "Iteration 1182, loss = 0.48479425\n",
      "Iteration 1183, loss = 0.48480570\n",
      "Iteration 1184, loss = 0.48504842\n",
      "Iteration 1185, loss = 0.48499334\n",
      "Iteration 1186, loss = 0.48482287\n",
      "Iteration 1187, loss = 0.48462330\n",
      "Iteration 1188, loss = 0.48468138\n",
      "Iteration 1189, loss = 0.48471916\n",
      "Iteration 1190, loss = 0.48472521\n",
      "Iteration 1191, loss = 0.48538169\n",
      "Iteration 1192, loss = 0.48637987\n",
      "Iteration 1193, loss = 0.48745730\n",
      "Iteration 1194, loss = 0.48824953\n",
      "Iteration 1195, loss = 0.48808306\n",
      "Iteration 1196, loss = 0.48660436\n",
      "Iteration 1197, loss = 0.48560146\n",
      "Iteration 1198, loss = 0.48464422\n",
      "Iteration 1199, loss = 0.48449970\n",
      "Iteration 1200, loss = 0.48589256\n",
      "Iteration 1201, loss = 0.48747291\n",
      "Iteration 1202, loss = 0.48707431\n",
      "Iteration 1203, loss = 0.48542328\n",
      "Iteration 1204, loss = 0.48486408\n",
      "Iteration 1205, loss = 0.48451383\n",
      "Iteration 1206, loss = 0.48484577\n",
      "Iteration 1207, loss = 0.48520619\n",
      "Iteration 1208, loss = 0.48565658\n",
      "Iteration 1209, loss = 0.48614424\n",
      "Iteration 1210, loss = 0.48585402\n",
      "Iteration 1211, loss = 0.48582322\n",
      "Iteration 1212, loss = 0.48493901\n",
      "Iteration 1213, loss = 0.48457368\n",
      "Iteration 1214, loss = 0.48492099\n",
      "Iteration 1215, loss = 0.48578465\n",
      "Iteration 1216, loss = 0.48689832\n",
      "Iteration 1217, loss = 0.48668837\n",
      "Iteration 1218, loss = 0.48522757\n",
      "Iteration 1219, loss = 0.48447585\n",
      "Iteration 1220, loss = 0.48468522\n",
      "Iteration 1221, loss = 0.48575822\n",
      "Iteration 1222, loss = 0.48592356\n",
      "Iteration 1223, loss = 0.48535034\n",
      "Iteration 1224, loss = 0.48487233\n",
      "Iteration 1225, loss = 0.48451483\n",
      "Iteration 1226, loss = 0.48504017\n",
      "Iteration 1227, loss = 0.48464478\n",
      "Iteration 1228, loss = 0.48454910\n",
      "Iteration 1229, loss = 0.48475985\n",
      "Iteration 1230, loss = 0.48507047\n",
      "Iteration 1231, loss = 0.48498928\n",
      "Iteration 1232, loss = 0.48460544\n",
      "Iteration 1233, loss = 0.48459225\n",
      "Iteration 1234, loss = 0.48449482\n",
      "Iteration 1235, loss = 0.48456584\n",
      "Iteration 1236, loss = 0.48528076\n",
      "Iteration 1237, loss = 0.48459603\n",
      "Iteration 1238, loss = 0.48454009\n",
      "Iteration 1239, loss = 0.48515264\n",
      "Iteration 1240, loss = 0.48512307\n",
      "Iteration 1241, loss = 0.48480631\n",
      "Iteration 1242, loss = 0.48468896\n",
      "Iteration 1243, loss = 0.48459657\n",
      "Iteration 1244, loss = 0.48462593\n",
      "Iteration 1245, loss = 0.48453585\n",
      "Iteration 1246, loss = 0.48455891\n",
      "Iteration 1247, loss = 0.48460766\n",
      "Iteration 1248, loss = 0.48458487\n",
      "Iteration 1249, loss = 0.48460310\n",
      "Iteration 1250, loss = 0.48469207\n",
      "Iteration 1251, loss = 0.48416271\n",
      "Iteration 1252, loss = 0.48578698\n",
      "Iteration 1253, loss = 0.48737404\n",
      "Iteration 1254, loss = 0.48799451\n",
      "Iteration 1255, loss = 0.48714779\n",
      "Iteration 1256, loss = 0.48571365\n",
      "Iteration 1257, loss = 0.48448015\n",
      "Iteration 1258, loss = 0.48416643\n",
      "Iteration 1259, loss = 0.48699026\n",
      "Iteration 1260, loss = 0.48709000\n",
      "Iteration 1261, loss = 0.48633411\n",
      "Iteration 1262, loss = 0.48559758\n",
      "Iteration 1263, loss = 0.48475814\n",
      "Iteration 1264, loss = 0.48423902\n",
      "Iteration 1265, loss = 0.48531647\n",
      "Iteration 1266, loss = 0.48592526\n",
      "Iteration 1267, loss = 0.48600531\n",
      "Iteration 1268, loss = 0.48582110\n",
      "Iteration 1269, loss = 0.48541632\n",
      "Iteration 1270, loss = 0.48491183\n",
      "Iteration 1271, loss = 0.48440823\n",
      "Iteration 1272, loss = 0.48490263\n",
      "Iteration 1273, loss = 0.48469616\n",
      "Iteration 1274, loss = 0.48449886\n",
      "Iteration 1275, loss = 0.48448876\n",
      "Iteration 1276, loss = 0.48464178\n",
      "Iteration 1277, loss = 0.48463934\n",
      "Iteration 1278, loss = 0.48506432\n",
      "Iteration 1279, loss = 0.48561820\n",
      "Iteration 1280, loss = 0.48554981\n",
      "Iteration 1281, loss = 0.48456358\n",
      "Iteration 1282, loss = 0.48399230\n",
      "Iteration 1283, loss = 0.48642504\n",
      "Iteration 1284, loss = 0.48985035\n",
      "Iteration 1285, loss = 0.48879553\n",
      "Iteration 1286, loss = 0.48546320\n",
      "Iteration 1287, loss = 0.48444299\n",
      "Iteration 1288, loss = 0.48689618\n",
      "Iteration 1289, loss = 0.48777385\n",
      "Iteration 1290, loss = 0.48740354\n",
      "Iteration 1291, loss = 0.48634373\n",
      "Iteration 1292, loss = 0.48535376\n",
      "Iteration 1293, loss = 0.48465948\n",
      "Iteration 1294, loss = 0.48482925\n",
      "Iteration 1295, loss = 0.48434502\n",
      "Iteration 1296, loss = 0.48505002\n",
      "Iteration 1297, loss = 0.48562162\n",
      "Iteration 1298, loss = 0.48587753\n",
      "Iteration 1299, loss = 0.48603737\n",
      "Iteration 1300, loss = 0.48658808\n",
      "Iteration 1301, loss = 0.48687195\n",
      "Iteration 1302, loss = 0.48656404\n",
      "Iteration 1303, loss = 0.48540086\n",
      "Iteration 1304, loss = 0.48453726\n",
      "Iteration 1305, loss = 0.48487135\n",
      "Iteration 1306, loss = 0.48513692\n",
      "Iteration 1307, loss = 0.48493832\n",
      "Iteration 1308, loss = 0.48420890\n",
      "Iteration 1309, loss = 0.48519317\n",
      "Iteration 1310, loss = 0.48534668\n",
      "Iteration 1311, loss = 0.48535840\n",
      "Iteration 1312, loss = 0.48490884\n",
      "Iteration 1313, loss = 0.48479582\n",
      "Iteration 1314, loss = 0.48466950\n",
      "Iteration 1315, loss = 0.48445824\n",
      "Iteration 1316, loss = 0.48453140\n",
      "Iteration 1317, loss = 0.48436782\n",
      "Iteration 1318, loss = 0.48495160\n",
      "Iteration 1319, loss = 0.48606173\n",
      "Iteration 1320, loss = 0.48654666\n",
      "Iteration 1321, loss = 0.48620307\n",
      "Iteration 1322, loss = 0.48515779\n",
      "Iteration 1323, loss = 0.48439605\n",
      "Iteration 1324, loss = 0.48564552\n",
      "Iteration 1325, loss = 0.48654924\n",
      "Iteration 1326, loss = 0.48682015\n",
      "Iteration 1327, loss = 0.48548428\n",
      "Iteration 1328, loss = 0.48533947\n",
      "Iteration 1329, loss = 0.48468575\n",
      "Iteration 1330, loss = 0.48446460\n",
      "Iteration 1331, loss = 0.48458768\n",
      "Iteration 1332, loss = 0.48542919\n",
      "Iteration 1333, loss = 0.48632153\n",
      "Iteration 1334, loss = 0.48632973\n",
      "Iteration 1335, loss = 0.48535695\n",
      "Iteration 1336, loss = 0.48508613\n",
      "Iteration 1337, loss = 0.48486905\n",
      "Iteration 1338, loss = 0.48466800\n",
      "Iteration 1339, loss = 0.48442889\n",
      "Iteration 1340, loss = 0.48420407\n",
      "Iteration 1341, loss = 0.48457113\n",
      "Iteration 1342, loss = 0.48533370\n",
      "Iteration 1343, loss = 0.48636771\n",
      "Iteration 1344, loss = 0.48601387\n",
      "Iteration 1345, loss = 0.48489370\n",
      "Iteration 1346, loss = 0.48483736\n",
      "Iteration 1347, loss = 0.48449432\n",
      "Iteration 1348, loss = 0.48430220\n",
      "Iteration 1349, loss = 0.48460493\n",
      "Iteration 1350, loss = 0.48502148\n",
      "Iteration 1351, loss = 0.48447516\n",
      "Iteration 1352, loss = 0.48429176\n",
      "Iteration 1353, loss = 0.48493608\n",
      "Iteration 1354, loss = 0.48656504\n",
      "Iteration 1355, loss = 0.48761368\n",
      "Iteration 1356, loss = 0.48785695\n",
      "Iteration 1357, loss = 0.48759051\n",
      "Iteration 1358, loss = 0.48716999\n",
      "Iteration 1359, loss = 0.48580817\n",
      "Iteration 1360, loss = 0.48539488\n",
      "Iteration 1361, loss = 0.48450451\n",
      "Iteration 1362, loss = 0.48439699\n",
      "Iteration 1363, loss = 0.48432926\n",
      "Iteration 1364, loss = 0.48426644\n",
      "Iteration 1365, loss = 0.48469175\n",
      "Iteration 1366, loss = 0.48620959\n",
      "Iteration 1367, loss = 0.48649453\n",
      "Iteration 1368, loss = 0.48531222\n",
      "Iteration 1369, loss = 0.48428528\n",
      "Iteration 1370, loss = 0.48486113\n",
      "Iteration 1371, loss = 0.48556560\n",
      "Iteration 1372, loss = 0.48640939\n",
      "Iteration 1373, loss = 0.48694445\n",
      "Iteration 1374, loss = 0.48539103\n",
      "Iteration 1375, loss = 0.48439700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1376, loss = 0.48582726\n",
      "Iteration 1377, loss = 0.48739674\n",
      "Iteration 1378, loss = 0.48743741\n",
      "Iteration 1379, loss = 0.48571083\n",
      "Iteration 1380, loss = 0.48387834\n",
      "Iteration 1381, loss = 0.48505153\n",
      "Iteration 1382, loss = 0.48783643\n",
      "Iteration 1383, loss = 0.49070914\n",
      "Iteration 1384, loss = 0.49086113\n",
      "Iteration 1385, loss = 0.48835183\n",
      "Iteration 1386, loss = 0.48579104\n",
      "Iteration 1387, loss = 0.48461011\n",
      "Iteration 1388, loss = 0.48484593\n",
      "Iteration 1389, loss = 0.48524585\n",
      "Iteration 1390, loss = 0.48521976\n",
      "Iteration 1391, loss = 0.48470616\n",
      "Iteration 1392, loss = 0.48454622\n",
      "Iteration 1393, loss = 0.48472168\n",
      "Iteration 1394, loss = 0.48444422\n",
      "Iteration 1395, loss = 0.48443800\n",
      "Iteration 1396, loss = 0.48449793\n",
      "Iteration 1397, loss = 0.48452224\n",
      "Iteration 1398, loss = 0.48445049\n",
      "Iteration 1399, loss = 0.48468552\n",
      "Iteration 1400, loss = 0.48441274\n",
      "Iteration 1401, loss = 0.48457092\n",
      "Iteration 1402, loss = 0.48433495\n",
      "Iteration 1403, loss = 0.48457117\n",
      "Iteration 1404, loss = 0.48485190\n",
      "Iteration 1405, loss = 0.48480386\n",
      "Iteration 1406, loss = 0.48441133\n",
      "Iteration 1407, loss = 0.48438837\n",
      "Iteration 1408, loss = 0.48437047\n",
      "Iteration 1409, loss = 0.48443669\n",
      "Iteration 1410, loss = 0.48461534\n",
      "Iteration 1411, loss = 0.48527763\n",
      "Iteration 1412, loss = 0.48510530\n",
      "Iteration 1413, loss = 0.48435204\n",
      "Iteration 1414, loss = 0.48395499\n",
      "Iteration 1415, loss = 0.48511554\n",
      "Iteration 1416, loss = 0.48697086\n",
      "Iteration 1417, loss = 0.48884895\n",
      "Iteration 1418, loss = 0.48656754\n",
      "Iteration 1419, loss = 0.48512104\n",
      "Iteration 1420, loss = 0.48570043\n",
      "Iteration 1421, loss = 0.48921622\n",
      "Iteration 1422, loss = 0.49106196\n",
      "Iteration 1423, loss = 0.49023192\n",
      "Iteration 1424, loss = 0.48762729\n",
      "Iteration 1425, loss = 0.48620738\n",
      "Iteration 1426, loss = 0.48544363\n",
      "Iteration 1427, loss = 0.48474031\n",
      "Iteration 1428, loss = 0.48439794\n",
      "Iteration 1429, loss = 0.48481683\n",
      "Iteration 1430, loss = 0.48491304\n",
      "Iteration 1431, loss = 0.48475935\n",
      "Iteration 1432, loss = 0.48480743\n",
      "Iteration 1433, loss = 0.48506373\n",
      "Iteration 1434, loss = 0.48579511\n",
      "Iteration 1435, loss = 0.48411159\n",
      "Iteration 1436, loss = 0.48518481\n",
      "Iteration 1437, loss = 0.48675142\n",
      "Iteration 1438, loss = 0.48661400\n",
      "Iteration 1439, loss = 0.48528897\n",
      "Iteration 1440, loss = 0.48457421\n",
      "Iteration 1441, loss = 0.48490808\n",
      "Iteration 1442, loss = 0.48389904\n",
      "Iteration 1443, loss = 0.48570102\n",
      "Iteration 1444, loss = 0.48676991\n",
      "Iteration 1445, loss = 0.48738193\n",
      "Iteration 1446, loss = 0.48797869\n",
      "Iteration 1447, loss = 0.48698888\n",
      "Iteration 1448, loss = 0.48451014\n",
      "Iteration 1449, loss = 0.48460761\n",
      "Iteration 1450, loss = 0.48458214\n",
      "Iteration 1451, loss = 0.48450008\n",
      "Iteration 1452, loss = 0.48518935\n",
      "Iteration 1453, loss = 0.48433068\n",
      "Iteration 1454, loss = 0.48457617\n",
      "Iteration 1455, loss = 0.48539206\n",
      "Iteration 1456, loss = 0.48580729\n",
      "Iteration 1457, loss = 0.48568819\n",
      "Iteration 1458, loss = 0.48480801\n",
      "Iteration 1459, loss = 0.48476517\n",
      "Iteration 1460, loss = 0.48492273\n",
      "Iteration 1461, loss = 0.48504156\n",
      "Iteration 1462, loss = 0.48513964\n",
      "Iteration 1463, loss = 0.48478113\n",
      "Iteration 1464, loss = 0.48435049\n",
      "Iteration 1465, loss = 0.48439972\n",
      "Iteration 1466, loss = 0.48446008\n",
      "Iteration 1467, loss = 0.48519813\n",
      "Iteration 1468, loss = 0.48569417\n",
      "Iteration 1469, loss = 0.48589587\n",
      "Iteration 1470, loss = 0.48593215\n",
      "Iteration 1471, loss = 0.48548251\n",
      "Iteration 1472, loss = 0.48479709\n",
      "Iteration 1473, loss = 0.48450195\n",
      "Iteration 1474, loss = 0.48469639\n",
      "Iteration 1475, loss = 0.48511786\n",
      "Iteration 1476, loss = 0.48522894\n",
      "Iteration 1477, loss = 0.48558726\n",
      "Iteration 1478, loss = 0.48524523\n",
      "Iteration 1479, loss = 0.48451086\n",
      "Iteration 1480, loss = 0.48450573\n",
      "Iteration 1481, loss = 0.48453338\n",
      "Iteration 1482, loss = 0.48509978\n",
      "Iteration 1483, loss = 0.48627242\n",
      "Iteration 1484, loss = 0.48654320\n",
      "Iteration 1485, loss = 0.48581889\n",
      "Iteration 1486, loss = 0.48512278\n",
      "Iteration 1487, loss = 0.48444817\n",
      "Iteration 1488, loss = 0.48435187\n",
      "Iteration 1489, loss = 0.48483263\n",
      "Iteration 1490, loss = 0.48511415\n",
      "Iteration 1491, loss = 0.48484323\n",
      "Iteration 1492, loss = 0.48456511\n",
      "Iteration 1493, loss = 0.48431433\n",
      "Iteration 1494, loss = 0.48472517\n",
      "Iteration 1495, loss = 0.48510231\n",
      "Iteration 1496, loss = 0.48507479\n",
      "Iteration 1497, loss = 0.48477079\n",
      "Iteration 1498, loss = 0.48441356\n",
      "Iteration 1499, loss = 0.48410484\n",
      "Iteration 1500, loss = 0.48496539\n",
      "Iteration 1501, loss = 0.48505113\n",
      "Iteration 1502, loss = 0.48503072\n",
      "Iteration 1503, loss = 0.48467526\n",
      "Iteration 1504, loss = 0.48473750\n",
      "Iteration 1505, loss = 0.48439251\n",
      "Iteration 1506, loss = 0.48469554\n",
      "Iteration 1507, loss = 0.48461958\n",
      "Iteration 1508, loss = 0.48433838\n",
      "Iteration 1509, loss = 0.48423041\n",
      "Iteration 1510, loss = 0.48468803\n",
      "Iteration 1511, loss = 0.48517276\n",
      "Iteration 1512, loss = 0.48588573\n",
      "Iteration 1513, loss = 0.48639716\n",
      "Iteration 1514, loss = 0.48569336\n",
      "Iteration 1515, loss = 0.48495286\n",
      "Iteration 1516, loss = 0.48419976\n",
      "Iteration 1517, loss = 0.48426533\n",
      "Iteration 1518, loss = 0.48507382\n",
      "Iteration 1519, loss = 0.48528051\n",
      "Iteration 1520, loss = 0.48491067\n",
      "Iteration 1521, loss = 0.48435739\n",
      "Iteration 1522, loss = 0.48425968\n",
      "Iteration 1523, loss = 0.48429254\n",
      "Iteration 1524, loss = 0.48432479\n",
      "Iteration 1525, loss = 0.48433531\n",
      "Iteration 1526, loss = 0.48440342\n",
      "Iteration 1527, loss = 0.48462906\n",
      "Iteration 1528, loss = 0.48454649\n",
      "Iteration 1529, loss = 0.48429078\n",
      "Iteration 1530, loss = 0.48417549\n",
      "Iteration 1531, loss = 0.48458855\n",
      "Iteration 1532, loss = 0.48477687\n",
      "Iteration 1533, loss = 0.48510569\n",
      "Iteration 1534, loss = 0.48398541\n",
      "Iteration 1535, loss = 0.48443914\n",
      "Iteration 1536, loss = 0.48731886\n",
      "Iteration 1537, loss = 0.48759724\n",
      "Iteration 1538, loss = 0.48768853\n",
      "Iteration 1539, loss = 0.48623958\n",
      "Iteration 1540, loss = 0.48726475\n",
      "Iteration 1541, loss = 0.48757785\n",
      "Iteration 1542, loss = 0.48601041\n",
      "Iteration 1543, loss = 0.48369399\n",
      "Iteration 1544, loss = 0.48534420\n",
      "Iteration 1545, loss = 0.48762041\n",
      "Iteration 1546, loss = 0.48842064\n",
      "Iteration 1547, loss = 0.48806564\n",
      "Iteration 1548, loss = 0.48686501\n",
      "Iteration 1549, loss = 0.48763778\n",
      "Iteration 1550, loss = 0.48703255\n",
      "Iteration 1551, loss = 0.48507143\n",
      "Iteration 1552, loss = 0.48414840\n",
      "Iteration 1553, loss = 0.48468278\n",
      "Iteration 1554, loss = 0.48706996\n",
      "Iteration 1555, loss = 0.48967343\n",
      "Iteration 1556, loss = 0.49095092\n",
      "Iteration 1557, loss = 0.49033262\n",
      "Iteration 1558, loss = 0.48800246\n",
      "Iteration 1559, loss = 0.48634244\n",
      "Iteration 1560, loss = 0.48458480\n",
      "Iteration 1561, loss = 0.48474374\n",
      "Iteration 1562, loss = 0.48429473\n",
      "Iteration 1563, loss = 0.48433008\n",
      "Iteration 1564, loss = 0.48459902\n",
      "Iteration 1565, loss = 0.48527961\n",
      "Iteration 1566, loss = 0.48517872\n",
      "Iteration 1567, loss = 0.48439235\n",
      "Iteration 1568, loss = 0.48493069\n",
      "Iteration 1569, loss = 0.48471434\n",
      "Iteration 1570, loss = 0.48470586\n",
      "Iteration 1571, loss = 0.48480630\n",
      "Iteration 1572, loss = 0.48468002\n",
      "Iteration 1573, loss = 0.48478842\n",
      "Iteration 1574, loss = 0.48453999\n",
      "Iteration 1575, loss = 0.48474967\n",
      "Iteration 1576, loss = 0.48528405\n",
      "Iteration 1577, loss = 0.48444494\n",
      "Iteration 1578, loss = 0.48532534\n",
      "Iteration 1579, loss = 0.48563232\n",
      "Iteration 1580, loss = 0.48528961\n",
      "Iteration 1581, loss = 0.48475940\n",
      "Iteration 1582, loss = 0.48459402\n",
      "Iteration 1583, loss = 0.48420569\n",
      "Iteration 1584, loss = 0.48425900\n",
      "Iteration 1585, loss = 0.48413641\n",
      "Iteration 1586, loss = 0.48468527\n",
      "Iteration 1587, loss = 0.48526999\n",
      "Iteration 1588, loss = 0.48524789\n",
      "Iteration 1589, loss = 0.48449777\n",
      "Iteration 1590, loss = 0.48460970\n",
      "Iteration 1591, loss = 0.48463992\n",
      "Iteration 1592, loss = 0.48477134\n",
      "Iteration 1593, loss = 0.48503230\n",
      "Iteration 1594, loss = 0.48434307\n",
      "Iteration 1595, loss = 0.48523997\n",
      "Iteration 1596, loss = 0.48486090\n",
      "Iteration 1597, loss = 0.48458271\n",
      "Iteration 1598, loss = 0.48475746\n",
      "Iteration 1599, loss = 0.48442651\n",
      "Iteration 1600, loss = 0.48449593\n",
      "Iteration 1601, loss = 0.48422431\n",
      "Iteration 1602, loss = 0.48419236\n",
      "Iteration 1603, loss = 0.48427143\n",
      "Iteration 1604, loss = 0.48488593\n",
      "Iteration 1605, loss = 0.48492300\n",
      "Iteration 1606, loss = 0.48482481\n",
      "Iteration 1607, loss = 0.48422459\n",
      "Iteration 1608, loss = 0.48413045\n",
      "Iteration 1609, loss = 0.48491239\n",
      "Iteration 1610, loss = 0.48485615\n",
      "Iteration 1611, loss = 0.48450709\n",
      "Iteration 1612, loss = 0.48530537\n",
      "Iteration 1613, loss = 0.48443837\n",
      "Iteration 1614, loss = 0.48441716\n",
      "Iteration 1615, loss = 0.48432942\n",
      "Iteration 1616, loss = 0.48428491\n",
      "Iteration 1617, loss = 0.48449788\n",
      "Iteration 1618, loss = 0.48450432\n",
      "Iteration 1619, loss = 0.48428771\n",
      "Iteration 1620, loss = 0.48409560\n",
      "Iteration 1621, loss = 0.48506722\n",
      "Iteration 1622, loss = 0.48711406\n",
      "Iteration 1623, loss = 0.48707775\n",
      "Iteration 1624, loss = 0.48558006\n",
      "Iteration 1625, loss = 0.48381588\n",
      "Iteration 1626, loss = 0.48417952\n",
      "Iteration 1627, loss = 0.48808438\n",
      "Iteration 1628, loss = 0.49219124\n",
      "Iteration 1629, loss = 0.49385285\n",
      "Iteration 1630, loss = 0.49092747\n",
      "Iteration 1631, loss = 0.48534545\n",
      "Iteration 1632, loss = 0.48307265\n",
      "Iteration 1633, loss = 0.48783438\n",
      "Iteration 1634, loss = 0.49628691\n",
      "Iteration 1635, loss = 0.49872586\n",
      "Iteration 1636, loss = 0.49307373\n",
      "Iteration 1637, loss = 0.48487211\n",
      "Iteration 1638, loss = 0.48437887\n",
      "Iteration 1639, loss = 0.49209717\n",
      "Iteration 1640, loss = 0.49760356\n",
      "Iteration 1641, loss = 0.49446561\n",
      "Iteration 1642, loss = 0.48816947\n",
      "Iteration 1643, loss = 0.48403582\n",
      "Iteration 1644, loss = 0.48669546\n",
      "Iteration 1645, loss = 0.48807242\n",
      "Iteration 1646, loss = 0.48866504\n",
      "Iteration 1647, loss = 0.48837239\n",
      "Iteration 1648, loss = 0.48699315\n",
      "Iteration 1649, loss = 0.48576964\n",
      "Iteration 1650, loss = 0.48453749\n",
      "Iteration 1651, loss = 0.48417324\n",
      "Iteration 1652, loss = 0.48457997\n",
      "Iteration 1653, loss = 0.48648703\n",
      "Iteration 1654, loss = 0.48714248\n",
      "Iteration 1655, loss = 0.48580333\n",
      "Iteration 1656, loss = 0.48422212\n",
      "Iteration 1657, loss = 0.48425048\n",
      "Iteration 1658, loss = 0.48620211\n",
      "Iteration 1659, loss = 0.48834785\n",
      "Iteration 1660, loss = 0.48954047\n",
      "Iteration 1661, loss = 0.48874638\n",
      "Iteration 1662, loss = 0.48634282\n",
      "Iteration 1663, loss = 0.48422278\n",
      "Iteration 1664, loss = 0.48461896\n",
      "Iteration 1665, loss = 0.48632967\n",
      "Iteration 1666, loss = 0.48804955\n",
      "Iteration 1667, loss = 0.48946403\n",
      "Iteration 1668, loss = 0.49060421\n",
      "Iteration 1669, loss = 0.48978723\n",
      "Iteration 1670, loss = 0.48686948\n",
      "Iteration 1671, loss = 0.48423048\n",
      "Iteration 1672, loss = 0.48492854\n",
      "Iteration 1673, loss = 0.48688909\n",
      "Iteration 1674, loss = 0.48934555\n",
      "Iteration 1675, loss = 0.49005687\n",
      "Iteration 1676, loss = 0.48853877\n",
      "Iteration 1677, loss = 0.48611548\n",
      "Iteration 1678, loss = 0.48507018\n",
      "Iteration 1679, loss = 0.48440333\n",
      "Iteration 1680, loss = 0.48432240\n",
      "Iteration 1681, loss = 0.48434083\n",
      "Iteration 1682, loss = 0.48448721\n",
      "Iteration 1683, loss = 0.48416573\n",
      "Iteration 1684, loss = 0.48425245\n",
      "Iteration 1685, loss = 0.48502055\n",
      "Iteration 1686, loss = 0.48557668\n",
      "Iteration 1687, loss = 0.48575163\n",
      "Iteration 1688, loss = 0.48496073\n",
      "Iteration 1689, loss = 0.48443228\n",
      "Iteration 1690, loss = 0.48439024\n",
      "Iteration 1691, loss = 0.48462525\n",
      "Iteration 1692, loss = 0.48458556\n",
      "Iteration 1693, loss = 0.48412280\n",
      "Iteration 1694, loss = 0.48493809\n",
      "Iteration 1695, loss = 0.48459653\n",
      "Iteration 1696, loss = 0.48429731\n",
      "Iteration 1697, loss = 0.48393946\n",
      "Iteration 1698, loss = 0.48419976\n",
      "Iteration 1699, loss = 0.48638833\n",
      "Iteration 1700, loss = 0.48740677\n",
      "Iteration 1701, loss = 0.48622109\n",
      "Iteration 1702, loss = 0.48436603\n",
      "Iteration 1703, loss = 0.48429015\n",
      "Iteration 1704, loss = 0.48508327\n",
      "Iteration 1705, loss = 0.48532235\n",
      "Iteration 1706, loss = 0.48504527\n",
      "Iteration 1707, loss = 0.48444939\n",
      "Iteration 1708, loss = 0.48385571\n",
      "Iteration 1709, loss = 0.48474870\n",
      "Iteration 1710, loss = 0.48627492\n",
      "Iteration 1711, loss = 0.48746079\n",
      "Iteration 1712, loss = 0.48710957\n",
      "Iteration 1713, loss = 0.48525574\n",
      "Iteration 1714, loss = 0.48396485\n",
      "Iteration 1715, loss = 0.48451813\n",
      "Iteration 1716, loss = 0.48683963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1717, loss = 0.48663031\n",
      "Iteration 1718, loss = 0.48614313\n",
      "Iteration 1719, loss = 0.48411428\n",
      "Iteration 1720, loss = 0.48400594\n",
      "Iteration 1721, loss = 0.48450618\n",
      "Iteration 1722, loss = 0.48694611\n",
      "Iteration 1723, loss = 0.48773082\n",
      "Iteration 1724, loss = 0.48634364\n",
      "Iteration 1725, loss = 0.48527665\n",
      "Iteration 1726, loss = 0.48415594\n",
      "Iteration 1727, loss = 0.48413612\n",
      "Iteration 1728, loss = 0.48425269\n",
      "Iteration 1729, loss = 0.48441682\n",
      "Iteration 1730, loss = 0.48456117\n",
      "Iteration 1731, loss = 0.48463379\n",
      "Iteration 1732, loss = 0.48468081\n",
      "Iteration 1733, loss = 0.48442987\n",
      "Iteration 1734, loss = 0.48402411\n",
      "Iteration 1735, loss = 0.48442043\n",
      "Iteration 1736, loss = 0.48509621\n",
      "Iteration 1737, loss = 0.48530184\n",
      "Iteration 1738, loss = 0.48491760\n",
      "Iteration 1739, loss = 0.48454290\n",
      "Iteration 1740, loss = 0.48403808\n",
      "Iteration 1741, loss = 0.48421362\n",
      "Iteration 1742, loss = 0.48571784\n",
      "Iteration 1743, loss = 0.48603474\n",
      "Iteration 1744, loss = 0.48575070\n",
      "Iteration 1745, loss = 0.48529368\n",
      "Iteration 1746, loss = 0.48438592\n",
      "Iteration 1747, loss = 0.48378682\n",
      "Iteration 1748, loss = 0.48486547\n",
      "Iteration 1749, loss = 0.48714629\n",
      "Iteration 1750, loss = 0.48840855\n",
      "Iteration 1751, loss = 0.48655136\n",
      "Iteration 1752, loss = 0.48508048\n",
      "Iteration 1753, loss = 0.48400144\n",
      "Iteration 1754, loss = 0.48600493\n",
      "Iteration 1755, loss = 0.48813459\n",
      "Iteration 1756, loss = 0.48833263\n",
      "Iteration 1757, loss = 0.48674392\n",
      "Iteration 1758, loss = 0.48557978\n",
      "Iteration 1759, loss = 0.48447965\n",
      "Iteration 1760, loss = 0.48429749\n",
      "Iteration 1761, loss = 0.48421928\n",
      "Iteration 1762, loss = 0.48442305\n",
      "Iteration 1763, loss = 0.48449102\n",
      "Iteration 1764, loss = 0.48501018\n",
      "Iteration 1765, loss = 0.48408930\n",
      "Iteration 1766, loss = 0.48482992\n",
      "Iteration 1767, loss = 0.48533228\n",
      "Iteration 1768, loss = 0.48456875\n",
      "Iteration 1769, loss = 0.48430297\n",
      "Iteration 1770, loss = 0.48475225\n",
      "Iteration 1771, loss = 0.48496512\n",
      "Iteration 1772, loss = 0.48492872\n",
      "Iteration 1773, loss = 0.48489435\n",
      "Iteration 1774, loss = 0.48521867\n",
      "Iteration 1775, loss = 0.48568686\n",
      "Iteration 1776, loss = 0.48455096\n",
      "Iteration 1777, loss = 0.48367259\n",
      "Iteration 1778, loss = 0.48612123\n",
      "Iteration 1779, loss = 0.48717195\n",
      "Iteration 1780, loss = 0.48481848\n",
      "Iteration 1781, loss = 0.48585937\n",
      "Iteration 1782, loss = 0.48560870\n",
      "Iteration 1783, loss = 0.48666293\n",
      "Iteration 1784, loss = 0.48478122\n",
      "Iteration 1785, loss = 0.48547851\n",
      "Iteration 1786, loss = 0.48489070\n",
      "Iteration 1787, loss = 0.48606116\n",
      "Iteration 1788, loss = 0.48628687\n",
      "Iteration 1789, loss = 0.48521431\n",
      "Iteration 1790, loss = 0.48381584\n",
      "Iteration 1791, loss = 0.48399100\n",
      "Iteration 1792, loss = 0.48627670\n",
      "Iteration 1793, loss = 0.48852211\n",
      "Iteration 1794, loss = 0.48878361\n",
      "Iteration 1795, loss = 0.48716490\n",
      "Iteration 1796, loss = 0.48521196\n",
      "Iteration 1797, loss = 0.48449720\n",
      "Iteration 1798, loss = 0.48420790\n",
      "Iteration 1799, loss = 0.48412483\n",
      "Iteration 1800, loss = 0.48417960\n",
      "Iteration 1801, loss = 0.48492106\n",
      "Iteration 1802, loss = 0.48539764\n",
      "Iteration 1803, loss = 0.48543236\n",
      "Iteration 1804, loss = 0.48554565\n",
      "Iteration 1805, loss = 0.48473049\n",
      "Iteration 1806, loss = 0.48502181\n",
      "Iteration 1807, loss = 0.48504379\n",
      "Iteration 1808, loss = 0.48416588\n",
      "Iteration 1809, loss = 0.48450583\n",
      "Iteration 1810, loss = 0.48473650\n",
      "Iteration 1811, loss = 0.48560728\n",
      "Iteration 1812, loss = 0.48560569\n",
      "Iteration 1813, loss = 0.48467053\n",
      "Iteration 1814, loss = 0.48462608\n",
      "Iteration 1815, loss = 0.48400251\n",
      "Iteration 1816, loss = 0.48422113\n",
      "Iteration 1817, loss = 0.48498949\n",
      "Iteration 1818, loss = 0.48450662\n",
      "Iteration 1819, loss = 0.48345937\n",
      "Iteration 1820, loss = 0.48501959\n",
      "Iteration 1821, loss = 0.48723115\n",
      "Iteration 1822, loss = 0.48803353\n",
      "Iteration 1823, loss = 0.48716780\n",
      "Iteration 1824, loss = 0.48607997\n",
      "Iteration 1825, loss = 0.48430739\n",
      "Iteration 1826, loss = 0.48371840\n",
      "Iteration 1827, loss = 0.48469668\n",
      "Iteration 1828, loss = 0.48609079\n",
      "Iteration 1829, loss = 0.48630467\n",
      "Iteration 1830, loss = 0.48572298\n",
      "Iteration 1831, loss = 0.48453670\n",
      "Iteration 1832, loss = 0.48389187\n",
      "Iteration 1833, loss = 0.48511838\n",
      "Iteration 1834, loss = 0.48725627\n",
      "Iteration 1835, loss = 0.48801674\n",
      "Iteration 1836, loss = 0.48710384\n",
      "Iteration 1837, loss = 0.48605337\n",
      "Iteration 1838, loss = 0.48460107\n",
      "Iteration 1839, loss = 0.48402114\n",
      "Iteration 1840, loss = 0.48403479\n",
      "Iteration 1841, loss = 0.48434771\n",
      "Iteration 1842, loss = 0.48386661\n",
      "Iteration 1843, loss = 0.48415623\n",
      "Iteration 1844, loss = 0.48541790\n",
      "Iteration 1845, loss = 0.48684027\n",
      "Iteration 1846, loss = 0.48618125\n",
      "Iteration 1847, loss = 0.48592280\n",
      "Iteration 1848, loss = 0.48398031\n",
      "Iteration 1849, loss = 0.48431187\n",
      "Iteration 1850, loss = 0.48473347\n",
      "Iteration 1851, loss = 0.48523892\n",
      "Iteration 1852, loss = 0.48495054\n",
      "Iteration 1853, loss = 0.48401182\n",
      "Iteration 1854, loss = 0.48434130\n",
      "Iteration 1855, loss = 0.48516934\n",
      "Iteration 1856, loss = 0.48505734\n",
      "Iteration 1857, loss = 0.48437680\n",
      "Iteration 1858, loss = 0.48381560\n",
      "Iteration 1859, loss = 0.48461898\n",
      "Iteration 1860, loss = 0.48499558\n",
      "Iteration 1861, loss = 0.48504688\n",
      "Iteration 1862, loss = 0.48460866\n",
      "Iteration 1863, loss = 0.48384046\n",
      "Iteration 1864, loss = 0.48416731\n",
      "Iteration 1865, loss = 0.48544618\n",
      "Iteration 1866, loss = 0.48617021\n",
      "Iteration 1867, loss = 0.48563435\n",
      "Iteration 1868, loss = 0.48435793\n",
      "Iteration 1869, loss = 0.48429483\n",
      "Iteration 1870, loss = 0.48414916\n",
      "Iteration 1871, loss = 0.48458051\n",
      "Iteration 1872, loss = 0.48558147\n",
      "Iteration 1873, loss = 0.48594026\n",
      "Iteration 1874, loss = 0.48530621\n",
      "Iteration 1875, loss = 0.48434920\n",
      "Iteration 1876, loss = 0.48385377\n",
      "Iteration 1877, loss = 0.48521146\n",
      "Iteration 1878, loss = 0.48528118\n",
      "Iteration 1879, loss = 0.48462782\n",
      "Iteration 1880, loss = 0.48417001\n",
      "Iteration 1881, loss = 0.48462097\n",
      "Iteration 1882, loss = 0.48451987\n",
      "Iteration 1883, loss = 0.48453254\n",
      "Iteration 1884, loss = 0.48421734\n",
      "Iteration 1885, loss = 0.48395090\n",
      "Iteration 1886, loss = 0.48427436\n",
      "Iteration 1887, loss = 0.48489193\n",
      "Iteration 1888, loss = 0.48513087\n",
      "Iteration 1889, loss = 0.48496241\n",
      "Iteration 1890, loss = 0.48478938\n",
      "Iteration 1891, loss = 0.48454944\n",
      "Iteration 1892, loss = 0.48484738\n",
      "Iteration 1893, loss = 0.48479153\n",
      "Iteration 1894, loss = 0.48443835\n",
      "Iteration 1895, loss = 0.48409916\n",
      "Iteration 1896, loss = 0.48397978\n",
      "Iteration 1897, loss = 0.48390739\n",
      "Iteration 1898, loss = 0.48505188\n",
      "Iteration 1899, loss = 0.48601099\n",
      "Iteration 1900, loss = 0.48627008\n",
      "Iteration 1901, loss = 0.48549119\n",
      "Iteration 1902, loss = 0.48376142\n",
      "Iteration 1903, loss = 0.48462706\n",
      "Iteration 1904, loss = 0.48592794\n",
      "Iteration 1905, loss = 0.48727966\n",
      "Iteration 1906, loss = 0.48741093\n",
      "Iteration 1907, loss = 0.48627818\n",
      "Iteration 1908, loss = 0.48431382\n",
      "Iteration 1909, loss = 0.48417864\n",
      "Iteration 1910, loss = 0.48490968\n",
      "Iteration 1911, loss = 0.48563803\n",
      "Iteration 1912, loss = 0.48541014\n",
      "Iteration 1913, loss = 0.48459605\n",
      "Iteration 1914, loss = 0.48436885\n",
      "Iteration 1915, loss = 0.48415965\n",
      "Iteration 1916, loss = 0.48410304\n",
      "Iteration 1917, loss = 0.48416470\n",
      "Iteration 1918, loss = 0.48405349\n",
      "Iteration 1919, loss = 0.48407904\n",
      "Iteration 1920, loss = 0.48418568\n",
      "Iteration 1921, loss = 0.48442329\n",
      "Iteration 1922, loss = 0.48468156\n",
      "Iteration 1923, loss = 0.48621000\n",
      "Iteration 1924, loss = 0.48669669\n",
      "Iteration 1925, loss = 0.48626940\n",
      "Iteration 1926, loss = 0.48557153\n",
      "Iteration 1927, loss = 0.48453389\n",
      "Iteration 1928, loss = 0.48440172\n",
      "Iteration 1929, loss = 0.48443725\n",
      "Iteration 1930, loss = 0.48436191\n",
      "Iteration 1931, loss = 0.48418343\n",
      "Iteration 1932, loss = 0.48417722\n",
      "Iteration 1933, loss = 0.48453484\n",
      "Iteration 1934, loss = 0.48521865\n",
      "Iteration 1935, loss = 0.48567144\n",
      "Iteration 1936, loss = 0.48464512\n",
      "Iteration 1937, loss = 0.48332117\n",
      "Iteration 1938, loss = 0.48477983\n",
      "Iteration 1939, loss = 0.48811749\n",
      "Iteration 1940, loss = 0.49273556\n",
      "Iteration 1941, loss = 0.49463824\n",
      "Iteration 1942, loss = 0.49080242\n",
      "Iteration 1943, loss = 0.48514432\n",
      "Iteration 1944, loss = 0.48351217\n",
      "Iteration 1945, loss = 0.48713534\n",
      "Iteration 1946, loss = 0.49047130\n",
      "Iteration 1947, loss = 0.49117326\n",
      "Iteration 1948, loss = 0.48976274\n",
      "Iteration 1949, loss = 0.48773543\n",
      "Iteration 1950, loss = 0.48569218\n",
      "Iteration 1951, loss = 0.48466800\n",
      "Iteration 1952, loss = 0.48426356\n",
      "Iteration 1953, loss = 0.48445379\n",
      "Iteration 1954, loss = 0.48420888\n",
      "Iteration 1955, loss = 0.48487105\n",
      "Iteration 1956, loss = 0.48533506\n",
      "Iteration 1957, loss = 0.48515671\n",
      "Iteration 1958, loss = 0.48565002\n",
      "Iteration 1959, loss = 0.48584729\n",
      "Iteration 1960, loss = 0.48585152\n",
      "Iteration 1961, loss = 0.48537516\n",
      "Iteration 1962, loss = 0.48461425\n",
      "Iteration 1963, loss = 0.48412891\n",
      "Iteration 1964, loss = 0.48450868\n",
      "Iteration 1965, loss = 0.48401225\n",
      "Iteration 1966, loss = 0.48426610\n",
      "Iteration 1967, loss = 0.48459619\n",
      "Iteration 1968, loss = 0.48490402\n",
      "Iteration 1969, loss = 0.48449673\n",
      "Iteration 1970, loss = 0.48503528\n",
      "Iteration 1971, loss = 0.48407642\n",
      "Iteration 1972, loss = 0.48420459\n",
      "Iteration 1973, loss = 0.48427508\n",
      "Iteration 1974, loss = 0.48423286\n",
      "Iteration 1975, loss = 0.48405764\n",
      "Iteration 1976, loss = 0.48399008\n",
      "Iteration 1977, loss = 0.48404751\n",
      "Iteration 1978, loss = 0.48405759\n",
      "Iteration 1979, loss = 0.48418506\n",
      "Iteration 1980, loss = 0.48465206\n",
      "Iteration 1981, loss = 0.48510798\n",
      "Iteration 1982, loss = 0.48542396\n",
      "Iteration 1983, loss = 0.48556349\n",
      "Iteration 1984, loss = 0.48506469\n",
      "Iteration 1985, loss = 0.48440116\n",
      "Iteration 1986, loss = 0.48403249\n",
      "Iteration 1987, loss = 0.48410223\n",
      "Iteration 1988, loss = 0.48413204\n",
      "Iteration 1989, loss = 0.48393126\n",
      "Iteration 1990, loss = 0.48411823\n",
      "Iteration 1991, loss = 0.48468177\n",
      "Iteration 1992, loss = 0.48457385\n",
      "Iteration 1993, loss = 0.48439936\n",
      "Iteration 1994, loss = 0.48409242\n",
      "Iteration 1995, loss = 0.48395463\n",
      "Iteration 1996, loss = 0.48395456\n",
      "Iteration 1997, loss = 0.48402887\n",
      "Iteration 1998, loss = 0.48443795\n",
      "Iteration 1999, loss = 0.48462140\n",
      "Iteration 2000, loss = 0.48442327\n",
      "Iteration 2001, loss = 0.48420282\n",
      "Iteration 2002, loss = 0.48411611\n",
      "Iteration 2003, loss = 0.48401965\n",
      "Iteration 2004, loss = 0.48427950\n",
      "Iteration 2005, loss = 0.48465088\n",
      "Iteration 2006, loss = 0.48477775\n",
      "Iteration 2007, loss = 0.48446032\n",
      "Iteration 2008, loss = 0.48400538\n",
      "Iteration 2009, loss = 0.48378220\n",
      "Iteration 2010, loss = 0.48392038\n",
      "Iteration 2011, loss = 0.48495886\n",
      "Iteration 2012, loss = 0.48676274\n",
      "Iteration 2013, loss = 0.48786052\n",
      "Iteration 2014, loss = 0.48786521\n",
      "Iteration 2015, loss = 0.48646178\n",
      "Iteration 2016, loss = 0.48357025\n",
      "Iteration 2017, loss = 0.48410709\n",
      "Iteration 2018, loss = 0.48587954\n",
      "Iteration 2019, loss = 0.48864197\n",
      "Iteration 2020, loss = 0.48915088\n",
      "Iteration 2021, loss = 0.48621711\n",
      "Iteration 2022, loss = 0.48432712\n",
      "Iteration 2023, loss = 0.48549875\n",
      "Iteration 2024, loss = 0.48588262\n",
      "Iteration 2025, loss = 0.48418560\n",
      "Iteration 2026, loss = 0.48312983\n",
      "Iteration 2027, loss = 0.48568192\n",
      "Iteration 2028, loss = 0.49039761\n",
      "Iteration 2029, loss = 0.49300973\n",
      "Iteration 2030, loss = 0.49211950\n",
      "Iteration 2031, loss = 0.48867527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2032, loss = 0.48587541\n",
      "Iteration 2033, loss = 0.48319045\n",
      "Iteration 2034, loss = 0.48559580\n",
      "Iteration 2035, loss = 0.48817185\n",
      "Iteration 2036, loss = 0.48954136\n",
      "Iteration 2037, loss = 0.48812373\n",
      "Iteration 2038, loss = 0.48544159\n",
      "Iteration 2039, loss = 0.48322572\n",
      "Iteration 2040, loss = 0.48464258\n",
      "Iteration 2041, loss = 0.48766736\n",
      "Iteration 2042, loss = 0.48988819\n",
      "Iteration 2043, loss = 0.48768603\n",
      "Iteration 2044, loss = 0.48453277\n",
      "Iteration 2045, loss = 0.48410670\n",
      "Iteration 2046, loss = 0.48727480\n",
      "Iteration 2047, loss = 0.49018714\n",
      "Iteration 2048, loss = 0.49091083\n",
      "Iteration 2049, loss = 0.48892951\n",
      "Iteration 2050, loss = 0.48604011\n",
      "Iteration 2051, loss = 0.48486641\n",
      "Iteration 2052, loss = 0.48421564\n",
      "Iteration 2053, loss = 0.48444633\n",
      "Iteration 2054, loss = 0.48509783\n",
      "Iteration 2055, loss = 0.48506054\n",
      "Iteration 2056, loss = 0.48427012\n",
      "Iteration 2057, loss = 0.48419888\n",
      "Iteration 2058, loss = 0.48413829\n",
      "Iteration 2059, loss = 0.48400457\n",
      "Iteration 2060, loss = 0.48427636\n",
      "Iteration 2061, loss = 0.48413328\n",
      "Iteration 2062, loss = 0.48404558\n",
      "Iteration 2063, loss = 0.48402635\n",
      "Iteration 2064, loss = 0.48407302\n",
      "Iteration 2065, loss = 0.48412090\n",
      "Iteration 2066, loss = 0.48390603\n",
      "Iteration 2067, loss = 0.48365727\n",
      "Iteration 2068, loss = 0.48470716\n",
      "Iteration 2069, loss = 0.48781270\n",
      "Iteration 2070, loss = 0.48963335\n",
      "Iteration 2071, loss = 0.48988776\n",
      "Iteration 2072, loss = 0.48901893\n",
      "Iteration 2073, loss = 0.48673585\n",
      "Iteration 2074, loss = 0.48395803\n",
      "Iteration 2075, loss = 0.48426450\n",
      "Iteration 2076, loss = 0.48646261\n",
      "Iteration 2077, loss = 0.48662329\n",
      "Iteration 2078, loss = 0.48560001\n",
      "Iteration 2079, loss = 0.48435826\n",
      "Iteration 2080, loss = 0.48395944\n",
      "Iteration 2081, loss = 0.48470662\n",
      "Iteration 2082, loss = 0.48455790\n",
      "Iteration 2083, loss = 0.48437221\n",
      "Iteration 2084, loss = 0.48441158\n",
      "Iteration 2085, loss = 0.48444958\n",
      "Iteration 2086, loss = 0.48471136\n",
      "Iteration 2087, loss = 0.48485275\n",
      "Iteration 2088, loss = 0.48480132\n",
      "Iteration 2089, loss = 0.48448140\n",
      "Iteration 2090, loss = 0.48350202\n",
      "Iteration 2091, loss = 0.48530041\n",
      "Iteration 2092, loss = 0.48722369\n",
      "Iteration 2093, loss = 0.48854392\n",
      "Iteration 2094, loss = 0.48939466\n",
      "Iteration 2095, loss = 0.48763487\n",
      "Iteration 2096, loss = 0.48417320\n",
      "Iteration 2097, loss = 0.48369392\n",
      "Iteration 2098, loss = 0.48697880\n",
      "Iteration 2099, loss = 0.49242267\n",
      "Iteration 2100, loss = 0.49546549\n",
      "Iteration 2101, loss = 0.49407354\n",
      "Iteration 2102, loss = 0.48928863\n",
      "Iteration 2103, loss = 0.48648445\n",
      "Iteration 2104, loss = 0.48390184\n",
      "Iteration 2105, loss = 0.48532128\n",
      "Iteration 2106, loss = 0.48558739\n",
      "Iteration 2107, loss = 0.48576520\n",
      "Iteration 2108, loss = 0.48522133\n",
      "Iteration 2109, loss = 0.48427473\n",
      "Iteration 2110, loss = 0.48389078\n",
      "Iteration 2111, loss = 0.48472378\n",
      "Iteration 2112, loss = 0.48553638\n",
      "Iteration 2113, loss = 0.48632560\n",
      "Iteration 2114, loss = 0.48617246\n",
      "Iteration 2115, loss = 0.48533961\n",
      "Iteration 2116, loss = 0.48409799\n",
      "Iteration 2117, loss = 0.48347878\n",
      "Iteration 2118, loss = 0.48489696\n",
      "Iteration 2119, loss = 0.49107887\n",
      "Iteration 2120, loss = 0.49385008\n",
      "Iteration 2121, loss = 0.49049815\n",
      "Iteration 2122, loss = 0.48498707\n",
      "Iteration 2123, loss = 0.48415577\n",
      "Iteration 2124, loss = 0.48691608\n",
      "Iteration 2125, loss = 0.49289696\n",
      "Iteration 2126, loss = 0.49443044\n",
      "Iteration 2127, loss = 0.49095454\n",
      "Iteration 2128, loss = 0.48569523\n",
      "Iteration 2129, loss = 0.48467755\n",
      "Iteration 2130, loss = 0.48599553\n",
      "Iteration 2131, loss = 0.48888448\n",
      "Iteration 2132, loss = 0.48916137\n",
      "Iteration 2133, loss = 0.48706881\n",
      "Iteration 2134, loss = 0.48595061\n",
      "Iteration 2135, loss = 0.48424525\n",
      "Iteration 2136, loss = 0.48377247\n",
      "Iteration 2137, loss = 0.48507369\n",
      "Iteration 2138, loss = 0.48706591\n",
      "Iteration 2139, loss = 0.48614872\n",
      "Iteration 2140, loss = 0.48502771\n",
      "Iteration 2141, loss = 0.48417699\n",
      "Iteration 2142, loss = 0.48430275\n",
      "Iteration 2143, loss = 0.48457674\n",
      "Iteration 2144, loss = 0.48386376\n",
      "Iteration 2145, loss = 0.48438241\n",
      "Iteration 2146, loss = 0.48520307\n",
      "Iteration 2147, loss = 0.48585371\n",
      "Iteration 2148, loss = 0.48642567\n",
      "Iteration 2149, loss = 0.48793666\n",
      "Iteration 2150, loss = 0.48759737\n",
      "Iteration 2151, loss = 0.48603807\n",
      "Iteration 2152, loss = 0.48532421\n",
      "Iteration 2153, loss = 0.48430874\n",
      "Iteration 2154, loss = 0.48412600\n",
      "Iteration 2155, loss = 0.48414877\n",
      "Iteration 2156, loss = 0.48414703\n",
      "Iteration 2157, loss = 0.48476199\n",
      "Iteration 2158, loss = 0.48612856\n",
      "Iteration 2159, loss = 0.48753267\n",
      "Iteration 2160, loss = 0.48619738\n",
      "Iteration 2161, loss = 0.48535852\n",
      "Iteration 2162, loss = 0.48409491\n",
      "Iteration 2163, loss = 0.48494496\n",
      "Iteration 2164, loss = 0.48543156\n",
      "Iteration 2165, loss = 0.48492193\n",
      "Iteration 2166, loss = 0.48403537\n",
      "Iteration 2167, loss = 0.48361376\n",
      "Iteration 2168, loss = 0.48567366\n",
      "Iteration 2169, loss = 0.48705943\n",
      "Iteration 2170, loss = 0.48869724\n",
      "Iteration 2171, loss = 0.48901253\n",
      "Iteration 2172, loss = 0.48699365\n",
      "Iteration 2173, loss = 0.48455037\n",
      "Iteration 2174, loss = 0.48337659\n",
      "Iteration 2175, loss = 0.48662131\n",
      "Iteration 2176, loss = 0.48845900\n",
      "Iteration 2177, loss = 0.48920008\n",
      "Iteration 2178, loss = 0.48846900\n",
      "Iteration 2179, loss = 0.48687458\n",
      "Iteration 2180, loss = 0.48504479\n",
      "Iteration 2181, loss = 0.48392044\n",
      "Iteration 2182, loss = 0.48477830\n",
      "Iteration 2183, loss = 0.48598354\n",
      "Iteration 2184, loss = 0.48688963\n",
      "Iteration 2185, loss = 0.48744238\n",
      "Iteration 2186, loss = 0.48739404\n",
      "Iteration 2187, loss = 0.48587109\n",
      "Iteration 2188, loss = 0.48470884\n",
      "Iteration 2189, loss = 0.48378045\n",
      "Iteration 2190, loss = 0.48468555\n",
      "Iteration 2191, loss = 0.48527719\n",
      "Iteration 2192, loss = 0.48508283\n",
      "Iteration 2193, loss = 0.48422479\n",
      "Iteration 2194, loss = 0.48402765\n",
      "Iteration 2195, loss = 0.48414791\n",
      "Iteration 2196, loss = 0.48633076\n",
      "Iteration 2197, loss = 0.49009811\n",
      "Iteration 2198, loss = 0.48966328\n",
      "Iteration 2199, loss = 0.48773400\n",
      "Iteration 2200, loss = 0.48441729\n",
      "Iteration 2201, loss = 0.48386691\n",
      "Iteration 2202, loss = 0.48446252\n",
      "Iteration 2203, loss = 0.48481306\n",
      "Iteration 2204, loss = 0.48497686\n",
      "Iteration 2205, loss = 0.48498522\n",
      "Iteration 2206, loss = 0.48447703\n",
      "Iteration 2207, loss = 0.48447196\n",
      "Iteration 2208, loss = 0.48456033\n",
      "Iteration 2209, loss = 0.48435027\n",
      "Iteration 2210, loss = 0.48481496\n",
      "Iteration 2211, loss = 0.48487827\n",
      "Iteration 2212, loss = 0.48425717\n",
      "Iteration 2213, loss = 0.48381155\n",
      "Iteration 2214, loss = 0.48425797\n",
      "Iteration 2215, loss = 0.48548334\n",
      "Iteration 2216, loss = 0.48717478\n",
      "Iteration 2217, loss = 0.48911481\n",
      "Iteration 2218, loss = 0.48897275\n",
      "Iteration 2219, loss = 0.48694003\n",
      "Iteration 2220, loss = 0.48494582\n",
      "Iteration 2221, loss = 0.48467211\n",
      "Iteration 2222, loss = 0.48418203\n",
      "Iteration 2223, loss = 0.48412968\n",
      "Iteration 2224, loss = 0.48414697\n",
      "Iteration 2225, loss = 0.48418551\n",
      "Iteration 2226, loss = 0.48422472\n",
      "Iteration 2227, loss = 0.48422063\n",
      "Iteration 2228, loss = 0.48441530\n",
      "Iteration 2229, loss = 0.48431209\n",
      "Iteration 2230, loss = 0.48578005\n",
      "Iteration 2231, loss = 0.48567139\n",
      "Iteration 2232, loss = 0.48509388\n",
      "Iteration 2233, loss = 0.48423567\n",
      "Iteration 2234, loss = 0.48389786\n",
      "Iteration 2235, loss = 0.48470051\n",
      "Iteration 2236, loss = 0.48432911\n",
      "Iteration 2237, loss = 0.48391906\n",
      "Iteration 2238, loss = 0.48415896\n",
      "Iteration 2239, loss = 0.48390732\n",
      "Iteration 2240, loss = 0.48435225\n",
      "Iteration 2241, loss = 0.48399517\n",
      "Iteration 2242, loss = 0.48391098\n",
      "Iteration 2243, loss = 0.48399237\n",
      "Iteration 2244, loss = 0.48383411\n",
      "Iteration 2245, loss = 0.48403602\n",
      "Iteration 2246, loss = 0.48452529\n",
      "Iteration 2247, loss = 0.48461554\n",
      "Iteration 2248, loss = 0.48473528\n",
      "Iteration 2249, loss = 0.48429544\n",
      "Iteration 2250, loss = 0.48415261\n",
      "Iteration 2251, loss = 0.48414629\n",
      "Iteration 2252, loss = 0.48389662\n",
      "Iteration 2253, loss = 0.48387236\n",
      "Iteration 2254, loss = 0.48395838\n",
      "Iteration 2255, loss = 0.48402859\n",
      "Iteration 2256, loss = 0.48388783\n",
      "Iteration 2257, loss = 0.48385000\n",
      "Iteration 2258, loss = 0.48385168\n",
      "Iteration 2259, loss = 0.48426508\n",
      "Iteration 2260, loss = 0.48465412\n",
      "Iteration 2261, loss = 0.48406663\n",
      "Iteration 2262, loss = 0.48382650\n",
      "Iteration 2263, loss = 0.48455885\n",
      "Iteration 2264, loss = 0.48477750\n",
      "Iteration 2265, loss = 0.48467920\n",
      "Iteration 2266, loss = 0.48433995\n",
      "Iteration 2267, loss = 0.48419361\n",
      "Iteration 2268, loss = 0.48429497\n",
      "Iteration 2269, loss = 0.48422765\n",
      "Iteration 2270, loss = 0.48447132\n",
      "Iteration 2271, loss = 0.48465610\n",
      "Iteration 2272, loss = 0.48457939\n",
      "Iteration 2273, loss = 0.48388743\n",
      "Iteration 2274, loss = 0.48351871\n",
      "Iteration 2275, loss = 0.48443383\n",
      "Iteration 2276, loss = 0.48665046\n",
      "Iteration 2277, loss = 0.48681830\n",
      "Iteration 2278, loss = 0.48525380\n",
      "Iteration 2279, loss = 0.48403293\n",
      "Iteration 2280, loss = 0.48355420\n",
      "Iteration 2281, loss = 0.48550148\n",
      "Iteration 2282, loss = 0.48683541\n",
      "Iteration 2283, loss = 0.48724045\n",
      "Iteration 2284, loss = 0.48657567\n",
      "Iteration 2285, loss = 0.48635723\n",
      "Iteration 2286, loss = 0.48573238\n",
      "Iteration 2287, loss = 0.48525104\n",
      "Iteration 2288, loss = 0.48450199\n",
      "Iteration 2289, loss = 0.48445569\n",
      "Iteration 2290, loss = 0.48390170\n",
      "Iteration 2291, loss = 0.48448575\n",
      "Iteration 2292, loss = 0.48455230\n",
      "Iteration 2293, loss = 0.48419449\n",
      "Iteration 2294, loss = 0.48365240\n",
      "Iteration 2295, loss = 0.48402203\n",
      "Iteration 2296, loss = 0.48490093\n",
      "Iteration 2297, loss = 0.48552459\n",
      "Iteration 2298, loss = 0.48506825\n",
      "Iteration 2299, loss = 0.48439543\n",
      "Iteration 2300, loss = 0.48383991\n",
      "Iteration 2301, loss = 0.48361749\n",
      "Iteration 2302, loss = 0.48417237\n",
      "Iteration 2303, loss = 0.48515709\n",
      "Iteration 2304, loss = 0.48639032\n",
      "Iteration 2305, loss = 0.48747916\n",
      "Iteration 2306, loss = 0.48829976\n",
      "Iteration 2307, loss = 0.48746921\n",
      "Iteration 2308, loss = 0.48500382\n",
      "Iteration 2309, loss = 0.48400949\n",
      "Iteration 2310, loss = 0.48444745\n",
      "Iteration 2311, loss = 0.48731904\n",
      "Iteration 2312, loss = 0.48786797\n",
      "Iteration 2313, loss = 0.48633325\n",
      "Iteration 2314, loss = 0.48404386\n",
      "Iteration 2315, loss = 0.48394128\n",
      "Iteration 2316, loss = 0.48440754\n",
      "Iteration 2317, loss = 0.48499795\n",
      "Iteration 2318, loss = 0.48513537\n",
      "Iteration 2319, loss = 0.48426601\n",
      "Iteration 2320, loss = 0.48422818\n",
      "Iteration 2321, loss = 0.48423376\n",
      "Iteration 2322, loss = 0.48550532\n",
      "Iteration 2323, loss = 0.48495949\n",
      "Iteration 2324, loss = 0.48378651\n",
      "Iteration 2325, loss = 0.48379417\n",
      "Iteration 2326, loss = 0.48544009\n",
      "Iteration 2327, loss = 0.48575214\n",
      "Iteration 2328, loss = 0.48537134\n",
      "Iteration 2329, loss = 0.48404168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2330, loss = 0.48390729\n",
      "Iteration 2331, loss = 0.48488998\n",
      "Iteration 2332, loss = 0.48591235\n",
      "Iteration 2333, loss = 0.48547586\n",
      "Iteration 2334, loss = 0.48413428\n",
      "Iteration 2335, loss = 0.48393439\n",
      "Iteration 2336, loss = 0.48492457\n",
      "Iteration 2337, loss = 0.48511525\n",
      "Iteration 2338, loss = 0.48482104\n",
      "Iteration 2339, loss = 0.48449859\n",
      "Iteration 2340, loss = 0.48397599\n",
      "Iteration 2341, loss = 0.48375934\n",
      "Iteration 2342, loss = 0.48431516\n",
      "Iteration 2343, loss = 0.48503592\n",
      "Iteration 2344, loss = 0.48612077\n",
      "Iteration 2345, loss = 0.48463238\n",
      "Iteration 2346, loss = 0.48426391\n",
      "Iteration 2347, loss = 0.48509713\n",
      "Iteration 2348, loss = 0.48517420\n",
      "Iteration 2349, loss = 0.48486432\n",
      "Iteration 2350, loss = 0.48472514\n",
      "Iteration 2351, loss = 0.48442370\n",
      "Iteration 2352, loss = 0.48452381\n",
      "Iteration 2353, loss = 0.48449814\n",
      "Iteration 2354, loss = 0.48408476\n",
      "Iteration 2355, loss = 0.48384291\n",
      "Iteration 2356, loss = 0.48421410\n",
      "Iteration 2357, loss = 0.48390516\n",
      "Iteration 2358, loss = 0.48383749\n",
      "Iteration 2359, loss = 0.48392449\n",
      "Iteration 2360, loss = 0.48407469\n",
      "Iteration 2361, loss = 0.48369871\n",
      "Iteration 2362, loss = 0.48406252\n",
      "Iteration 2363, loss = 0.48466745\n",
      "Iteration 2364, loss = 0.48559040\n",
      "Iteration 2365, loss = 0.48473269\n",
      "Iteration 2366, loss = 0.48347781\n",
      "Iteration 2367, loss = 0.48429105\n",
      "Iteration 2368, loss = 0.48546316\n",
      "Iteration 2369, loss = 0.48611681\n",
      "Iteration 2370, loss = 0.48586758\n",
      "Iteration 2371, loss = 0.48480421\n",
      "Iteration 2372, loss = 0.48409584\n",
      "Iteration 2373, loss = 0.48371744\n",
      "Iteration 2374, loss = 0.48457451\n",
      "Iteration 2375, loss = 0.48515268\n",
      "Iteration 2376, loss = 0.48467447\n",
      "Iteration 2377, loss = 0.48374602\n",
      "Iteration 2378, loss = 0.48357438\n",
      "Iteration 2379, loss = 0.48452337\n",
      "Iteration 2380, loss = 0.48607616\n",
      "Iteration 2381, loss = 0.48973902\n",
      "Iteration 2382, loss = 0.49002975\n",
      "Iteration 2383, loss = 0.48630380\n",
      "Iteration 2384, loss = 0.48408643\n",
      "Iteration 2385, loss = 0.48552847\n",
      "Iteration 2386, loss = 0.48742596\n",
      "Iteration 2387, loss = 0.48664411\n",
      "Iteration 2388, loss = 0.48418971\n",
      "Iteration 2389, loss = 0.48350431\n",
      "Iteration 2390, loss = 0.48519572\n",
      "Iteration 2391, loss = 0.48898521\n",
      "Iteration 2392, loss = 0.48852033\n",
      "Iteration 2393, loss = 0.48586130\n",
      "Iteration 2394, loss = 0.48415469\n",
      "Iteration 2395, loss = 0.48432648\n",
      "Iteration 2396, loss = 0.48532130\n",
      "Iteration 2397, loss = 0.48602510\n",
      "Iteration 2398, loss = 0.48589433\n",
      "Iteration 2399, loss = 0.48473560\n",
      "Iteration 2400, loss = 0.48424284\n",
      "Iteration 2401, loss = 0.48436044\n",
      "Iteration 2402, loss = 0.48418750\n",
      "Iteration 2403, loss = 0.48412991\n",
      "Iteration 2404, loss = 0.48399017\n",
      "Iteration 2405, loss = 0.48383740\n",
      "Iteration 2406, loss = 0.48393469\n",
      "Iteration 2407, loss = 0.48428728\n",
      "Iteration 2408, loss = 0.48432240\n",
      "Iteration 2409, loss = 0.48413377\n",
      "Iteration 2410, loss = 0.48439069\n",
      "Iteration 2411, loss = 0.48384066\n",
      "Iteration 2412, loss = 0.48392392\n",
      "Iteration 2413, loss = 0.48464988\n",
      "Iteration 2414, loss = 0.48509131\n",
      "Iteration 2415, loss = 0.48497394\n",
      "Iteration 2416, loss = 0.48400636\n",
      "Iteration 2417, loss = 0.48440371\n",
      "Iteration 2418, loss = 0.48442220\n",
      "Iteration 2419, loss = 0.48504042\n",
      "Iteration 2420, loss = 0.48588721\n",
      "Iteration 2421, loss = 0.48570202\n",
      "Iteration 2422, loss = 0.48450897\n",
      "Iteration 2423, loss = 0.48344342\n",
      "Iteration 2424, loss = 0.48386774\n",
      "Iteration 2425, loss = 0.48616458\n",
      "Iteration 2426, loss = 0.48836117\n",
      "Iteration 2427, loss = 0.48879001\n",
      "Iteration 2428, loss = 0.48720033\n",
      "Iteration 2429, loss = 0.48534230\n",
      "Iteration 2430, loss = 0.48396073\n",
      "Iteration 2431, loss = 0.48391762\n",
      "Iteration 2432, loss = 0.48549899\n",
      "Iteration 2433, loss = 0.48748721\n",
      "Iteration 2434, loss = 0.48657248\n",
      "Iteration 2435, loss = 0.48470656\n",
      "Iteration 2436, loss = 0.48375320\n",
      "Iteration 2437, loss = 0.48481732\n",
      "Iteration 2438, loss = 0.48684414\n",
      "Iteration 2439, loss = 0.48928957\n",
      "Iteration 2440, loss = 0.48927580\n",
      "Iteration 2441, loss = 0.48681590\n",
      "Iteration 2442, loss = 0.48617833\n",
      "Iteration 2443, loss = 0.48365168\n",
      "Iteration 2444, loss = 0.48383589\n",
      "Iteration 2445, loss = 0.48505115\n",
      "Iteration 2446, loss = 0.48633266\n",
      "Iteration 2447, loss = 0.48641681\n",
      "Iteration 2448, loss = 0.48554157\n",
      "Iteration 2449, loss = 0.48489040\n",
      "Iteration 2450, loss = 0.48467920\n",
      "Iteration 2451, loss = 0.48411363\n",
      "Iteration 2452, loss = 0.48411367\n",
      "Iteration 2453, loss = 0.48430633\n",
      "Iteration 2454, loss = 0.48414951\n",
      "Iteration 2455, loss = 0.48380244\n",
      "Iteration 2456, loss = 0.48386689\n",
      "Iteration 2457, loss = 0.48417711\n",
      "Iteration 2458, loss = 0.48436276\n",
      "Iteration 2459, loss = 0.48419378\n",
      "Iteration 2460, loss = 0.48473085\n",
      "Iteration 2461, loss = 0.48510377\n",
      "Iteration 2462, loss = 0.48523240\n",
      "Iteration 2463, loss = 0.48504683\n",
      "Iteration 2464, loss = 0.48395725\n",
      "Iteration 2465, loss = 0.48386145\n",
      "Iteration 2466, loss = 0.48424006\n",
      "Iteration 2467, loss = 0.48488104\n",
      "Iteration 2468, loss = 0.48534260\n",
      "Iteration 2469, loss = 0.48552288\n",
      "Iteration 2470, loss = 0.48529351\n",
      "Iteration 2471, loss = 0.48440649\n",
      "Iteration 2472, loss = 0.48338574\n",
      "Iteration 2473, loss = 0.48530409\n",
      "Iteration 2474, loss = 0.48612845\n",
      "Iteration 2475, loss = 0.48529138\n",
      "Iteration 2476, loss = 0.48432892\n",
      "Iteration 2477, loss = 0.48420969\n",
      "Iteration 2478, loss = 0.48472553\n",
      "Iteration 2479, loss = 0.48521883\n",
      "Iteration 2480, loss = 0.48497989\n",
      "Iteration 2481, loss = 0.48475043\n",
      "Iteration 2482, loss = 0.48435468\n",
      "Iteration 2483, loss = 0.48406063\n",
      "Iteration 2484, loss = 0.48384170\n",
      "Iteration 2485, loss = 0.48431265\n",
      "Iteration 2486, loss = 0.48410901\n",
      "Iteration 2487, loss = 0.48425006\n",
      "Iteration 2488, loss = 0.48407748\n",
      "Iteration 2489, loss = 0.48371913\n",
      "Iteration 2490, loss = 0.48391385\n",
      "Iteration 2491, loss = 0.48839331\n",
      "Iteration 2492, loss = 0.49025653\n",
      "Iteration 2493, loss = 0.48666446\n",
      "Iteration 2494, loss = 0.48441590\n",
      "Iteration 2495, loss = 0.48538581\n",
      "Iteration 2496, loss = 0.48720435\n",
      "Iteration 2497, loss = 0.48777390\n",
      "Iteration 2498, loss = 0.48636508\n",
      "Iteration 2499, loss = 0.48426325\n",
      "Iteration 2500, loss = 0.48411160\n",
      "Iteration 2501, loss = 0.48527966\n",
      "Iteration 2502, loss = 0.48623010\n",
      "Iteration 2503, loss = 0.48539227\n",
      "Iteration 2504, loss = 0.48361432\n",
      "Iteration 2505, loss = 0.48407107\n",
      "Iteration 2506, loss = 0.48520444\n",
      "Iteration 2507, loss = 0.48655557\n",
      "Iteration 2508, loss = 0.48657575\n",
      "Iteration 2509, loss = 0.48538794\n",
      "Iteration 2510, loss = 0.48490615\n",
      "Iteration 2511, loss = 0.48400878\n",
      "Iteration 2512, loss = 0.48381367\n",
      "Iteration 2513, loss = 0.48413367\n",
      "Iteration 2514, loss = 0.48408880\n",
      "Iteration 2515, loss = 0.48398973\n",
      "Iteration 2516, loss = 0.48378871\n",
      "Iteration 2517, loss = 0.48399124\n",
      "Iteration 2518, loss = 0.48418281\n",
      "Iteration 2519, loss = 0.48461850\n",
      "Iteration 2520, loss = 0.48446137\n",
      "Iteration 2521, loss = 0.48398569\n",
      "Iteration 2522, loss = 0.48421016\n",
      "Iteration 2523, loss = 0.48384988\n",
      "Iteration 2524, loss = 0.48386574\n",
      "Iteration 2525, loss = 0.48404513\n",
      "Iteration 2526, loss = 0.48408713\n",
      "Iteration 2527, loss = 0.48408339\n",
      "Iteration 2528, loss = 0.48369072\n",
      "Iteration 2529, loss = 0.48382432\n",
      "Iteration 2530, loss = 0.48507355\n",
      "Iteration 2531, loss = 0.48574155\n",
      "Iteration 2532, loss = 0.48579643\n",
      "Iteration 2533, loss = 0.48552748\n",
      "Iteration 2534, loss = 0.48493864\n",
      "Iteration 2535, loss = 0.48380592\n",
      "Iteration 2536, loss = 0.48399486\n",
      "Iteration 2537, loss = 0.48460217\n",
      "Iteration 2538, loss = 0.48571053\n",
      "Iteration 2539, loss = 0.48717955\n",
      "Iteration 2540, loss = 0.48746237\n",
      "Iteration 2541, loss = 0.48653709\n",
      "Iteration 2542, loss = 0.48492136\n",
      "Iteration 2543, loss = 0.48396411\n",
      "Iteration 2544, loss = 0.48380213\n",
      "Iteration 2545, loss = 0.48409751\n",
      "Iteration 2546, loss = 0.48383542\n",
      "Iteration 2547, loss = 0.48394545\n",
      "Iteration 2548, loss = 0.48385965\n",
      "Iteration 2549, loss = 0.48386212\n",
      "Iteration 2550, loss = 0.48415381\n",
      "Iteration 2551, loss = 0.48535562\n",
      "Iteration 2552, loss = 0.48592791\n",
      "Iteration 2553, loss = 0.48580284\n",
      "Iteration 2554, loss = 0.48614740\n",
      "Iteration 2555, loss = 0.48616355\n",
      "Iteration 2556, loss = 0.48536237\n",
      "Iteration 2557, loss = 0.48441923\n",
      "Iteration 2558, loss = 0.48406598\n",
      "Iteration 2559, loss = 0.48393103\n",
      "Iteration 2560, loss = 0.48454315\n",
      "Iteration 2561, loss = 0.48517293\n",
      "Iteration 2562, loss = 0.48710952\n",
      "Iteration 2563, loss = 0.48834071\n",
      "Iteration 2564, loss = 0.48752379\n",
      "Iteration 2565, loss = 0.48568921\n",
      "Iteration 2566, loss = 0.48396539\n",
      "Iteration 2567, loss = 0.48378534\n",
      "Iteration 2568, loss = 0.48445496\n",
      "Iteration 2569, loss = 0.48534355\n",
      "Iteration 2570, loss = 0.48604163\n",
      "Iteration 2571, loss = 0.48648535\n",
      "Iteration 2572, loss = 0.48608770\n",
      "Iteration 2573, loss = 0.48439002\n",
      "Iteration 2574, loss = 0.48334825\n",
      "Iteration 2575, loss = 0.48551064\n",
      "Iteration 2576, loss = 0.48792400\n",
      "Iteration 2577, loss = 0.48719746\n",
      "Iteration 2578, loss = 0.48729746\n",
      "Iteration 2579, loss = 0.48390477\n",
      "Iteration 2580, loss = 0.48400973\n",
      "Iteration 2581, loss = 0.48386269\n",
      "Iteration 2582, loss = 0.48372932\n",
      "Iteration 2583, loss = 0.48505747\n",
      "Iteration 2584, loss = 0.48449500\n",
      "Iteration 2585, loss = 0.48409612\n",
      "Iteration 2586, loss = 0.48447118\n",
      "Iteration 2587, loss = 0.48450168\n",
      "Iteration 2588, loss = 0.48400238\n",
      "Iteration 2589, loss = 0.48387294\n",
      "Iteration 2590, loss = 0.48373299\n",
      "Iteration 2591, loss = 0.48385206\n",
      "Iteration 2592, loss = 0.48423550\n",
      "Iteration 2593, loss = 0.48463129\n",
      "Iteration 2594, loss = 0.48474957\n",
      "Iteration 2595, loss = 0.48464417\n",
      "Iteration 2596, loss = 0.48430348\n",
      "Iteration 2597, loss = 0.48406808\n",
      "Iteration 2598, loss = 0.48407853\n",
      "Iteration 2599, loss = 0.48382804\n",
      "Iteration 2600, loss = 0.48387997\n",
      "Iteration 2601, loss = 0.48413024\n",
      "Iteration 2602, loss = 0.48499768\n",
      "Iteration 2603, loss = 0.48512746\n",
      "Iteration 2604, loss = 0.48456568\n",
      "Iteration 2605, loss = 0.48400084\n",
      "Iteration 2606, loss = 0.48362025\n",
      "Iteration 2607, loss = 0.48429076\n",
      "Iteration 2608, loss = 0.48502654\n",
      "Iteration 2609, loss = 0.48616650\n",
      "Iteration 2610, loss = 0.48805478\n",
      "Iteration 2611, loss = 0.48684911\n",
      "Iteration 2612, loss = 0.48613845\n",
      "Iteration 2613, loss = 0.48420000\n",
      "Iteration 2614, loss = 0.48452258\n",
      "Iteration 2615, loss = 0.48427645\n",
      "Iteration 2616, loss = 0.48471752\n",
      "Iteration 2617, loss = 0.48533470\n",
      "Iteration 2618, loss = 0.48547238\n",
      "Iteration 2619, loss = 0.48495872\n",
      "Iteration 2620, loss = 0.48400241\n",
      "Iteration 2621, loss = 0.48409082\n",
      "Iteration 2622, loss = 0.48403579\n",
      "Iteration 2623, loss = 0.48410243\n",
      "Iteration 2624, loss = 0.48405171\n",
      "Iteration 2625, loss = 0.48421476\n",
      "Iteration 2626, loss = 0.48420097\n",
      "Iteration 2627, loss = 0.48496468\n",
      "Iteration 2628, loss = 0.48547187\n",
      "Iteration 2629, loss = 0.48547848\n",
      "Iteration 2630, loss = 0.48498186\n",
      "Iteration 2631, loss = 0.48505072\n",
      "Iteration 2632, loss = 0.48400610\n",
      "Iteration 2633, loss = 0.48408296\n",
      "Iteration 2634, loss = 0.48450266\n",
      "Iteration 2635, loss = 0.48442762\n",
      "Iteration 2636, loss = 0.48386446\n",
      "Iteration 2637, loss = 0.48450193\n",
      "Iteration 2638, loss = 0.48421645\n",
      "Iteration 2639, loss = 0.48401637\n",
      "Iteration 2640, loss = 0.48378395\n",
      "Iteration 2641, loss = 0.48387615\n",
      "Iteration 2642, loss = 0.48452828\n",
      "Iteration 2643, loss = 0.48485969\n",
      "Iteration 2644, loss = 0.48473834\n",
      "Iteration 2645, loss = 0.48441764\n",
      "Iteration 2646, loss = 0.48388507\n",
      "Iteration 2647, loss = 0.48364451\n",
      "Iteration 2648, loss = 0.48443456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2649, loss = 0.48458779\n",
      "Iteration 2650, loss = 0.48435988\n",
      "Iteration 2651, loss = 0.48399695\n",
      "Iteration 2652, loss = 0.48385601\n",
      "Iteration 2653, loss = 0.48387384\n",
      "Iteration 2654, loss = 0.48390130\n",
      "Iteration 2655, loss = 0.48373718\n",
      "Iteration 2656, loss = 0.48388510\n",
      "Iteration 2657, loss = 0.48410545\n",
      "Iteration 2658, loss = 0.48401866\n",
      "Iteration 2659, loss = 0.48500761\n",
      "Iteration 2660, loss = 0.48483908\n",
      "Iteration 2661, loss = 0.48444194\n",
      "Iteration 2662, loss = 0.48375989\n",
      "Iteration 2663, loss = 0.48363126\n",
      "Iteration 2664, loss = 0.48384965\n",
      "Iteration 2665, loss = 0.48478972\n",
      "Iteration 2666, loss = 0.48552816\n",
      "Iteration 2667, loss = 0.48553737\n",
      "Iteration 2668, loss = 0.48484201\n",
      "Iteration 2669, loss = 0.48413506\n",
      "Iteration 2670, loss = 0.48366760\n",
      "Iteration 2671, loss = 0.48394210\n",
      "Iteration 2672, loss = 0.48543377\n",
      "Iteration 2673, loss = 0.48656796\n",
      "Iteration 2674, loss = 0.48666032\n",
      "Iteration 2675, loss = 0.48559780\n",
      "Iteration 2676, loss = 0.48425358\n",
      "Iteration 2677, loss = 0.48373029\n",
      "Iteration 2678, loss = 0.48395496\n",
      "Iteration 2679, loss = 0.48451326\n",
      "Iteration 2680, loss = 0.48637715\n",
      "Iteration 2681, loss = 0.48664358\n",
      "Iteration 2682, loss = 0.48530623\n",
      "Iteration 2683, loss = 0.48434498\n",
      "Iteration 2684, loss = 0.48370285\n",
      "Iteration 2685, loss = 0.48404810\n",
      "Iteration 2686, loss = 0.48477897\n",
      "Iteration 2687, loss = 0.48358519\n",
      "Iteration 2688, loss = 0.48411767\n",
      "Iteration 2689, loss = 0.48610857\n",
      "Iteration 2690, loss = 0.48783502\n",
      "Iteration 2691, loss = 0.48719989\n",
      "Iteration 2692, loss = 0.48541919\n",
      "Iteration 2693, loss = 0.48385221\n",
      "Iteration 2694, loss = 0.48468001\n",
      "Iteration 2695, loss = 0.48399888\n",
      "Iteration 2696, loss = 0.48385245\n",
      "Iteration 2697, loss = 0.48375830\n",
      "Iteration 2698, loss = 0.48368314\n",
      "Iteration 2699, loss = 0.48387953\n",
      "Iteration 2700, loss = 0.48508427\n",
      "Iteration 2701, loss = 0.48524920\n",
      "Iteration 2702, loss = 0.48516758\n",
      "Iteration 2703, loss = 0.48415090\n",
      "Iteration 2704, loss = 0.48414804\n",
      "Iteration 2705, loss = 0.48448419\n",
      "Iteration 2706, loss = 0.48479752\n",
      "Iteration 2707, loss = 0.48457301\n",
      "Iteration 2708, loss = 0.48422288\n",
      "Iteration 2709, loss = 0.48413630\n",
      "Iteration 2710, loss = 0.48405851\n",
      "Iteration 2711, loss = 0.48391480\n",
      "Iteration 2712, loss = 0.48420126\n",
      "Iteration 2713, loss = 0.48520252\n",
      "Iteration 2714, loss = 0.48599842\n",
      "Iteration 2715, loss = 0.48504122\n",
      "Iteration 2716, loss = 0.48408442\n",
      "Iteration 2717, loss = 0.48364495\n",
      "Iteration 2718, loss = 0.48417431\n",
      "Iteration 2719, loss = 0.48628251\n",
      "Iteration 2720, loss = 0.48675991\n",
      "Iteration 2721, loss = 0.48559411\n",
      "Iteration 2722, loss = 0.48406701\n",
      "Iteration 2723, loss = 0.48354990\n",
      "Iteration 2724, loss = 0.48463953\n",
      "Iteration 2725, loss = 0.48584716\n",
      "Iteration 2726, loss = 0.48546794\n",
      "Iteration 2727, loss = 0.48455995\n",
      "Iteration 2728, loss = 0.48404552\n",
      "Iteration 2729, loss = 0.48386287\n",
      "Iteration 2730, loss = 0.48376586\n",
      "Iteration 2731, loss = 0.48369879\n",
      "Iteration 2732, loss = 0.48442937\n",
      "Iteration 2733, loss = 0.48440563\n",
      "Iteration 2734, loss = 0.48451183\n",
      "Iteration 2735, loss = 0.48386144\n",
      "Iteration 2736, loss = 0.48362248\n",
      "Iteration 2737, loss = 0.48359885\n",
      "Iteration 2738, loss = 0.48454260\n",
      "Iteration 2739, loss = 0.48532938\n",
      "Iteration 2740, loss = 0.48577685\n",
      "Iteration 2741, loss = 0.48631489\n",
      "Iteration 2742, loss = 0.48636465\n",
      "Iteration 2743, loss = 0.48526890\n",
      "Iteration 2744, loss = 0.48381082\n",
      "Iteration 2745, loss = 0.48344231\n",
      "Iteration 2746, loss = 0.48453761\n",
      "Iteration 2747, loss = 0.48673315\n",
      "Iteration 2748, loss = 0.48852587\n",
      "Iteration 2749, loss = 0.48937105\n",
      "Iteration 2750, loss = 0.48926323\n",
      "Iteration 2751, loss = 0.48612744\n",
      "Iteration 2752, loss = 0.48441236\n",
      "Iteration 2753, loss = 0.48491730\n",
      "Iteration 2754, loss = 0.48818167\n",
      "Iteration 2755, loss = 0.48919219\n",
      "Iteration 2756, loss = 0.48762397\n",
      "Iteration 2757, loss = 0.48515783\n",
      "Iteration 2758, loss = 0.48401038\n",
      "Iteration 2759, loss = 0.48413995\n",
      "Iteration 2760, loss = 0.48379653\n",
      "Iteration 2761, loss = 0.48353340\n",
      "Iteration 2762, loss = 0.48426917\n",
      "Iteration 2763, loss = 0.48524894\n",
      "Iteration 2764, loss = 0.48618758\n",
      "Iteration 2765, loss = 0.48527686\n",
      "Iteration 2766, loss = 0.48405664\n",
      "Iteration 2767, loss = 0.48444990\n",
      "Iteration 2768, loss = 0.48561877\n",
      "Iteration 2769, loss = 0.48541869\n",
      "Iteration 2770, loss = 0.48482022\n",
      "Iteration 2771, loss = 0.48410533\n",
      "Iteration 2772, loss = 0.48400315\n",
      "Iteration 2773, loss = 0.48381769\n",
      "Iteration 2774, loss = 0.48380031\n",
      "Iteration 2775, loss = 0.48449037\n",
      "Iteration 2776, loss = 0.48419098\n",
      "Iteration 2777, loss = 0.48381583\n",
      "Iteration 2778, loss = 0.48429138\n",
      "Iteration 2779, loss = 0.48458257\n",
      "Iteration 2780, loss = 0.48549531\n",
      "Iteration 2781, loss = 0.48553080\n",
      "Iteration 2782, loss = 0.48518457\n",
      "Iteration 2783, loss = 0.48462990\n",
      "Iteration 2784, loss = 0.48463421\n",
      "Iteration 2785, loss = 0.48451435\n",
      "Iteration 2786, loss = 0.48444046\n",
      "Iteration 2787, loss = 0.48383451\n",
      "Iteration 2788, loss = 0.48391217\n",
      "Iteration 2789, loss = 0.48391897\n",
      "Iteration 2790, loss = 0.48388675\n",
      "Iteration 2791, loss = 0.48403666\n",
      "Iteration 2792, loss = 0.48413172\n",
      "Iteration 2793, loss = 0.48383145\n",
      "Iteration 2794, loss = 0.48377946\n",
      "Iteration 2795, loss = 0.48402999\n",
      "Iteration 2796, loss = 0.48445616\n",
      "Iteration 2797, loss = 0.48498726\n",
      "Iteration 2798, loss = 0.48581042\n",
      "Iteration 2799, loss = 0.48614774\n",
      "Iteration 2800, loss = 0.48582992\n",
      "Iteration 2801, loss = 0.48522935\n",
      "Iteration 2802, loss = 0.48477644\n",
      "Iteration 2803, loss = 0.48392408\n",
      "Iteration 2804, loss = 0.48379716\n",
      "Iteration 2805, loss = 0.48404070\n",
      "Iteration 2806, loss = 0.48393912\n",
      "Iteration 2807, loss = 0.48404340\n",
      "Iteration 2808, loss = 0.48388367\n",
      "Iteration 2809, loss = 0.48367827\n",
      "Iteration 2810, loss = 0.48397654\n",
      "Iteration 2811, loss = 0.48438447\n",
      "Iteration 2812, loss = 0.48449866\n",
      "Iteration 2813, loss = 0.48419056\n",
      "Iteration 2814, loss = 0.48378859\n",
      "Iteration 2815, loss = 0.48421143\n",
      "Iteration 2816, loss = 0.48498760\n",
      "Iteration 2817, loss = 0.48501343\n",
      "Iteration 2818, loss = 0.48453838\n",
      "Iteration 2819, loss = 0.48367932\n",
      "Iteration 2820, loss = 0.48423005\n",
      "Iteration 2821, loss = 0.48542068\n",
      "Iteration 2822, loss = 0.48544271\n",
      "Iteration 2823, loss = 0.48453290\n",
      "Iteration 2824, loss = 0.48385953\n",
      "Iteration 2825, loss = 0.48366203\n",
      "Iteration 2826, loss = 0.48533261\n",
      "Iteration 2827, loss = 0.48731102\n",
      "Iteration 2828, loss = 0.48822729\n",
      "Iteration 2829, loss = 0.48751710\n",
      "Iteration 2830, loss = 0.48548035\n",
      "Iteration 2831, loss = 0.48491851\n",
      "Iteration 2832, loss = 0.48383782\n",
      "Iteration 2833, loss = 0.48388094\n",
      "Iteration 2834, loss = 0.48397012\n",
      "Iteration 2835, loss = 0.48391975\n",
      "Iteration 2836, loss = 0.48419031\n",
      "Iteration 2837, loss = 0.48490823\n",
      "Iteration 2838, loss = 0.48541665\n",
      "Iteration 2839, loss = 0.48531256\n",
      "Iteration 2840, loss = 0.48697615\n",
      "Iteration 2841, loss = 0.48701423\n",
      "Iteration 2842, loss = 0.48572527\n",
      "Iteration 2843, loss = 0.48457797\n",
      "Iteration 2844, loss = 0.48394345\n",
      "Iteration 2845, loss = 0.48381786\n",
      "Iteration 2846, loss = 0.48390851\n",
      "Iteration 2847, loss = 0.48389710\n",
      "Iteration 2848, loss = 0.48385537\n",
      "Iteration 2849, loss = 0.48379515\n",
      "Iteration 2850, loss = 0.48389808\n",
      "Iteration 2851, loss = 0.48458816\n",
      "Iteration 2852, loss = 0.48492084\n",
      "Iteration 2853, loss = 0.48457802\n",
      "Iteration 2854, loss = 0.48383890\n",
      "Iteration 2855, loss = 0.48388898\n",
      "Iteration 2856, loss = 0.48448632\n",
      "Iteration 2857, loss = 0.48495379\n",
      "Iteration 2858, loss = 0.48548004\n",
      "Iteration 2859, loss = 0.48564056\n",
      "Iteration 2860, loss = 0.48504292\n",
      "Iteration 2861, loss = 0.48425593\n",
      "Iteration 2862, loss = 0.48329152\n",
      "Iteration 2863, loss = 0.48494568\n",
      "Iteration 2864, loss = 0.48732924\n",
      "Iteration 2865, loss = 0.48701109\n",
      "Iteration 2866, loss = 0.48482294\n",
      "Iteration 2867, loss = 0.48406177\n",
      "Iteration 2868, loss = 0.48490190\n",
      "Iteration 2869, loss = 0.48609356\n",
      "Iteration 2870, loss = 0.48527507\n",
      "Iteration 2871, loss = 0.48441795\n",
      "Iteration 2872, loss = 0.48391336\n",
      "Iteration 2873, loss = 0.48377283\n",
      "Iteration 2874, loss = 0.48388398\n",
      "Iteration 2875, loss = 0.48386940\n",
      "Iteration 2876, loss = 0.48379049\n",
      "Iteration 2877, loss = 0.48367361\n",
      "Iteration 2878, loss = 0.48449314\n",
      "Iteration 2879, loss = 0.48415846\n",
      "Iteration 2880, loss = 0.48387831\n",
      "Iteration 2881, loss = 0.48394385\n",
      "Iteration 2882, loss = 0.48422941\n",
      "Iteration 2883, loss = 0.48423660\n",
      "Iteration 2884, loss = 0.48396647\n",
      "Iteration 2885, loss = 0.48385243\n",
      "Iteration 2886, loss = 0.48367699\n",
      "Iteration 2887, loss = 0.48416299\n",
      "Iteration 2888, loss = 0.48423216\n",
      "Iteration 2889, loss = 0.48454165\n",
      "Iteration 2890, loss = 0.48528508\n",
      "Iteration 2891, loss = 0.48515204\n",
      "Iteration 2892, loss = 0.48461610\n",
      "Iteration 2893, loss = 0.48359822\n",
      "Iteration 2894, loss = 0.48361561\n",
      "Iteration 2895, loss = 0.48457065\n",
      "Iteration 2896, loss = 0.48604193\n",
      "Iteration 2897, loss = 0.48714732\n",
      "Iteration 2898, loss = 0.48722345\n",
      "Iteration 2899, loss = 0.48598694\n",
      "Iteration 2900, loss = 0.48447096\n",
      "Iteration 2901, loss = 0.48359770\n",
      "Iteration 2902, loss = 0.48413939\n",
      "Iteration 2903, loss = 0.48454833\n",
      "Iteration 2904, loss = 0.48409311\n",
      "Iteration 2905, loss = 0.48345881\n",
      "Iteration 2906, loss = 0.48470820\n",
      "Iteration 2907, loss = 0.48502223\n",
      "Iteration 2908, loss = 0.48552963\n",
      "Iteration 2909, loss = 0.48508600\n",
      "Iteration 2910, loss = 0.48444657\n",
      "Iteration 2911, loss = 0.48371922\n",
      "Iteration 2912, loss = 0.48396562\n",
      "Iteration 2913, loss = 0.48389000\n",
      "Iteration 2914, loss = 0.48363236\n",
      "Iteration 2915, loss = 0.48433873\n",
      "Iteration 2916, loss = 0.48416818\n",
      "Iteration 2917, loss = 0.48391700\n",
      "Iteration 2918, loss = 0.48376510\n",
      "Iteration 2919, loss = 0.48378360\n",
      "Iteration 2920, loss = 0.48453953\n",
      "Iteration 2921, loss = 0.48513675\n",
      "Iteration 2922, loss = 0.48596202\n",
      "Iteration 2923, loss = 0.48694488\n",
      "Iteration 2924, loss = 0.48607196\n",
      "Iteration 2925, loss = 0.48427997\n",
      "Iteration 2926, loss = 0.48392669\n",
      "Iteration 2927, loss = 0.48412415\n",
      "Iteration 2928, loss = 0.48396515\n",
      "Iteration 2929, loss = 0.48413667\n",
      "Iteration 2930, loss = 0.48404701\n",
      "Iteration 2931, loss = 0.48408934\n",
      "Iteration 2932, loss = 0.48401455\n",
      "Iteration 2933, loss = 0.48399388\n",
      "Iteration 2934, loss = 0.48405353\n",
      "Iteration 2935, loss = 0.48413950\n",
      "Iteration 2936, loss = 0.48423214\n",
      "Iteration 2937, loss = 0.48394918\n",
      "Iteration 2938, loss = 0.48348497\n",
      "Iteration 2939, loss = 0.48348836\n",
      "Iteration 2940, loss = 0.48717581\n",
      "Iteration 2941, loss = 0.48669904\n",
      "Iteration 2942, loss = 0.48393359\n",
      "Iteration 2943, loss = 0.48336328\n",
      "Iteration 2944, loss = 0.48725220\n",
      "Iteration 2945, loss = 0.48948675\n",
      "Iteration 2946, loss = 0.48765568\n",
      "Iteration 2947, loss = 0.48464879\n",
      "Iteration 2948, loss = 0.48430670\n",
      "Iteration 2949, loss = 0.48513328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2950, loss = 0.48618210\n",
      "Iteration 2951, loss = 0.48555674\n",
      "Iteration 2952, loss = 0.48405616\n",
      "Iteration 2953, loss = 0.48386641\n",
      "Iteration 2954, loss = 0.48411884\n",
      "Iteration 2955, loss = 0.48491084\n",
      "Iteration 2956, loss = 0.48555885\n",
      "Iteration 2957, loss = 0.48592803\n",
      "Iteration 2958, loss = 0.48521528\n",
      "Iteration 2959, loss = 0.48435307\n",
      "Iteration 2960, loss = 0.48377917\n",
      "Iteration 2961, loss = 0.48376687\n",
      "Iteration 2962, loss = 0.48405971\n",
      "Iteration 2963, loss = 0.48470364\n",
      "Iteration 2964, loss = 0.48596712\n",
      "Iteration 2965, loss = 0.48681491\n",
      "Iteration 2966, loss = 0.48661674\n",
      "Iteration 2967, loss = 0.48583406\n",
      "Iteration 2968, loss = 0.48416773\n",
      "Iteration 2969, loss = 0.48344121\n",
      "Iteration 2970, loss = 0.48470599\n",
      "Iteration 2971, loss = 0.48683600\n",
      "Iteration 2972, loss = 0.48735973\n",
      "Iteration 2973, loss = 0.48563958\n",
      "Iteration 2974, loss = 0.48368517\n",
      "Iteration 2975, loss = 0.48326273\n",
      "Iteration 2976, loss = 0.48586456\n",
      "Iteration 2977, loss = 0.49173426\n",
      "Iteration 2978, loss = 0.49428583\n",
      "Iteration 2979, loss = 0.49165237\n",
      "Iteration 2980, loss = 0.48742994\n",
      "Iteration 2981, loss = 0.48418816\n",
      "Iteration 2982, loss = 0.48353930\n",
      "Iteration 2983, loss = 0.48438521\n",
      "Iteration 2984, loss = 0.48593331\n",
      "Iteration 2985, loss = 0.48678264\n",
      "Iteration 2986, loss = 0.48634670\n",
      "Iteration 2987, loss = 0.48531000\n",
      "Iteration 2988, loss = 0.48439942\n",
      "Iteration 2989, loss = 0.48472833\n",
      "Iteration 2990, loss = 0.48387773\n",
      "Iteration 2991, loss = 0.48385438\n",
      "Iteration 2992, loss = 0.48409025\n",
      "Iteration 2993, loss = 0.48472747\n",
      "Iteration 2994, loss = 0.48445568\n",
      "Iteration 2995, loss = 0.48376810\n",
      "Iteration 2996, loss = 0.48354333\n",
      "Iteration 2997, loss = 0.48518506\n",
      "Iteration 2998, loss = 0.48575401\n",
      "Iteration 2999, loss = 0.48521735\n",
      "Iteration 3000, loss = 0.48402343\n",
      "Iteration 3001, loss = 0.48372641\n",
      "Iteration 3002, loss = 0.48483128\n",
      "Iteration 3003, loss = 0.48520480\n",
      "Iteration 3004, loss = 0.48421183\n",
      "Iteration 3005, loss = 0.48347000\n",
      "Iteration 3006, loss = 0.48452465\n",
      "Iteration 3007, loss = 0.48598152\n",
      "Iteration 3008, loss = 0.48619905\n",
      "Iteration 3009, loss = 0.48399426\n",
      "Iteration 3010, loss = 0.48500822\n",
      "Iteration 3011, loss = 0.48560742\n",
      "Iteration 3012, loss = 0.48622432\n",
      "Iteration 3013, loss = 0.48589683\n",
      "Iteration 3014, loss = 0.48556606\n",
      "Iteration 3015, loss = 0.48514093\n",
      "Iteration 3016, loss = 0.48417326\n",
      "Iteration 3017, loss = 0.48362665\n",
      "Iteration 3018, loss = 0.48447920\n",
      "Iteration 3019, loss = 0.48479167\n",
      "Iteration 3020, loss = 0.48547223\n",
      "Iteration 3021, loss = 0.48462950\n",
      "Iteration 3022, loss = 0.48435113\n",
      "Iteration 3023, loss = 0.48389519\n",
      "Iteration 3024, loss = 0.48414407\n",
      "Iteration 3025, loss = 0.48443433\n",
      "Iteration 3026, loss = 0.48464773\n",
      "Iteration 3027, loss = 0.48467134\n",
      "Iteration 3028, loss = 0.48455670\n",
      "Iteration 3029, loss = 0.48410292\n",
      "Iteration 3030, loss = 0.48367881\n",
      "Iteration 3031, loss = 0.48368912\n",
      "Iteration 3032, loss = 0.48447582\n",
      "Iteration 3033, loss = 0.48622398\n",
      "Iteration 3034, loss = 0.48712100\n",
      "Iteration 3035, loss = 0.48659524\n",
      "Iteration 3036, loss = 0.48533436\n",
      "Iteration 3037, loss = 0.48444847\n",
      "Iteration 3038, loss = 0.48377354\n",
      "Iteration 3039, loss = 0.48318518\n",
      "Iteration 3040, loss = 0.48542865\n",
      "Iteration 3041, loss = 0.48852878\n",
      "Iteration 3042, loss = 0.49091479\n",
      "Iteration 3043, loss = 0.49135318\n",
      "Iteration 3044, loss = 0.48854099\n",
      "Iteration 3045, loss = 0.48507552\n",
      "Iteration 3046, loss = 0.48410266\n",
      "Iteration 3047, loss = 0.48442582\n",
      "Iteration 3048, loss = 0.48451374\n",
      "Iteration 3049, loss = 0.48441138\n",
      "Iteration 3050, loss = 0.48431151\n",
      "Iteration 3051, loss = 0.48400929\n",
      "Iteration 3052, loss = 0.48380860\n",
      "Iteration 3053, loss = 0.48425873\n",
      "Iteration 3054, loss = 0.48443542\n",
      "Iteration 3055, loss = 0.48451362\n",
      "Iteration 3056, loss = 0.48341190\n",
      "Iteration 3057, loss = 0.48512068\n",
      "Iteration 3058, loss = 0.48560166\n",
      "Iteration 3059, loss = 0.48639493\n",
      "Iteration 3060, loss = 0.48609388\n",
      "Iteration 3061, loss = 0.48497058\n",
      "Iteration 3062, loss = 0.48346043\n",
      "Iteration 3063, loss = 0.48514184\n",
      "Iteration 3064, loss = 0.48536969\n",
      "Iteration 3065, loss = 0.48524055\n",
      "Iteration 3066, loss = 0.48449566\n",
      "Iteration 3067, loss = 0.48438333\n",
      "Iteration 3068, loss = 0.48417719\n",
      "Iteration 3069, loss = 0.48509217\n",
      "Iteration 3070, loss = 0.48527674\n",
      "Iteration 3071, loss = 0.48366362\n",
      "Iteration 3072, loss = 0.48458937\n",
      "Iteration 3073, loss = 0.48522881\n",
      "Iteration 3074, loss = 0.48619130\n",
      "Iteration 3075, loss = 0.48666488\n",
      "Iteration 3076, loss = 0.48636974\n",
      "Iteration 3077, loss = 0.48434151\n",
      "Iteration 3078, loss = 0.48472892\n",
      "Iteration 3079, loss = 0.48476367\n",
      "Iteration 3080, loss = 0.48450359\n",
      "Iteration 3081, loss = 0.48389707\n",
      "Iteration 3082, loss = 0.48404213\n",
      "Iteration 3083, loss = 0.48423772\n",
      "Iteration 3084, loss = 0.48442449\n",
      "Iteration 3085, loss = 0.48444315\n",
      "Iteration 3086, loss = 0.48476831\n",
      "Iteration 3087, loss = 0.48432089\n",
      "Iteration 3088, loss = 0.48469260\n",
      "Iteration 3089, loss = 0.48488178\n",
      "Iteration 3090, loss = 0.48465897\n",
      "Iteration 3091, loss = 0.48411131\n",
      "Iteration 3092, loss = 0.48405067\n",
      "Iteration 3093, loss = 0.48401548\n",
      "Iteration 3094, loss = 0.48374354\n",
      "Iteration 3095, loss = 0.48378456\n",
      "Iteration 3096, loss = 0.48370136\n",
      "Iteration 3097, loss = 0.48370285\n",
      "Iteration 3098, loss = 0.48406718\n",
      "Iteration 3099, loss = 0.48368039\n",
      "Iteration 3100, loss = 0.48388298\n",
      "Iteration 3101, loss = 0.48395283\n",
      "Iteration 3102, loss = 0.48454196\n",
      "Iteration 3103, loss = 0.48443898\n",
      "Iteration 3104, loss = 0.48393660\n",
      "Iteration 3105, loss = 0.48350585\n",
      "Iteration 3106, loss = 0.48456915\n",
      "Iteration 3107, loss = 0.48436374\n",
      "Iteration 3108, loss = 0.48380861\n",
      "Iteration 3109, loss = 0.48374834\n",
      "Iteration 3110, loss = 0.48414661\n",
      "Iteration 3111, loss = 0.48462215\n",
      "Iteration 3112, loss = 0.48386047\n",
      "Iteration 3113, loss = 0.48328028\n",
      "Iteration 3114, loss = 0.48392707\n",
      "Iteration 3115, loss = 0.48715542\n",
      "Iteration 3116, loss = 0.48977562\n",
      "Iteration 3117, loss = 0.48985751\n",
      "Iteration 3118, loss = 0.48768667\n",
      "Iteration 3119, loss = 0.48452756\n",
      "Iteration 3120, loss = 0.48461589\n",
      "Iteration 3121, loss = 0.48431534\n",
      "Iteration 3122, loss = 0.48652994\n",
      "Iteration 3123, loss = 0.48602610\n",
      "Iteration 3124, loss = 0.48436064\n",
      "Iteration 3125, loss = 0.48385063\n",
      "Iteration 3126, loss = 0.48342459\n",
      "Iteration 3127, loss = 0.48583718\n",
      "Iteration 3128, loss = 0.48895313\n",
      "Iteration 3129, loss = 0.49009718\n",
      "Iteration 3130, loss = 0.48879032\n",
      "Iteration 3131, loss = 0.48624136\n",
      "Iteration 3132, loss = 0.48456613\n",
      "Iteration 3133, loss = 0.48382961\n",
      "Iteration 3134, loss = 0.48398073\n",
      "Iteration 3135, loss = 0.48417713\n",
      "Iteration 3136, loss = 0.48427300\n",
      "Iteration 3137, loss = 0.48476450\n",
      "Iteration 3138, loss = 0.48486889\n",
      "Iteration 3139, loss = 0.48485086\n",
      "Iteration 3140, loss = 0.48449506\n",
      "Iteration 3141, loss = 0.48456320\n",
      "Iteration 3142, loss = 0.48438946\n",
      "Iteration 3143, loss = 0.48436464\n",
      "Iteration 3144, loss = 0.48397956\n",
      "Iteration 3145, loss = 0.48404548\n",
      "Iteration 3146, loss = 0.48431853\n",
      "Iteration 3147, loss = 0.48493221\n",
      "Iteration 3148, loss = 0.48549274\n",
      "Iteration 3149, loss = 0.48535980\n",
      "Iteration 3150, loss = 0.48490067\n",
      "Iteration 3151, loss = 0.48428414\n",
      "Iteration 3152, loss = 0.48385704\n",
      "Iteration 3153, loss = 0.48416869\n",
      "Iteration 3154, loss = 0.48378458\n",
      "Iteration 3155, loss = 0.48390420\n",
      "Iteration 3156, loss = 0.48389816\n",
      "Iteration 3157, loss = 0.48376714\n",
      "Iteration 3158, loss = 0.48364405\n",
      "Iteration 3159, loss = 0.48385502\n",
      "Iteration 3160, loss = 0.48357550\n",
      "Iteration 3161, loss = 0.48418209\n",
      "Iteration 3162, loss = 0.48451662\n",
      "Iteration 3163, loss = 0.48478031\n",
      "Iteration 3164, loss = 0.48467002\n",
      "Iteration 3165, loss = 0.48426979\n",
      "Iteration 3166, loss = 0.48342523\n",
      "Iteration 3167, loss = 0.48328752\n",
      "Iteration 3168, loss = 0.48622848\n",
      "Iteration 3169, loss = 0.48684775\n",
      "Iteration 3170, loss = 0.48512376\n",
      "Iteration 3171, loss = 0.48329593\n",
      "Iteration 3172, loss = 0.48366334\n",
      "Iteration 3173, loss = 0.48675658\n",
      "Iteration 3174, loss = 0.48785453\n",
      "Iteration 3175, loss = 0.48700187\n",
      "Iteration 3176, loss = 0.48464817\n",
      "Iteration 3177, loss = 0.48398257\n",
      "Iteration 3178, loss = 0.48498942\n",
      "Iteration 3179, loss = 0.48603903\n",
      "Iteration 3180, loss = 0.48603481\n",
      "Iteration 3181, loss = 0.48531560\n",
      "Iteration 3182, loss = 0.48469001\n",
      "Iteration 3183, loss = 0.48405152\n",
      "Iteration 3184, loss = 0.48401959\n",
      "Iteration 3185, loss = 0.48361332\n",
      "Iteration 3186, loss = 0.48375531\n",
      "Iteration 3187, loss = 0.48362399\n",
      "Iteration 3188, loss = 0.48418075\n",
      "Iteration 3189, loss = 0.48454583\n",
      "Iteration 3190, loss = 0.48479666\n",
      "Iteration 3191, loss = 0.48414798\n",
      "Iteration 3192, loss = 0.48345457\n",
      "Iteration 3193, loss = 0.48394375\n",
      "Iteration 3194, loss = 0.48462362\n",
      "Iteration 3195, loss = 0.48495681\n",
      "Iteration 3196, loss = 0.48436299\n",
      "Iteration 3197, loss = 0.48382124\n",
      "Iteration 3198, loss = 0.48395718\n",
      "Iteration 3199, loss = 0.48458071\n",
      "Iteration 3200, loss = 0.48531486\n",
      "Iteration 3201, loss = 0.48464943\n",
      "Iteration 3202, loss = 0.48355340\n",
      "Iteration 3203, loss = 0.48396122\n",
      "Iteration 3204, loss = 0.48534945\n",
      "Iteration 3205, loss = 0.48578284\n",
      "Iteration 3206, loss = 0.48563070\n",
      "Iteration 3207, loss = 0.48575161\n",
      "Iteration 3208, loss = 0.48584695\n",
      "Iteration 3209, loss = 0.48574034\n",
      "Iteration 3210, loss = 0.48511601\n",
      "Iteration 3211, loss = 0.48532849\n",
      "Iteration 3212, loss = 0.48480320\n",
      "Iteration 3213, loss = 0.48464856\n",
      "Iteration 3214, loss = 0.48366502\n",
      "Iteration 3215, loss = 0.48387683\n",
      "Iteration 3216, loss = 0.48438710\n",
      "Iteration 3217, loss = 0.48527002\n",
      "Iteration 3218, loss = 0.48475253\n",
      "Iteration 3219, loss = 0.48431906\n",
      "Iteration 3220, loss = 0.48365849\n",
      "Iteration 3221, loss = 0.48417621\n",
      "Iteration 3222, loss = 0.48400626\n",
      "Iteration 3223, loss = 0.48362847\n",
      "Iteration 3224, loss = 0.48361242\n",
      "Iteration 3225, loss = 0.48420627\n",
      "Iteration 3226, loss = 0.48378773\n",
      "Iteration 3227, loss = 0.48386358\n",
      "Iteration 3228, loss = 0.48373821\n",
      "Iteration 3229, loss = 0.48445037\n",
      "Iteration 3230, loss = 0.48418614\n",
      "Iteration 3231, loss = 0.48367802\n",
      "Iteration 3232, loss = 0.48360345\n",
      "Iteration 3233, loss = 0.48432181\n",
      "Iteration 3234, loss = 0.48419813\n",
      "Iteration 3235, loss = 0.48387260\n",
      "Iteration 3236, loss = 0.48364944\n",
      "Iteration 3237, loss = 0.48354955\n",
      "Iteration 3238, loss = 0.48428162\n",
      "Iteration 3239, loss = 0.48439330\n",
      "Iteration 3240, loss = 0.48429233\n",
      "Iteration 3241, loss = 0.48429291\n",
      "Iteration 3242, loss = 0.48411468\n",
      "Iteration 3243, loss = 0.48430235\n",
      "Iteration 3244, loss = 0.48390539\n",
      "Iteration 3245, loss = 0.48425158\n",
      "Iteration 3246, loss = 0.48542065\n",
      "Iteration 3247, loss = 0.48600795\n",
      "Iteration 3248, loss = 0.48564434\n",
      "Iteration 3249, loss = 0.48435196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3250, loss = 0.48381286\n",
      "Iteration 3251, loss = 0.48344052\n",
      "Iteration 3252, loss = 0.48409901\n",
      "Iteration 3253, loss = 0.48421775\n",
      "Iteration 3254, loss = 0.48387640\n",
      "Iteration 3255, loss = 0.48317528\n",
      "Iteration 3256, loss = 0.48402838\n",
      "Iteration 3257, loss = 0.48695082\n",
      "Iteration 3258, loss = 0.48496318\n",
      "Iteration 3259, loss = 0.48290483\n",
      "Iteration 3260, loss = 0.48569781\n",
      "Iteration 3261, loss = 0.48825698\n",
      "Iteration 3262, loss = 0.48948007\n",
      "Iteration 3263, loss = 0.48834522\n",
      "Iteration 3264, loss = 0.48597364\n",
      "Iteration 3265, loss = 0.48400739\n",
      "Iteration 3266, loss = 0.48436890\n",
      "Iteration 3267, loss = 0.48405398\n",
      "Iteration 3268, loss = 0.48390552\n",
      "Iteration 3269, loss = 0.48360101\n",
      "Iteration 3270, loss = 0.48373639\n",
      "Iteration 3271, loss = 0.48378765\n",
      "Iteration 3272, loss = 0.48356116\n",
      "Iteration 3273, loss = 0.48354418\n",
      "Iteration 3274, loss = 0.48437904\n",
      "Iteration 3275, loss = 0.48485500\n",
      "Iteration 3276, loss = 0.48472125\n",
      "Iteration 3277, loss = 0.48433694\n",
      "Iteration 3278, loss = 0.48380905\n",
      "Iteration 3279, loss = 0.48362007\n",
      "Iteration 3280, loss = 0.48360041\n",
      "Iteration 3281, loss = 0.48364467\n",
      "Iteration 3282, loss = 0.48388594\n",
      "Iteration 3283, loss = 0.48558637\n",
      "Iteration 3284, loss = 0.48559067\n",
      "Iteration 3285, loss = 0.48504079\n",
      "Iteration 3286, loss = 0.48390040\n",
      "Iteration 3287, loss = 0.48363872\n",
      "Iteration 3288, loss = 0.48383449\n",
      "Iteration 3289, loss = 0.48377278\n",
      "Iteration 3290, loss = 0.48387003\n",
      "Iteration 3291, loss = 0.48396837\n",
      "Iteration 3292, loss = 0.48311164\n",
      "Iteration 3293, loss = 0.48458439\n",
      "Iteration 3294, loss = 0.48639627\n",
      "Iteration 3295, loss = 0.48735715\n",
      "Iteration 3296, loss = 0.48579754\n",
      "Iteration 3297, loss = 0.48349096\n",
      "Iteration 3298, loss = 0.48359233\n",
      "Iteration 3299, loss = 0.48657084\n",
      "Iteration 3300, loss = 0.48784602\n",
      "Iteration 3301, loss = 0.48678375\n",
      "Iteration 3302, loss = 0.48428941\n",
      "Iteration 3303, loss = 0.48369179\n",
      "Iteration 3304, loss = 0.48410065\n",
      "Iteration 3305, loss = 0.48393136\n",
      "Iteration 3306, loss = 0.48356602\n",
      "Iteration 3307, loss = 0.48364289\n",
      "Iteration 3308, loss = 0.48444431\n",
      "Iteration 3309, loss = 0.48469593\n",
      "Iteration 3310, loss = 0.48454121\n",
      "Iteration 3311, loss = 0.48460522\n",
      "Iteration 3312, loss = 0.48438812\n",
      "Iteration 3313, loss = 0.48446040\n",
      "Iteration 3314, loss = 0.48435893\n",
      "Iteration 3315, loss = 0.48410820\n",
      "Iteration 3316, loss = 0.48379892\n",
      "Iteration 3317, loss = 0.48376667\n",
      "Iteration 3318, loss = 0.48353740\n",
      "Iteration 3319, loss = 0.48374733\n",
      "Iteration 3320, loss = 0.48371847\n",
      "Iteration 3321, loss = 0.48372450\n",
      "Iteration 3322, loss = 0.48373093\n",
      "Iteration 3323, loss = 0.48413559\n",
      "Iteration 3324, loss = 0.48422357\n",
      "Iteration 3325, loss = 0.48380406\n",
      "Iteration 3326, loss = 0.48370969\n",
      "Iteration 3327, loss = 0.48358043\n",
      "Iteration 3328, loss = 0.48363850\n",
      "Iteration 3329, loss = 0.48366060\n",
      "Iteration 3330, loss = 0.48329952\n",
      "Iteration 3331, loss = 0.48378614\n",
      "Iteration 3332, loss = 0.48572683\n",
      "Iteration 3333, loss = 0.48624944\n",
      "Iteration 3334, loss = 0.48555171\n",
      "Iteration 3335, loss = 0.48462111\n",
      "Iteration 3336, loss = 0.48387988\n",
      "Iteration 3337, loss = 0.48356165\n",
      "Iteration 3338, loss = 0.48349323\n",
      "Iteration 3339, loss = 0.48361385\n",
      "Iteration 3340, loss = 0.48405321\n",
      "Iteration 3341, loss = 0.48448870\n",
      "Iteration 3342, loss = 0.48492451\n",
      "Iteration 3343, loss = 0.48563158\n",
      "Iteration 3344, loss = 0.48606103\n",
      "Iteration 3345, loss = 0.48554571\n",
      "Iteration 3346, loss = 0.48427226\n",
      "Iteration 3347, loss = 0.48336808\n",
      "Iteration 3348, loss = 0.48361976\n",
      "Iteration 3349, loss = 0.48609721\n",
      "Iteration 3350, loss = 0.48767920\n",
      "Iteration 3351, loss = 0.48746236\n",
      "Iteration 3352, loss = 0.48657509\n",
      "Iteration 3353, loss = 0.48462927\n",
      "Iteration 3354, loss = 0.48382548\n",
      "Iteration 3355, loss = 0.48291676\n",
      "Iteration 3356, loss = 0.48483901\n",
      "Iteration 3357, loss = 0.48650971\n",
      "Iteration 3358, loss = 0.48756627\n",
      "Iteration 3359, loss = 0.48680251\n",
      "Iteration 3360, loss = 0.48539416\n",
      "Iteration 3361, loss = 0.48386180\n",
      "Iteration 3362, loss = 0.48374320\n",
      "Iteration 3363, loss = 0.48406895\n",
      "Iteration 3364, loss = 0.48414848\n",
      "Iteration 3365, loss = 0.48394002\n",
      "Iteration 3366, loss = 0.48355441\n",
      "Iteration 3367, loss = 0.48365222\n",
      "Iteration 3368, loss = 0.48367644\n",
      "Iteration 3369, loss = 0.48371881\n",
      "Iteration 3370, loss = 0.48440041\n",
      "Iteration 3371, loss = 0.48539564\n",
      "Iteration 3372, loss = 0.48640153\n",
      "Iteration 3373, loss = 0.48660516\n",
      "Iteration 3374, loss = 0.48448231\n",
      "Iteration 3375, loss = 0.48471523\n",
      "Iteration 3376, loss = 0.48452879\n",
      "Iteration 3377, loss = 0.48547003\n",
      "Iteration 3378, loss = 0.48580852\n",
      "Iteration 3379, loss = 0.48527092\n",
      "Iteration 3380, loss = 0.48450449\n",
      "Iteration 3381, loss = 0.48370174\n",
      "Iteration 3382, loss = 0.48362298\n",
      "Iteration 3383, loss = 0.48376153\n",
      "Iteration 3384, loss = 0.48365935\n",
      "Iteration 3385, loss = 0.48348155\n",
      "Iteration 3386, loss = 0.48360214\n",
      "Iteration 3387, loss = 0.48414045\n",
      "Iteration 3388, loss = 0.48492835\n",
      "Iteration 3389, loss = 0.48508614\n",
      "Iteration 3390, loss = 0.48516465\n",
      "Iteration 3391, loss = 0.48405263\n",
      "Iteration 3392, loss = 0.48386523\n",
      "Iteration 3393, loss = 0.48370555\n",
      "Iteration 3394, loss = 0.48368562\n",
      "Iteration 3395, loss = 0.48345196\n",
      "Iteration 3396, loss = 0.48381616\n",
      "Iteration 3397, loss = 0.48486823\n",
      "Iteration 3398, loss = 0.48484136\n",
      "Iteration 3399, loss = 0.48488897\n",
      "Iteration 3400, loss = 0.48399927\n",
      "Iteration 3401, loss = 0.48345555\n",
      "Iteration 3402, loss = 0.48455783\n",
      "Iteration 3403, loss = 0.48540826\n",
      "Iteration 3404, loss = 0.48555850\n",
      "Iteration 3405, loss = 0.48484386\n",
      "Iteration 3406, loss = 0.48339544\n",
      "Iteration 3407, loss = 0.48396956\n",
      "Iteration 3408, loss = 0.48510240\n",
      "Iteration 3409, loss = 0.48684577\n",
      "Iteration 3410, loss = 0.48816386\n",
      "Iteration 3411, loss = 0.48752745\n",
      "Iteration 3412, loss = 0.48538182\n",
      "Iteration 3413, loss = 0.48484724\n",
      "Iteration 3414, loss = 0.48350683\n",
      "Iteration 3415, loss = 0.48375837\n",
      "Iteration 3416, loss = 0.48434190\n",
      "Iteration 3417, loss = 0.48611580\n",
      "Iteration 3418, loss = 0.48863905\n",
      "Iteration 3419, loss = 0.48846632\n",
      "Iteration 3420, loss = 0.48455218\n",
      "Iteration 3421, loss = 0.48316354\n",
      "Iteration 3422, loss = 0.48637380\n",
      "Iteration 3423, loss = 0.49115215\n",
      "Iteration 3424, loss = 0.49256761\n",
      "Iteration 3425, loss = 0.49036036\n",
      "Iteration 3426, loss = 0.48757253\n",
      "Iteration 3427, loss = 0.48498249\n",
      "Iteration 3428, loss = 0.48361015\n",
      "Iteration 3429, loss = 0.48367261\n",
      "Iteration 3430, loss = 0.48415549\n",
      "Iteration 3431, loss = 0.48439690\n",
      "Iteration 3432, loss = 0.48404065\n",
      "Iteration 3433, loss = 0.48487692\n",
      "Iteration 3434, loss = 0.48522574\n",
      "Iteration 3435, loss = 0.48476310\n",
      "Iteration 3436, loss = 0.48399293\n",
      "Iteration 3437, loss = 0.48381505\n",
      "Iteration 3438, loss = 0.48385159\n",
      "Iteration 3439, loss = 0.48409714\n",
      "Iteration 3440, loss = 0.48418712\n",
      "Iteration 3441, loss = 0.48411790\n",
      "Iteration 3442, loss = 0.48468464\n",
      "Iteration 3443, loss = 0.48437080\n",
      "Iteration 3444, loss = 0.48420898\n",
      "Iteration 3445, loss = 0.48355729\n",
      "Iteration 3446, loss = 0.48358979\n",
      "Iteration 3447, loss = 0.48367249\n",
      "Iteration 3448, loss = 0.48376230\n",
      "Iteration 3449, loss = 0.48341369\n",
      "Iteration 3450, loss = 0.48308553\n",
      "Iteration 3451, loss = 0.48469706\n",
      "Iteration 3452, loss = 0.48722438\n",
      "Iteration 3453, loss = 0.48942317\n",
      "Iteration 3454, loss = 0.49025624\n",
      "Iteration 3455, loss = 0.48932773\n",
      "Iteration 3456, loss = 0.48687346\n",
      "Iteration 3457, loss = 0.48354099\n",
      "Iteration 3458, loss = 0.48363441\n",
      "Iteration 3459, loss = 0.48667194\n",
      "Iteration 3460, loss = 0.48848781\n",
      "Iteration 3461, loss = 0.48821937\n",
      "Iteration 3462, loss = 0.48646023\n",
      "Iteration 3463, loss = 0.48459582\n",
      "Iteration 3464, loss = 0.48343738\n",
      "Iteration 3465, loss = 0.48392802\n",
      "Iteration 3466, loss = 0.48413468\n",
      "Iteration 3467, loss = 0.48428412\n",
      "Iteration 3468, loss = 0.48462329\n",
      "Iteration 3469, loss = 0.48446139\n",
      "Iteration 3470, loss = 0.48408801\n",
      "Iteration 3471, loss = 0.48349539\n",
      "Iteration 3472, loss = 0.48382897\n",
      "Iteration 3473, loss = 0.48388965\n",
      "Iteration 3474, loss = 0.48388301\n",
      "Iteration 3475, loss = 0.48371381\n",
      "Iteration 3476, loss = 0.48390437\n",
      "Iteration 3477, loss = 0.48385199\n",
      "Iteration 3478, loss = 0.48358957\n",
      "Iteration 3479, loss = 0.48351060\n",
      "Iteration 3480, loss = 0.48369707\n",
      "Iteration 3481, loss = 0.48391375\n",
      "Iteration 3482, loss = 0.48416507\n",
      "Iteration 3483, loss = 0.48386681\n",
      "Iteration 3484, loss = 0.48367438\n",
      "Iteration 3485, loss = 0.48392636\n",
      "Iteration 3486, loss = 0.48405158\n",
      "Iteration 3487, loss = 0.48357071\n",
      "Iteration 3488, loss = 0.48354112\n",
      "Iteration 3489, loss = 0.48355436\n",
      "Iteration 3490, loss = 0.48383666\n",
      "Iteration 3491, loss = 0.48428907\n",
      "Iteration 3492, loss = 0.48437569\n",
      "Iteration 3493, loss = 0.48353505\n",
      "Iteration 3494, loss = 0.48352477\n",
      "Iteration 3495, loss = 0.48353407\n",
      "Iteration 3496, loss = 0.48356382\n",
      "Iteration 3497, loss = 0.48360497\n",
      "Iteration 3498, loss = 0.48408821\n",
      "Iteration 3499, loss = 0.48451569\n",
      "Iteration 3500, loss = 0.48483240\n",
      "Iteration 3501, loss = 0.48476776\n",
      "Iteration 3502, loss = 0.48388302\n",
      "Iteration 3503, loss = 0.48447076\n",
      "Iteration 3504, loss = 0.48382617\n",
      "Iteration 3505, loss = 0.48355653\n",
      "Iteration 3506, loss = 0.48342868\n",
      "Iteration 3507, loss = 0.48341759\n",
      "Iteration 3508, loss = 0.48490627\n",
      "Iteration 3509, loss = 0.48501144\n",
      "Iteration 3510, loss = 0.48399802\n",
      "Iteration 3511, loss = 0.48335582\n",
      "Iteration 3512, loss = 0.48470455\n",
      "Iteration 3513, loss = 0.48506675\n",
      "Iteration 3514, loss = 0.48512779\n",
      "Iteration 3515, loss = 0.48512343\n",
      "Iteration 3516, loss = 0.48462640\n",
      "Iteration 3517, loss = 0.48399353\n",
      "Iteration 3518, loss = 0.48381394\n",
      "Iteration 3519, loss = 0.48348620\n",
      "Iteration 3520, loss = 0.48355161\n",
      "Iteration 3521, loss = 0.48359275\n",
      "Iteration 3522, loss = 0.48363900\n",
      "Iteration 3523, loss = 0.48421027\n",
      "Iteration 3524, loss = 0.48367212\n",
      "Iteration 3525, loss = 0.48357172\n",
      "Iteration 3526, loss = 0.48369385\n",
      "Iteration 3527, loss = 0.48393027\n",
      "Iteration 3528, loss = 0.48394950\n",
      "Iteration 3529, loss = 0.48359802\n",
      "Iteration 3530, loss = 0.48326775\n",
      "Iteration 3531, loss = 0.48448221\n",
      "Iteration 3532, loss = 0.48558187\n",
      "Iteration 3533, loss = 0.48565802\n",
      "Iteration 3534, loss = 0.48484867\n",
      "Iteration 3535, loss = 0.48371106\n",
      "Iteration 3536, loss = 0.48342530\n",
      "Iteration 3537, loss = 0.48443811\n",
      "Iteration 3538, loss = 0.48439660\n",
      "Iteration 3539, loss = 0.48436274\n",
      "Iteration 3540, loss = 0.48386082\n",
      "Iteration 3541, loss = 0.48392500\n",
      "Iteration 3542, loss = 0.48352377\n",
      "Iteration 3543, loss = 0.48336807\n",
      "Iteration 3544, loss = 0.48397654\n",
      "Iteration 3545, loss = 0.48458701\n",
      "Iteration 3546, loss = 0.48448336\n",
      "Iteration 3547, loss = 0.48407090\n",
      "Iteration 3548, loss = 0.48355942\n",
      "Iteration 3549, loss = 0.48323512\n",
      "Iteration 3550, loss = 0.48416543\n",
      "Iteration 3551, loss = 0.48457159\n",
      "Iteration 3552, loss = 0.48434991\n",
      "Iteration 3553, loss = 0.48376670\n",
      "Iteration 3554, loss = 0.48364087\n",
      "Iteration 3555, loss = 0.48368397\n",
      "Iteration 3556, loss = 0.48355437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3557, loss = 0.48343839\n",
      "Iteration 3558, loss = 0.48360371\n",
      "Iteration 3559, loss = 0.48448517\n",
      "Iteration 3560, loss = 0.48524722\n",
      "Iteration 3561, loss = 0.48513319\n",
      "Iteration 3562, loss = 0.48418832\n",
      "Iteration 3563, loss = 0.48346290\n",
      "Iteration 3564, loss = 0.48353883\n",
      "Iteration 3565, loss = 0.48488591\n",
      "Iteration 3566, loss = 0.48583105\n",
      "Iteration 3567, loss = 0.48666739\n",
      "Iteration 3568, loss = 0.48636266\n",
      "Iteration 3569, loss = 0.48477716\n",
      "Iteration 3570, loss = 0.48351189\n",
      "Iteration 3571, loss = 0.48452608\n",
      "Iteration 3572, loss = 0.48473668\n",
      "Iteration 3573, loss = 0.48449247\n",
      "Iteration 3574, loss = 0.48394255\n",
      "Iteration 3575, loss = 0.48382219\n",
      "Iteration 3576, loss = 0.48377182\n",
      "Iteration 3577, loss = 0.48383739\n",
      "Iteration 3578, loss = 0.48380887\n",
      "Iteration 3579, loss = 0.48350547\n",
      "Iteration 3580, loss = 0.48393339\n",
      "Iteration 3581, loss = 0.48408945\n",
      "Iteration 3582, loss = 0.48434510\n",
      "Iteration 3583, loss = 0.48441210\n",
      "Iteration 3584, loss = 0.48398017\n",
      "Iteration 3585, loss = 0.48388887\n",
      "Iteration 3586, loss = 0.48355610\n",
      "Iteration 3587, loss = 0.48365451\n",
      "Iteration 3588, loss = 0.48362136\n",
      "Iteration 3589, loss = 0.48464137\n",
      "Iteration 3590, loss = 0.48394215\n",
      "Iteration 3591, loss = 0.48355740\n",
      "Iteration 3592, loss = 0.48377855\n",
      "Iteration 3593, loss = 0.48397166\n",
      "Iteration 3594, loss = 0.48368214\n",
      "Iteration 3595, loss = 0.48343064\n",
      "Iteration 3596, loss = 0.48376462\n",
      "Iteration 3597, loss = 0.48407429\n",
      "Iteration 3598, loss = 0.48431180\n",
      "Iteration 3599, loss = 0.48447851\n",
      "Iteration 3600, loss = 0.48465957\n",
      "Iteration 3601, loss = 0.48365187\n",
      "Iteration 3602, loss = 0.48298420\n",
      "Iteration 3603, loss = 0.48441994\n",
      "Iteration 3604, loss = 0.48862613\n",
      "Iteration 3605, loss = 0.49211755\n",
      "Iteration 3606, loss = 0.49031675\n",
      "Iteration 3607, loss = 0.48745318\n",
      "Iteration 3608, loss = 0.48411571\n",
      "Iteration 3609, loss = 0.48353131\n",
      "Iteration 3610, loss = 0.48342105\n",
      "Iteration 3611, loss = 0.48383796\n",
      "Iteration 3612, loss = 0.48373562\n",
      "Iteration 3613, loss = 0.48363716\n",
      "Iteration 3614, loss = 0.48386922\n",
      "Iteration 3615, loss = 0.48326321\n",
      "Iteration 3616, loss = 0.48485118\n",
      "Iteration 3617, loss = 0.48444198\n",
      "Iteration 3618, loss = 0.48370258\n",
      "Iteration 3619, loss = 0.48360858\n",
      "Iteration 3620, loss = 0.48373848\n",
      "Iteration 3621, loss = 0.48424527\n",
      "Iteration 3622, loss = 0.48513369\n",
      "Iteration 3623, loss = 0.48572282\n",
      "Iteration 3624, loss = 0.48547055\n",
      "Iteration 3625, loss = 0.48447202\n",
      "Iteration 3626, loss = 0.48467123\n",
      "Iteration 3627, loss = 0.48361036\n",
      "Iteration 3628, loss = 0.48335041\n",
      "Iteration 3629, loss = 0.48359979\n",
      "Iteration 3630, loss = 0.48447823\n",
      "Iteration 3631, loss = 0.48402090\n",
      "Iteration 3632, loss = 0.48286734\n",
      "Iteration 3633, loss = 0.48559134\n",
      "Iteration 3634, loss = 0.48654439\n",
      "Iteration 3635, loss = 0.48579389\n",
      "Iteration 3636, loss = 0.48376159\n",
      "Iteration 3637, loss = 0.48321614\n",
      "Iteration 3638, loss = 0.48518100\n",
      "Iteration 3639, loss = 0.48640061\n",
      "Iteration 3640, loss = 0.48648087\n",
      "Iteration 3641, loss = 0.48567104\n",
      "Iteration 3642, loss = 0.48444089\n",
      "Iteration 3643, loss = 0.48373719\n",
      "Iteration 3644, loss = 0.48348463\n",
      "Iteration 3645, loss = 0.48356222\n",
      "Iteration 3646, loss = 0.48384885\n",
      "Iteration 3647, loss = 0.48364189\n",
      "Iteration 3648, loss = 0.48422894\n",
      "Iteration 3649, loss = 0.48551186\n",
      "Iteration 3650, loss = 0.48522700\n",
      "Iteration 3651, loss = 0.48424748\n",
      "Iteration 3652, loss = 0.48368657\n",
      "Iteration 3653, loss = 0.48364928\n",
      "Iteration 3654, loss = 0.48358015\n",
      "Iteration 3655, loss = 0.48362414\n",
      "Iteration 3656, loss = 0.48364574\n",
      "Iteration 3657, loss = 0.48348119\n",
      "Iteration 3658, loss = 0.48361798\n",
      "Iteration 3659, loss = 0.48380923\n",
      "Iteration 3660, loss = 0.48418317\n",
      "Iteration 3661, loss = 0.48427242\n",
      "Iteration 3662, loss = 0.48375743\n",
      "Iteration 3663, loss = 0.48387628\n",
      "Iteration 3664, loss = 0.48361302\n",
      "Iteration 3665, loss = 0.48411248\n",
      "Iteration 3666, loss = 0.48422537\n",
      "Iteration 3667, loss = 0.48414694\n",
      "Iteration 3668, loss = 0.48425665\n",
      "Iteration 3669, loss = 0.48389372\n",
      "Iteration 3670, loss = 0.48347055\n",
      "Iteration 3671, loss = 0.48356528\n",
      "Iteration 3672, loss = 0.48427344\n",
      "Iteration 3673, loss = 0.48478764\n",
      "Iteration 3674, loss = 0.48481352\n",
      "Iteration 3675, loss = 0.48438029\n",
      "Iteration 3676, loss = 0.48382299\n",
      "Iteration 3677, loss = 0.48367076\n",
      "Iteration 3678, loss = 0.48418489\n",
      "Iteration 3679, loss = 0.48432711\n",
      "Iteration 3680, loss = 0.48419500\n",
      "Iteration 3681, loss = 0.48425111\n",
      "Iteration 3682, loss = 0.48359616\n",
      "Iteration 3683, loss = 0.48352014\n",
      "Iteration 3684, loss = 0.48353723\n",
      "Iteration 3685, loss = 0.48376846\n",
      "Iteration 3686, loss = 0.48407056\n",
      "Iteration 3687, loss = 0.48363359\n",
      "Iteration 3688, loss = 0.48484842\n",
      "Iteration 3689, loss = 0.48631078\n",
      "Iteration 3690, loss = 0.48683159\n",
      "Iteration 3691, loss = 0.48563005\n",
      "Iteration 3692, loss = 0.48490962\n",
      "Iteration 3693, loss = 0.48375947\n",
      "Iteration 3694, loss = 0.48360629\n",
      "Iteration 3695, loss = 0.48331046\n",
      "Iteration 3696, loss = 0.48350572\n",
      "Iteration 3697, loss = 0.48399194\n",
      "Iteration 3698, loss = 0.48516975\n",
      "Iteration 3699, loss = 0.48832943\n",
      "Iteration 3700, loss = 0.49143604\n",
      "Iteration 3701, loss = 0.48924701\n",
      "Iteration 3702, loss = 0.48552787\n",
      "Iteration 3703, loss = 0.48453746\n",
      "Iteration 3704, loss = 0.48395325\n",
      "Iteration 3705, loss = 0.48435336\n",
      "Iteration 3706, loss = 0.48447699\n",
      "Iteration 3707, loss = 0.48358149\n",
      "Iteration 3708, loss = 0.48391023\n",
      "Iteration 3709, loss = 0.48409262\n",
      "Iteration 3710, loss = 0.48418412\n",
      "Iteration 3711, loss = 0.48432361\n",
      "Iteration 3712, loss = 0.48344593\n",
      "Iteration 3713, loss = 0.48343067\n",
      "Iteration 3714, loss = 0.48342011\n",
      "Iteration 3715, loss = 0.48357707\n",
      "Iteration 3716, loss = 0.48366504\n",
      "Iteration 3717, loss = 0.48356866\n",
      "Iteration 3718, loss = 0.48362586\n",
      "Iteration 3719, loss = 0.48419226\n",
      "Iteration 3720, loss = 0.48428264\n",
      "Iteration 3721, loss = 0.48393708\n",
      "Iteration 3722, loss = 0.48376564\n",
      "Iteration 3723, loss = 0.48335938\n",
      "Iteration 3724, loss = 0.48436091\n",
      "Iteration 3725, loss = 0.48544417\n",
      "Iteration 3726, loss = 0.48613598\n",
      "Iteration 3727, loss = 0.48589159\n",
      "Iteration 3728, loss = 0.48496356\n",
      "Iteration 3729, loss = 0.48424457\n",
      "Iteration 3730, loss = 0.48377962\n",
      "Iteration 3731, loss = 0.48383610\n",
      "Iteration 3732, loss = 0.48394577\n",
      "Iteration 3733, loss = 0.48410734\n",
      "Iteration 3734, loss = 0.48415509\n",
      "Iteration 3735, loss = 0.48401613\n",
      "Iteration 3736, loss = 0.48351892\n",
      "Iteration 3737, loss = 0.48379458\n",
      "Iteration 3738, loss = 0.48383833\n",
      "Iteration 3739, loss = 0.48422188\n",
      "Iteration 3740, loss = 0.48451883\n",
      "Iteration 3741, loss = 0.48448281\n",
      "Iteration 3742, loss = 0.48388200\n",
      "Iteration 3743, loss = 0.48373454\n",
      "Iteration 3744, loss = 0.48348635\n",
      "Iteration 3745, loss = 0.48340020\n",
      "Iteration 3746, loss = 0.48379897\n",
      "Iteration 3747, loss = 0.48414395\n",
      "Iteration 3748, loss = 0.48389563\n",
      "Iteration 3749, loss = 0.48349078\n",
      "Iteration 3750, loss = 0.48339272\n",
      "Iteration 3751, loss = 0.48362821\n",
      "Iteration 3752, loss = 0.48447291\n",
      "Iteration 3753, loss = 0.48567294\n",
      "Iteration 3754, loss = 0.48649495\n",
      "Iteration 3755, loss = 0.48546032\n",
      "Iteration 3756, loss = 0.48433875\n",
      "Iteration 3757, loss = 0.48426042\n",
      "Iteration 3758, loss = 0.48364112\n",
      "Iteration 3759, loss = 0.48348052\n",
      "Iteration 3760, loss = 0.48428926\n",
      "Iteration 3761, loss = 0.48362243\n",
      "Iteration 3762, loss = 0.48345715\n",
      "Iteration 3763, loss = 0.48362866\n",
      "Iteration 3764, loss = 0.48410706\n",
      "Iteration 3765, loss = 0.48398370\n",
      "Iteration 3766, loss = 0.48341955\n",
      "Iteration 3767, loss = 0.48317175\n",
      "Iteration 3768, loss = 0.48404207\n",
      "Iteration 3769, loss = 0.48622800\n",
      "Iteration 3770, loss = 0.48794958\n",
      "Iteration 3771, loss = 0.48799630\n",
      "Iteration 3772, loss = 0.48641621\n",
      "Iteration 3773, loss = 0.48531488\n",
      "Iteration 3774, loss = 0.48391185\n",
      "Iteration 3775, loss = 0.48347891\n",
      "Iteration 3776, loss = 0.48375698\n",
      "Iteration 3777, loss = 0.48363428\n",
      "Iteration 3778, loss = 0.48285889\n",
      "Iteration 3779, loss = 0.48572461\n",
      "Iteration 3780, loss = 0.48697128\n",
      "Iteration 3781, loss = 0.48529884\n",
      "Iteration 3782, loss = 0.48349902\n",
      "Iteration 3783, loss = 0.48382609\n",
      "Iteration 3784, loss = 0.48561812\n",
      "Iteration 3785, loss = 0.48655480\n",
      "Iteration 3786, loss = 0.48639398\n",
      "Iteration 3787, loss = 0.48602034\n",
      "Iteration 3788, loss = 0.48557943\n",
      "Iteration 3789, loss = 0.48440451\n",
      "Iteration 3790, loss = 0.48379439\n",
      "Iteration 3791, loss = 0.48341066\n",
      "Iteration 3792, loss = 0.48374556\n",
      "Iteration 3793, loss = 0.48548964\n",
      "Iteration 3794, loss = 0.48720792\n",
      "Iteration 3795, loss = 0.48716290\n",
      "Iteration 3796, loss = 0.48538298\n",
      "Iteration 3797, loss = 0.48502151\n",
      "Iteration 3798, loss = 0.48460508\n",
      "Iteration 3799, loss = 0.48379268\n",
      "Iteration 3800, loss = 0.48369129\n",
      "Iteration 3801, loss = 0.48379234\n",
      "Iteration 3802, loss = 0.48389200\n",
      "Iteration 3803, loss = 0.48390550\n",
      "Iteration 3804, loss = 0.48396946\n",
      "Iteration 3805, loss = 0.48364998\n",
      "Iteration 3806, loss = 0.48342282\n",
      "Iteration 3807, loss = 0.48416080\n",
      "Iteration 3808, loss = 0.48478018\n",
      "Iteration 3809, loss = 0.48459667\n",
      "Iteration 3810, loss = 0.48381377\n",
      "Iteration 3811, loss = 0.48349421\n",
      "Iteration 3812, loss = 0.48395755\n",
      "Iteration 3813, loss = 0.48531574\n",
      "Iteration 3814, loss = 0.48770318\n",
      "Iteration 3815, loss = 0.48909751\n",
      "Iteration 3816, loss = 0.48857936\n",
      "Iteration 3817, loss = 0.48728249\n",
      "Iteration 3818, loss = 0.48566874\n",
      "Iteration 3819, loss = 0.48410836\n",
      "Iteration 3820, loss = 0.48407002\n",
      "Iteration 3821, loss = 0.48383903\n",
      "Iteration 3822, loss = 0.48378908\n",
      "Iteration 3823, loss = 0.48358166\n",
      "Iteration 3824, loss = 0.48354683\n",
      "Iteration 3825, loss = 0.48406567\n",
      "Iteration 3826, loss = 0.48452400\n",
      "Iteration 3827, loss = 0.48447638\n",
      "Iteration 3828, loss = 0.48388992\n",
      "Iteration 3829, loss = 0.48347648\n",
      "Iteration 3830, loss = 0.48336704\n",
      "Iteration 3831, loss = 0.48429723\n",
      "Iteration 3832, loss = 0.48741433\n",
      "Iteration 3833, loss = 0.48684112\n",
      "Iteration 3834, loss = 0.48498418\n",
      "Iteration 3835, loss = 0.48475145\n",
      "Iteration 3836, loss = 0.48397117\n",
      "Iteration 3837, loss = 0.48381221\n",
      "Iteration 3838, loss = 0.48370527\n",
      "Iteration 3839, loss = 0.48365662\n",
      "Iteration 3840, loss = 0.48355137\n",
      "Iteration 3841, loss = 0.48387771\n",
      "Iteration 3842, loss = 0.48385138\n",
      "Iteration 3843, loss = 0.48348492\n",
      "Iteration 3844, loss = 0.48335451\n",
      "Iteration 3845, loss = 0.48463679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3846, loss = 0.48634044\n",
      "Iteration 3847, loss = 0.48844674\n",
      "Iteration 3848, loss = 0.49006207\n",
      "Iteration 3849, loss = 0.48875870\n",
      "Iteration 3850, loss = 0.48560936\n",
      "Iteration 3851, loss = 0.48372716\n",
      "Iteration 3852, loss = 0.48406940\n",
      "Iteration 3853, loss = 0.48625259\n",
      "Iteration 3854, loss = 0.48666378\n",
      "Iteration 3855, loss = 0.48519018\n",
      "Iteration 3856, loss = 0.48453276\n",
      "Iteration 3857, loss = 0.48522911\n",
      "Iteration 3858, loss = 0.48352354\n",
      "Iteration 3859, loss = 0.48414940\n",
      "Iteration 3860, loss = 0.48394064\n",
      "Iteration 3861, loss = 0.48345827\n",
      "Iteration 3862, loss = 0.48356378\n",
      "Iteration 3863, loss = 0.48445954\n",
      "Iteration 3864, loss = 0.48545214\n",
      "Iteration 3865, loss = 0.48596390\n",
      "Iteration 3866, loss = 0.48491043\n",
      "Iteration 3867, loss = 0.48385453\n",
      "Iteration 3868, loss = 0.48363389\n",
      "Iteration 3869, loss = 0.48514085\n",
      "Iteration 3870, loss = 0.48562382\n",
      "Iteration 3871, loss = 0.48529130\n",
      "Iteration 3872, loss = 0.48438254\n",
      "Iteration 3873, loss = 0.48365425\n",
      "Iteration 3874, loss = 0.48338803\n",
      "Iteration 3875, loss = 0.48468640\n",
      "Iteration 3876, loss = 0.48515638\n",
      "Iteration 3877, loss = 0.48502969\n",
      "Iteration 3878, loss = 0.48403448\n",
      "Iteration 3879, loss = 0.48378165\n",
      "Iteration 3880, loss = 0.48351568\n",
      "Iteration 3881, loss = 0.48360594\n",
      "Iteration 3882, loss = 0.48380295\n",
      "Iteration 3883, loss = 0.48394829\n",
      "Iteration 3884, loss = 0.48395189\n",
      "Iteration 3885, loss = 0.48389861\n",
      "Iteration 3886, loss = 0.48353486\n",
      "Iteration 3887, loss = 0.48350184\n",
      "Iteration 3888, loss = 0.48378508\n",
      "Iteration 3889, loss = 0.48471927\n",
      "Iteration 3890, loss = 0.48484885\n",
      "Iteration 3891, loss = 0.48437227\n",
      "Iteration 3892, loss = 0.48381625\n",
      "Iteration 3893, loss = 0.48353399\n",
      "Iteration 3894, loss = 0.48369593\n",
      "Iteration 3895, loss = 0.48528582\n",
      "Iteration 3896, loss = 0.48368983\n",
      "Iteration 3897, loss = 0.48330896\n",
      "Iteration 3898, loss = 0.48612698\n",
      "Iteration 3899, loss = 0.48572381\n",
      "Iteration 3900, loss = 0.48442283\n",
      "Iteration 3901, loss = 0.48396093\n",
      "Iteration 3902, loss = 0.48398620\n",
      "Iteration 3903, loss = 0.48450077\n",
      "Iteration 3904, loss = 0.48435623\n",
      "Iteration 3905, loss = 0.48431159\n",
      "Iteration 3906, loss = 0.48356584\n",
      "Iteration 3907, loss = 0.48360556\n",
      "Iteration 3908, loss = 0.48371692\n",
      "Iteration 3909, loss = 0.48354392\n",
      "Iteration 3910, loss = 0.48392969\n",
      "Iteration 3911, loss = 0.48360197\n",
      "Iteration 3912, loss = 0.48349516\n",
      "Iteration 3913, loss = 0.48350415\n",
      "Iteration 3914, loss = 0.48354887\n",
      "Iteration 3915, loss = 0.48372767\n",
      "Iteration 3916, loss = 0.48374130\n",
      "Iteration 3917, loss = 0.48357596\n",
      "Iteration 3918, loss = 0.48364693\n",
      "Iteration 3919, loss = 0.48360681\n",
      "Iteration 3920, loss = 0.48379255\n",
      "Iteration 3921, loss = 0.48355286\n",
      "Iteration 3922, loss = 0.48355258\n",
      "Iteration 3923, loss = 0.48361698\n",
      "Iteration 3924, loss = 0.48386672\n",
      "Iteration 3925, loss = 0.48356846\n",
      "Iteration 3926, loss = 0.48345273\n",
      "Iteration 3927, loss = 0.48365402\n",
      "Iteration 3928, loss = 0.48374003\n",
      "Iteration 3929, loss = 0.48343641\n",
      "Iteration 3930, loss = 0.48416357\n",
      "Iteration 3931, loss = 0.48514967\n",
      "Iteration 3932, loss = 0.48553092\n",
      "Iteration 3933, loss = 0.48491814\n",
      "Iteration 3934, loss = 0.48385441\n",
      "Iteration 3935, loss = 0.48366688\n",
      "Iteration 3936, loss = 0.48380485\n",
      "Iteration 3937, loss = 0.48357048\n",
      "Iteration 3938, loss = 0.48356347\n",
      "Iteration 3939, loss = 0.48362700\n",
      "Iteration 3940, loss = 0.48345013\n",
      "Iteration 3941, loss = 0.48501199\n",
      "Iteration 3942, loss = 0.48570148\n",
      "Iteration 3943, loss = 0.48541465\n",
      "Iteration 3944, loss = 0.48460002\n",
      "Iteration 3945, loss = 0.48372423\n",
      "Iteration 3946, loss = 0.48351503\n",
      "Iteration 3947, loss = 0.48392232\n",
      "Iteration 3948, loss = 0.48392691\n",
      "Iteration 3949, loss = 0.48362233\n",
      "Iteration 3950, loss = 0.48443767\n",
      "Iteration 3951, loss = 0.48691228\n",
      "Iteration 3952, loss = 0.48778319\n",
      "Iteration 3953, loss = 0.48644091\n",
      "Iteration 3954, loss = 0.48459450\n",
      "Iteration 3955, loss = 0.48332198\n",
      "Iteration 3956, loss = 0.48362072\n",
      "Iteration 3957, loss = 0.48533425\n",
      "Iteration 3958, loss = 0.48623100\n",
      "Iteration 3959, loss = 0.48563907\n",
      "Iteration 3960, loss = 0.48470696\n",
      "Iteration 3961, loss = 0.48381219\n",
      "Iteration 3962, loss = 0.48371516\n",
      "Iteration 3963, loss = 0.48367869\n",
      "Iteration 3964, loss = 0.48357055\n",
      "Iteration 3965, loss = 0.48352173\n",
      "Iteration 3966, loss = 0.48360839\n",
      "Iteration 3967, loss = 0.48376198\n",
      "Iteration 3968, loss = 0.48391276\n",
      "Iteration 3969, loss = 0.48385865\n",
      "Iteration 3970, loss = 0.48350957\n",
      "Iteration 3971, loss = 0.48304481\n",
      "Iteration 3972, loss = 0.48400551\n",
      "Iteration 3973, loss = 0.48613214\n",
      "Iteration 3974, loss = 0.48624570\n",
      "Iteration 3975, loss = 0.48498895\n",
      "Iteration 3976, loss = 0.48427929\n",
      "Iteration 3977, loss = 0.48361253\n",
      "Iteration 3978, loss = 0.48338180\n",
      "Iteration 3979, loss = 0.48371726\n",
      "Iteration 3980, loss = 0.48470902\n",
      "Iteration 3981, loss = 0.48477274\n",
      "Iteration 3982, loss = 0.48406162\n",
      "Iteration 3983, loss = 0.48348736\n",
      "Iteration 3984, loss = 0.48320424\n",
      "Iteration 3985, loss = 0.48403297\n",
      "Iteration 3986, loss = 0.48554190\n",
      "Iteration 3987, loss = 0.48709322\n",
      "Iteration 3988, loss = 0.48606980\n",
      "Iteration 3989, loss = 0.48459819\n",
      "Iteration 3990, loss = 0.48326553\n",
      "Iteration 3991, loss = 0.48456792\n",
      "Iteration 3992, loss = 0.48397587\n",
      "Iteration 3993, loss = 0.48358170\n",
      "Iteration 3994, loss = 0.48433238\n",
      "Iteration 3995, loss = 0.48361209\n",
      "Iteration 3996, loss = 0.48381461\n",
      "Iteration 3997, loss = 0.48449850\n",
      "Iteration 3998, loss = 0.48469933\n",
      "Iteration 3999, loss = 0.48431521\n",
      "Iteration 4000, loss = 0.48400351\n",
      "Iteration 4001, loss = 0.48338455\n",
      "Iteration 4002, loss = 0.48402088\n",
      "Iteration 4003, loss = 0.48400968\n",
      "Iteration 4004, loss = 0.48396278\n",
      "Iteration 4005, loss = 0.48389132\n",
      "Iteration 4006, loss = 0.48355055\n",
      "Iteration 4007, loss = 0.48344212\n",
      "Iteration 4008, loss = 0.48381938\n",
      "Iteration 4009, loss = 0.48358978\n",
      "Iteration 4010, loss = 0.48348538\n",
      "Iteration 4011, loss = 0.48349246\n",
      "Iteration 4012, loss = 0.48352590\n",
      "Iteration 4013, loss = 0.48447000\n",
      "Iteration 4014, loss = 0.48339507\n",
      "Iteration 4015, loss = 0.48395543\n",
      "Iteration 4016, loss = 0.48461819\n",
      "Iteration 4017, loss = 0.48489302\n",
      "Iteration 4018, loss = 0.48459450\n",
      "Iteration 4019, loss = 0.48434876\n",
      "Iteration 4020, loss = 0.48353237\n",
      "Iteration 4021, loss = 0.48339367\n",
      "Iteration 4022, loss = 0.48387875\n",
      "Iteration 4023, loss = 0.48449824\n",
      "Iteration 4024, loss = 0.48472444\n",
      "Iteration 4025, loss = 0.48452133\n",
      "Iteration 4026, loss = 0.48376749\n",
      "Iteration 4027, loss = 0.48359062\n",
      "Iteration 4028, loss = 0.48400143\n",
      "Iteration 4029, loss = 0.48451830\n",
      "Iteration 4030, loss = 0.48421123\n",
      "Iteration 4031, loss = 0.48365114\n",
      "Iteration 4032, loss = 0.48364666\n",
      "Iteration 4033, loss = 0.48345726\n",
      "Iteration 4034, loss = 0.48368850\n",
      "Iteration 4035, loss = 0.48355316\n",
      "Iteration 4036, loss = 0.48347878\n",
      "Iteration 4037, loss = 0.48379899\n",
      "Iteration 4038, loss = 0.48378102\n",
      "Iteration 4039, loss = 0.48389164\n",
      "Iteration 4040, loss = 0.48392327\n",
      "Iteration 4041, loss = 0.48449197\n",
      "Iteration 4042, loss = 0.48623132\n",
      "Iteration 4043, loss = 0.48618168\n",
      "Iteration 4044, loss = 0.48528054\n",
      "Iteration 4045, loss = 0.48378878\n",
      "Iteration 4046, loss = 0.48288608\n",
      "Iteration 4047, loss = 0.48530373\n",
      "Iteration 4048, loss = 0.48553248\n",
      "Iteration 4049, loss = 0.48461682\n",
      "Iteration 4050, loss = 0.48347727\n",
      "Iteration 4051, loss = 0.48305630\n",
      "Iteration 4052, loss = 0.48472614\n",
      "Iteration 4053, loss = 0.48667768\n",
      "Iteration 4054, loss = 0.48758716\n",
      "Iteration 4055, loss = 0.48688171\n",
      "Iteration 4056, loss = 0.48464959\n",
      "Iteration 4057, loss = 0.48349544\n",
      "Iteration 4058, loss = 0.48432947\n",
      "Iteration 4059, loss = 0.48726014\n",
      "Iteration 4060, loss = 0.48758215\n",
      "Iteration 4061, loss = 0.48585700\n",
      "Iteration 4062, loss = 0.48348769\n",
      "Iteration 4063, loss = 0.48474792\n",
      "Iteration 4064, loss = 0.48464512\n",
      "Iteration 4065, loss = 0.48503604\n",
      "Iteration 4066, loss = 0.48483042\n",
      "Iteration 4067, loss = 0.48445613\n",
      "Iteration 4068, loss = 0.48371908\n",
      "Iteration 4069, loss = 0.48346023\n",
      "Iteration 4070, loss = 0.48347590\n",
      "Iteration 4071, loss = 0.48313216\n",
      "Iteration 4072, loss = 0.48371938\n",
      "Iteration 4073, loss = 0.48657411\n",
      "Iteration 4074, loss = 0.48777175\n",
      "Iteration 4075, loss = 0.48471396\n",
      "Iteration 4076, loss = 0.48428372\n",
      "Iteration 4077, loss = 0.48536184\n",
      "Iteration 4078, loss = 0.48637240\n",
      "Iteration 4079, loss = 0.48657173\n",
      "Iteration 4080, loss = 0.48556742\n",
      "Iteration 4081, loss = 0.48376327\n",
      "Iteration 4082, loss = 0.48371456\n",
      "Iteration 4083, loss = 0.48487255\n",
      "Iteration 4084, loss = 0.48632290\n",
      "Iteration 4085, loss = 0.48637878\n",
      "Iteration 4086, loss = 0.48486138\n",
      "Iteration 4087, loss = 0.48310385\n",
      "Iteration 4088, loss = 0.48482782\n",
      "Iteration 4089, loss = 0.48494661\n",
      "Iteration 4090, loss = 0.48429419\n",
      "Iteration 4091, loss = 0.48418378\n",
      "Iteration 4092, loss = 0.48351494\n",
      "Iteration 4093, loss = 0.48349668\n",
      "Iteration 4094, loss = 0.48372348\n",
      "Iteration 4095, loss = 0.48345218\n",
      "Iteration 4096, loss = 0.48330505\n",
      "Iteration 4097, loss = 0.48377938\n",
      "Iteration 4098, loss = 0.48415849\n",
      "Iteration 4099, loss = 0.48429718\n",
      "Iteration 4100, loss = 0.48424761\n",
      "Iteration 4101, loss = 0.48393430\n",
      "Iteration 4102, loss = 0.48381611\n",
      "Iteration 4103, loss = 0.48390627\n",
      "Iteration 4104, loss = 0.48398862\n",
      "Iteration 4105, loss = 0.48360288\n",
      "Iteration 4106, loss = 0.48314980\n",
      "Iteration 4107, loss = 0.48373648\n",
      "Iteration 4108, loss = 0.48499907\n",
      "Iteration 4109, loss = 0.48501943\n",
      "Iteration 4110, loss = 0.48411847\n",
      "Iteration 4111, loss = 0.48319187\n",
      "Iteration 4112, loss = 0.48384942\n",
      "Iteration 4113, loss = 0.48484386\n",
      "Iteration 4114, loss = 0.48485556\n",
      "Iteration 4115, loss = 0.48424277\n",
      "Iteration 4116, loss = 0.48325613\n",
      "Iteration 4117, loss = 0.48396397\n",
      "Iteration 4118, loss = 0.48501679\n",
      "Iteration 4119, loss = 0.48539182\n",
      "Iteration 4120, loss = 0.48481379\n",
      "Iteration 4121, loss = 0.48476944\n",
      "Iteration 4122, loss = 0.48379417\n",
      "Iteration 4123, loss = 0.48374912\n",
      "Iteration 4124, loss = 0.48394848\n",
      "Iteration 4125, loss = 0.48359058\n",
      "Iteration 4126, loss = 0.48337665\n",
      "Iteration 4127, loss = 0.48466195\n",
      "Iteration 4128, loss = 0.48494269\n",
      "Iteration 4129, loss = 0.48502387\n",
      "Iteration 4130, loss = 0.48477044\n",
      "Iteration 4131, loss = 0.48446586\n",
      "Iteration 4132, loss = 0.48374080\n",
      "Iteration 4133, loss = 0.48355243\n",
      "Iteration 4134, loss = 0.48320436\n",
      "Iteration 4135, loss = 0.48438642\n",
      "Iteration 4136, loss = 0.48450980\n",
      "Iteration 4137, loss = 0.48428458\n",
      "Iteration 4138, loss = 0.48387595\n",
      "Iteration 4139, loss = 0.48346733\n",
      "Iteration 4140, loss = 0.48319789\n",
      "Iteration 4141, loss = 0.48397557\n",
      "Iteration 4142, loss = 0.48541460\n",
      "Iteration 4143, loss = 0.48452100\n",
      "Iteration 4144, loss = 0.48359406\n",
      "Iteration 4145, loss = 0.48358072\n",
      "Iteration 4146, loss = 0.48446767\n",
      "Iteration 4147, loss = 0.48595482\n",
      "Iteration 4148, loss = 0.48762796\n",
      "Iteration 4149, loss = 0.48775581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4150, loss = 0.48634433\n",
      "Iteration 4151, loss = 0.48406581\n",
      "Iteration 4152, loss = 0.48373369\n",
      "Iteration 4153, loss = 0.48388107\n",
      "Iteration 4154, loss = 0.48357086\n",
      "Iteration 4155, loss = 0.48356889\n",
      "Iteration 4156, loss = 0.48368397\n",
      "Iteration 4157, loss = 0.48396286\n",
      "Iteration 4158, loss = 0.48428287\n",
      "Iteration 4159, loss = 0.48397163\n",
      "Iteration 4160, loss = 0.48369376\n",
      "Iteration 4161, loss = 0.48377209\n",
      "Iteration 4162, loss = 0.48380642\n",
      "Iteration 4163, loss = 0.48391253\n",
      "Iteration 4164, loss = 0.48399111\n",
      "Iteration 4165, loss = 0.48379786\n",
      "Iteration 4166, loss = 0.48344399\n",
      "Iteration 4167, loss = 0.48389795\n",
      "Iteration 4168, loss = 0.48514009\n",
      "Iteration 4169, loss = 0.48487485\n",
      "Iteration 4170, loss = 0.48379523\n",
      "Iteration 4171, loss = 0.48351876\n",
      "Iteration 4172, loss = 0.48493143\n",
      "Iteration 4173, loss = 0.48514288\n",
      "Iteration 4174, loss = 0.48504712\n",
      "Iteration 4175, loss = 0.48517918\n",
      "Iteration 4176, loss = 0.48410693\n",
      "Iteration 4177, loss = 0.48298173\n",
      "Iteration 4178, loss = 0.48575769\n",
      "Iteration 4179, loss = 0.48870955\n",
      "Iteration 4180, loss = 0.48851741\n",
      "Iteration 4181, loss = 0.48633635\n",
      "Iteration 4182, loss = 0.48410805\n",
      "Iteration 4183, loss = 0.48418766\n",
      "Iteration 4184, loss = 0.48385019\n",
      "Iteration 4185, loss = 0.48394186\n",
      "Iteration 4186, loss = 0.48364919\n",
      "Iteration 4187, loss = 0.48399834\n",
      "Iteration 4188, loss = 0.48402016\n",
      "Iteration 4189, loss = 0.48363111\n",
      "Iteration 4190, loss = 0.48365459\n",
      "Iteration 4191, loss = 0.48377246\n",
      "Iteration 4192, loss = 0.48378923\n",
      "Iteration 4193, loss = 0.48368263\n",
      "Iteration 4194, loss = 0.48366631\n",
      "Iteration 4195, loss = 0.48350760\n",
      "Iteration 4196, loss = 0.48355268\n",
      "Iteration 4197, loss = 0.48367598\n",
      "Iteration 4198, loss = 0.48386961\n",
      "Iteration 4199, loss = 0.48448431\n",
      "Iteration 4200, loss = 0.48463692\n",
      "Iteration 4201, loss = 0.48399918\n",
      "Iteration 4202, loss = 0.48403347\n",
      "Iteration 4203, loss = 0.48433472\n",
      "Iteration 4204, loss = 0.48501485\n",
      "Iteration 4205, loss = 0.48561050\n",
      "Iteration 4206, loss = 0.48544647\n",
      "Iteration 4207, loss = 0.48481298\n",
      "Iteration 4208, loss = 0.48332619\n",
      "Iteration 4209, loss = 0.48376820\n",
      "Iteration 4210, loss = 0.48436373\n",
      "Iteration 4211, loss = 0.48548234\n",
      "Iteration 4212, loss = 0.48560459\n",
      "Iteration 4213, loss = 0.48461890\n",
      "Iteration 4214, loss = 0.48411139\n",
      "Iteration 4215, loss = 0.48322610\n",
      "Iteration 4216, loss = 0.48381777\n",
      "Iteration 4217, loss = 0.48439447\n",
      "Iteration 4218, loss = 0.48454440\n",
      "Iteration 4219, loss = 0.48435354\n",
      "Iteration 4220, loss = 0.48435915\n",
      "Iteration 4221, loss = 0.48413541\n",
      "Iteration 4222, loss = 0.48511368\n",
      "Iteration 4223, loss = 0.48523343\n",
      "Iteration 4224, loss = 0.48457708\n",
      "Iteration 4225, loss = 0.48394882\n",
      "Iteration 4226, loss = 0.48349906\n",
      "Iteration 4227, loss = 0.48362654\n",
      "Iteration 4228, loss = 0.48373904\n",
      "Iteration 4229, loss = 0.48351588\n",
      "Iteration 4230, loss = 0.48372952\n",
      "Iteration 4231, loss = 0.48344575\n",
      "Iteration 4232, loss = 0.48370196\n",
      "Iteration 4233, loss = 0.48359154\n",
      "Iteration 4234, loss = 0.48400205\n",
      "Iteration 4235, loss = 0.48449570\n",
      "Iteration 4236, loss = 0.48461269\n",
      "Iteration 4237, loss = 0.48440096\n",
      "Iteration 4238, loss = 0.48421222\n",
      "Iteration 4239, loss = 0.48396120\n",
      "Iteration 4240, loss = 0.48387038\n",
      "Iteration 4241, loss = 0.48391272\n",
      "Iteration 4242, loss = 0.48374687\n",
      "Iteration 4243, loss = 0.48366439\n",
      "Iteration 4244, loss = 0.48334966\n",
      "Iteration 4245, loss = 0.48367020\n",
      "Iteration 4246, loss = 0.48440094\n",
      "Iteration 4247, loss = 0.48400973\n",
      "Iteration 4248, loss = 0.48350207\n",
      "Iteration 4249, loss = 0.48342876\n",
      "Iteration 4250, loss = 0.48349659\n",
      "Iteration 4251, loss = 0.48383171\n",
      "Iteration 4252, loss = 0.48394412\n",
      "Iteration 4253, loss = 0.48382670\n",
      "Iteration 4254, loss = 0.48366540\n",
      "Iteration 4255, loss = 0.48332494\n",
      "Iteration 4256, loss = 0.48323480\n",
      "Iteration 4257, loss = 0.48344389\n",
      "Iteration 4258, loss = 0.48463347\n",
      "Iteration 4259, loss = 0.48602104\n",
      "Iteration 4260, loss = 0.48537364\n",
      "Iteration 4261, loss = 0.48376436\n",
      "Iteration 4262, loss = 0.48334593\n",
      "Iteration 4263, loss = 0.48339613\n",
      "Iteration 4264, loss = 0.48368375\n",
      "Iteration 4265, loss = 0.48455329\n",
      "Iteration 4266, loss = 0.48576908\n",
      "Iteration 4267, loss = 0.48665177\n",
      "Iteration 4268, loss = 0.48716124\n",
      "Iteration 4269, loss = 0.48704457\n",
      "Iteration 4270, loss = 0.48381090\n",
      "Iteration 4271, loss = 0.48333002\n",
      "Iteration 4272, loss = 0.48349233\n",
      "Iteration 4273, loss = 0.48436529\n",
      "Iteration 4274, loss = 0.48452871\n",
      "Iteration 4275, loss = 0.48422562\n",
      "Iteration 4276, loss = 0.48371199\n",
      "Iteration 4277, loss = 0.48354376\n",
      "Iteration 4278, loss = 0.48343792\n",
      "Iteration 4279, loss = 0.48330738\n",
      "Iteration 4280, loss = 0.48384940\n",
      "Iteration 4281, loss = 0.48436101\n",
      "Iteration 4282, loss = 0.48476365\n",
      "Iteration 4283, loss = 0.48415984\n",
      "Iteration 4284, loss = 0.48361768\n",
      "Iteration 4285, loss = 0.48325866\n",
      "Iteration 4286, loss = 0.48376816\n",
      "Iteration 4287, loss = 0.48469748\n",
      "Iteration 4288, loss = 0.48455551\n",
      "Iteration 4289, loss = 0.48397863\n",
      "Iteration 4290, loss = 0.48336215\n",
      "Iteration 4291, loss = 0.48355172\n",
      "Iteration 4292, loss = 0.48360838\n",
      "Iteration 4293, loss = 0.48369742\n",
      "Iteration 4294, loss = 0.48374171\n",
      "Iteration 4295, loss = 0.48334643\n",
      "Iteration 4296, loss = 0.48336498\n",
      "Iteration 4297, loss = 0.48337958\n",
      "Iteration 4298, loss = 0.48342880\n",
      "Iteration 4299, loss = 0.48337014\n",
      "Iteration 4300, loss = 0.48342670\n",
      "Iteration 4301, loss = 0.48357041\n",
      "Iteration 4302, loss = 0.48357018\n",
      "Iteration 4303, loss = 0.48346032\n",
      "Iteration 4304, loss = 0.48361705\n",
      "Iteration 4305, loss = 0.48337573\n",
      "Iteration 4306, loss = 0.48348070\n",
      "Iteration 4307, loss = 0.48326085\n",
      "Iteration 4308, loss = 0.48331469\n",
      "Iteration 4309, loss = 0.48382826\n",
      "Iteration 4310, loss = 0.48410716\n",
      "Iteration 4311, loss = 0.48370079\n",
      "Iteration 4312, loss = 0.48317998\n",
      "Iteration 4313, loss = 0.48413688\n",
      "Iteration 4314, loss = 0.48377550\n",
      "Iteration 4315, loss = 0.48363678\n",
      "Iteration 4316, loss = 0.48331315\n",
      "Iteration 4317, loss = 0.48382819\n",
      "Iteration 4318, loss = 0.48430684\n",
      "Iteration 4319, loss = 0.48459061\n",
      "Iteration 4320, loss = 0.48477151\n",
      "Iteration 4321, loss = 0.48544931\n",
      "Iteration 4322, loss = 0.48474929\n",
      "Iteration 4323, loss = 0.48364612\n",
      "Iteration 4324, loss = 0.48340240\n",
      "Iteration 4325, loss = 0.48378655\n",
      "Iteration 4326, loss = 0.48425791\n",
      "Iteration 4327, loss = 0.48444984\n",
      "Iteration 4328, loss = 0.48433754\n",
      "Iteration 4329, loss = 0.48416059\n",
      "Iteration 4330, loss = 0.48393219\n",
      "Iteration 4331, loss = 0.48363263\n",
      "Iteration 4332, loss = 0.48381824\n",
      "Iteration 4333, loss = 0.48436980\n",
      "Iteration 4334, loss = 0.48400329\n",
      "Iteration 4335, loss = 0.48325850\n",
      "Iteration 4336, loss = 0.48358981\n",
      "Iteration 4337, loss = 0.48373546\n",
      "Iteration 4338, loss = 0.48389514\n",
      "Iteration 4339, loss = 0.48377990\n",
      "Iteration 4340, loss = 0.48331635\n",
      "Iteration 4341, loss = 0.48393678\n",
      "Iteration 4342, loss = 0.48443243\n",
      "Iteration 4343, loss = 0.48457312\n",
      "Iteration 4344, loss = 0.48424204\n",
      "Iteration 4345, loss = 0.48409569\n",
      "Iteration 4346, loss = 0.48338429\n",
      "Iteration 4347, loss = 0.48415784\n",
      "Iteration 4348, loss = 0.48368392\n",
      "Iteration 4349, loss = 0.48341712\n",
      "Iteration 4350, loss = 0.48316426\n",
      "Iteration 4351, loss = 0.48349326\n",
      "Iteration 4352, loss = 0.48448861\n",
      "Iteration 4353, loss = 0.48456457\n",
      "Iteration 4354, loss = 0.48392730\n",
      "Iteration 4355, loss = 0.48365704\n",
      "Iteration 4356, loss = 0.48325690\n",
      "Iteration 4357, loss = 0.48362303\n",
      "Iteration 4358, loss = 0.48357043\n",
      "Iteration 4359, loss = 0.48341632\n",
      "Iteration 4360, loss = 0.48339839\n",
      "Iteration 4361, loss = 0.48336257\n",
      "Iteration 4362, loss = 0.48378458\n",
      "Iteration 4363, loss = 0.48345396\n",
      "Iteration 4364, loss = 0.48357705\n",
      "Iteration 4365, loss = 0.48342863\n",
      "Iteration 4366, loss = 0.48325903\n",
      "Iteration 4367, loss = 0.48372228\n",
      "Iteration 4368, loss = 0.48324061\n",
      "Iteration 4369, loss = 0.48343200\n",
      "Iteration 4370, loss = 0.48392982\n",
      "Iteration 4371, loss = 0.48503960\n",
      "Iteration 4372, loss = 0.48569430\n",
      "Iteration 4373, loss = 0.48448240\n",
      "Iteration 4374, loss = 0.48256787\n",
      "Iteration 4375, loss = 0.48421494\n",
      "Iteration 4376, loss = 0.48680281\n",
      "Iteration 4377, loss = 0.48879896\n",
      "Iteration 4378, loss = 0.48944091\n",
      "Iteration 4379, loss = 0.48906273\n",
      "Iteration 4380, loss = 0.48757203\n",
      "Iteration 4381, loss = 0.48634571\n",
      "Iteration 4382, loss = 0.48498615\n",
      "Iteration 4383, loss = 0.48388691\n",
      "Iteration 4384, loss = 0.48409120\n",
      "Iteration 4385, loss = 0.48424694\n",
      "Iteration 4386, loss = 0.48372280\n",
      "Iteration 4387, loss = 0.48362294\n",
      "Iteration 4388, loss = 0.48349391\n",
      "Iteration 4389, loss = 0.48331609\n",
      "Iteration 4390, loss = 0.48372626\n",
      "Iteration 4391, loss = 0.48401380\n",
      "Iteration 4392, loss = 0.48423495\n",
      "Iteration 4393, loss = 0.48452168\n",
      "Iteration 4394, loss = 0.48379305\n",
      "Iteration 4395, loss = 0.48308929\n",
      "Iteration 4396, loss = 0.48414857\n",
      "Iteration 4397, loss = 0.48464928\n",
      "Iteration 4398, loss = 0.48380290\n",
      "Iteration 4399, loss = 0.48396416\n",
      "Iteration 4400, loss = 0.48348434\n",
      "Iteration 4401, loss = 0.48339361\n",
      "Iteration 4402, loss = 0.48321164\n",
      "Iteration 4403, loss = 0.48340420\n",
      "Iteration 4404, loss = 0.48403009\n",
      "Iteration 4405, loss = 0.48387334\n",
      "Iteration 4406, loss = 0.48345733\n",
      "Iteration 4407, loss = 0.48326477\n",
      "Iteration 4408, loss = 0.48353604\n",
      "Iteration 4409, loss = 0.48341395\n",
      "Iteration 4410, loss = 0.48297554\n",
      "Iteration 4411, loss = 0.48354656\n",
      "Iteration 4412, loss = 0.48571118\n",
      "Iteration 4413, loss = 0.48577994\n",
      "Iteration 4414, loss = 0.48439582\n",
      "Iteration 4415, loss = 0.48284758\n",
      "Iteration 4416, loss = 0.48373763\n",
      "Iteration 4417, loss = 0.48614498\n",
      "Iteration 4418, loss = 0.48864916\n",
      "Iteration 4419, loss = 0.48751992\n",
      "Iteration 4420, loss = 0.48472558\n",
      "Iteration 4421, loss = 0.48439600\n",
      "Iteration 4422, loss = 0.48451014\n",
      "Iteration 4423, loss = 0.48474744\n",
      "Iteration 4424, loss = 0.48386510\n",
      "Iteration 4425, loss = 0.48350389\n",
      "Iteration 4426, loss = 0.48386689\n",
      "Iteration 4427, loss = 0.48359042\n",
      "Iteration 4428, loss = 0.48332773\n",
      "Iteration 4429, loss = 0.48337791\n",
      "Iteration 4430, loss = 0.48336907\n",
      "Iteration 4431, loss = 0.48329422\n",
      "Iteration 4432, loss = 0.48359951\n",
      "Iteration 4433, loss = 0.48332863\n",
      "Iteration 4434, loss = 0.48322373\n",
      "Iteration 4435, loss = 0.48402139\n",
      "Iteration 4436, loss = 0.48432399\n",
      "Iteration 4437, loss = 0.48387286\n",
      "Iteration 4438, loss = 0.48374701\n",
      "Iteration 4439, loss = 0.48338547\n",
      "Iteration 4440, loss = 0.48342950\n",
      "Iteration 4441, loss = 0.48329418\n",
      "Iteration 4442, loss = 0.48386542\n",
      "Iteration 4443, loss = 0.48373833\n",
      "Iteration 4444, loss = 0.48374429\n",
      "Iteration 4445, loss = 0.48352519\n",
      "Iteration 4446, loss = 0.48344780\n",
      "Iteration 4447, loss = 0.48334760\n",
      "Iteration 4448, loss = 0.48399100\n",
      "Iteration 4449, loss = 0.48393917\n",
      "Iteration 4450, loss = 0.48387418\n",
      "Iteration 4451, loss = 0.48343274\n",
      "Iteration 4452, loss = 0.48362608\n",
      "Iteration 4453, loss = 0.48340876\n",
      "Iteration 4454, loss = 0.48354448\n",
      "Iteration 4455, loss = 0.48377234\n",
      "Iteration 4456, loss = 0.48313560\n",
      "Iteration 4457, loss = 0.48347314\n",
      "Iteration 4458, loss = 0.48434279\n",
      "Iteration 4459, loss = 0.48494873\n",
      "Iteration 4460, loss = 0.48481004\n",
      "Iteration 4461, loss = 0.48372085\n",
      "Iteration 4462, loss = 0.48425741\n",
      "Iteration 4463, loss = 0.48366101\n",
      "Iteration 4464, loss = 0.48367432\n",
      "Iteration 4465, loss = 0.48357096\n",
      "Iteration 4466, loss = 0.48329021\n",
      "Iteration 4467, loss = 0.48320072\n",
      "Iteration 4468, loss = 0.48351850\n",
      "Iteration 4469, loss = 0.48442930\n",
      "Iteration 4470, loss = 0.48542297\n",
      "Iteration 4471, loss = 0.48528380\n",
      "Iteration 4472, loss = 0.48396971\n",
      "Iteration 4473, loss = 0.48345097\n",
      "Iteration 4474, loss = 0.48331719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4475, loss = 0.48366197\n",
      "Iteration 4476, loss = 0.48395120\n",
      "Iteration 4477, loss = 0.48403911\n",
      "Iteration 4478, loss = 0.48407327\n",
      "Iteration 4479, loss = 0.48345976\n",
      "Iteration 4480, loss = 0.48300920\n",
      "Iteration 4481, loss = 0.48478428\n",
      "Iteration 4482, loss = 0.48493871\n",
      "Iteration 4483, loss = 0.48437546\n",
      "Iteration 4484, loss = 0.48395698\n",
      "Iteration 4485, loss = 0.48328635\n",
      "Iteration 4486, loss = 0.48331603\n",
      "Iteration 4487, loss = 0.48356400\n",
      "Iteration 4488, loss = 0.48369821\n",
      "Iteration 4489, loss = 0.48374075\n",
      "Iteration 4490, loss = 0.48376892\n",
      "Iteration 4491, loss = 0.48343896\n",
      "Iteration 4492, loss = 0.48337342\n",
      "Iteration 4493, loss = 0.48375409\n",
      "Iteration 4494, loss = 0.48396800\n",
      "Iteration 4495, loss = 0.48333990\n",
      "Iteration 4496, loss = 0.48354606\n",
      "Iteration 4497, loss = 0.48401697\n",
      "Iteration 4498, loss = 0.48455290\n",
      "Iteration 4499, loss = 0.48549806\n",
      "Iteration 4500, loss = 0.48632277\n",
      "Iteration 4501, loss = 0.48676633\n",
      "Iteration 4502, loss = 0.48465162\n",
      "Iteration 4503, loss = 0.48479662\n",
      "Iteration 4504, loss = 0.48458626\n",
      "Iteration 4505, loss = 0.48434290\n",
      "Iteration 4506, loss = 0.48289676\n",
      "Iteration 4507, loss = 0.48353606\n",
      "Iteration 4508, loss = 0.48594057\n",
      "Iteration 4509, loss = 0.48803545\n",
      "Iteration 4510, loss = 0.48774719\n",
      "Iteration 4511, loss = 0.48587225\n",
      "Iteration 4512, loss = 0.48411154\n",
      "Iteration 4513, loss = 0.48383157\n",
      "Iteration 4514, loss = 0.48343747\n",
      "Iteration 4515, loss = 0.48332344\n",
      "Iteration 4516, loss = 0.48297949\n",
      "Iteration 4517, loss = 0.48315788\n",
      "Iteration 4518, loss = 0.48656613\n",
      "Iteration 4519, loss = 0.48831164\n",
      "Iteration 4520, loss = 0.48772847\n",
      "Iteration 4521, loss = 0.48530301\n",
      "Iteration 4522, loss = 0.48345569\n",
      "Iteration 4523, loss = 0.48297968\n",
      "Iteration 4524, loss = 0.48664331\n",
      "Iteration 4525, loss = 0.48787141\n",
      "Iteration 4526, loss = 0.48646696\n",
      "Iteration 4527, loss = 0.48443678\n",
      "Iteration 4528, loss = 0.48327882\n",
      "Iteration 4529, loss = 0.48346699\n",
      "Iteration 4530, loss = 0.48367819\n",
      "Iteration 4531, loss = 0.48352739\n",
      "Iteration 4532, loss = 0.48363810\n",
      "Iteration 4533, loss = 0.48365674\n",
      "Iteration 4534, loss = 0.48367148\n",
      "Iteration 4535, loss = 0.48327352\n",
      "Iteration 4536, loss = 0.48328670\n",
      "Iteration 4537, loss = 0.48326362\n",
      "Iteration 4538, loss = 0.48314002\n",
      "Iteration 4539, loss = 0.48410259\n",
      "Iteration 4540, loss = 0.48381139\n",
      "Iteration 4541, loss = 0.48347821\n",
      "Iteration 4542, loss = 0.48327748\n",
      "Iteration 4543, loss = 0.48317065\n",
      "Iteration 4544, loss = 0.48344137\n",
      "Iteration 4545, loss = 0.48486603\n",
      "Iteration 4546, loss = 0.48718185\n",
      "Iteration 4547, loss = 0.48926150\n",
      "Iteration 4548, loss = 0.48653386\n",
      "Iteration 4549, loss = 0.48424504\n",
      "Iteration 4550, loss = 0.48468915\n",
      "Iteration 4551, loss = 0.48555515\n",
      "Iteration 4552, loss = 0.48520744\n",
      "Iteration 4553, loss = 0.48475085\n",
      "Iteration 4554, loss = 0.48331163\n",
      "Iteration 4555, loss = 0.48336548\n",
      "Iteration 4556, loss = 0.48332429\n",
      "Iteration 4557, loss = 0.48330627\n",
      "Iteration 4558, loss = 0.48366844\n",
      "Iteration 4559, loss = 0.48467494\n",
      "Iteration 4560, loss = 0.48643745\n",
      "Iteration 4561, loss = 0.48599863\n",
      "Iteration 4562, loss = 0.48412837\n",
      "Iteration 4563, loss = 0.48382673\n",
      "Iteration 4564, loss = 0.48331890\n",
      "Iteration 4565, loss = 0.48331721\n",
      "Iteration 4566, loss = 0.48327696\n",
      "Iteration 4567, loss = 0.48341734\n",
      "Iteration 4568, loss = 0.48367452\n",
      "Iteration 4569, loss = 0.48396839\n",
      "Iteration 4570, loss = 0.48423240\n",
      "Iteration 4571, loss = 0.48444793\n",
      "Iteration 4572, loss = 0.48439843\n",
      "Iteration 4573, loss = 0.48425987\n",
      "Iteration 4574, loss = 0.48433218\n",
      "Iteration 4575, loss = 0.48399330\n",
      "Iteration 4576, loss = 0.48390537\n",
      "Iteration 4577, loss = 0.48326532\n",
      "Iteration 4578, loss = 0.48317792\n",
      "Iteration 4579, loss = 0.48328306\n",
      "Iteration 4580, loss = 0.48375793\n",
      "Iteration 4581, loss = 0.48371146\n",
      "Iteration 4582, loss = 0.48369388\n",
      "Iteration 4583, loss = 0.48332963\n",
      "Iteration 4584, loss = 0.48372673\n",
      "Iteration 4585, loss = 0.48307426\n",
      "Iteration 4586, loss = 0.48376728\n",
      "Iteration 4587, loss = 0.48471792\n",
      "Iteration 4588, loss = 0.48397443\n",
      "Iteration 4589, loss = 0.48321149\n",
      "Iteration 4590, loss = 0.48329661\n",
      "Iteration 4591, loss = 0.48401020\n",
      "Iteration 4592, loss = 0.48554506\n",
      "Iteration 4593, loss = 0.48580843\n",
      "Iteration 4594, loss = 0.48435369\n",
      "Iteration 4595, loss = 0.48376045\n",
      "Iteration 4596, loss = 0.48322732\n",
      "Iteration 4597, loss = 0.48344153\n",
      "Iteration 4598, loss = 0.48437028\n",
      "Iteration 4599, loss = 0.48438432\n",
      "Iteration 4600, loss = 0.48386955\n",
      "Iteration 4601, loss = 0.48362659\n",
      "Iteration 4602, loss = 0.48339570\n",
      "Iteration 4603, loss = 0.48353337\n",
      "Iteration 4604, loss = 0.48380767\n",
      "Iteration 4605, loss = 0.48350092\n",
      "Iteration 4606, loss = 0.48319752\n",
      "Iteration 4607, loss = 0.48373624\n",
      "Iteration 4608, loss = 0.48420254\n",
      "Iteration 4609, loss = 0.48405211\n",
      "Iteration 4610, loss = 0.48349172\n",
      "Iteration 4611, loss = 0.48320817\n",
      "Iteration 4612, loss = 0.48354825\n",
      "Iteration 4613, loss = 0.48389753\n",
      "Iteration 4614, loss = 0.48391178\n",
      "Iteration 4615, loss = 0.48359211\n",
      "Iteration 4616, loss = 0.48353134\n",
      "Iteration 4617, loss = 0.48364008\n",
      "Iteration 4618, loss = 0.48365086\n",
      "Iteration 4619, loss = 0.48331509\n",
      "Iteration 4620, loss = 0.48290580\n",
      "Iteration 4621, loss = 0.48465967\n",
      "Iteration 4622, loss = 0.48485818\n",
      "Iteration 4623, loss = 0.48492176\n",
      "Iteration 4624, loss = 0.48534811\n",
      "Iteration 4625, loss = 0.48642577\n",
      "Iteration 4626, loss = 0.48889264\n",
      "Iteration 4627, loss = 0.48871620\n",
      "Iteration 4628, loss = 0.48579858\n",
      "Iteration 4629, loss = 0.48371132\n",
      "Iteration 4630, loss = 0.48376037\n",
      "Iteration 4631, loss = 0.48483705\n",
      "Iteration 4632, loss = 0.48659363\n",
      "Iteration 4633, loss = 0.48896726\n",
      "Iteration 4634, loss = 0.48840027\n",
      "Iteration 4635, loss = 0.48480567\n",
      "Iteration 4636, loss = 0.48277446\n",
      "Iteration 4637, loss = 0.48536574\n",
      "Iteration 4638, loss = 0.48925998\n",
      "Iteration 4639, loss = 0.48851243\n",
      "Iteration 4640, loss = 0.48573139\n",
      "Iteration 4641, loss = 0.48377152\n",
      "Iteration 4642, loss = 0.48370846\n",
      "Iteration 4643, loss = 0.48419950\n",
      "Iteration 4644, loss = 0.48495053\n",
      "Iteration 4645, loss = 0.48554416\n",
      "Iteration 4646, loss = 0.48548937\n",
      "Iteration 4647, loss = 0.48496266\n",
      "Iteration 4648, loss = 0.48435863\n",
      "Iteration 4649, loss = 0.48377664\n",
      "Iteration 4650, loss = 0.48362967\n",
      "Iteration 4651, loss = 0.48340081\n",
      "Iteration 4652, loss = 0.48362648\n",
      "Iteration 4653, loss = 0.48336974\n",
      "Iteration 4654, loss = 0.48309457\n",
      "Iteration 4655, loss = 0.48458684\n",
      "Iteration 4656, loss = 0.48676239\n",
      "Iteration 4657, loss = 0.48735283\n",
      "Iteration 4658, loss = 0.48602245\n",
      "Iteration 4659, loss = 0.48445377\n",
      "Iteration 4660, loss = 0.48382077\n",
      "Iteration 4661, loss = 0.48351118\n",
      "Iteration 4662, loss = 0.48412169\n",
      "Iteration 4663, loss = 0.48607649\n",
      "Iteration 4664, loss = 0.48574436\n",
      "Iteration 4665, loss = 0.48363035\n",
      "Iteration 4666, loss = 0.48447687\n",
      "Iteration 4667, loss = 0.48469698\n",
      "Iteration 4668, loss = 0.48541247\n",
      "Iteration 4669, loss = 0.48500560\n",
      "Iteration 4670, loss = 0.48436882\n",
      "Iteration 4671, loss = 0.48355513\n",
      "Iteration 4672, loss = 0.48356626\n",
      "Iteration 4673, loss = 0.48347411\n",
      "Iteration 4674, loss = 0.48341130\n",
      "Iteration 4675, loss = 0.48334073\n",
      "Iteration 4676, loss = 0.48346066\n",
      "Iteration 4677, loss = 0.48336286\n",
      "Iteration 4678, loss = 0.48410970\n",
      "Iteration 4679, loss = 0.48440215\n",
      "Iteration 4680, loss = 0.48433260\n",
      "Iteration 4681, loss = 0.48374445\n",
      "Iteration 4682, loss = 0.48372419\n",
      "Iteration 4683, loss = 0.48355823\n",
      "Iteration 4684, loss = 0.48323552\n",
      "Iteration 4685, loss = 0.48382966\n",
      "Iteration 4686, loss = 0.48370407\n",
      "Iteration 4687, loss = 0.48353748\n",
      "Iteration 4688, loss = 0.48268375\n",
      "Iteration 4689, loss = 0.48468926\n",
      "Iteration 4690, loss = 0.48682375\n",
      "Iteration 4691, loss = 0.48729419\n",
      "Iteration 4692, loss = 0.48583113\n",
      "Iteration 4693, loss = 0.48473592\n",
      "Iteration 4694, loss = 0.48357870\n",
      "Iteration 4695, loss = 0.48401055\n",
      "Iteration 4696, loss = 0.48531915\n",
      "Iteration 4697, loss = 0.48582333\n",
      "Iteration 4698, loss = 0.48419610\n",
      "Iteration 4699, loss = 0.48345010\n",
      "Iteration 4700, loss = 0.48369864\n",
      "Iteration 4701, loss = 0.48544341\n",
      "Iteration 4702, loss = 0.48615465\n",
      "Iteration 4703, loss = 0.48521901\n",
      "Iteration 4704, loss = 0.48372536\n",
      "Iteration 4705, loss = 0.48334523\n",
      "Iteration 4706, loss = 0.48447188\n",
      "Iteration 4707, loss = 0.48598354\n",
      "Iteration 4708, loss = 0.48695289\n",
      "Iteration 4709, loss = 0.48601923\n",
      "Iteration 4710, loss = 0.48352111\n",
      "Iteration 4711, loss = 0.48219297\n",
      "Iteration 4712, loss = 0.48654494\n",
      "Iteration 4713, loss = 0.48889075\n",
      "Iteration 4714, loss = 0.48906808\n",
      "Iteration 4715, loss = 0.48689782\n",
      "Iteration 4716, loss = 0.48521433\n",
      "Iteration 4717, loss = 0.48355864\n",
      "Iteration 4718, loss = 0.48369612\n",
      "Iteration 4719, loss = 0.48402402\n",
      "Iteration 4720, loss = 0.48482885\n",
      "Iteration 4721, loss = 0.48611324\n",
      "Iteration 4722, loss = 0.48626271\n",
      "Iteration 4723, loss = 0.48482735\n",
      "Iteration 4724, loss = 0.48384526\n",
      "Iteration 4725, loss = 0.48339622\n",
      "Iteration 4726, loss = 0.48461995\n",
      "Iteration 4727, loss = 0.48467020\n",
      "Iteration 4728, loss = 0.48404732\n",
      "Iteration 4729, loss = 0.48332779\n",
      "Iteration 4730, loss = 0.48391165\n",
      "Iteration 4731, loss = 0.48392994\n",
      "Iteration 4732, loss = 0.48372305\n",
      "Iteration 4733, loss = 0.48323889\n",
      "Iteration 4734, loss = 0.48313793\n",
      "Iteration 4735, loss = 0.48435373\n",
      "Iteration 4736, loss = 0.48536659\n",
      "Iteration 4737, loss = 0.48503361\n",
      "Iteration 4738, loss = 0.48421444\n",
      "Iteration 4739, loss = 0.48358209\n",
      "Iteration 4740, loss = 0.48369178\n",
      "Iteration 4741, loss = 0.48421629\n",
      "Iteration 4742, loss = 0.48462739\n",
      "Iteration 4743, loss = 0.48413237\n",
      "Iteration 4744, loss = 0.48342606\n",
      "Iteration 4745, loss = 0.48313149\n",
      "Iteration 4746, loss = 0.48401738\n",
      "Iteration 4747, loss = 0.48448719\n",
      "Iteration 4748, loss = 0.48364919\n",
      "Iteration 4749, loss = 0.48350701\n",
      "Iteration 4750, loss = 0.48373219\n",
      "Iteration 4751, loss = 0.48487942\n",
      "Iteration 4752, loss = 0.48462015\n",
      "Iteration 4753, loss = 0.48395056\n",
      "Iteration 4754, loss = 0.48351982\n",
      "Iteration 4755, loss = 0.48336311\n",
      "Iteration 4756, loss = 0.48335905\n",
      "Iteration 4757, loss = 0.48341179\n",
      "Iteration 4758, loss = 0.48362011\n",
      "Iteration 4759, loss = 0.48352237\n",
      "Iteration 4760, loss = 0.48331827\n",
      "Iteration 4761, loss = 0.48345603\n",
      "Iteration 4762, loss = 0.48387590\n",
      "Iteration 4763, loss = 0.48387288\n",
      "Iteration 4764, loss = 0.48341911\n",
      "Iteration 4765, loss = 0.48341161\n",
      "Iteration 4766, loss = 0.48332825\n",
      "Iteration 4767, loss = 0.48336698\n",
      "Iteration 4768, loss = 0.48339650\n",
      "Iteration 4769, loss = 0.48348472\n",
      "Iteration 4770, loss = 0.48357138\n",
      "Iteration 4771, loss = 0.48388258\n",
      "Iteration 4772, loss = 0.48384333\n",
      "Iteration 4773, loss = 0.48352208\n",
      "Iteration 4774, loss = 0.48338946\n",
      "Iteration 4775, loss = 0.48340093\n",
      "Iteration 4776, loss = 0.48342752\n",
      "Iteration 4777, loss = 0.48342121\n",
      "Iteration 4778, loss = 0.48332373\n",
      "Iteration 4779, loss = 0.48327542\n",
      "Iteration 4780, loss = 0.48357190\n",
      "Iteration 4781, loss = 0.48345397\n",
      "Iteration 4782, loss = 0.48356087\n",
      "Iteration 4783, loss = 0.48385581\n",
      "Iteration 4784, loss = 0.48618040\n",
      "Iteration 4785, loss = 0.48882557\n",
      "Iteration 4786, loss = 0.48799130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4787, loss = 0.48536221\n",
      "Iteration 4788, loss = 0.48424302\n",
      "Iteration 4789, loss = 0.48357431\n",
      "Iteration 4790, loss = 0.48358064\n",
      "Iteration 4791, loss = 0.48347145\n",
      "Iteration 4792, loss = 0.48334509\n",
      "Iteration 4793, loss = 0.48351366\n",
      "Iteration 4794, loss = 0.48339202\n",
      "Iteration 4795, loss = 0.48324052\n",
      "Iteration 4796, loss = 0.48333035\n",
      "Iteration 4797, loss = 0.48388789\n",
      "Iteration 4798, loss = 0.48440795\n",
      "Iteration 4799, loss = 0.48397215\n",
      "Iteration 4800, loss = 0.48357828\n",
      "Iteration 4801, loss = 0.48330780\n",
      "Iteration 4802, loss = 0.48364781\n",
      "Iteration 4803, loss = 0.48325926\n",
      "Iteration 4804, loss = 0.48355815\n",
      "Iteration 4805, loss = 0.48364818\n",
      "Iteration 4806, loss = 0.48410077\n",
      "Iteration 4807, loss = 0.48430955\n",
      "Iteration 4808, loss = 0.48391393\n",
      "Iteration 4809, loss = 0.48404861\n",
      "Iteration 4810, loss = 0.48385833\n",
      "Iteration 4811, loss = 0.48344950\n",
      "Iteration 4812, loss = 0.48348659\n",
      "Iteration 4813, loss = 0.48339400\n",
      "Iteration 4814, loss = 0.48328792\n",
      "Iteration 4815, loss = 0.48357317\n",
      "Iteration 4816, loss = 0.48342583\n",
      "Iteration 4817, loss = 0.48395417\n",
      "Iteration 4818, loss = 0.48321549\n",
      "Iteration 4819, loss = 0.48368383\n",
      "Iteration 4820, loss = 0.48379557\n",
      "Iteration 4821, loss = 0.48375635\n",
      "Iteration 4822, loss = 0.48340820\n",
      "Iteration 4823, loss = 0.48254179\n",
      "Iteration 4824, loss = 0.48443937\n",
      "Iteration 4825, loss = 0.48980098\n",
      "Iteration 4826, loss = 0.49133079\n",
      "Iteration 4827, loss = 0.48728905\n",
      "Iteration 4828, loss = 0.48328766\n",
      "Iteration 4829, loss = 0.48598994\n",
      "Iteration 4830, loss = 0.48640690\n",
      "Iteration 4831, loss = 0.48637621\n",
      "Iteration 4832, loss = 0.48620401\n",
      "Iteration 4833, loss = 0.48463444\n",
      "Iteration 4834, loss = 0.48450038\n",
      "Iteration 4835, loss = 0.48408959\n",
      "Iteration 4836, loss = 0.48340797\n",
      "Iteration 4837, loss = 0.48330347\n",
      "Iteration 4838, loss = 0.48349169\n",
      "Iteration 4839, loss = 0.48373049\n",
      "Iteration 4840, loss = 0.48368981\n",
      "Iteration 4841, loss = 0.48345302\n",
      "Iteration 4842, loss = 0.48327225\n",
      "Iteration 4843, loss = 0.48323073\n",
      "Iteration 4844, loss = 0.48361005\n",
      "Iteration 4845, loss = 0.48430520\n",
      "Iteration 4846, loss = 0.48411714\n",
      "Iteration 4847, loss = 0.48386055\n",
      "Iteration 4848, loss = 0.48344335\n",
      "Iteration 4849, loss = 0.48350590\n",
      "Iteration 4850, loss = 0.48388395\n",
      "Iteration 4851, loss = 0.48328690\n",
      "Iteration 4852, loss = 0.48357993\n",
      "Iteration 4853, loss = 0.48349215\n",
      "Iteration 4854, loss = 0.48312805\n",
      "Iteration 4855, loss = 0.48387439\n",
      "Iteration 4856, loss = 0.48422659\n",
      "Iteration 4857, loss = 0.48412288\n",
      "Iteration 4858, loss = 0.48339949\n",
      "Iteration 4859, loss = 0.48331379\n",
      "Iteration 4860, loss = 0.48324514\n",
      "Iteration 4861, loss = 0.48344304\n",
      "Iteration 4862, loss = 0.48420858\n",
      "Iteration 4863, loss = 0.48422125\n",
      "Iteration 4864, loss = 0.48365398\n",
      "Iteration 4865, loss = 0.48352587\n",
      "Iteration 4866, loss = 0.48352034\n",
      "Iteration 4867, loss = 0.48333095\n",
      "Iteration 4868, loss = 0.48341406\n",
      "Iteration 4869, loss = 0.48310889\n",
      "Iteration 4870, loss = 0.48430255\n",
      "Iteration 4871, loss = 0.48418178\n",
      "Iteration 4872, loss = 0.48380210\n",
      "Iteration 4873, loss = 0.48335953\n",
      "Iteration 4874, loss = 0.48335899\n",
      "Iteration 4875, loss = 0.48359803\n",
      "Iteration 4876, loss = 0.48335863\n",
      "Iteration 4877, loss = 0.48350918\n",
      "Iteration 4878, loss = 0.48342135\n",
      "Iteration 4879, loss = 0.48338525\n",
      "Iteration 4880, loss = 0.48344724\n",
      "Iteration 4881, loss = 0.48342108\n",
      "Iteration 4882, loss = 0.48332871\n",
      "Iteration 4883, loss = 0.48329699\n",
      "Iteration 4884, loss = 0.48328303\n",
      "Iteration 4885, loss = 0.48328834\n",
      "Iteration 4886, loss = 0.48317482\n",
      "Iteration 4887, loss = 0.48357726\n",
      "Iteration 4888, loss = 0.48440118\n",
      "Iteration 4889, loss = 0.48441292\n",
      "Iteration 4890, loss = 0.48393010\n",
      "Iteration 4891, loss = 0.48322946\n",
      "Iteration 4892, loss = 0.48319550\n",
      "Iteration 4893, loss = 0.48415567\n",
      "Iteration 4894, loss = 0.48478854\n",
      "Iteration 4895, loss = 0.48499778\n",
      "Iteration 4896, loss = 0.48474035\n",
      "Iteration 4897, loss = 0.48376705\n",
      "Iteration 4898, loss = 0.48278503\n",
      "Iteration 4899, loss = 0.48381711\n",
      "Iteration 4900, loss = 0.48702104\n",
      "Iteration 4901, loss = 0.48744897\n",
      "Iteration 4902, loss = 0.48580438\n",
      "Iteration 4903, loss = 0.48452537\n",
      "Iteration 4904, loss = 0.48331957\n",
      "Iteration 4905, loss = 0.48327569\n",
      "Iteration 4906, loss = 0.48357962\n",
      "Iteration 4907, loss = 0.48391673\n",
      "Iteration 4908, loss = 0.48402676\n",
      "Iteration 4909, loss = 0.48381220\n",
      "Iteration 4910, loss = 0.48332162\n",
      "Iteration 4911, loss = 0.48367537\n",
      "Iteration 4912, loss = 0.48336586\n",
      "Iteration 4913, loss = 0.48332123\n",
      "Iteration 4914, loss = 0.48332957\n",
      "Iteration 4915, loss = 0.48332453\n",
      "Iteration 4916, loss = 0.48323650\n",
      "Iteration 4917, loss = 0.48354153\n",
      "Iteration 4918, loss = 0.48339416\n",
      "Iteration 4919, loss = 0.48333590\n",
      "Iteration 4920, loss = 0.48334454\n",
      "Iteration 4921, loss = 0.48326053\n",
      "Iteration 4922, loss = 0.48330862\n",
      "Iteration 4923, loss = 0.48371371\n",
      "Iteration 4924, loss = 0.48437599\n",
      "Iteration 4925, loss = 0.48486850\n",
      "Iteration 4926, loss = 0.48522429\n",
      "Iteration 4927, loss = 0.48506488\n",
      "Iteration 4928, loss = 0.48417186\n",
      "Iteration 4929, loss = 0.48335821\n",
      "Iteration 4930, loss = 0.48327279\n",
      "Iteration 4931, loss = 0.48379607\n",
      "Iteration 4932, loss = 0.48422663\n",
      "Iteration 4933, loss = 0.48469562\n",
      "Iteration 4934, loss = 0.48461300\n",
      "Iteration 4935, loss = 0.48388697\n",
      "Iteration 4936, loss = 0.48355679\n",
      "Iteration 4937, loss = 0.48317380\n",
      "Iteration 4938, loss = 0.48315526\n",
      "Iteration 4939, loss = 0.48509753\n",
      "Iteration 4940, loss = 0.48535363\n",
      "Iteration 4941, loss = 0.48469326\n",
      "Iteration 4942, loss = 0.48351388\n",
      "Iteration 4943, loss = 0.48366234\n",
      "Iteration 4944, loss = 0.48477026\n",
      "Iteration 4945, loss = 0.48696799\n",
      "Iteration 4946, loss = 0.48899121\n",
      "Iteration 4947, loss = 0.48958717\n",
      "Iteration 4948, loss = 0.48791387\n",
      "Iteration 4949, loss = 0.48547170\n",
      "Iteration 4950, loss = 0.48357229\n",
      "Iteration 4951, loss = 0.48318037\n",
      "Iteration 4952, loss = 0.48551994\n",
      "Iteration 4953, loss = 0.48920448\n",
      "Iteration 4954, loss = 0.48694488\n",
      "Iteration 4955, loss = 0.48463573\n",
      "Iteration 4956, loss = 0.48316954\n",
      "Iteration 4957, loss = 0.48559771\n",
      "Iteration 4958, loss = 0.48820956\n",
      "Iteration 4959, loss = 0.48864593\n",
      "Iteration 4960, loss = 0.48723435\n",
      "Iteration 4961, loss = 0.48510450\n",
      "Iteration 4962, loss = 0.48328593\n",
      "Iteration 4963, loss = 0.48503497\n",
      "Iteration 4964, loss = 0.48446494\n",
      "Iteration 4965, loss = 0.48449740\n",
      "Iteration 4966, loss = 0.48410303\n",
      "Iteration 4967, loss = 0.48395855\n",
      "Iteration 4968, loss = 0.48328748\n",
      "Iteration 4969, loss = 0.48337310\n",
      "Iteration 4970, loss = 0.48330130\n",
      "Iteration 4971, loss = 0.48299979\n",
      "Iteration 4972, loss = 0.48488439\n",
      "Iteration 4973, loss = 0.48461392\n",
      "Iteration 4974, loss = 0.48366853\n",
      "Iteration 4975, loss = 0.48444818\n",
      "Iteration 4976, loss = 0.48326170\n",
      "Iteration 4977, loss = 0.48305880\n",
      "Iteration 4978, loss = 0.48451492\n",
      "Iteration 4979, loss = 0.48560263\n",
      "Iteration 4980, loss = 0.48594664\n",
      "Iteration 4981, loss = 0.48576684\n",
      "Iteration 4982, loss = 0.48484693\n",
      "Iteration 4983, loss = 0.48330044\n",
      "Iteration 4984, loss = 0.48358322\n",
      "Iteration 4985, loss = 0.48434737\n",
      "Iteration 4986, loss = 0.48583221\n",
      "Iteration 4987, loss = 0.48623512\n",
      "Iteration 4988, loss = 0.48520592\n",
      "Iteration 4989, loss = 0.48430782\n",
      "Iteration 4990, loss = 0.48317912\n",
      "Iteration 4991, loss = 0.48339220\n",
      "Iteration 4992, loss = 0.48458600\n",
      "Iteration 4993, loss = 0.48564724\n",
      "Iteration 4994, loss = 0.48549533\n",
      "Iteration 4995, loss = 0.48488467\n",
      "Iteration 4996, loss = 0.48389020\n",
      "Iteration 4997, loss = 0.48362444\n",
      "Iteration 4998, loss = 0.48358981\n",
      "Iteration 4999, loss = 0.48358230\n",
      "Iteration 5000, loss = 0.48342638\n",
      "Iteration 5001, loss = 0.48331891\n",
      "Iteration 5002, loss = 0.48328241\n",
      "Iteration 5003, loss = 0.48366357\n",
      "Iteration 5004, loss = 0.48433988\n",
      "Iteration 5005, loss = 0.48505137\n",
      "Iteration 5006, loss = 0.48617217\n",
      "Iteration 5007, loss = 0.48677875\n",
      "Iteration 5008, loss = 0.48605210\n",
      "Iteration 5009, loss = 0.48578665\n",
      "Iteration 5010, loss = 0.48511403\n",
      "Iteration 5011, loss = 0.48486804\n",
      "Iteration 5012, loss = 0.48436134\n",
      "Iteration 5013, loss = 0.48425362\n",
      "Iteration 5014, loss = 0.48353218\n",
      "Iteration 5015, loss = 0.48340014\n",
      "Iteration 5016, loss = 0.48323696\n",
      "Iteration 5017, loss = 0.48366447\n",
      "Iteration 5018, loss = 0.48352977\n",
      "Iteration 5019, loss = 0.48330469\n",
      "Iteration 5020, loss = 0.48337241\n",
      "Iteration 5021, loss = 0.48334449\n",
      "Iteration 5022, loss = 0.48326663\n",
      "Iteration 5023, loss = 0.48321388\n",
      "Iteration 5024, loss = 0.48321984\n",
      "Iteration 5025, loss = 0.48325333\n",
      "Iteration 5026, loss = 0.48356220\n",
      "Iteration 5027, loss = 0.48362397\n",
      "Iteration 5028, loss = 0.48362025\n",
      "Iteration 5029, loss = 0.48338984\n",
      "Iteration 5030, loss = 0.48342877\n",
      "Iteration 5031, loss = 0.48341054\n",
      "Iteration 5032, loss = 0.48341447\n",
      "Iteration 5033, loss = 0.48340457\n",
      "Iteration 5034, loss = 0.48305844\n",
      "Iteration 5035, loss = 0.48330254\n",
      "Iteration 5036, loss = 0.48397852\n",
      "Iteration 5037, loss = 0.48578431\n",
      "Iteration 5038, loss = 0.48741261\n",
      "Iteration 5039, loss = 0.48505858\n",
      "Iteration 5040, loss = 0.48394241\n",
      "Iteration 5041, loss = 0.48366050\n",
      "Iteration 5042, loss = 0.48502376\n",
      "Iteration 5043, loss = 0.48586063\n",
      "Iteration 5044, loss = 0.48532421\n",
      "Iteration 5045, loss = 0.48411894\n",
      "Iteration 5046, loss = 0.48375654\n",
      "Iteration 5047, loss = 0.48331205\n",
      "Iteration 5048, loss = 0.48359531\n",
      "Iteration 5049, loss = 0.48533657\n",
      "Iteration 5050, loss = 0.48613880\n",
      "Iteration 5051, loss = 0.48512631\n",
      "Iteration 5052, loss = 0.48432361\n",
      "Iteration 5053, loss = 0.48332601\n",
      "Iteration 5054, loss = 0.48325871\n",
      "Iteration 5055, loss = 0.48332163\n",
      "Iteration 5056, loss = 0.48365669\n",
      "Iteration 5057, loss = 0.48341506\n",
      "Iteration 5058, loss = 0.48339761\n",
      "Iteration 5059, loss = 0.48318964\n",
      "Iteration 5060, loss = 0.48393965\n",
      "Iteration 5061, loss = 0.48313373\n",
      "Iteration 5062, loss = 0.48338931\n",
      "Iteration 5063, loss = 0.48388963\n",
      "Iteration 5064, loss = 0.48417300\n",
      "Iteration 5065, loss = 0.48424672\n",
      "Iteration 5066, loss = 0.48304341\n",
      "Iteration 5067, loss = 0.48370247\n",
      "Iteration 5068, loss = 0.48534842\n",
      "Iteration 5069, loss = 0.48665731\n",
      "Iteration 5070, loss = 0.48625639\n",
      "Iteration 5071, loss = 0.48428722\n",
      "Iteration 5072, loss = 0.48343498\n",
      "Iteration 5073, loss = 0.48458014\n",
      "Iteration 5074, loss = 0.48499178\n",
      "Iteration 5075, loss = 0.48465132\n",
      "Iteration 5076, loss = 0.48434889\n",
      "Iteration 5077, loss = 0.48373819\n",
      "Iteration 5078, loss = 0.48421593\n",
      "Iteration 5079, loss = 0.48359473\n",
      "Iteration 5080, loss = 0.48347995\n",
      "Iteration 5081, loss = 0.48344037\n",
      "Iteration 5082, loss = 0.48369108\n",
      "Iteration 5083, loss = 0.48383128\n",
      "Iteration 5084, loss = 0.48386605\n",
      "Iteration 5085, loss = 0.48398507\n",
      "Iteration 5086, loss = 0.48381884\n",
      "Iteration 5087, loss = 0.48476881\n",
      "Iteration 5088, loss = 0.48548103\n",
      "Iteration 5089, loss = 0.48471424\n",
      "Iteration 5090, loss = 0.48335141\n",
      "Iteration 5091, loss = 0.48309655\n",
      "Iteration 5092, loss = 0.48533483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5093, loss = 0.48659666\n",
      "Iteration 5094, loss = 0.48621556\n",
      "Iteration 5095, loss = 0.48460871\n",
      "Iteration 5096, loss = 0.48338689\n",
      "Iteration 5097, loss = 0.48355317\n",
      "Iteration 5098, loss = 0.48407980\n",
      "Iteration 5099, loss = 0.48406933\n",
      "Iteration 5100, loss = 0.48376173\n",
      "Iteration 5101, loss = 0.48328984\n",
      "Iteration 5102, loss = 0.48319260\n",
      "Iteration 5103, loss = 0.48357477\n",
      "Iteration 5104, loss = 0.48371964\n",
      "Iteration 5105, loss = 0.48317668\n",
      "Iteration 5106, loss = 0.48349854\n",
      "Iteration 5107, loss = 0.48408119\n",
      "Iteration 5108, loss = 0.48397998\n",
      "Iteration 5109, loss = 0.48393445\n",
      "Iteration 5110, loss = 0.48327899\n",
      "Iteration 5111, loss = 0.48331710\n",
      "Iteration 5112, loss = 0.48332400\n",
      "Iteration 5113, loss = 0.48334445\n",
      "Iteration 5114, loss = 0.48375802\n",
      "Iteration 5115, loss = 0.48383662\n",
      "Iteration 5116, loss = 0.48383150\n",
      "Iteration 5117, loss = 0.48343735\n",
      "Iteration 5118, loss = 0.48360680\n",
      "Iteration 5119, loss = 0.48383787\n",
      "Iteration 5120, loss = 0.48445143\n",
      "Iteration 5121, loss = 0.48428872\n",
      "Iteration 5122, loss = 0.48362378\n",
      "Iteration 5123, loss = 0.48319959\n",
      "Iteration 5124, loss = 0.48341396\n",
      "Iteration 5125, loss = 0.48388547\n",
      "Iteration 5126, loss = 0.48419613\n",
      "Iteration 5127, loss = 0.48423505\n",
      "Iteration 5128, loss = 0.48400883\n",
      "Iteration 5129, loss = 0.48399791\n",
      "Iteration 5130, loss = 0.48403473\n",
      "Iteration 5131, loss = 0.48390697\n",
      "Iteration 5132, loss = 0.48381767\n",
      "Iteration 5133, loss = 0.48372530\n",
      "Iteration 5134, loss = 0.48368482\n",
      "Iteration 5135, loss = 0.48336496\n",
      "Iteration 5136, loss = 0.48299086\n",
      "Iteration 5137, loss = 0.48341208\n",
      "Iteration 5138, loss = 0.48497146\n",
      "Iteration 5139, loss = 0.48694347\n",
      "Iteration 5140, loss = 0.48708818\n",
      "Iteration 5141, loss = 0.48524118\n",
      "Iteration 5142, loss = 0.48333806\n",
      "Iteration 5143, loss = 0.48313908\n",
      "Iteration 5144, loss = 0.48414125\n",
      "Iteration 5145, loss = 0.48759677\n",
      "Iteration 5146, loss = 0.49063498\n",
      "Iteration 5147, loss = 0.48891307\n",
      "Iteration 5148, loss = 0.48507019\n",
      "Iteration 5149, loss = 0.48249467\n",
      "Iteration 5150, loss = 0.48347627\n",
      "Iteration 5151, loss = 0.49023050\n",
      "Iteration 5152, loss = 0.49411487\n",
      "Iteration 5153, loss = 0.49051856\n",
      "Iteration 5154, loss = 0.48439364\n",
      "Iteration 5155, loss = 0.48392307\n",
      "Iteration 5156, loss = 0.48536085\n",
      "Iteration 5157, loss = 0.48934443\n",
      "Iteration 5158, loss = 0.49011083\n",
      "Iteration 5159, loss = 0.48826106\n",
      "Iteration 5160, loss = 0.48613957\n",
      "Iteration 5161, loss = 0.48451485\n",
      "Iteration 5162, loss = 0.48307326\n",
      "Iteration 5163, loss = 0.48301495\n",
      "Iteration 5164, loss = 0.48503865\n",
      "Iteration 5165, loss = 0.48879304\n",
      "Iteration 5166, loss = 0.49080737\n",
      "Iteration 5167, loss = 0.48880831\n",
      "Iteration 5168, loss = 0.48424554\n",
      "Iteration 5169, loss = 0.48395214\n",
      "Iteration 5170, loss = 0.48426381\n",
      "Iteration 5171, loss = 0.48607400\n",
      "Iteration 5172, loss = 0.48757723\n",
      "Iteration 5173, loss = 0.48810336\n",
      "Iteration 5174, loss = 0.48663022\n",
      "Iteration 5175, loss = 0.48472260\n",
      "Iteration 5176, loss = 0.48450152\n",
      "Iteration 5177, loss = 0.48380334\n",
      "Iteration 5178, loss = 0.48405240\n",
      "Iteration 5179, loss = 0.48379671\n",
      "Iteration 5180, loss = 0.48335109\n",
      "Iteration 5181, loss = 0.48320233\n",
      "Iteration 5182, loss = 0.48355654\n",
      "Iteration 5183, loss = 0.48374177\n",
      "Iteration 5184, loss = 0.48365436\n",
      "Iteration 5185, loss = 0.48343080\n",
      "Iteration 5186, loss = 0.48319537\n",
      "Iteration 5187, loss = 0.48286007\n",
      "Iteration 5188, loss = 0.48543381\n",
      "Iteration 5189, loss = 0.48702196\n",
      "Iteration 5190, loss = 0.48740571\n",
      "Iteration 5191, loss = 0.48598002\n",
      "Iteration 5192, loss = 0.48429625\n",
      "Iteration 5193, loss = 0.48340510\n",
      "Iteration 5194, loss = 0.48346111\n",
      "Iteration 5195, loss = 0.48364519\n",
      "Iteration 5196, loss = 0.48374010\n",
      "Iteration 5197, loss = 0.48332164\n",
      "Iteration 5198, loss = 0.48332348\n",
      "Iteration 5199, loss = 0.48324228\n",
      "Iteration 5200, loss = 0.48345463\n",
      "Iteration 5201, loss = 0.48352336\n",
      "Iteration 5202, loss = 0.48365440\n",
      "Iteration 5203, loss = 0.48383258\n",
      "Iteration 5204, loss = 0.48364866\n",
      "Iteration 5205, loss = 0.48378903\n",
      "Iteration 5206, loss = 0.48326706\n",
      "Iteration 5207, loss = 0.48327473\n",
      "Iteration 5208, loss = 0.48335566\n",
      "Iteration 5209, loss = 0.48344535\n",
      "Iteration 5210, loss = 0.48324750\n",
      "Iteration 5211, loss = 0.48305164\n",
      "Iteration 5212, loss = 0.48376144\n",
      "Iteration 5213, loss = 0.48430272\n",
      "Iteration 5214, loss = 0.48425446\n",
      "Iteration 5215, loss = 0.48380959\n",
      "Iteration 5216, loss = 0.48353935\n",
      "Iteration 5217, loss = 0.48336173\n",
      "Iteration 5218, loss = 0.48307320\n",
      "Iteration 5219, loss = 0.48371231\n",
      "Iteration 5220, loss = 0.48423486\n",
      "Iteration 5221, loss = 0.48420858\n",
      "Iteration 5222, loss = 0.48346118\n",
      "Iteration 5223, loss = 0.48307021\n",
      "Iteration 5224, loss = 0.48293255\n",
      "Iteration 5225, loss = 0.48438386\n",
      "Iteration 5226, loss = 0.48602975\n",
      "Iteration 5227, loss = 0.48677538\n",
      "Iteration 5228, loss = 0.48584088\n",
      "Iteration 5229, loss = 0.48316139\n",
      "Iteration 5230, loss = 0.48412889\n",
      "Iteration 5231, loss = 0.48623722\n",
      "Iteration 5232, loss = 0.48863940\n",
      "Iteration 5233, loss = 0.48775886\n",
      "Iteration 5234, loss = 0.48619045\n",
      "Iteration 5235, loss = 0.48329574\n",
      "Iteration 5236, loss = 0.48325664\n",
      "Iteration 5237, loss = 0.48387202\n",
      "Iteration 5238, loss = 0.48404275\n",
      "Iteration 5239, loss = 0.48371149\n",
      "Iteration 5240, loss = 0.48308545\n",
      "Iteration 5241, loss = 0.48391123\n",
      "Iteration 5242, loss = 0.48395855\n",
      "Iteration 5243, loss = 0.48399043\n",
      "Iteration 5244, loss = 0.48366873\n",
      "Iteration 5245, loss = 0.48296246\n",
      "Iteration 5246, loss = 0.48318345\n",
      "Iteration 5247, loss = 0.48529871\n",
      "Iteration 5248, loss = 0.48715495\n",
      "Iteration 5249, loss = 0.48831066\n",
      "Iteration 5250, loss = 0.48674342\n",
      "Iteration 5251, loss = 0.48402014\n",
      "Iteration 5252, loss = 0.48391756\n",
      "Iteration 5253, loss = 0.48383554\n",
      "Iteration 5254, loss = 0.48499462\n",
      "Iteration 5255, loss = 0.48724749\n",
      "Iteration 5256, loss = 0.48764052\n",
      "Iteration 5257, loss = 0.48443342\n",
      "Iteration 5258, loss = 0.48253979\n",
      "Iteration 5259, loss = 0.48645754\n",
      "Iteration 5260, loss = 0.48838702\n",
      "Iteration 5261, loss = 0.48697012\n",
      "Iteration 5262, loss = 0.48406045\n",
      "Iteration 5263, loss = 0.48401739\n",
      "Iteration 5264, loss = 0.48438054\n",
      "Iteration 5265, loss = 0.48530653\n",
      "Iteration 5266, loss = 0.48580159\n",
      "Iteration 5267, loss = 0.48528949\n",
      "Iteration 5268, loss = 0.48372670\n",
      "Iteration 5269, loss = 0.48304344\n",
      "Iteration 5270, loss = 0.48401369\n",
      "Iteration 5271, loss = 0.48460581\n",
      "Iteration 5272, loss = 0.48446299\n",
      "Iteration 5273, loss = 0.48428490\n",
      "Iteration 5274, loss = 0.48341209\n",
      "Iteration 5275, loss = 0.48331447\n",
      "Iteration 5276, loss = 0.48328642\n",
      "Iteration 5277, loss = 0.48336738\n",
      "Iteration 5278, loss = 0.48359231\n",
      "Iteration 5279, loss = 0.48336313\n",
      "Iteration 5280, loss = 0.48321048\n",
      "Iteration 5281, loss = 0.48357911\n",
      "Iteration 5282, loss = 0.48345033\n",
      "Iteration 5283, loss = 0.48323291\n",
      "Iteration 5284, loss = 0.48330692\n",
      "Iteration 5285, loss = 0.48339761\n",
      "Iteration 5286, loss = 0.48355574\n",
      "Iteration 5287, loss = 0.48377339\n",
      "Iteration 5288, loss = 0.48387188\n",
      "Iteration 5289, loss = 0.48394977\n",
      "Iteration 5290, loss = 0.48369247\n",
      "Iteration 5291, loss = 0.48363813\n",
      "Iteration 5292, loss = 0.48375999\n",
      "Iteration 5293, loss = 0.48345477\n",
      "Iteration 5294, loss = 0.48334163\n",
      "Iteration 5295, loss = 0.48320625\n",
      "Iteration 5296, loss = 0.48326597\n",
      "Iteration 5297, loss = 0.48342676\n",
      "Iteration 5298, loss = 0.48340421\n",
      "Iteration 5299, loss = 0.48340363\n",
      "Iteration 5300, loss = 0.48326040\n",
      "Iteration 5301, loss = 0.48328682\n",
      "Iteration 5302, loss = 0.48377295\n",
      "Iteration 5303, loss = 0.48395998\n",
      "Iteration 5304, loss = 0.48349592\n",
      "Iteration 5305, loss = 0.48340273\n",
      "Iteration 5306, loss = 0.48324689\n",
      "Iteration 5307, loss = 0.48355364\n",
      "Iteration 5308, loss = 0.48473459\n",
      "Iteration 5309, loss = 0.48455967\n",
      "Iteration 5310, loss = 0.48443865\n",
      "Iteration 5311, loss = 0.48343914\n",
      "Iteration 5312, loss = 0.48392214\n",
      "Iteration 5313, loss = 0.48355229\n",
      "Iteration 5314, loss = 0.48290375\n",
      "Iteration 5315, loss = 0.48349914\n",
      "Iteration 5316, loss = 0.48515537\n",
      "Iteration 5317, loss = 0.48620282\n",
      "Iteration 5318, loss = 0.48584584\n",
      "Iteration 5319, loss = 0.48397532\n",
      "Iteration 5320, loss = 0.48391962\n",
      "Iteration 5321, loss = 0.48410088\n",
      "Iteration 5322, loss = 0.48493563\n",
      "Iteration 5323, loss = 0.48545627\n",
      "Iteration 5324, loss = 0.48565641\n",
      "Iteration 5325, loss = 0.48508808\n",
      "Iteration 5326, loss = 0.48453764\n",
      "Iteration 5327, loss = 0.48440086\n",
      "Iteration 5328, loss = 0.48344478\n",
      "Iteration 5329, loss = 0.48347662\n",
      "Iteration 5330, loss = 0.48343187\n",
      "Iteration 5331, loss = 0.48373360\n",
      "Iteration 5332, loss = 0.48412279\n",
      "Iteration 5333, loss = 0.48541132\n",
      "Iteration 5334, loss = 0.48543587\n",
      "Iteration 5335, loss = 0.48460962\n",
      "Iteration 5336, loss = 0.48368163\n",
      "Iteration 5337, loss = 0.48334145\n",
      "Iteration 5338, loss = 0.48326850\n",
      "Iteration 5339, loss = 0.48336276\n",
      "Iteration 5340, loss = 0.48345952\n",
      "Iteration 5341, loss = 0.48358645\n",
      "Iteration 5342, loss = 0.48401683\n",
      "Iteration 5343, loss = 0.48433489\n",
      "Iteration 5344, loss = 0.48446522\n",
      "Iteration 5345, loss = 0.48466078\n",
      "Iteration 5346, loss = 0.48513975\n",
      "Iteration 5347, loss = 0.48502915\n",
      "Iteration 5348, loss = 0.48421852\n",
      "Iteration 5349, loss = 0.48389009\n",
      "Iteration 5350, loss = 0.48314396\n",
      "Iteration 5351, loss = 0.48333198\n",
      "Iteration 5352, loss = 0.48412469\n",
      "Iteration 5353, loss = 0.48489375\n",
      "Iteration 5354, loss = 0.48474868\n",
      "Iteration 5355, loss = 0.48418849\n",
      "Iteration 5356, loss = 0.48356415\n",
      "Iteration 5357, loss = 0.48355691\n",
      "Iteration 5358, loss = 0.48359385\n",
      "Iteration 5359, loss = 0.48349883\n",
      "Iteration 5360, loss = 0.48380016\n",
      "Iteration 5361, loss = 0.48371963\n",
      "Iteration 5362, loss = 0.48334292\n",
      "Iteration 5363, loss = 0.48310390\n",
      "Iteration 5364, loss = 0.48363922\n",
      "Iteration 5365, loss = 0.48695908\n",
      "Iteration 5366, loss = 0.48676848\n",
      "Iteration 5367, loss = 0.48446570\n",
      "Iteration 5368, loss = 0.48366020\n",
      "Iteration 5369, loss = 0.48387444\n",
      "Iteration 5370, loss = 0.48436576\n",
      "Iteration 5371, loss = 0.48473701\n",
      "Iteration 5372, loss = 0.48493136\n",
      "Iteration 5373, loss = 0.48506210\n",
      "Iteration 5374, loss = 0.48330101\n",
      "Iteration 5375, loss = 0.48206412\n",
      "Iteration 5376, loss = 0.48661877\n",
      "Iteration 5377, loss = 0.49294443\n",
      "Iteration 5378, loss = 0.49428431\n",
      "Iteration 5379, loss = 0.48969705\n",
      "Iteration 5380, loss = 0.48525736\n",
      "Iteration 5381, loss = 0.48305292\n",
      "Iteration 5382, loss = 0.48536622\n",
      "Iteration 5383, loss = 0.48555643\n",
      "Iteration 5384, loss = 0.48491458\n",
      "Iteration 5385, loss = 0.48384960\n",
      "Iteration 5386, loss = 0.48376340\n",
      "Iteration 5387, loss = 0.48346573\n",
      "Iteration 5388, loss = 0.48327061\n",
      "Iteration 5389, loss = 0.48399390\n",
      "Iteration 5390, loss = 0.48360118\n",
      "Iteration 5391, loss = 0.48333916\n",
      "Iteration 5392, loss = 0.48314383\n",
      "Iteration 5393, loss = 0.48373236\n",
      "Iteration 5394, loss = 0.48498179\n",
      "Iteration 5395, loss = 0.48913894\n",
      "Iteration 5396, loss = 0.48931567\n",
      "Iteration 5397, loss = 0.48631691\n",
      "Iteration 5398, loss = 0.48465209\n",
      "Iteration 5399, loss = 0.48314321\n",
      "Iteration 5400, loss = 0.48378990\n",
      "Iteration 5401, loss = 0.48480189\n",
      "Iteration 5402, loss = 0.48663699\n",
      "Iteration 5403, loss = 0.48690656\n",
      "Iteration 5404, loss = 0.48567215\n",
      "Iteration 5405, loss = 0.48481658\n",
      "Iteration 5406, loss = 0.48379964\n",
      "Iteration 5407, loss = 0.48338842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5408, loss = 0.48317301\n",
      "Iteration 5409, loss = 0.48441815\n",
      "Iteration 5410, loss = 0.48421590\n",
      "Iteration 5411, loss = 0.48392814\n",
      "Iteration 5412, loss = 0.48366426\n",
      "Iteration 5413, loss = 0.48377515\n",
      "Iteration 5414, loss = 0.48336784\n",
      "Iteration 5415, loss = 0.48336020\n",
      "Iteration 5416, loss = 0.48334105\n",
      "Iteration 5417, loss = 0.48345759\n",
      "Iteration 5418, loss = 0.48333555\n",
      "Iteration 5419, loss = 0.48357750\n",
      "Iteration 5420, loss = 0.48511993\n",
      "Iteration 5421, loss = 0.48517106\n",
      "Iteration 5422, loss = 0.48417396\n",
      "Iteration 5423, loss = 0.48288609\n",
      "Iteration 5424, loss = 0.48332696\n",
      "Iteration 5425, loss = 0.48511668\n",
      "Iteration 5426, loss = 0.48807925\n",
      "Iteration 5427, loss = 0.48937652\n",
      "Iteration 5428, loss = 0.48814573\n",
      "Iteration 5429, loss = 0.48525339\n",
      "Iteration 5430, loss = 0.48352588\n",
      "Iteration 5431, loss = 0.48211967\n",
      "Iteration 5432, loss = 0.48657406\n",
      "Iteration 5433, loss = 0.48951842\n",
      "Iteration 5434, loss = 0.49160413\n",
      "Iteration 5435, loss = 0.49251447\n",
      "Iteration 5436, loss = 0.49036559\n",
      "Iteration 5437, loss = 0.48613374\n",
      "Iteration 5438, loss = 0.48381004\n",
      "Iteration 5439, loss = 0.48322437\n",
      "Iteration 5440, loss = 0.48381364\n",
      "Iteration 5441, loss = 0.48439273\n",
      "Iteration 5442, loss = 0.48510345\n",
      "Iteration 5443, loss = 0.48560075\n",
      "Iteration 5444, loss = 0.48537962\n",
      "Iteration 5445, loss = 0.48453844\n",
      "Iteration 5446, loss = 0.48442259\n",
      "Iteration 5447, loss = 0.48334168\n",
      "Iteration 5448, loss = 0.48316824\n",
      "Iteration 5449, loss = 0.48339147\n",
      "Iteration 5450, loss = 0.48381078\n",
      "Iteration 5451, loss = 0.48449136\n",
      "Iteration 5452, loss = 0.48554977\n",
      "Iteration 5453, loss = 0.48476629\n",
      "Iteration 5454, loss = 0.48356511\n",
      "Iteration 5455, loss = 0.48346620\n",
      "Iteration 5456, loss = 0.48384785\n",
      "Iteration 5457, loss = 0.48439114\n",
      "Iteration 5458, loss = 0.48516189\n",
      "Iteration 5459, loss = 0.48650451\n",
      "Iteration 5460, loss = 0.48734415\n",
      "Iteration 5461, loss = 0.48700110\n",
      "Iteration 5462, loss = 0.48549742\n",
      "Iteration 5463, loss = 0.48382106\n",
      "Iteration 5464, loss = 0.48324515\n",
      "Iteration 5465, loss = 0.48500221\n",
      "Iteration 5466, loss = 0.48661777\n",
      "Iteration 5467, loss = 0.48779209\n",
      "Iteration 5468, loss = 0.48971719\n",
      "Iteration 5469, loss = 0.48978106\n",
      "Iteration 5470, loss = 0.48805856\n",
      "Iteration 5471, loss = 0.48445847\n",
      "Iteration 5472, loss = 0.48352649\n",
      "Iteration 5473, loss = 0.48300375\n",
      "Iteration 5474, loss = 0.48612718\n",
      "Iteration 5475, loss = 0.49072503\n",
      "Iteration 5476, loss = 0.49234270\n",
      "Iteration 5477, loss = 0.49074442\n",
      "Iteration 5478, loss = 0.48796842\n",
      "Iteration 5479, loss = 0.48518633\n",
      "Iteration 5480, loss = 0.48350450\n",
      "Iteration 5481, loss = 0.48346119\n",
      "Iteration 5482, loss = 0.48373429\n",
      "Iteration 5483, loss = 0.48533061\n",
      "Iteration 5484, loss = 0.48784502\n",
      "Iteration 5485, loss = 0.48881228\n",
      "Iteration 5486, loss = 0.48727065\n",
      "Iteration 5487, loss = 0.48461193\n",
      "Iteration 5488, loss = 0.48354385\n",
      "Iteration 5489, loss = 0.48418998\n",
      "Iteration 5490, loss = 0.48531199\n",
      "Iteration 5491, loss = 0.48521205\n",
      "Iteration 5492, loss = 0.48417359\n",
      "Iteration 5493, loss = 0.48365045\n",
      "Iteration 5494, loss = 0.48320335\n",
      "Iteration 5495, loss = 0.48369485\n",
      "Iteration 5496, loss = 0.48478145\n",
      "Iteration 5497, loss = 0.48512856\n",
      "Iteration 5498, loss = 0.48514969\n",
      "Iteration 5499, loss = 0.48329841\n",
      "Iteration 5500, loss = 0.48336341\n",
      "Iteration 5501, loss = 0.48353936\n",
      "Iteration 5502, loss = 0.48350593\n",
      "Iteration 5503, loss = 0.48338358\n",
      "Iteration 5504, loss = 0.48339023\n",
      "Iteration 5505, loss = 0.48340190\n",
      "Iteration 5506, loss = 0.48338496\n",
      "Iteration 5507, loss = 0.48339034\n",
      "Iteration 5508, loss = 0.48336573\n",
      "Iteration 5509, loss = 0.48319810\n",
      "Iteration 5510, loss = 0.48409221\n",
      "Iteration 5511, loss = 0.48730753\n",
      "Iteration 5512, loss = 0.48796718\n",
      "Iteration 5513, loss = 0.48697684\n",
      "Iteration 5514, loss = 0.48527397\n",
      "Iteration 5515, loss = 0.48404344\n",
      "Iteration 5516, loss = 0.48322553\n",
      "Iteration 5517, loss = 0.48392322\n",
      "Iteration 5518, loss = 0.48428737\n",
      "Iteration 5519, loss = 0.48416475\n",
      "Iteration 5520, loss = 0.48369040\n",
      "Iteration 5521, loss = 0.48321765\n",
      "Iteration 5522, loss = 0.48352436\n",
      "Iteration 5523, loss = 0.48420240\n",
      "Iteration 5524, loss = 0.48442937\n",
      "Iteration 5525, loss = 0.48437053\n",
      "Iteration 5526, loss = 0.48395615\n",
      "Iteration 5527, loss = 0.48381810\n",
      "Iteration 5528, loss = 0.48373468\n",
      "Iteration 5529, loss = 0.48329857\n",
      "Iteration 5530, loss = 0.48325466\n",
      "Iteration 5531, loss = 0.48325078\n",
      "Iteration 5532, loss = 0.48327410\n",
      "Iteration 5533, loss = 0.48331253\n",
      "Iteration 5534, loss = 0.48344478\n",
      "Iteration 5535, loss = 0.48335523\n",
      "Iteration 5536, loss = 0.48343068\n",
      "Iteration 5537, loss = 0.48328264\n",
      "Iteration 5538, loss = 0.48348334\n",
      "Iteration 5539, loss = 0.48382803\n",
      "Iteration 5540, loss = 0.48334680\n",
      "Iteration 5541, loss = 0.48318706\n",
      "Iteration 5542, loss = 0.48321090\n",
      "Iteration 5543, loss = 0.48376724\n",
      "Iteration 5544, loss = 0.48395386\n",
      "Iteration 5545, loss = 0.48388545\n",
      "Iteration 5546, loss = 0.48360618\n",
      "Iteration 5547, loss = 0.48326559\n",
      "Iteration 5548, loss = 0.48321654\n",
      "Iteration 5549, loss = 0.48337228\n",
      "Iteration 5550, loss = 0.48365906\n",
      "Iteration 5551, loss = 0.48373174\n",
      "Iteration 5552, loss = 0.48350918\n",
      "Iteration 5553, loss = 0.48332608\n",
      "Iteration 5554, loss = 0.48323889\n",
      "Iteration 5555, loss = 0.48333480\n",
      "Iteration 5556, loss = 0.48321769\n",
      "Iteration 5557, loss = 0.48329311\n",
      "Iteration 5558, loss = 0.48313746\n",
      "Iteration 5559, loss = 0.48354877\n",
      "Iteration 5560, loss = 0.48353807\n",
      "Iteration 5561, loss = 0.48336261\n",
      "Iteration 5562, loss = 0.48339272\n",
      "Iteration 5563, loss = 0.48329846\n",
      "Iteration 5564, loss = 0.48312799\n",
      "Iteration 5565, loss = 0.48294944\n",
      "Iteration 5566, loss = 0.48345872\n",
      "Iteration 5567, loss = 0.48483809\n",
      "Iteration 5568, loss = 0.48576562\n",
      "Iteration 5569, loss = 0.48548063\n",
      "Iteration 5570, loss = 0.48407643\n",
      "Iteration 5571, loss = 0.48339567\n",
      "Iteration 5572, loss = 0.48282813\n",
      "Iteration 5573, loss = 0.48314246\n",
      "Iteration 5574, loss = 0.48498220\n",
      "Iteration 5575, loss = 0.48785245\n",
      "Iteration 5576, loss = 0.48821700\n",
      "Iteration 5577, loss = 0.48577154\n",
      "Iteration 5578, loss = 0.48479898\n",
      "Iteration 5579, loss = 0.48297090\n",
      "Iteration 5580, loss = 0.48342962\n",
      "Iteration 5581, loss = 0.48433732\n",
      "Iteration 5582, loss = 0.48671446\n",
      "Iteration 5583, loss = 0.48698021\n",
      "Iteration 5584, loss = 0.48625152\n",
      "Iteration 5585, loss = 0.48504885\n",
      "Iteration 5586, loss = 0.48460376\n",
      "Iteration 5587, loss = 0.48478037\n",
      "Iteration 5588, loss = 0.48455686\n",
      "Iteration 5589, loss = 0.48515202\n",
      "Iteration 5590, loss = 0.48493147\n",
      "Iteration 5591, loss = 0.48496085\n",
      "Iteration 5592, loss = 0.48398864\n",
      "Iteration 5593, loss = 0.48379628\n",
      "Iteration 5594, loss = 0.48351512\n",
      "Iteration 5595, loss = 0.48325527\n",
      "Iteration 5596, loss = 0.48326664\n",
      "Iteration 5597, loss = 0.48319722\n",
      "Iteration 5598, loss = 0.48316979\n",
      "Iteration 5599, loss = 0.48325668\n",
      "Iteration 5600, loss = 0.48350149\n",
      "Iteration 5601, loss = 0.48366352\n",
      "Iteration 5602, loss = 0.48361303\n",
      "Iteration 5603, loss = 0.48341295\n",
      "Iteration 5604, loss = 0.48332168\n",
      "Iteration 5605, loss = 0.48326016\n",
      "Iteration 5606, loss = 0.48319414\n",
      "Iteration 5607, loss = 0.48309568\n",
      "Iteration 5608, loss = 0.48330433\n",
      "Iteration 5609, loss = 0.48441254\n",
      "Iteration 5610, loss = 0.48567418\n",
      "Iteration 5611, loss = 0.48616875\n",
      "Iteration 5612, loss = 0.48553565\n",
      "Iteration 5613, loss = 0.48408362\n",
      "Iteration 5614, loss = 0.48374934\n",
      "Iteration 5615, loss = 0.48326185\n",
      "Iteration 5616, loss = 0.48328114\n",
      "Iteration 5617, loss = 0.48353135\n",
      "Iteration 5618, loss = 0.48359081\n",
      "Iteration 5619, loss = 0.48255966\n",
      "Iteration 5620, loss = 0.48349029\n",
      "Iteration 5621, loss = 0.48674944\n",
      "Iteration 5622, loss = 0.48888091\n",
      "Iteration 5623, loss = 0.48753342\n",
      "Iteration 5624, loss = 0.48526059\n",
      "Iteration 5625, loss = 0.48329783\n",
      "Iteration 5626, loss = 0.48277812\n",
      "Iteration 5627, loss = 0.48432617\n",
      "Iteration 5628, loss = 0.48658715\n",
      "Iteration 5629, loss = 0.48650454\n",
      "Iteration 5630, loss = 0.48456477\n",
      "Iteration 5631, loss = 0.48254736\n",
      "Iteration 5632, loss = 0.48403526\n",
      "Iteration 5633, loss = 0.48616565\n",
      "Iteration 5634, loss = 0.48803276\n",
      "Iteration 5635, loss = 0.48787859\n",
      "Iteration 5636, loss = 0.48636703\n",
      "Iteration 5637, loss = 0.48425237\n",
      "Iteration 5638, loss = 0.48294487\n",
      "Iteration 5639, loss = 0.48352884\n",
      "Iteration 5640, loss = 0.48565730\n",
      "Iteration 5641, loss = 0.48949631\n",
      "Iteration 5642, loss = 0.49368599\n",
      "Iteration 5643, loss = 0.49286141\n",
      "Iteration 5644, loss = 0.48857295\n",
      "Iteration 5645, loss = 0.48478983\n",
      "Iteration 5646, loss = 0.48320521\n",
      "Iteration 5647, loss = 0.48318285\n",
      "Iteration 5648, loss = 0.48449129\n",
      "Iteration 5649, loss = 0.48492295\n",
      "Iteration 5650, loss = 0.48478657\n",
      "Iteration 5651, loss = 0.48407674\n",
      "Iteration 5652, loss = 0.48460070\n",
      "Iteration 5653, loss = 0.48320793\n",
      "Iteration 5654, loss = 0.48339090\n",
      "Iteration 5655, loss = 0.48321171\n",
      "Iteration 5656, loss = 0.48313039\n",
      "Iteration 5657, loss = 0.48328961\n",
      "Iteration 5658, loss = 0.48431782\n",
      "Iteration 5659, loss = 0.48460347\n",
      "Iteration 5660, loss = 0.48404749\n",
      "Iteration 5661, loss = 0.48327409\n",
      "Iteration 5662, loss = 0.48324287\n",
      "Iteration 5663, loss = 0.48376923\n",
      "Iteration 5664, loss = 0.48495951\n",
      "Iteration 5665, loss = 0.48680755\n",
      "Iteration 5666, loss = 0.48777202\n",
      "Iteration 5667, loss = 0.48745509\n",
      "Iteration 5668, loss = 0.48683610\n",
      "Iteration 5669, loss = 0.48559385\n",
      "Iteration 5670, loss = 0.48433786\n",
      "Iteration 5671, loss = 0.48362939\n",
      "Iteration 5672, loss = 0.48335996\n",
      "Iteration 5673, loss = 0.48373895\n",
      "Iteration 5674, loss = 0.48345988\n",
      "Iteration 5675, loss = 0.48289311\n",
      "Iteration 5676, loss = 0.48428279\n",
      "Iteration 5677, loss = 0.48430474\n",
      "Iteration 5678, loss = 0.48449385\n",
      "Iteration 5679, loss = 0.48508092\n",
      "Iteration 5680, loss = 0.48505574\n",
      "Iteration 5681, loss = 0.48439905\n",
      "Iteration 5682, loss = 0.48387835\n",
      "Iteration 5683, loss = 0.48320253\n",
      "Iteration 5684, loss = 0.48323359\n",
      "Iteration 5685, loss = 0.48355133\n",
      "Iteration 5686, loss = 0.48347196\n",
      "Iteration 5687, loss = 0.48375221\n",
      "Iteration 5688, loss = 0.48323951\n",
      "Iteration 5689, loss = 0.48322641\n",
      "Iteration 5690, loss = 0.48336795\n",
      "Iteration 5691, loss = 0.48359975\n",
      "Iteration 5692, loss = 0.48379831\n",
      "Iteration 5693, loss = 0.48364289\n",
      "Iteration 5694, loss = 0.48431873\n",
      "Iteration 5695, loss = 0.48417587\n",
      "Iteration 5696, loss = 0.48323258\n",
      "Iteration 5697, loss = 0.48322976\n",
      "Iteration 5698, loss = 0.48314684\n",
      "Iteration 5699, loss = 0.48321207\n",
      "Iteration 5700, loss = 0.48336309\n",
      "Iteration 5701, loss = 0.48458205\n",
      "Iteration 5702, loss = 0.48475305\n",
      "Iteration 5703, loss = 0.48422355\n",
      "Iteration 5704, loss = 0.48328446\n",
      "Iteration 5705, loss = 0.48374526\n",
      "Iteration 5706, loss = 0.48298429\n",
      "Iteration 5707, loss = 0.48317913\n",
      "Iteration 5708, loss = 0.48407848\n",
      "Iteration 5709, loss = 0.48627482\n",
      "Iteration 5710, loss = 0.48805806\n",
      "Iteration 5711, loss = 0.48549776\n",
      "Iteration 5712, loss = 0.48246181\n",
      "Iteration 5713, loss = 0.48469757\n",
      "Iteration 5714, loss = 0.48669118\n",
      "Iteration 5715, loss = 0.48886441\n",
      "Iteration 5716, loss = 0.48819967\n",
      "Iteration 5717, loss = 0.48549935\n",
      "Iteration 5718, loss = 0.48362279\n",
      "Iteration 5719, loss = 0.48414153\n",
      "Iteration 5720, loss = 0.48392115\n",
      "Iteration 5721, loss = 0.48430152\n",
      "Iteration 5722, loss = 0.48482839\n",
      "Iteration 5723, loss = 0.48465851\n",
      "Iteration 5724, loss = 0.48369323\n",
      "Iteration 5725, loss = 0.48312342\n",
      "Iteration 5726, loss = 0.48393700\n",
      "Iteration 5727, loss = 0.48431261\n",
      "Iteration 5728, loss = 0.48407609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5729, loss = 0.48404178\n",
      "Iteration 5730, loss = 0.48350651\n",
      "Iteration 5731, loss = 0.48323838\n",
      "Iteration 5732, loss = 0.48358042\n",
      "Iteration 5733, loss = 0.48335627\n",
      "Iteration 5734, loss = 0.48318698\n",
      "Iteration 5735, loss = 0.48318637\n",
      "Iteration 5736, loss = 0.48322832\n",
      "Iteration 5737, loss = 0.48323755\n",
      "Iteration 5738, loss = 0.48321854\n",
      "Iteration 5739, loss = 0.48349369\n",
      "Iteration 5740, loss = 0.48349125\n",
      "Iteration 5741, loss = 0.48382963\n",
      "Iteration 5742, loss = 0.48468194\n",
      "Iteration 5743, loss = 0.48524318\n",
      "Iteration 5744, loss = 0.48396844\n",
      "Iteration 5745, loss = 0.48290879\n",
      "Iteration 5746, loss = 0.48399372\n",
      "Iteration 5747, loss = 0.48626891\n",
      "Iteration 5748, loss = 0.48733531\n",
      "Iteration 5749, loss = 0.48701710\n",
      "Iteration 5750, loss = 0.48519376\n",
      "Iteration 5751, loss = 0.48447059\n",
      "Iteration 5752, loss = 0.48337775\n",
      "Iteration 5753, loss = 0.48310057\n",
      "Iteration 5754, loss = 0.48375513\n",
      "Iteration 5755, loss = 0.48463233\n",
      "Iteration 5756, loss = 0.48470013\n",
      "Iteration 5757, loss = 0.48368880\n",
      "Iteration 5758, loss = 0.48390256\n",
      "Iteration 5759, loss = 0.48329467\n",
      "Iteration 5760, loss = 0.48339270\n",
      "Iteration 5761, loss = 0.48404267\n",
      "Iteration 5762, loss = 0.48502580\n",
      "Iteration 5763, loss = 0.48461673\n",
      "Iteration 5764, loss = 0.48359527\n",
      "Iteration 5765, loss = 0.48289250\n",
      "Iteration 5766, loss = 0.48494392\n",
      "Iteration 5767, loss = 0.48669831\n",
      "Iteration 5768, loss = 0.48642815\n",
      "Iteration 5769, loss = 0.48429702\n",
      "Iteration 5770, loss = 0.48361176\n",
      "Iteration 5771, loss = 0.48403049\n",
      "Iteration 5772, loss = 0.48437888\n",
      "Iteration 5773, loss = 0.48360180\n",
      "Iteration 5774, loss = 0.48299818\n",
      "Iteration 5775, loss = 0.48402525\n",
      "Iteration 5776, loss = 0.48512511\n",
      "Iteration 5777, loss = 0.48447029\n",
      "Iteration 5778, loss = 0.48345822\n",
      "Iteration 5779, loss = 0.48302173\n",
      "Iteration 5780, loss = 0.48356763\n",
      "Iteration 5781, loss = 0.48419840\n",
      "Iteration 5782, loss = 0.48591690\n",
      "Iteration 5783, loss = 0.48486950\n",
      "Iteration 5784, loss = 0.48348357\n",
      "Iteration 5785, loss = 0.48297305\n",
      "Iteration 5786, loss = 0.48392201\n",
      "Iteration 5787, loss = 0.48507475\n",
      "Iteration 5788, loss = 0.48607303\n",
      "Iteration 5789, loss = 0.48518301\n",
      "Iteration 5790, loss = 0.48342084\n",
      "Iteration 5791, loss = 0.48323971\n",
      "Iteration 5792, loss = 0.48417480\n",
      "Iteration 5793, loss = 0.48484016\n",
      "Iteration 5794, loss = 0.48481258\n",
      "Iteration 5795, loss = 0.48416029\n",
      "Iteration 5796, loss = 0.48287628\n",
      "Iteration 5797, loss = 0.48459979\n",
      "Iteration 5798, loss = 0.48468878\n",
      "Iteration 5799, loss = 0.48519824\n",
      "Iteration 5800, loss = 0.48689039\n",
      "Iteration 5801, loss = 0.48662707\n",
      "Iteration 5802, loss = 0.48529712\n",
      "Iteration 5803, loss = 0.48330903\n",
      "Iteration 5804, loss = 0.48313167\n",
      "Iteration 5805, loss = 0.48370738\n",
      "Iteration 5806, loss = 0.48462386\n",
      "Iteration 5807, loss = 0.48544034\n",
      "Iteration 5808, loss = 0.48445604\n",
      "Iteration 5809, loss = 0.48311901\n",
      "Iteration 5810, loss = 0.48363952\n",
      "Iteration 5811, loss = 0.48530416\n",
      "Iteration 5812, loss = 0.48673206\n",
      "Iteration 5813, loss = 0.48555724\n",
      "Iteration 5814, loss = 0.48345583\n",
      "Iteration 5815, loss = 0.48371322\n",
      "Iteration 5816, loss = 0.48372856\n",
      "Iteration 5817, loss = 0.48472623\n",
      "Iteration 5818, loss = 0.48664850\n",
      "Iteration 5819, loss = 0.48676364\n",
      "Iteration 5820, loss = 0.48587489\n",
      "Iteration 5821, loss = 0.48468936\n",
      "Iteration 5822, loss = 0.48420141\n",
      "Iteration 5823, loss = 0.48416202\n",
      "Iteration 5824, loss = 0.48376108\n",
      "Iteration 5825, loss = 0.48327455\n",
      "Iteration 5826, loss = 0.48310822\n",
      "Iteration 5827, loss = 0.48418350\n",
      "Iteration 5828, loss = 0.48438925\n",
      "Iteration 5829, loss = 0.48389788\n",
      "Iteration 5830, loss = 0.48367388\n",
      "Iteration 5831, loss = 0.48317462\n",
      "Iteration 5832, loss = 0.48322024\n",
      "Iteration 5833, loss = 0.48324482\n",
      "Iteration 5834, loss = 0.48324148\n",
      "Iteration 5835, loss = 0.48340405\n",
      "Iteration 5836, loss = 0.48314821\n",
      "Iteration 5837, loss = 0.48384174\n",
      "Iteration 5838, loss = 0.48601884\n",
      "Iteration 5839, loss = 0.48584270\n",
      "Iteration 5840, loss = 0.48488917\n",
      "Iteration 5841, loss = 0.48367584\n",
      "Iteration 5842, loss = 0.48335761\n",
      "Iteration 5843, loss = 0.48316917\n",
      "Iteration 5844, loss = 0.48324558\n",
      "Iteration 5845, loss = 0.48331860\n",
      "Iteration 5846, loss = 0.48358456\n",
      "Iteration 5847, loss = 0.48422011\n",
      "Iteration 5848, loss = 0.48339243\n",
      "Iteration 5849, loss = 0.48433198\n",
      "Iteration 5850, loss = 0.48346761\n",
      "Iteration 5851, loss = 0.48318185\n",
      "Iteration 5852, loss = 0.48390718\n",
      "Iteration 5853, loss = 0.48397259\n",
      "Iteration 5854, loss = 0.48422675\n",
      "Iteration 5855, loss = 0.48403051\n",
      "Iteration 5856, loss = 0.48350504\n",
      "Iteration 5857, loss = 0.48326882\n",
      "Iteration 5858, loss = 0.48305016\n",
      "Iteration 5859, loss = 0.48422980\n",
      "Iteration 5860, loss = 0.48446077\n",
      "Iteration 5861, loss = 0.48433093\n",
      "Iteration 5862, loss = 0.48429337\n",
      "Iteration 5863, loss = 0.48390054\n",
      "Iteration 5864, loss = 0.48313444\n",
      "Iteration 5865, loss = 0.48344878\n",
      "Iteration 5866, loss = 0.48332166\n",
      "Iteration 5867, loss = 0.48387329\n",
      "Iteration 5868, loss = 0.48424244\n",
      "Iteration 5869, loss = 0.48422455\n",
      "Iteration 5870, loss = 0.48372449\n",
      "Iteration 5871, loss = 0.48304199\n",
      "Iteration 5872, loss = 0.48322762\n",
      "Iteration 5873, loss = 0.48388040\n",
      "Iteration 5874, loss = 0.48493191\n",
      "Iteration 5875, loss = 0.48490060\n",
      "Iteration 5876, loss = 0.48333964\n",
      "Iteration 5877, loss = 0.48313474\n",
      "Iteration 5878, loss = 0.48344497\n",
      "Iteration 5879, loss = 0.48309678\n",
      "Iteration 5880, loss = 0.48315158\n",
      "Iteration 5881, loss = 0.48319545\n",
      "Iteration 5882, loss = 0.48304187\n",
      "Iteration 5883, loss = 0.48304466\n",
      "Iteration 5884, loss = 0.48358852\n",
      "Iteration 5885, loss = 0.48416555\n",
      "Iteration 5886, loss = 0.48469234\n",
      "Iteration 5887, loss = 0.48500514\n",
      "Iteration 5888, loss = 0.48465285\n",
      "Iteration 5889, loss = 0.48421460\n",
      "Iteration 5890, loss = 0.48352211\n",
      "Iteration 5891, loss = 0.48304376\n",
      "Iteration 5892, loss = 0.48284617\n",
      "Iteration 5893, loss = 0.48439914\n",
      "Iteration 5894, loss = 0.48574855\n",
      "Iteration 5895, loss = 0.48632334\n",
      "Iteration 5896, loss = 0.48555374\n",
      "Iteration 5897, loss = 0.48453087\n",
      "Iteration 5898, loss = 0.48372970\n",
      "Iteration 5899, loss = 0.48350145\n",
      "Iteration 5900, loss = 0.48411780\n",
      "Iteration 5901, loss = 0.48519583\n",
      "Iteration 5902, loss = 0.48652158\n",
      "Iteration 5903, loss = 0.48655007\n",
      "Iteration 5904, loss = 0.48498239\n",
      "Iteration 5905, loss = 0.48375300\n",
      "Iteration 5906, loss = 0.48314804\n",
      "Iteration 5907, loss = 0.48384090\n",
      "Iteration 5908, loss = 0.48423859\n",
      "Iteration 5909, loss = 0.48431072\n",
      "Iteration 5910, loss = 0.48424855\n",
      "Iteration 5911, loss = 0.48346690\n",
      "Iteration 5912, loss = 0.48377912\n",
      "Iteration 5913, loss = 0.48393923\n",
      "Iteration 5914, loss = 0.48449993\n",
      "Iteration 5915, loss = 0.48426488\n",
      "Iteration 5916, loss = 0.48373731\n",
      "Iteration 5917, loss = 0.48324602\n",
      "Iteration 5918, loss = 0.48326067\n",
      "Iteration 5919, loss = 0.48310853\n",
      "Iteration 5920, loss = 0.48318310\n",
      "Iteration 5921, loss = 0.48332415\n",
      "Iteration 5922, loss = 0.48356930\n",
      "Iteration 5923, loss = 0.48439769\n",
      "Iteration 5924, loss = 0.48436086\n",
      "Iteration 5925, loss = 0.48335581\n",
      "Iteration 5926, loss = 0.48332919\n",
      "Iteration 5927, loss = 0.48415525\n",
      "Iteration 5928, loss = 0.48602899\n",
      "Iteration 5929, loss = 0.48549373\n",
      "Iteration 5930, loss = 0.48447965\n",
      "Iteration 5931, loss = 0.48334442\n",
      "Iteration 5932, loss = 0.48305080\n",
      "Iteration 5933, loss = 0.48350029\n",
      "Iteration 5934, loss = 0.48486500\n",
      "Iteration 5935, loss = 0.48420635\n",
      "Iteration 5936, loss = 0.48319521\n",
      "Iteration 5937, loss = 0.48342809\n",
      "Iteration 5938, loss = 0.48454340\n",
      "Iteration 5939, loss = 0.48463194\n",
      "Iteration 5940, loss = 0.48403455\n",
      "Iteration 5941, loss = 0.48335275\n",
      "Iteration 5942, loss = 0.48299480\n",
      "Iteration 5943, loss = 0.48392454\n",
      "Iteration 5944, loss = 0.48428506\n",
      "Iteration 5945, loss = 0.48426914\n",
      "Iteration 5946, loss = 0.48476693\n",
      "Iteration 5947, loss = 0.48449696\n",
      "Iteration 5948, loss = 0.48350877\n",
      "Iteration 5949, loss = 0.48314927\n",
      "Iteration 5950, loss = 0.48369427\n",
      "Iteration 5951, loss = 0.48418720\n",
      "Iteration 5952, loss = 0.48456693\n",
      "Iteration 5953, loss = 0.48502564\n",
      "Iteration 5954, loss = 0.48558568\n",
      "Iteration 5955, loss = 0.48546913\n",
      "Iteration 5956, loss = 0.48419692\n",
      "Iteration 5957, loss = 0.48295405\n",
      "Iteration 5958, loss = 0.48370609\n",
      "Iteration 5959, loss = 0.48519308\n",
      "Iteration 5960, loss = 0.48628972\n",
      "Iteration 5961, loss = 0.48576950\n",
      "Iteration 5962, loss = 0.48405614\n",
      "Iteration 5963, loss = 0.48323219\n",
      "Iteration 5964, loss = 0.48278904\n",
      "Iteration 5965, loss = 0.48356509\n",
      "Iteration 5966, loss = 0.48564087\n",
      "Iteration 5967, loss = 0.48634970\n",
      "Iteration 5968, loss = 0.48463255\n",
      "Iteration 5969, loss = 0.48298328\n",
      "Iteration 5970, loss = 0.48337071\n",
      "Iteration 5971, loss = 0.48675236\n",
      "Iteration 5972, loss = 0.48843808\n",
      "Iteration 5973, loss = 0.48598200\n",
      "Iteration 5974, loss = 0.48248200\n",
      "Iteration 5975, loss = 0.48476806\n",
      "Iteration 5976, loss = 0.48650017\n",
      "Iteration 5977, loss = 0.48780790\n",
      "Iteration 5978, loss = 0.48724026\n",
      "Iteration 5979, loss = 0.48521755\n",
      "Iteration 5980, loss = 0.48417568\n",
      "Iteration 5981, loss = 0.48360939\n",
      "Iteration 5982, loss = 0.48322693\n",
      "Iteration 5983, loss = 0.48323800\n",
      "Iteration 5984, loss = 0.48321773\n",
      "Iteration 5985, loss = 0.48311092\n",
      "Iteration 5986, loss = 0.48310620\n",
      "Iteration 5987, loss = 0.48401566\n",
      "Iteration 5988, loss = 0.48353905\n",
      "Iteration 5989, loss = 0.48354087\n",
      "Iteration 5990, loss = 0.48326897\n",
      "Iteration 5991, loss = 0.48323545\n",
      "Iteration 5992, loss = 0.48385699\n",
      "Iteration 5993, loss = 0.48293939\n",
      "Iteration 5994, loss = 0.48379156\n",
      "Iteration 5995, loss = 0.48412278\n",
      "Iteration 5996, loss = 0.48513044\n",
      "Iteration 5997, loss = 0.48506078\n",
      "Iteration 5998, loss = 0.48401235\n",
      "Iteration 5999, loss = 0.48354836\n",
      "Iteration 6000, loss = 0.48338420\n",
      "Iteration 6001, loss = 0.48314593\n",
      "Iteration 6002, loss = 0.48307357\n",
      "Iteration 6003, loss = 0.48361420\n",
      "Iteration 6004, loss = 0.48380315\n",
      "Iteration 6005, loss = 0.48383484\n",
      "Iteration 6006, loss = 0.48422962\n",
      "Iteration 6007, loss = 0.48454005\n",
      "Iteration 6008, loss = 0.48496409\n",
      "Iteration 6009, loss = 0.48490644\n",
      "Iteration 6010, loss = 0.48467106\n",
      "Iteration 6011, loss = 0.48402979\n",
      "Iteration 6012, loss = 0.48358416\n",
      "Iteration 6013, loss = 0.48306516\n",
      "Iteration 6014, loss = 0.48347288\n",
      "Iteration 6015, loss = 0.48363695\n",
      "Iteration 6016, loss = 0.48480419\n",
      "Iteration 6017, loss = 0.48457570\n",
      "Iteration 6018, loss = 0.48419107\n",
      "Iteration 6019, loss = 0.48340762\n",
      "Iteration 6020, loss = 0.48358165\n",
      "Iteration 6021, loss = 0.48324578\n",
      "Iteration 6022, loss = 0.48383460\n",
      "Iteration 6023, loss = 0.48347597\n",
      "Iteration 6024, loss = 0.48380062\n",
      "Iteration 6025, loss = 0.48383968\n",
      "Iteration 6026, loss = 0.48353167\n",
      "Iteration 6027, loss = 0.48350633\n",
      "Iteration 6028, loss = 0.48337420\n",
      "Iteration 6029, loss = 0.48322443\n",
      "Iteration 6030, loss = 0.48308784\n",
      "Iteration 6031, loss = 0.48308684\n",
      "Iteration 6032, loss = 0.48327019\n",
      "Iteration 6033, loss = 0.48358048\n",
      "Iteration 6034, loss = 0.48418389\n",
      "Iteration 6035, loss = 0.48417876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6036, loss = 0.48352654\n",
      "Iteration 6037, loss = 0.48286193\n",
      "Iteration 6038, loss = 0.48349153\n",
      "Iteration 6039, loss = 0.48456492\n",
      "Iteration 6040, loss = 0.48486311\n",
      "Iteration 6041, loss = 0.48418226\n",
      "Iteration 6042, loss = 0.48360169\n",
      "Iteration 6043, loss = 0.48291544\n",
      "Iteration 6044, loss = 0.48341353\n",
      "Iteration 6045, loss = 0.48545991\n",
      "Iteration 6046, loss = 0.48531005\n",
      "Iteration 6047, loss = 0.48329192\n",
      "Iteration 6048, loss = 0.48430978\n",
      "Iteration 6049, loss = 0.48493562\n",
      "Iteration 6050, loss = 0.48428005\n",
      "Iteration 6051, loss = 0.48364329\n",
      "Iteration 6052, loss = 0.48297049\n",
      "Iteration 6053, loss = 0.48369296\n",
      "Iteration 6054, loss = 0.48464977\n",
      "Iteration 6055, loss = 0.48415034\n",
      "Iteration 6056, loss = 0.48410878\n",
      "Iteration 6057, loss = 0.48311822\n",
      "Iteration 6058, loss = 0.48333566\n",
      "Iteration 6059, loss = 0.48311387\n",
      "Iteration 6060, loss = 0.48315166\n",
      "Iteration 6061, loss = 0.48340606\n",
      "Iteration 6062, loss = 0.48423068\n",
      "Iteration 6063, loss = 0.48485385\n",
      "Iteration 6064, loss = 0.48452885\n",
      "Iteration 6065, loss = 0.48351140\n",
      "Iteration 6066, loss = 0.48370039\n",
      "Iteration 6067, loss = 0.48323920\n",
      "Iteration 6068, loss = 0.48333845\n",
      "Iteration 6069, loss = 0.48320292\n",
      "Iteration 6070, loss = 0.48332395\n",
      "Iteration 6071, loss = 0.48296090\n",
      "Iteration 6072, loss = 0.48349164\n",
      "Iteration 6073, loss = 0.48464156\n",
      "Iteration 6074, loss = 0.48496762\n",
      "Iteration 6075, loss = 0.48468042\n",
      "Iteration 6076, loss = 0.48424001\n",
      "Iteration 6077, loss = 0.48370567\n",
      "Iteration 6078, loss = 0.48354052\n",
      "Iteration 6079, loss = 0.48326410\n",
      "Iteration 6080, loss = 0.48313566\n",
      "Iteration 6081, loss = 0.48317533\n",
      "Iteration 6082, loss = 0.48351562\n",
      "Iteration 6083, loss = 0.48378732\n",
      "Iteration 6084, loss = 0.48377674\n",
      "Iteration 6085, loss = 0.48346800\n",
      "Iteration 6086, loss = 0.48313160\n",
      "Iteration 6087, loss = 0.48306732\n",
      "Iteration 6088, loss = 0.48353846\n",
      "Iteration 6089, loss = 0.48449545\n",
      "Iteration 6090, loss = 0.48604831\n",
      "Iteration 6091, loss = 0.48851921\n",
      "Iteration 6092, loss = 0.48726468\n",
      "Iteration 6093, loss = 0.48473166\n",
      "Iteration 6094, loss = 0.48321854\n",
      "Iteration 6095, loss = 0.48328770\n",
      "Iteration 6096, loss = 0.48353714\n",
      "Iteration 6097, loss = 0.48397478\n",
      "Iteration 6098, loss = 0.48446597\n",
      "Iteration 6099, loss = 0.48411608\n",
      "Iteration 6100, loss = 0.48339434\n",
      "Iteration 6101, loss = 0.48342695\n",
      "Iteration 6102, loss = 0.48324926\n",
      "Iteration 6103, loss = 0.48323188\n",
      "Iteration 6104, loss = 0.48339165\n",
      "Iteration 6105, loss = 0.48350826\n",
      "Iteration 6106, loss = 0.48349086\n",
      "Iteration 6107, loss = 0.48352375\n",
      "Iteration 6108, loss = 0.48331071\n",
      "Iteration 6109, loss = 0.48338789\n",
      "Iteration 6110, loss = 0.48338033\n",
      "Iteration 6111, loss = 0.48326948\n",
      "Iteration 6112, loss = 0.48312977\n",
      "Iteration 6113, loss = 0.48364099\n",
      "Iteration 6114, loss = 0.48309091\n",
      "Iteration 6115, loss = 0.48306774\n",
      "Iteration 6116, loss = 0.48383544\n",
      "Iteration 6117, loss = 0.48382615\n",
      "Iteration 6118, loss = 0.48344531\n",
      "Iteration 6119, loss = 0.48295206\n",
      "Iteration 6120, loss = 0.48303899\n",
      "Iteration 6121, loss = 0.48424892\n",
      "Iteration 6122, loss = 0.48695941\n",
      "Iteration 6123, loss = 0.49123310\n",
      "Iteration 6124, loss = 0.49248605\n",
      "Iteration 6125, loss = 0.48955564\n",
      "Iteration 6126, loss = 0.48432538\n",
      "Iteration 6127, loss = 0.48254067\n",
      "Iteration 6128, loss = 0.48808850\n",
      "Iteration 6129, loss = 0.48942714\n",
      "Iteration 6130, loss = 0.48898545\n",
      "Iteration 6131, loss = 0.48674422\n",
      "Iteration 6132, loss = 0.48453483\n",
      "Iteration 6133, loss = 0.48305186\n",
      "Iteration 6134, loss = 0.48356310\n",
      "Iteration 6135, loss = 0.48546260\n",
      "Iteration 6136, loss = 0.48629825\n",
      "Iteration 6137, loss = 0.48597210\n",
      "Iteration 6138, loss = 0.48504436\n",
      "Iteration 6139, loss = 0.48364503\n",
      "Iteration 6140, loss = 0.48257897\n",
      "Iteration 6141, loss = 0.48480940\n",
      "Iteration 6142, loss = 0.48654055\n",
      "Iteration 6143, loss = 0.48746325\n",
      "Iteration 6144, loss = 0.48631611\n",
      "Iteration 6145, loss = 0.48460305\n",
      "Iteration 6146, loss = 0.48346737\n",
      "Iteration 6147, loss = 0.48383133\n",
      "Iteration 6148, loss = 0.48337761\n",
      "Iteration 6149, loss = 0.48321595\n",
      "Iteration 6150, loss = 0.48375329\n",
      "Iteration 6151, loss = 0.48374258\n",
      "Iteration 6152, loss = 0.48422391\n",
      "Iteration 6153, loss = 0.48464409\n",
      "Iteration 6154, loss = 0.48442145\n",
      "Iteration 6155, loss = 0.48379463\n",
      "Iteration 6156, loss = 0.48339662\n",
      "Iteration 6157, loss = 0.48320300\n",
      "Iteration 6158, loss = 0.48295822\n",
      "Iteration 6159, loss = 0.48351677\n",
      "Iteration 6160, loss = 0.48448204\n",
      "Iteration 6161, loss = 0.48541549\n",
      "Iteration 6162, loss = 0.48469643\n",
      "Iteration 6163, loss = 0.48358169\n",
      "Iteration 6164, loss = 0.48370962\n",
      "Iteration 6165, loss = 0.48367089\n",
      "Iteration 6166, loss = 0.48362311\n",
      "Iteration 6167, loss = 0.48320600\n",
      "Iteration 6168, loss = 0.48291188\n",
      "Iteration 6169, loss = 0.48375254\n",
      "Iteration 6170, loss = 0.48599660\n",
      "Iteration 6171, loss = 0.48716706\n",
      "Iteration 6172, loss = 0.48699635\n",
      "Iteration 6173, loss = 0.48601174\n",
      "Iteration 6174, loss = 0.48404366\n",
      "Iteration 6175, loss = 0.48301277\n",
      "Iteration 6176, loss = 0.48370329\n",
      "Iteration 6177, loss = 0.48605977\n",
      "Iteration 6178, loss = 0.48672312\n",
      "Iteration 6179, loss = 0.48615745\n",
      "Iteration 6180, loss = 0.48449207\n",
      "Iteration 6181, loss = 0.48375768\n",
      "Iteration 6182, loss = 0.48388475\n",
      "Iteration 6183, loss = 0.48317637\n",
      "Iteration 6184, loss = 0.48325523\n",
      "Iteration 6185, loss = 0.48310667\n",
      "Iteration 6186, loss = 0.48360694\n",
      "Iteration 6187, loss = 0.48322736\n",
      "Iteration 6188, loss = 0.48345729\n",
      "Iteration 6189, loss = 0.48321927\n",
      "Iteration 6190, loss = 0.48317717\n",
      "Iteration 6191, loss = 0.48322857\n",
      "Iteration 6192, loss = 0.48324317\n",
      "Iteration 6193, loss = 0.48316791\n",
      "Iteration 6194, loss = 0.48327637\n",
      "Iteration 6195, loss = 0.48316411\n",
      "Iteration 6196, loss = 0.48315726\n",
      "Iteration 6197, loss = 0.48372373\n",
      "Iteration 6198, loss = 0.48349501\n",
      "Iteration 6199, loss = 0.48357521\n",
      "Iteration 6200, loss = 0.48373853\n",
      "Iteration 6201, loss = 0.48317993\n",
      "Iteration 6202, loss = 0.48317880\n",
      "Iteration 6203, loss = 0.48313623\n",
      "Iteration 6204, loss = 0.48306114\n",
      "Iteration 6205, loss = 0.48343427\n",
      "Iteration 6206, loss = 0.48415257\n",
      "Iteration 6207, loss = 0.48519759\n",
      "Iteration 6208, loss = 0.48518374\n",
      "Iteration 6209, loss = 0.48395450\n",
      "Iteration 6210, loss = 0.48295721\n",
      "Iteration 6211, loss = 0.48261204\n",
      "Iteration 6212, loss = 0.48554609\n",
      "Iteration 6213, loss = 0.48703120\n",
      "Iteration 6214, loss = 0.48667464\n",
      "Iteration 6215, loss = 0.48466049\n",
      "Iteration 6216, loss = 0.48315277\n",
      "Iteration 6217, loss = 0.48320857\n",
      "Iteration 6218, loss = 0.48559886\n",
      "Iteration 6219, loss = 0.48857761\n",
      "Iteration 6220, loss = 0.48767939\n",
      "Iteration 6221, loss = 0.48406978\n",
      "Iteration 6222, loss = 0.48297956\n",
      "Iteration 6223, loss = 0.48432148\n",
      "Iteration 6224, loss = 0.48795826\n",
      "Iteration 6225, loss = 0.49081468\n",
      "Iteration 6226, loss = 0.49106587\n",
      "Iteration 6227, loss = 0.48902712\n",
      "Iteration 6228, loss = 0.48608618\n",
      "Iteration 6229, loss = 0.48517790\n",
      "Iteration 6230, loss = 0.48328357\n",
      "Iteration 6231, loss = 0.48301302\n",
      "Iteration 6232, loss = 0.48345395\n",
      "Iteration 6233, loss = 0.48419811\n",
      "Iteration 6234, loss = 0.48558878\n",
      "Iteration 6235, loss = 0.48664545\n",
      "Iteration 6236, loss = 0.48520014\n",
      "Iteration 6237, loss = 0.48407448\n",
      "Iteration 6238, loss = 0.48332630\n",
      "Iteration 6239, loss = 0.48370963\n",
      "Iteration 6240, loss = 0.48437534\n",
      "Iteration 6241, loss = 0.48395441\n",
      "Iteration 6242, loss = 0.48334680\n",
      "Iteration 6243, loss = 0.48302982\n",
      "Iteration 6244, loss = 0.48384427\n",
      "Iteration 6245, loss = 0.48408310\n",
      "Iteration 6246, loss = 0.48395913\n",
      "Iteration 6247, loss = 0.48349376\n",
      "Iteration 6248, loss = 0.48332547\n",
      "Iteration 6249, loss = 0.48347204\n",
      "Iteration 6250, loss = 0.48310440\n",
      "Iteration 6251, loss = 0.48340958\n",
      "Iteration 6252, loss = 0.48360209\n",
      "Iteration 6253, loss = 0.48396006\n",
      "Iteration 6254, loss = 0.48400499\n",
      "Iteration 6255, loss = 0.48351612\n",
      "Iteration 6256, loss = 0.48284212\n",
      "Iteration 6257, loss = 0.48426609\n",
      "Iteration 6258, loss = 0.48447481\n",
      "Iteration 6259, loss = 0.48448029\n",
      "Iteration 6260, loss = 0.48414963\n",
      "Iteration 6261, loss = 0.48357853\n",
      "Iteration 6262, loss = 0.48351855\n",
      "Iteration 6263, loss = 0.48323399\n",
      "Iteration 6264, loss = 0.48329211\n",
      "Iteration 6265, loss = 0.48305706\n",
      "Iteration 6266, loss = 0.48376093\n",
      "Iteration 6267, loss = 0.48367987\n",
      "Iteration 6268, loss = 0.48362093\n",
      "Iteration 6269, loss = 0.48357290\n",
      "Iteration 6270, loss = 0.48331958\n",
      "Iteration 6271, loss = 0.48330725\n",
      "Iteration 6272, loss = 0.48321898\n",
      "Iteration 6273, loss = 0.48316115\n",
      "Iteration 6274, loss = 0.48370582\n",
      "Iteration 6275, loss = 0.48342196\n",
      "Iteration 6276, loss = 0.48340585\n",
      "Iteration 6277, loss = 0.48336342\n",
      "Iteration 6278, loss = 0.48322496\n",
      "Iteration 6279, loss = 0.48320272\n",
      "Iteration 6280, loss = 0.48336254\n",
      "Iteration 6281, loss = 0.48336285\n",
      "Iteration 6282, loss = 0.48328071\n",
      "Iteration 6283, loss = 0.48328878\n",
      "Iteration 6284, loss = 0.48327899\n",
      "Iteration 6285, loss = 0.48323300\n",
      "Iteration 6286, loss = 0.48385696\n",
      "Iteration 6287, loss = 0.48367778\n",
      "Iteration 6288, loss = 0.48325693\n",
      "Iteration 6289, loss = 0.48328705\n",
      "Iteration 6290, loss = 0.48364993\n",
      "Iteration 6291, loss = 0.48404635\n",
      "Iteration 6292, loss = 0.48371831\n",
      "Iteration 6293, loss = 0.48344733\n",
      "Iteration 6294, loss = 0.48331803\n",
      "Iteration 6295, loss = 0.48337571\n",
      "Iteration 6296, loss = 0.48335170\n",
      "Iteration 6297, loss = 0.48327636\n",
      "Iteration 6298, loss = 0.48353019\n",
      "Iteration 6299, loss = 0.48317502\n",
      "Iteration 6300, loss = 0.48357345\n",
      "Iteration 6301, loss = 0.48360189\n",
      "Iteration 6302, loss = 0.48345501\n",
      "Iteration 6303, loss = 0.48359950\n",
      "Iteration 6304, loss = 0.48321552\n",
      "Iteration 6305, loss = 0.48312611\n",
      "Iteration 6306, loss = 0.48373236\n",
      "Iteration 6307, loss = 0.48421531\n",
      "Iteration 6308, loss = 0.48442514\n",
      "Iteration 6309, loss = 0.48376373\n",
      "Iteration 6310, loss = 0.48338121\n",
      "Iteration 6311, loss = 0.48316956\n",
      "Iteration 6312, loss = 0.48458303\n",
      "Iteration 6313, loss = 0.48540608\n",
      "Iteration 6314, loss = 0.48402761\n",
      "Iteration 6315, loss = 0.48339418\n",
      "Iteration 6316, loss = 0.48370691\n",
      "Iteration 6317, loss = 0.48493997\n",
      "Iteration 6318, loss = 0.48682366\n",
      "Iteration 6319, loss = 0.48681861\n",
      "Iteration 6320, loss = 0.48517654\n",
      "Iteration 6321, loss = 0.48450255\n",
      "Iteration 6322, loss = 0.48335542\n",
      "Iteration 6323, loss = 0.48315140\n",
      "Iteration 6324, loss = 0.48347973\n",
      "Iteration 6325, loss = 0.48309573\n",
      "Iteration 6326, loss = 0.48281697\n",
      "Iteration 6327, loss = 0.48488842\n",
      "Iteration 6328, loss = 0.48554115\n",
      "Iteration 6329, loss = 0.48580823\n",
      "Iteration 6330, loss = 0.48711482\n",
      "Iteration 6331, loss = 0.48719995\n",
      "Iteration 6332, loss = 0.48522649\n",
      "Iteration 6333, loss = 0.48414856\n",
      "Iteration 6334, loss = 0.48439638\n",
      "Iteration 6335, loss = 0.48439754\n",
      "Iteration 6336, loss = 0.48442704\n",
      "Iteration 6337, loss = 0.48439062\n",
      "Iteration 6338, loss = 0.48438300\n",
      "Iteration 6339, loss = 0.48389427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6340, loss = 0.48413016\n",
      "Iteration 6341, loss = 0.48378200\n",
      "Iteration 6342, loss = 0.48329174\n",
      "Iteration 6343, loss = 0.48315384\n",
      "Iteration 6344, loss = 0.48329953\n",
      "Iteration 6345, loss = 0.48365582\n",
      "Iteration 6346, loss = 0.48395823\n",
      "Iteration 6347, loss = 0.48387999\n",
      "Iteration 6348, loss = 0.48358577\n",
      "Iteration 6349, loss = 0.48343021\n",
      "Iteration 6350, loss = 0.48331121\n",
      "Iteration 6351, loss = 0.48366479\n",
      "Iteration 6352, loss = 0.48357236\n",
      "Iteration 6353, loss = 0.48325620\n",
      "Iteration 6354, loss = 0.48336818\n",
      "Iteration 6355, loss = 0.48328805\n",
      "Iteration 6356, loss = 0.48359021\n",
      "Iteration 6357, loss = 0.48396468\n",
      "Iteration 6358, loss = 0.48362109\n",
      "Iteration 6359, loss = 0.48312511\n",
      "Iteration 6360, loss = 0.48414178\n",
      "Iteration 6361, loss = 0.48339951\n",
      "Iteration 6362, loss = 0.48329570\n",
      "Iteration 6363, loss = 0.48342409\n",
      "Iteration 6364, loss = 0.48364315\n",
      "Iteration 6365, loss = 0.48324870\n",
      "Iteration 6366, loss = 0.48336387\n",
      "Iteration 6367, loss = 0.48326825\n",
      "Iteration 6368, loss = 0.48367775\n",
      "Iteration 6369, loss = 0.48381493\n",
      "Iteration 6370, loss = 0.48371181\n",
      "Iteration 6371, loss = 0.48349753\n",
      "Iteration 6372, loss = 0.48313230\n",
      "Iteration 6373, loss = 0.48311271\n",
      "Iteration 6374, loss = 0.48374345\n",
      "Iteration 6375, loss = 0.48335836\n",
      "Iteration 6376, loss = 0.48316808\n",
      "Iteration 6377, loss = 0.48303134\n",
      "Iteration 6378, loss = 0.48392097\n",
      "Iteration 6379, loss = 0.48384717\n",
      "Iteration 6380, loss = 0.48350633\n",
      "Iteration 6381, loss = 0.48305580\n",
      "Iteration 6382, loss = 0.48343706\n",
      "Iteration 6383, loss = 0.48383869\n",
      "Iteration 6384, loss = 0.48412640\n",
      "Iteration 6385, loss = 0.48357092\n",
      "Iteration 6386, loss = 0.48379501\n",
      "Iteration 6387, loss = 0.48347791\n",
      "Iteration 6388, loss = 0.48290940\n",
      "Iteration 6389, loss = 0.48306952\n",
      "Iteration 6390, loss = 0.48508013\n",
      "Iteration 6391, loss = 0.48724074\n",
      "Iteration 6392, loss = 0.48722874\n",
      "Iteration 6393, loss = 0.48496201\n",
      "Iteration 6394, loss = 0.48323156\n",
      "Iteration 6395, loss = 0.48386852\n",
      "Iteration 6396, loss = 0.48487694\n",
      "Iteration 6397, loss = 0.48576110\n",
      "Iteration 6398, loss = 0.48565974\n",
      "Iteration 6399, loss = 0.48456806\n",
      "Iteration 6400, loss = 0.48329811\n",
      "Iteration 6401, loss = 0.48340552\n",
      "Iteration 6402, loss = 0.48360790\n",
      "Iteration 6403, loss = 0.48330453\n",
      "Iteration 6404, loss = 0.48285415\n",
      "Iteration 6405, loss = 0.48412049\n",
      "Iteration 6406, loss = 0.48381395\n",
      "Iteration 6407, loss = 0.48344996\n",
      "Iteration 6408, loss = 0.48317073\n",
      "Iteration 6409, loss = 0.48295167\n",
      "Iteration 6410, loss = 0.48340260\n",
      "Iteration 6411, loss = 0.48368730\n",
      "Iteration 6412, loss = 0.48401929\n",
      "Iteration 6413, loss = 0.48394444\n",
      "Iteration 6414, loss = 0.48381823\n",
      "Iteration 6415, loss = 0.48323874\n",
      "Iteration 6416, loss = 0.48316048\n",
      "Iteration 6417, loss = 0.48317321\n",
      "Iteration 6418, loss = 0.48314686\n",
      "Iteration 6419, loss = 0.48315440\n",
      "Iteration 6420, loss = 0.48318911\n",
      "Iteration 6421, loss = 0.48345619\n",
      "Iteration 6422, loss = 0.48330274\n",
      "Iteration 6423, loss = 0.48312681\n",
      "Iteration 6424, loss = 0.48309142\n",
      "Iteration 6425, loss = 0.48341763\n",
      "Iteration 6426, loss = 0.48270850\n",
      "Iteration 6427, loss = 0.48328212\n",
      "Iteration 6428, loss = 0.48631624\n",
      "Iteration 6429, loss = 0.48695291\n",
      "Iteration 6430, loss = 0.48578859\n",
      "Iteration 6431, loss = 0.48485774\n",
      "Iteration 6432, loss = 0.48342394\n",
      "Iteration 6433, loss = 0.48310602\n",
      "Iteration 6434, loss = 0.48338764\n",
      "Iteration 6435, loss = 0.48462532\n",
      "Iteration 6436, loss = 0.48506633\n",
      "Iteration 6437, loss = 0.48489297\n",
      "Iteration 6438, loss = 0.48448455\n",
      "Iteration 6439, loss = 0.48360231\n",
      "Iteration 6440, loss = 0.48293604\n",
      "Iteration 6441, loss = 0.48407697\n",
      "Iteration 6442, loss = 0.48471743\n",
      "Iteration 6443, loss = 0.48498187\n",
      "Iteration 6444, loss = 0.48493184\n",
      "Iteration 6445, loss = 0.48454694\n",
      "Iteration 6446, loss = 0.48353894\n",
      "Iteration 6447, loss = 0.48320702\n",
      "Iteration 6448, loss = 0.48340275\n",
      "Iteration 6449, loss = 0.48364079\n",
      "Iteration 6450, loss = 0.48370805\n",
      "Iteration 6451, loss = 0.48345145\n",
      "Iteration 6452, loss = 0.48332035\n",
      "Iteration 6453, loss = 0.48318020\n",
      "Iteration 6454, loss = 0.48310958\n",
      "Iteration 6455, loss = 0.48315296\n",
      "Iteration 6456, loss = 0.48312939\n",
      "Iteration 6457, loss = 0.48313614\n",
      "Iteration 6458, loss = 0.48314921\n",
      "Iteration 6459, loss = 0.48301921\n",
      "Iteration 6460, loss = 0.48413002\n",
      "Iteration 6461, loss = 0.48366510\n",
      "Iteration 6462, loss = 0.48345272\n",
      "Iteration 6463, loss = 0.48311758\n",
      "Iteration 6464, loss = 0.48324225\n",
      "Iteration 6465, loss = 0.48326234\n",
      "Iteration 6466, loss = 0.48322398\n",
      "Iteration 6467, loss = 0.48309151\n",
      "Iteration 6468, loss = 0.48299963\n",
      "Iteration 6469, loss = 0.48368879\n",
      "Iteration 6470, loss = 0.48414192\n",
      "Iteration 6471, loss = 0.48366591\n",
      "Iteration 6472, loss = 0.48385147\n",
      "Iteration 6473, loss = 0.48312440\n",
      "Iteration 6474, loss = 0.48336282\n",
      "Iteration 6475, loss = 0.48351310\n",
      "Iteration 6476, loss = 0.48345406\n",
      "Iteration 6477, loss = 0.48333217\n",
      "Iteration 6478, loss = 0.48306606\n",
      "Iteration 6479, loss = 0.48324790\n",
      "Iteration 6480, loss = 0.48318197\n",
      "Iteration 6481, loss = 0.48315246\n",
      "Iteration 6482, loss = 0.48360848\n",
      "Iteration 6483, loss = 0.48396407\n",
      "Iteration 6484, loss = 0.48441257\n",
      "Iteration 6485, loss = 0.48399535\n",
      "Iteration 6486, loss = 0.48348741\n",
      "Iteration 6487, loss = 0.48311743\n",
      "Iteration 6488, loss = 0.48376800\n",
      "Iteration 6489, loss = 0.48343645\n",
      "Iteration 6490, loss = 0.48296572\n",
      "Iteration 6491, loss = 0.48293205\n",
      "Iteration 6492, loss = 0.48379841\n",
      "Iteration 6493, loss = 0.48459535\n",
      "Iteration 6494, loss = 0.48472321\n",
      "Iteration 6495, loss = 0.48422673\n",
      "Iteration 6496, loss = 0.48336307\n",
      "Iteration 6497, loss = 0.48292089\n",
      "Iteration 6498, loss = 0.48331445\n",
      "Iteration 6499, loss = 0.48364127\n",
      "Iteration 6500, loss = 0.48385368\n",
      "Iteration 6501, loss = 0.48320482\n",
      "Iteration 6502, loss = 0.48268644\n",
      "Iteration 6503, loss = 0.48389759\n",
      "Iteration 6504, loss = 0.48536148\n",
      "Iteration 6505, loss = 0.48577420\n",
      "Iteration 6506, loss = 0.48539593\n",
      "Iteration 6507, loss = 0.48350341\n",
      "Iteration 6508, loss = 0.48376655\n",
      "Iteration 6509, loss = 0.48490231\n",
      "Iteration 6510, loss = 0.48575553\n",
      "Iteration 6511, loss = 0.48536835\n",
      "Iteration 6512, loss = 0.48334352\n",
      "Iteration 6513, loss = 0.48271526\n",
      "Iteration 6514, loss = 0.48498234\n",
      "Iteration 6515, loss = 0.48794779\n",
      "Iteration 6516, loss = 0.48991544\n",
      "Iteration 6517, loss = 0.48896669\n",
      "Iteration 6518, loss = 0.48671368\n",
      "Iteration 6519, loss = 0.48300717\n",
      "Iteration 6520, loss = 0.48337918\n",
      "Iteration 6521, loss = 0.48515995\n",
      "Iteration 6522, loss = 0.48551559\n",
      "Iteration 6523, loss = 0.48449082\n",
      "Iteration 6524, loss = 0.48309425\n",
      "Iteration 6525, loss = 0.48298333\n",
      "Iteration 6526, loss = 0.48484979\n",
      "Iteration 6527, loss = 0.48683676\n",
      "Iteration 6528, loss = 0.48628927\n",
      "Iteration 6529, loss = 0.48411674\n",
      "Iteration 6530, loss = 0.48359602\n",
      "Iteration 6531, loss = 0.48413149\n",
      "Iteration 6532, loss = 0.48414907\n",
      "Iteration 6533, loss = 0.48376074\n",
      "Iteration 6534, loss = 0.48374191\n",
      "Iteration 6535, loss = 0.48323671\n",
      "Iteration 6536, loss = 0.48320473\n",
      "Iteration 6537, loss = 0.48313852\n",
      "Iteration 6538, loss = 0.48318036\n",
      "Iteration 6539, loss = 0.48330479\n",
      "Iteration 6540, loss = 0.48349086\n",
      "Iteration 6541, loss = 0.48391227\n",
      "Iteration 6542, loss = 0.48428830\n",
      "Iteration 6543, loss = 0.48431331\n",
      "Iteration 6544, loss = 0.48347622\n",
      "Iteration 6545, loss = 0.48323645\n",
      "Iteration 6546, loss = 0.48352615\n",
      "Iteration 6547, loss = 0.48416455\n",
      "Iteration 6548, loss = 0.48421977\n",
      "Iteration 6549, loss = 0.48459406\n",
      "Iteration 6550, loss = 0.48335895\n",
      "Iteration 6551, loss = 0.48410887\n",
      "Iteration 6552, loss = 0.48424968\n",
      "Iteration 6553, loss = 0.48304694\n",
      "Iteration 6554, loss = 0.48310025\n",
      "Iteration 6555, loss = 0.48440644\n",
      "Iteration 6556, loss = 0.48599468\n",
      "Iteration 6557, loss = 0.48612252\n",
      "Iteration 6558, loss = 0.48542850\n",
      "Iteration 6559, loss = 0.48403896\n",
      "Iteration 6560, loss = 0.48334331\n",
      "Iteration 6561, loss = 0.48335592\n",
      "Iteration 6562, loss = 0.48369758\n",
      "Iteration 6563, loss = 0.48384831\n",
      "Iteration 6564, loss = 0.48315293\n",
      "Iteration 6565, loss = 0.48358824\n",
      "Iteration 6566, loss = 0.48385315\n",
      "Iteration 6567, loss = 0.48316930\n",
      "Iteration 6568, loss = 0.48263397\n",
      "Iteration 6569, loss = 0.48462042\n",
      "Iteration 6570, loss = 0.48574374\n",
      "Iteration 6571, loss = 0.48667917\n",
      "Iteration 6572, loss = 0.48673764\n",
      "Iteration 6573, loss = 0.48562479\n",
      "Iteration 6574, loss = 0.48530043\n",
      "Iteration 6575, loss = 0.48552548\n",
      "Iteration 6576, loss = 0.48531206\n",
      "Iteration 6577, loss = 0.48478838\n",
      "Iteration 6578, loss = 0.48451880\n",
      "Iteration 6579, loss = 0.48441714\n",
      "Iteration 6580, loss = 0.48421757\n",
      "Iteration 6581, loss = 0.48422931\n",
      "Iteration 6582, loss = 0.48405720\n",
      "Iteration 6583, loss = 0.48356336\n",
      "Iteration 6584, loss = 0.48338306\n",
      "Iteration 6585, loss = 0.48326180\n",
      "Iteration 6586, loss = 0.48309084\n",
      "Iteration 6587, loss = 0.48293239\n",
      "Iteration 6588, loss = 0.48418168\n",
      "Iteration 6589, loss = 0.48465100\n",
      "Iteration 6590, loss = 0.48443354\n",
      "Iteration 6591, loss = 0.48399071\n",
      "Iteration 6592, loss = 0.48308512\n",
      "Iteration 6593, loss = 0.48356199\n",
      "Iteration 6594, loss = 0.48325420\n",
      "Iteration 6595, loss = 0.48336199\n",
      "Iteration 6596, loss = 0.48327721\n",
      "Iteration 6597, loss = 0.48384576\n",
      "Iteration 6598, loss = 0.48365549\n",
      "Iteration 6599, loss = 0.48292167\n",
      "Iteration 6600, loss = 0.48270997\n",
      "Iteration 6601, loss = 0.48395610\n",
      "Iteration 6602, loss = 0.48666922\n",
      "Iteration 6603, loss = 0.48852204\n",
      "Iteration 6604, loss = 0.48752812\n",
      "Iteration 6605, loss = 0.48552865\n",
      "Iteration 6606, loss = 0.48354482\n",
      "Iteration 6607, loss = 0.48305639\n",
      "Iteration 6608, loss = 0.48397838\n",
      "Iteration 6609, loss = 0.48367589\n",
      "Iteration 6610, loss = 0.48308518\n",
      "Iteration 6611, loss = 0.48311793\n",
      "Iteration 6612, loss = 0.48303359\n",
      "Iteration 6613, loss = 0.48305074\n",
      "Iteration 6614, loss = 0.48309451\n",
      "Iteration 6615, loss = 0.48312278\n",
      "Iteration 6616, loss = 0.48308201\n",
      "Iteration 6617, loss = 0.48305678\n",
      "Iteration 6618, loss = 0.48315226\n",
      "Iteration 6619, loss = 0.48318888\n",
      "Iteration 6620, loss = 0.48314000\n",
      "Iteration 6621, loss = 0.48302350\n",
      "Iteration 6622, loss = 0.48317143\n",
      "Iteration 6623, loss = 0.48443331\n",
      "Iteration 6624, loss = 0.48454852\n",
      "Iteration 6625, loss = 0.48354116\n",
      "Iteration 6626, loss = 0.48280978\n",
      "Iteration 6627, loss = 0.48385999\n",
      "Iteration 6628, loss = 0.48484495\n",
      "Iteration 6629, loss = 0.48577201\n",
      "Iteration 6630, loss = 0.48664101\n",
      "Iteration 6631, loss = 0.48629182\n",
      "Iteration 6632, loss = 0.48495047\n",
      "Iteration 6633, loss = 0.48325048\n",
      "Iteration 6634, loss = 0.48414269\n",
      "Iteration 6635, loss = 0.48358150\n",
      "Iteration 6636, loss = 0.48328204\n",
      "Iteration 6637, loss = 0.48332054\n",
      "Iteration 6638, loss = 0.48329544\n",
      "Iteration 6639, loss = 0.48326016\n",
      "Iteration 6640, loss = 0.48312901\n",
      "Iteration 6641, loss = 0.48318262\n",
      "Iteration 6642, loss = 0.48325086\n",
      "Iteration 6643, loss = 0.48309264\n",
      "Iteration 6644, loss = 0.48320542\n",
      "Iteration 6645, loss = 0.48304218\n",
      "Iteration 6646, loss = 0.48328318\n",
      "Iteration 6647, loss = 0.48356452\n",
      "Iteration 6648, loss = 0.48381495\n",
      "Iteration 6649, loss = 0.48448192\n",
      "Iteration 6650, loss = 0.48553486\n",
      "Iteration 6651, loss = 0.48531118\n",
      "Iteration 6652, loss = 0.48357279\n",
      "Iteration 6653, loss = 0.48296471\n",
      "Iteration 6654, loss = 0.48388255\n",
      "Iteration 6655, loss = 0.48404606\n",
      "Iteration 6656, loss = 0.48426522\n",
      "Iteration 6657, loss = 0.48422834\n",
      "Iteration 6658, loss = 0.48369954\n",
      "Iteration 6659, loss = 0.48332803\n",
      "Iteration 6660, loss = 0.48314269\n",
      "Iteration 6661, loss = 0.48324569\n",
      "Iteration 6662, loss = 0.48417835\n",
      "Iteration 6663, loss = 0.48468160\n",
      "Iteration 6664, loss = 0.48446623\n",
      "Iteration 6665, loss = 0.48388792\n",
      "Iteration 6666, loss = 0.48358795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6667, loss = 0.48355491\n",
      "Iteration 6668, loss = 0.48353135\n",
      "Iteration 6669, loss = 0.48334374\n",
      "Iteration 6670, loss = 0.48317994\n",
      "Iteration 6671, loss = 0.48308562\n",
      "Iteration 6672, loss = 0.48384503\n",
      "Iteration 6673, loss = 0.48441253\n",
      "Iteration 6674, loss = 0.48464830\n",
      "Iteration 6675, loss = 0.48447913\n",
      "Iteration 6676, loss = 0.48416868\n",
      "Iteration 6677, loss = 0.48390432\n",
      "Iteration 6678, loss = 0.48382673\n",
      "Iteration 6679, loss = 0.48327194\n",
      "Iteration 6680, loss = 0.48321831\n",
      "Iteration 6681, loss = 0.48318106\n",
      "Iteration 6682, loss = 0.48314743\n",
      "Iteration 6683, loss = 0.48328674\n",
      "Iteration 6684, loss = 0.48336261\n",
      "Iteration 6685, loss = 0.48354431\n",
      "Iteration 6686, loss = 0.48349112\n",
      "Iteration 6687, loss = 0.48321374\n",
      "Iteration 6688, loss = 0.48289248\n",
      "Iteration 6689, loss = 0.48287445\n",
      "Iteration 6690, loss = 0.48574378\n",
      "Iteration 6691, loss = 0.48693629\n",
      "Iteration 6692, loss = 0.48565123\n",
      "Iteration 6693, loss = 0.48430511\n",
      "Iteration 6694, loss = 0.48411608\n",
      "Iteration 6695, loss = 0.48375871\n",
      "Iteration 6696, loss = 0.48365533\n",
      "Iteration 6697, loss = 0.48326747\n",
      "Iteration 6698, loss = 0.48356713\n",
      "Iteration 6699, loss = 0.48319046\n",
      "Iteration 6700, loss = 0.48337282\n",
      "Iteration 6701, loss = 0.48311598\n",
      "Iteration 6702, loss = 0.48345590\n",
      "Iteration 6703, loss = 0.48440171\n",
      "Iteration 6704, loss = 0.48547849\n",
      "Iteration 6705, loss = 0.48557561\n",
      "Iteration 6706, loss = 0.48477321\n",
      "Iteration 6707, loss = 0.48331160\n",
      "Iteration 6708, loss = 0.48318914\n",
      "Iteration 6709, loss = 0.48320962\n",
      "Iteration 6710, loss = 0.48389926\n",
      "Iteration 6711, loss = 0.48446005\n",
      "Iteration 6712, loss = 0.48452308\n",
      "Iteration 6713, loss = 0.48403352\n",
      "Iteration 6714, loss = 0.48394056\n",
      "Iteration 6715, loss = 0.48360961\n",
      "Iteration 6716, loss = 0.48298389\n",
      "Iteration 6717, loss = 0.48305827\n",
      "Iteration 6718, loss = 0.48488123\n",
      "Iteration 6719, loss = 0.48551321\n",
      "Iteration 6720, loss = 0.48486037\n",
      "Iteration 6721, loss = 0.48355580\n",
      "Iteration 6722, loss = 0.48297106\n",
      "Iteration 6723, loss = 0.48355818\n",
      "Iteration 6724, loss = 0.48493685\n",
      "Iteration 6725, loss = 0.48495888\n",
      "Iteration 6726, loss = 0.48428095\n",
      "Iteration 6727, loss = 0.48385113\n",
      "Iteration 6728, loss = 0.48371317\n",
      "Iteration 6729, loss = 0.48372046\n",
      "Iteration 6730, loss = 0.48349522\n",
      "Iteration 6731, loss = 0.48352643\n",
      "Iteration 6732, loss = 0.48358077\n",
      "Iteration 6733, loss = 0.48326877\n",
      "Iteration 6734, loss = 0.48327151\n",
      "Iteration 6735, loss = 0.48323981\n",
      "Iteration 6736, loss = 0.48315608\n",
      "Iteration 6737, loss = 0.48317893\n",
      "Iteration 6738, loss = 0.48338278\n",
      "Iteration 6739, loss = 0.48421821\n",
      "Iteration 6740, loss = 0.48482286\n",
      "Iteration 6741, loss = 0.48409493\n",
      "Iteration 6742, loss = 0.48331166\n",
      "Iteration 6743, loss = 0.48320297\n",
      "Iteration 6744, loss = 0.48374217\n",
      "Iteration 6745, loss = 0.48472440\n",
      "Iteration 6746, loss = 0.48560656\n",
      "Iteration 6747, loss = 0.48537125\n",
      "Iteration 6748, loss = 0.48397071\n",
      "Iteration 6749, loss = 0.48354283\n",
      "Iteration 6750, loss = 0.48404676\n",
      "Iteration 6751, loss = 0.48413402\n",
      "Iteration 6752, loss = 0.48365728\n",
      "Iteration 6753, loss = 0.48344086\n",
      "Iteration 6754, loss = 0.48311825\n",
      "Iteration 6755, loss = 0.48376622\n",
      "Iteration 6756, loss = 0.48323005\n",
      "Iteration 6757, loss = 0.48320293\n",
      "Iteration 6758, loss = 0.48352032\n",
      "Iteration 6759, loss = 0.48345259\n",
      "Iteration 6760, loss = 0.48330400\n",
      "Iteration 6761, loss = 0.48324779\n",
      "Iteration 6762, loss = 0.48326615\n",
      "Iteration 6763, loss = 0.48320691\n",
      "Iteration 6764, loss = 0.48324823\n",
      "Iteration 6765, loss = 0.48323221\n",
      "Iteration 6766, loss = 0.48315280\n",
      "Iteration 6767, loss = 0.48315132\n",
      "Iteration 6768, loss = 0.48326152\n",
      "Iteration 6769, loss = 0.48359025\n",
      "Iteration 6770, loss = 0.48515499\n",
      "Iteration 6771, loss = 0.48592627\n",
      "Iteration 6772, loss = 0.48484483\n",
      "Iteration 6773, loss = 0.48377823\n",
      "Iteration 6774, loss = 0.48308368\n",
      "Iteration 6775, loss = 0.48345162\n",
      "Iteration 6776, loss = 0.48379361\n",
      "Iteration 6777, loss = 0.48349810\n",
      "Iteration 6778, loss = 0.48329763\n",
      "Iteration 6779, loss = 0.48337308\n",
      "Iteration 6780, loss = 0.48476283\n",
      "Iteration 6781, loss = 0.48495192\n",
      "Iteration 6782, loss = 0.48419744\n",
      "Iteration 6783, loss = 0.48330565\n",
      "Iteration 6784, loss = 0.48315293\n",
      "Iteration 6785, loss = 0.48323982\n",
      "Iteration 6786, loss = 0.48402682\n",
      "Iteration 6787, loss = 0.48564074\n",
      "Iteration 6788, loss = 0.48657951\n",
      "Iteration 6789, loss = 0.48518361\n",
      "Iteration 6790, loss = 0.48344029\n",
      "Iteration 6791, loss = 0.48322851\n",
      "Iteration 6792, loss = 0.48397242\n",
      "Iteration 6793, loss = 0.48464206\n",
      "Iteration 6794, loss = 0.48422417\n",
      "Iteration 6795, loss = 0.48414814\n",
      "Iteration 6796, loss = 0.48325962\n",
      "Iteration 6797, loss = 0.48309736\n",
      "Iteration 6798, loss = 0.48376408\n",
      "Iteration 6799, loss = 0.48350378\n",
      "Iteration 6800, loss = 0.48374829\n",
      "Iteration 6801, loss = 0.48378544\n",
      "Iteration 6802, loss = 0.48344274\n",
      "Iteration 6803, loss = 0.48342643\n",
      "Iteration 6804, loss = 0.48318657\n",
      "Iteration 6805, loss = 0.48299951\n",
      "Iteration 6806, loss = 0.48296380\n",
      "Iteration 6807, loss = 0.48393125\n",
      "Iteration 6808, loss = 0.48421610\n",
      "Iteration 6809, loss = 0.48416356\n",
      "Iteration 6810, loss = 0.48391611\n",
      "Iteration 6811, loss = 0.48369603\n",
      "Iteration 6812, loss = 0.48356015\n",
      "Iteration 6813, loss = 0.48304938\n",
      "Iteration 6814, loss = 0.48314232\n",
      "Iteration 6815, loss = 0.48325352\n",
      "Iteration 6816, loss = 0.48323931\n",
      "Iteration 6817, loss = 0.48343305\n",
      "Iteration 6818, loss = 0.48306661\n",
      "Iteration 6819, loss = 0.48339586\n",
      "Iteration 6820, loss = 0.48386203\n",
      "Iteration 6821, loss = 0.48356561\n",
      "Iteration 6822, loss = 0.48304166\n",
      "Iteration 6823, loss = 0.48330750\n",
      "Iteration 6824, loss = 0.48347072\n",
      "Iteration 6825, loss = 0.48330847\n",
      "Iteration 6826, loss = 0.48336861\n",
      "Iteration 6827, loss = 0.48348130\n",
      "Iteration 6828, loss = 0.48319633\n",
      "Iteration 6829, loss = 0.48341574\n",
      "Iteration 6830, loss = 0.48366546\n",
      "Iteration 6831, loss = 0.48397849\n",
      "Iteration 6832, loss = 0.48349143\n",
      "Iteration 6833, loss = 0.48291727\n",
      "Iteration 6834, loss = 0.48334421\n",
      "Iteration 6835, loss = 0.48440919\n",
      "Iteration 6836, loss = 0.48425305\n",
      "Iteration 6837, loss = 0.48350997\n",
      "Iteration 6838, loss = 0.48334910\n",
      "Iteration 6839, loss = 0.48310606\n",
      "Iteration 6840, loss = 0.48323817\n",
      "Iteration 6841, loss = 0.48429532\n",
      "Iteration 6842, loss = 0.48490324\n",
      "Iteration 6843, loss = 0.48422218\n",
      "Iteration 6844, loss = 0.48337224\n",
      "Iteration 6845, loss = 0.48302810\n",
      "Iteration 6846, loss = 0.48331554\n",
      "Iteration 6847, loss = 0.48339858\n",
      "Iteration 6848, loss = 0.48347577\n",
      "Iteration 6849, loss = 0.48316815\n",
      "Iteration 6850, loss = 0.48352330\n",
      "Iteration 6851, loss = 0.48341821\n",
      "Iteration 6852, loss = 0.48308200\n",
      "Iteration 6853, loss = 0.48247995\n",
      "Iteration 6854, loss = 0.48535007\n",
      "Iteration 6855, loss = 0.48615025\n",
      "Iteration 6856, loss = 0.48564775\n",
      "Iteration 6857, loss = 0.48377364\n",
      "Iteration 6858, loss = 0.48240996\n",
      "Iteration 6859, loss = 0.48387074\n",
      "Iteration 6860, loss = 0.48748361\n",
      "Iteration 6861, loss = 0.49113285\n",
      "Iteration 6862, loss = 0.49144419\n",
      "Iteration 6863, loss = 0.48845550\n",
      "Iteration 6864, loss = 0.48522708\n",
      "Iteration 6865, loss = 0.48306092\n",
      "Iteration 6866, loss = 0.48342941\n",
      "Iteration 6867, loss = 0.48521377\n",
      "Iteration 6868, loss = 0.48499690\n",
      "Iteration 6869, loss = 0.48404001\n",
      "Iteration 6870, loss = 0.48353712\n",
      "Iteration 6871, loss = 0.48313710\n",
      "Iteration 6872, loss = 0.48348439\n",
      "Iteration 6873, loss = 0.48327093\n",
      "Iteration 6874, loss = 0.48328604\n",
      "Iteration 6875, loss = 0.48323526\n",
      "Iteration 6876, loss = 0.48306633\n",
      "Iteration 6877, loss = 0.48289622\n",
      "Iteration 6878, loss = 0.48315103\n",
      "Iteration 6879, loss = 0.48650339\n",
      "Iteration 6880, loss = 0.48631516\n",
      "Iteration 6881, loss = 0.48445037\n",
      "Iteration 6882, loss = 0.48281336\n",
      "Iteration 6883, loss = 0.48350038\n",
      "Iteration 6884, loss = 0.48491846\n",
      "Iteration 6885, loss = 0.48564577\n",
      "Iteration 6886, loss = 0.48547945\n",
      "Iteration 6887, loss = 0.48476355\n",
      "Iteration 6888, loss = 0.48380271\n",
      "Iteration 6889, loss = 0.48282463\n",
      "Iteration 6890, loss = 0.48409805\n",
      "Iteration 6891, loss = 0.48461671\n",
      "Iteration 6892, loss = 0.48421293\n",
      "Iteration 6893, loss = 0.48323087\n",
      "Iteration 6894, loss = 0.48307284\n",
      "Iteration 6895, loss = 0.48380620\n",
      "Iteration 6896, loss = 0.48528321\n",
      "Iteration 6897, loss = 0.48463720\n",
      "Iteration 6898, loss = 0.48382463\n",
      "Iteration 6899, loss = 0.48342575\n",
      "Iteration 6900, loss = 0.48324366\n",
      "Iteration 6901, loss = 0.48322303\n",
      "Iteration 6902, loss = 0.48332154\n",
      "Iteration 6903, loss = 0.48323060\n",
      "Iteration 6904, loss = 0.48332954\n",
      "Iteration 6905, loss = 0.48303969\n",
      "Iteration 6906, loss = 0.48319010\n",
      "Iteration 6907, loss = 0.48327457\n",
      "Iteration 6908, loss = 0.48329591\n",
      "Iteration 6909, loss = 0.48330874\n",
      "Iteration 6910, loss = 0.48304932\n",
      "Iteration 6911, loss = 0.48302547\n",
      "Iteration 6912, loss = 0.48301460\n",
      "Iteration 6913, loss = 0.48311443\n",
      "Iteration 6914, loss = 0.48306212\n",
      "Iteration 6915, loss = 0.48297565\n",
      "Iteration 6916, loss = 0.48305918\n",
      "Iteration 6917, loss = 0.48325977\n",
      "Iteration 6918, loss = 0.48350990\n",
      "Iteration 6919, loss = 0.48365468\n",
      "Iteration 6920, loss = 0.48361365\n",
      "Iteration 6921, loss = 0.48336358\n",
      "Iteration 6922, loss = 0.48288126\n",
      "Iteration 6923, loss = 0.48273999\n",
      "Iteration 6924, loss = 0.48632093\n",
      "Iteration 6925, loss = 0.48585283\n",
      "Iteration 6926, loss = 0.48453936\n",
      "Iteration 6927, loss = 0.48331685\n",
      "Iteration 6928, loss = 0.48371266\n",
      "Iteration 6929, loss = 0.48371821\n",
      "Iteration 6930, loss = 0.48404249\n",
      "Iteration 6931, loss = 0.48407284\n",
      "Iteration 6932, loss = 0.48382122\n",
      "Iteration 6933, loss = 0.48376484\n",
      "Iteration 6934, loss = 0.48386216\n",
      "Iteration 6935, loss = 0.48332919\n",
      "Iteration 6936, loss = 0.48332303\n",
      "Iteration 6937, loss = 0.48316721\n",
      "Iteration 6938, loss = 0.48326154\n",
      "Iteration 6939, loss = 0.48307552\n",
      "Iteration 6940, loss = 0.48316122\n",
      "Iteration 6941, loss = 0.48339636\n",
      "Iteration 6942, loss = 0.48327101\n",
      "Iteration 6943, loss = 0.48332616\n",
      "Iteration 6944, loss = 0.48326252\n",
      "Iteration 6945, loss = 0.48316668\n",
      "Iteration 6946, loss = 0.48297899\n",
      "Iteration 6947, loss = 0.48332218\n",
      "Iteration 6948, loss = 0.48382835\n",
      "Iteration 6949, loss = 0.48431809\n",
      "Iteration 6950, loss = 0.48458247\n",
      "Iteration 6951, loss = 0.48444675\n",
      "Iteration 6952, loss = 0.48390534\n",
      "Iteration 6953, loss = 0.48364091\n",
      "Iteration 6954, loss = 0.48301575\n",
      "Iteration 6955, loss = 0.48377225\n",
      "Iteration 6956, loss = 0.48414365\n",
      "Iteration 6957, loss = 0.48357585\n",
      "Iteration 6958, loss = 0.48256236\n",
      "Iteration 6959, loss = 0.48405884\n",
      "Iteration 6960, loss = 0.48645537\n",
      "Iteration 6961, loss = 0.48626919\n",
      "Iteration 6962, loss = 0.48434366\n",
      "Iteration 6963, loss = 0.48378565\n",
      "Iteration 6964, loss = 0.48300299\n",
      "Iteration 6965, loss = 0.48372969\n",
      "Iteration 6966, loss = 0.48400035\n",
      "Iteration 6967, loss = 0.48407247\n",
      "Iteration 6968, loss = 0.48368118\n",
      "Iteration 6969, loss = 0.48295742\n",
      "Iteration 6970, loss = 0.48403317\n",
      "Iteration 6971, loss = 0.48398055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6972, loss = 0.48400707\n",
      "Iteration 6973, loss = 0.48398424\n",
      "Iteration 6974, loss = 0.48382011\n",
      "Iteration 6975, loss = 0.48378844\n",
      "Iteration 6976, loss = 0.48366530\n",
      "Iteration 6977, loss = 0.48396145\n",
      "Iteration 6978, loss = 0.48388479\n",
      "Iteration 6979, loss = 0.48344458\n",
      "Iteration 6980, loss = 0.48301145\n",
      "Iteration 6981, loss = 0.48276185\n",
      "Iteration 6982, loss = 0.48393195\n",
      "Iteration 6983, loss = 0.48726719\n",
      "Iteration 6984, loss = 0.48915956\n",
      "Iteration 6985, loss = 0.48711200\n",
      "Iteration 6986, loss = 0.48406962\n",
      "Iteration 6987, loss = 0.48235668\n",
      "Iteration 6988, loss = 0.48628210\n",
      "Iteration 6989, loss = 0.49003478\n",
      "Iteration 6990, loss = 0.48999216\n",
      "Iteration 6991, loss = 0.48727649\n",
      "Iteration 6992, loss = 0.48464353\n",
      "Iteration 6993, loss = 0.48316275\n",
      "Iteration 6994, loss = 0.48306130\n",
      "Iteration 6995, loss = 0.48354426\n",
      "Iteration 6996, loss = 0.48397541\n",
      "Iteration 6997, loss = 0.48384616\n",
      "Iteration 6998, loss = 0.48325156\n",
      "Iteration 6999, loss = 0.48309693\n",
      "Iteration 7000, loss = 0.48334945\n",
      "Iteration 7001, loss = 0.48350980\n",
      "Iteration 7002, loss = 0.48431614\n",
      "Iteration 7003, loss = 0.48464207\n",
      "Iteration 7004, loss = 0.48368214\n",
      "Iteration 7005, loss = 0.48283667\n",
      "Iteration 7006, loss = 0.48287400\n",
      "Iteration 7007, loss = 0.48493997\n",
      "Iteration 7008, loss = 0.48972805\n",
      "Iteration 7009, loss = 0.49525984\n",
      "Iteration 7010, loss = 0.49688779\n",
      "Iteration 7011, loss = 0.49235438\n",
      "Iteration 7012, loss = 0.48748049\n",
      "Iteration 7013, loss = 0.48311352\n",
      "Iteration 7014, loss = 0.48342908\n",
      "Iteration 7015, loss = 0.48445790\n",
      "Iteration 7016, loss = 0.48498142\n",
      "Iteration 7017, loss = 0.48478792\n",
      "Iteration 7018, loss = 0.48466542\n",
      "Iteration 7019, loss = 0.48386750\n",
      "Iteration 7020, loss = 0.48332709\n",
      "Iteration 7021, loss = 0.48339640\n",
      "Iteration 7022, loss = 0.48315532\n",
      "Iteration 7023, loss = 0.48332123\n",
      "Iteration 7024, loss = 0.48330668\n",
      "Iteration 7025, loss = 0.48318375\n",
      "Iteration 7026, loss = 0.48374750\n",
      "Iteration 7027, loss = 0.48372529\n",
      "Iteration 7028, loss = 0.48340163\n",
      "Iteration 7029, loss = 0.48332434\n",
      "Iteration 7030, loss = 0.48306810\n",
      "Iteration 7031, loss = 0.48344609\n",
      "Iteration 7032, loss = 0.48395852\n",
      "Iteration 7033, loss = 0.48483615\n",
      "Iteration 7034, loss = 0.48436729\n",
      "Iteration 7035, loss = 0.48378297\n",
      "Iteration 7036, loss = 0.48290398\n",
      "Iteration 7037, loss = 0.48333925\n",
      "Iteration 7038, loss = 0.48482020\n",
      "Iteration 7039, loss = 0.48460891\n",
      "Iteration 7040, loss = 0.48356099\n",
      "Iteration 7041, loss = 0.48320004\n",
      "Iteration 7042, loss = 0.48333226\n",
      "Iteration 7043, loss = 0.48336182\n",
      "Iteration 7044, loss = 0.48335138\n",
      "Iteration 7045, loss = 0.48331844\n",
      "Iteration 7046, loss = 0.48327142\n",
      "Iteration 7047, loss = 0.48314253\n",
      "Iteration 7048, loss = 0.48308725\n",
      "Iteration 7049, loss = 0.48311662\n",
      "Iteration 7050, loss = 0.48302283\n",
      "Iteration 7051, loss = 0.48305432\n",
      "Iteration 7052, loss = 0.48307594\n",
      "Iteration 7053, loss = 0.48342283\n",
      "Iteration 7054, loss = 0.48364327\n",
      "Iteration 7055, loss = 0.48366410\n",
      "Iteration 7056, loss = 0.48348158\n",
      "Iteration 7057, loss = 0.48345431\n",
      "Iteration 7058, loss = 0.48306978\n",
      "Iteration 7059, loss = 0.48303827\n",
      "Iteration 7060, loss = 0.48298868\n",
      "Iteration 7061, loss = 0.48312207\n",
      "Iteration 7062, loss = 0.48324034\n",
      "Iteration 7063, loss = 0.48336959\n",
      "Iteration 7064, loss = 0.48344490\n",
      "Iteration 7065, loss = 0.48334398\n",
      "Iteration 7066, loss = 0.48316263\n",
      "Iteration 7067, loss = 0.48306681\n",
      "Iteration 7068, loss = 0.48308683\n",
      "Iteration 7069, loss = 0.48310050\n",
      "Iteration 7070, loss = 0.48306024\n",
      "Iteration 7071, loss = 0.48300020\n",
      "Iteration 7072, loss = 0.48304989\n",
      "Iteration 7073, loss = 0.48318477\n",
      "Iteration 7074, loss = 0.48393828\n",
      "Iteration 7075, loss = 0.48377042\n",
      "Iteration 7076, loss = 0.48265010\n",
      "Iteration 7077, loss = 0.48336309\n",
      "Iteration 7078, loss = 0.48527233\n",
      "Iteration 7079, loss = 0.48895848\n",
      "Iteration 7080, loss = 0.48994716\n",
      "Iteration 7081, loss = 0.48552424\n",
      "Iteration 7082, loss = 0.48192592\n",
      "Iteration 7083, loss = 0.48637861\n",
      "Iteration 7084, loss = 0.49201365\n",
      "Iteration 7085, loss = 0.49434293\n",
      "Iteration 7086, loss = 0.49156135\n",
      "Iteration 7087, loss = 0.48583740\n",
      "Iteration 7088, loss = 0.48413031\n",
      "Iteration 7089, loss = 0.48332965\n",
      "Iteration 7090, loss = 0.48518818\n",
      "Iteration 7091, loss = 0.48828798\n",
      "Iteration 7092, loss = 0.48934575\n",
      "Iteration 7093, loss = 0.48690138\n",
      "Iteration 7094, loss = 0.48420636\n",
      "Iteration 7095, loss = 0.48326991\n",
      "Iteration 7096, loss = 0.48342249\n",
      "Iteration 7097, loss = 0.48495891\n",
      "Iteration 7098, loss = 0.48545559\n",
      "Iteration 7099, loss = 0.48480577\n",
      "Iteration 7100, loss = 0.48385704\n",
      "Iteration 7101, loss = 0.48270763\n",
      "Iteration 7102, loss = 0.48400457\n",
      "Iteration 7103, loss = 0.48470572\n",
      "Iteration 7104, loss = 0.48444834\n",
      "Iteration 7105, loss = 0.48270992\n",
      "Iteration 7106, loss = 0.48354271\n",
      "Iteration 7107, loss = 0.48646471\n",
      "Iteration 7108, loss = 0.48814887\n",
      "Iteration 7109, loss = 0.48714954\n",
      "Iteration 7110, loss = 0.48415216\n",
      "Iteration 7111, loss = 0.48369834\n",
      "Iteration 7112, loss = 0.48372301\n",
      "Iteration 7113, loss = 0.48347276\n",
      "Iteration 7114, loss = 0.48334860\n",
      "Iteration 7115, loss = 0.48319518\n",
      "Iteration 7116, loss = 0.48377176\n",
      "Iteration 7117, loss = 0.48541965\n",
      "Iteration 7118, loss = 0.48602976\n",
      "Iteration 7119, loss = 0.48502265\n",
      "Iteration 7120, loss = 0.48379141\n",
      "Iteration 7121, loss = 0.48251307\n",
      "Iteration 7122, loss = 0.48369650\n",
      "Iteration 7123, loss = 0.48733189\n",
      "Iteration 7124, loss = 0.49098893\n",
      "Iteration 7125, loss = 0.49007772\n",
      "Iteration 7126, loss = 0.48714887\n",
      "Iteration 7127, loss = 0.48387869\n",
      "Iteration 7128, loss = 0.48309779\n",
      "Iteration 7129, loss = 0.48380770\n",
      "Iteration 7130, loss = 0.48413155\n",
      "Iteration 7131, loss = 0.48406495\n",
      "Iteration 7132, loss = 0.48354182\n",
      "Iteration 7133, loss = 0.48309703\n",
      "Iteration 7134, loss = 0.48419983\n",
      "Iteration 7135, loss = 0.48420024\n",
      "Iteration 7136, loss = 0.48399979\n",
      "Iteration 7137, loss = 0.48343385\n",
      "Iteration 7138, loss = 0.48375406\n",
      "Iteration 7139, loss = 0.48341677\n",
      "Iteration 7140, loss = 0.48345129\n",
      "Iteration 7141, loss = 0.48333900\n",
      "Iteration 7142, loss = 0.48341058\n",
      "Iteration 7143, loss = 0.48346104\n",
      "Iteration 7144, loss = 0.48356713\n",
      "Iteration 7145, loss = 0.48326058\n",
      "Iteration 7146, loss = 0.48330087\n",
      "Iteration 7147, loss = 0.48346463\n",
      "Iteration 7148, loss = 0.48348543\n",
      "Iteration 7149, loss = 0.48320221\n",
      "Iteration 7150, loss = 0.48318788\n",
      "Iteration 7151, loss = 0.48331163\n",
      "Iteration 7152, loss = 0.48420249\n",
      "Iteration 7153, loss = 0.48453131\n",
      "Iteration 7154, loss = 0.48333821\n",
      "Iteration 7155, loss = 0.48287854\n",
      "Iteration 7156, loss = 0.48440835\n",
      "Iteration 7157, loss = 0.48605466\n",
      "Iteration 7158, loss = 0.48687099\n",
      "Iteration 7159, loss = 0.48516621\n",
      "Iteration 7160, loss = 0.48445669\n",
      "Iteration 7161, loss = 0.48318854\n",
      "Iteration 7162, loss = 0.48366325\n",
      "Iteration 7163, loss = 0.48413295\n",
      "Iteration 7164, loss = 0.48406301\n",
      "Iteration 7165, loss = 0.48502242\n",
      "Iteration 7166, loss = 0.48650269\n",
      "Iteration 7167, loss = 0.48690612\n",
      "Iteration 7168, loss = 0.48518119\n",
      "Iteration 7169, loss = 0.48312757\n",
      "Iteration 7170, loss = 0.48275952\n",
      "Iteration 7171, loss = 0.48443137\n",
      "Iteration 7172, loss = 0.48724299\n",
      "Iteration 7173, loss = 0.49023424\n",
      "Iteration 7174, loss = 0.49294863\n",
      "Iteration 7175, loss = 0.49305253\n",
      "Iteration 7176, loss = 0.48914115\n",
      "Iteration 7177, loss = 0.48531796\n",
      "Iteration 7178, loss = 0.48358964\n",
      "Iteration 7179, loss = 0.48362701\n",
      "Iteration 7180, loss = 0.48512656\n",
      "Iteration 7181, loss = 0.48645062\n",
      "Iteration 7182, loss = 0.48551264\n",
      "Iteration 7183, loss = 0.48376883\n",
      "Iteration 7184, loss = 0.48308499\n",
      "Iteration 7185, loss = 0.48302701\n",
      "Iteration 7186, loss = 0.48302004\n",
      "Iteration 7187, loss = 0.48431465\n",
      "Iteration 7188, loss = 0.48525545\n",
      "Iteration 7189, loss = 0.48545292\n",
      "Iteration 7190, loss = 0.48508002\n",
      "Iteration 7191, loss = 0.48471042\n",
      "Iteration 7192, loss = 0.48392451\n",
      "Iteration 7193, loss = 0.48373160\n",
      "Iteration 7194, loss = 0.48316285\n",
      "Iteration 7195, loss = 0.48327064\n",
      "Iteration 7196, loss = 0.48349137\n",
      "Iteration 7197, loss = 0.48365156\n",
      "Iteration 7198, loss = 0.48383534\n",
      "Iteration 7199, loss = 0.48419979\n",
      "Iteration 7200, loss = 0.48418422\n",
      "Iteration 7201, loss = 0.48429135\n",
      "Iteration 7202, loss = 0.48332794\n",
      "Iteration 7203, loss = 0.48364433\n",
      "Iteration 7204, loss = 0.48414711\n",
      "Iteration 7205, loss = 0.48483158\n",
      "Iteration 7206, loss = 0.48494519\n",
      "Iteration 7207, loss = 0.48401652\n",
      "Iteration 7208, loss = 0.48313689\n",
      "Iteration 7209, loss = 0.48294237\n",
      "Iteration 7210, loss = 0.48431858\n",
      "Iteration 7211, loss = 0.48412065\n",
      "Iteration 7212, loss = 0.48405938\n",
      "Iteration 7213, loss = 0.48338101\n",
      "Iteration 7214, loss = 0.48354452\n",
      "Iteration 7215, loss = 0.48329046\n",
      "Iteration 7216, loss = 0.48404335\n",
      "Iteration 7217, loss = 0.48398844\n",
      "Iteration 7218, loss = 0.48415847\n",
      "Iteration 7219, loss = 0.48356823\n",
      "Iteration 7220, loss = 0.48374442\n",
      "Iteration 7221, loss = 0.48375946\n",
      "Iteration 7222, loss = 0.48360726\n",
      "Iteration 7223, loss = 0.48343609\n",
      "Iteration 7224, loss = 0.48342315\n",
      "Iteration 7225, loss = 0.48313198\n",
      "Iteration 7226, loss = 0.48368813\n",
      "Iteration 7227, loss = 0.48378761\n",
      "Iteration 7228, loss = 0.48365260\n",
      "Iteration 7229, loss = 0.48332764\n",
      "Iteration 7230, loss = 0.48308544\n",
      "Iteration 7231, loss = 0.48320092\n",
      "Iteration 7232, loss = 0.48366070\n",
      "Iteration 7233, loss = 0.48409725\n",
      "Iteration 7234, loss = 0.48379597\n",
      "Iteration 7235, loss = 0.48366394\n",
      "Iteration 7236, loss = 0.48317559\n",
      "Iteration 7237, loss = 0.48317462\n",
      "Iteration 7238, loss = 0.48326970\n",
      "Iteration 7239, loss = 0.48338951\n",
      "Iteration 7240, loss = 0.48318612\n",
      "Iteration 7241, loss = 0.48398306\n",
      "Iteration 7242, loss = 0.48392587\n",
      "Iteration 7243, loss = 0.48387894\n",
      "Iteration 7244, loss = 0.48327331\n",
      "Iteration 7245, loss = 0.48324696\n",
      "Iteration 7246, loss = 0.48315945\n",
      "Iteration 7247, loss = 0.48329892\n",
      "Iteration 7248, loss = 0.48288179\n",
      "Iteration 7249, loss = 0.48336267\n",
      "Iteration 7250, loss = 0.48588907\n",
      "Iteration 7251, loss = 0.48698876\n",
      "Iteration 7252, loss = 0.48558581\n",
      "Iteration 7253, loss = 0.48419167\n",
      "Iteration 7254, loss = 0.48287660\n",
      "Iteration 7255, loss = 0.48320544\n",
      "Iteration 7256, loss = 0.48409404\n",
      "Iteration 7257, loss = 0.48531004\n",
      "Iteration 7258, loss = 0.48478849\n",
      "Iteration 7259, loss = 0.48403627\n",
      "Iteration 7260, loss = 0.48311336\n",
      "Iteration 7261, loss = 0.48305190\n",
      "Iteration 7262, loss = 0.48339497\n",
      "Iteration 7263, loss = 0.48300429\n",
      "Iteration 7264, loss = 0.48339352\n",
      "Iteration 7265, loss = 0.48350844\n",
      "Iteration 7266, loss = 0.48330743\n",
      "Iteration 7267, loss = 0.48416134\n",
      "Iteration 7268, loss = 0.48568697\n",
      "Iteration 7269, loss = 0.48649411\n",
      "Iteration 7270, loss = 0.48655326\n",
      "Iteration 7271, loss = 0.48614072\n",
      "Iteration 7272, loss = 0.48465757\n",
      "Iteration 7273, loss = 0.48316722\n",
      "Iteration 7274, loss = 0.48358922\n",
      "Iteration 7275, loss = 0.48343007\n",
      "Iteration 7276, loss = 0.48427196\n",
      "Iteration 7277, loss = 0.48452855\n",
      "Iteration 7278, loss = 0.48396902\n",
      "Iteration 7279, loss = 0.48308041\n",
      "Iteration 7280, loss = 0.48361755\n",
      "Iteration 7281, loss = 0.48315682\n",
      "Iteration 7282, loss = 0.48303821\n",
      "Iteration 7283, loss = 0.48308247\n",
      "Iteration 7284, loss = 0.48303849\n",
      "Iteration 7285, loss = 0.48297343\n",
      "Iteration 7286, loss = 0.48362815\n",
      "Iteration 7287, loss = 0.48385521\n",
      "Iteration 7288, loss = 0.48391848\n",
      "Iteration 7289, loss = 0.48325917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7290, loss = 0.48317823\n",
      "Iteration 7291, loss = 0.48301716\n",
      "Iteration 7292, loss = 0.48314655\n",
      "Iteration 7293, loss = 0.48304728\n",
      "Iteration 7294, loss = 0.48302214\n",
      "Iteration 7295, loss = 0.48304909\n",
      "Iteration 7296, loss = 0.48301404\n",
      "Iteration 7297, loss = 0.48302300\n",
      "Iteration 7298, loss = 0.48319727\n",
      "Iteration 7299, loss = 0.48313987\n",
      "Iteration 7300, loss = 0.48325423\n",
      "Iteration 7301, loss = 0.48309780\n",
      "Iteration 7302, loss = 0.48282837\n",
      "Iteration 7303, loss = 0.48412593\n",
      "Iteration 7304, loss = 0.48410848\n",
      "Iteration 7305, loss = 0.48362859\n",
      "Iteration 7306, loss = 0.48362537\n",
      "Iteration 7307, loss = 0.48319288\n",
      "Iteration 7308, loss = 0.48355321\n",
      "Iteration 7309, loss = 0.48261488\n",
      "Iteration 7310, loss = 0.48389899\n",
      "Iteration 7311, loss = 0.48458195\n",
      "Iteration 7312, loss = 0.48474426\n",
      "Iteration 7313, loss = 0.48390734\n",
      "Iteration 7314, loss = 0.48325895\n",
      "Iteration 7315, loss = 0.48279658\n",
      "Iteration 7316, loss = 0.48435340\n",
      "Iteration 7317, loss = 0.48521753\n",
      "Iteration 7318, loss = 0.48484009\n",
      "Iteration 7319, loss = 0.48365589\n",
      "Iteration 7320, loss = 0.48302529\n",
      "Iteration 7321, loss = 0.48320505\n",
      "Iteration 7322, loss = 0.48376268\n",
      "Iteration 7323, loss = 0.48444260\n",
      "Iteration 7324, loss = 0.48442805\n",
      "Iteration 7325, loss = 0.48389452\n",
      "Iteration 7326, loss = 0.48354149\n",
      "Iteration 7327, loss = 0.48306855\n",
      "Iteration 7328, loss = 0.48286845\n",
      "Iteration 7329, loss = 0.48298000\n",
      "Iteration 7330, loss = 0.48430999\n",
      "Iteration 7331, loss = 0.48415667\n",
      "Iteration 7332, loss = 0.48324664\n",
      "Iteration 7333, loss = 0.48248796\n",
      "Iteration 7334, loss = 0.48338477\n",
      "Iteration 7335, loss = 0.48567561\n",
      "Iteration 7336, loss = 0.48821035\n",
      "Iteration 7337, loss = 0.48879493\n",
      "Iteration 7338, loss = 0.48678868\n",
      "Iteration 7339, loss = 0.48443607\n",
      "Iteration 7340, loss = 0.48202683\n",
      "Iteration 7341, loss = 0.48456319\n",
      "Iteration 7342, loss = 0.48796748\n",
      "Iteration 7343, loss = 0.48980407\n",
      "Iteration 7344, loss = 0.48895727\n",
      "Iteration 7345, loss = 0.48618862\n",
      "Iteration 7346, loss = 0.48324064\n",
      "Iteration 7347, loss = 0.48237006\n",
      "Iteration 7348, loss = 0.48599656\n",
      "Iteration 7349, loss = 0.48845357\n",
      "Iteration 7350, loss = 0.49028346\n",
      "Iteration 7351, loss = 0.48917205\n",
      "Iteration 7352, loss = 0.48539866\n",
      "Iteration 7353, loss = 0.48298690\n",
      "Iteration 7354, loss = 0.48491791\n",
      "Iteration 7355, loss = 0.48517171\n",
      "Iteration 7356, loss = 0.48545137\n",
      "Iteration 7357, loss = 0.48510629\n",
      "Iteration 7358, loss = 0.48405284\n",
      "Iteration 7359, loss = 0.48342984\n",
      "Iteration 7360, loss = 0.48312122\n",
      "Iteration 7361, loss = 0.48293104\n",
      "Iteration 7362, loss = 0.48322190\n",
      "Iteration 7363, loss = 0.48358326\n",
      "Iteration 7364, loss = 0.48259144\n",
      "Iteration 7365, loss = 0.48380409\n",
      "Iteration 7366, loss = 0.48525582\n",
      "Iteration 7367, loss = 0.48505785\n",
      "Iteration 7368, loss = 0.48434959\n",
      "Iteration 7369, loss = 0.48332013\n",
      "Iteration 7370, loss = 0.48308380\n",
      "Iteration 7371, loss = 0.48305427\n",
      "Iteration 7372, loss = 0.48311700\n",
      "Iteration 7373, loss = 0.48302055\n",
      "Iteration 7374, loss = 0.48328464\n",
      "Iteration 7375, loss = 0.48296452\n",
      "Iteration 7376, loss = 0.48316404\n",
      "Iteration 7377, loss = 0.48366238\n",
      "Iteration 7378, loss = 0.48349973\n",
      "Iteration 7379, loss = 0.48307232\n",
      "Iteration 7380, loss = 0.48348634\n",
      "Iteration 7381, loss = 0.48320129\n",
      "Iteration 7382, loss = 0.48264900\n",
      "Iteration 7383, loss = 0.48332020\n",
      "Iteration 7384, loss = 0.48495404\n",
      "Iteration 7385, loss = 0.48680650\n",
      "Iteration 7386, loss = 0.48679437\n",
      "Iteration 7387, loss = 0.48470676\n",
      "Iteration 7388, loss = 0.48288149\n",
      "Iteration 7389, loss = 0.48392297\n",
      "Iteration 7390, loss = 0.48571210\n",
      "Iteration 7391, loss = 0.48566376\n",
      "Iteration 7392, loss = 0.48446131\n",
      "Iteration 7393, loss = 0.48338932\n",
      "Iteration 7394, loss = 0.48310350\n",
      "Iteration 7395, loss = 0.48366214\n",
      "Iteration 7396, loss = 0.48322930\n",
      "Iteration 7397, loss = 0.48325080\n",
      "Iteration 7398, loss = 0.48324304\n",
      "Iteration 7399, loss = 0.48304766\n",
      "Iteration 7400, loss = 0.48414567\n",
      "Iteration 7401, loss = 0.48464949\n",
      "Iteration 7402, loss = 0.48436275\n",
      "Iteration 7403, loss = 0.48394495\n",
      "Iteration 7404, loss = 0.48335641\n",
      "Iteration 7405, loss = 0.48344281\n",
      "Iteration 7406, loss = 0.48330897\n",
      "Iteration 7407, loss = 0.48338833\n",
      "Iteration 7408, loss = 0.48295265\n",
      "Iteration 7409, loss = 0.48316973\n",
      "Iteration 7410, loss = 0.48563226\n",
      "Iteration 7411, loss = 0.48748272\n",
      "Iteration 7412, loss = 0.48628765\n",
      "Iteration 7413, loss = 0.48428324\n",
      "Iteration 7414, loss = 0.48318835\n",
      "Iteration 7415, loss = 0.48343186\n",
      "Iteration 7416, loss = 0.48486590\n",
      "Iteration 7417, loss = 0.48603158\n",
      "Iteration 7418, loss = 0.48499722\n",
      "Iteration 7419, loss = 0.48379660\n",
      "Iteration 7420, loss = 0.48327815\n",
      "Iteration 7421, loss = 0.48392856\n",
      "Iteration 7422, loss = 0.48465151\n",
      "Iteration 7423, loss = 0.48533732\n",
      "Iteration 7424, loss = 0.48530804\n",
      "Iteration 7425, loss = 0.48452504\n",
      "Iteration 7426, loss = 0.48328131\n",
      "Iteration 7427, loss = 0.48258289\n",
      "Iteration 7428, loss = 0.48551490\n",
      "Iteration 7429, loss = 0.48654436\n",
      "Iteration 7430, loss = 0.48654564\n",
      "Iteration 7431, loss = 0.48596892\n",
      "Iteration 7432, loss = 0.48537696\n",
      "Iteration 7433, loss = 0.48396770\n",
      "Iteration 7434, loss = 0.48460490\n",
      "Iteration 7435, loss = 0.48357216\n",
      "Iteration 7436, loss = 0.48362827\n",
      "Iteration 7437, loss = 0.48344410\n",
      "Iteration 7438, loss = 0.48350223\n",
      "Iteration 7439, loss = 0.48354314\n",
      "Iteration 7440, loss = 0.48357089\n",
      "Iteration 7441, loss = 0.48350821\n",
      "Iteration 7442, loss = 0.48332859\n",
      "Iteration 7443, loss = 0.48323512\n",
      "Iteration 7444, loss = 0.48313722\n",
      "Iteration 7445, loss = 0.48313114\n",
      "Iteration 7446, loss = 0.48317919\n",
      "Iteration 7447, loss = 0.48319165\n",
      "Iteration 7448, loss = 0.48317067\n",
      "Iteration 7449, loss = 0.48320087\n",
      "Iteration 7450, loss = 0.48312328\n",
      "Iteration 7451, loss = 0.48273199\n",
      "Iteration 7452, loss = 0.48398468\n",
      "Iteration 7453, loss = 0.48397910\n",
      "Iteration 7454, loss = 0.48351174\n",
      "Iteration 7455, loss = 0.48298316\n",
      "Iteration 7456, loss = 0.48349123\n",
      "Iteration 7457, loss = 0.48462146\n",
      "Iteration 7458, loss = 0.48558120\n",
      "Iteration 7459, loss = 0.48513467\n",
      "Iteration 7460, loss = 0.48418260\n",
      "Iteration 7461, loss = 0.48310011\n",
      "Iteration 7462, loss = 0.48299658\n",
      "Iteration 7463, loss = 0.48350267\n",
      "Iteration 7464, loss = 0.48431951\n",
      "Iteration 7465, loss = 0.48377347\n",
      "Iteration 7466, loss = 0.48370013\n",
      "Iteration 7467, loss = 0.48297107\n",
      "Iteration 7468, loss = 0.48336935\n",
      "Iteration 7469, loss = 0.48378285\n",
      "Iteration 7470, loss = 0.48411945\n",
      "Iteration 7471, loss = 0.48400416\n",
      "Iteration 7472, loss = 0.48316908\n",
      "Iteration 7473, loss = 0.48240867\n",
      "Iteration 7474, loss = 0.48453844\n",
      "Iteration 7475, loss = 0.48517162\n",
      "Iteration 7476, loss = 0.48495331\n",
      "Iteration 7477, loss = 0.48384003\n",
      "Iteration 7478, loss = 0.48308254\n",
      "Iteration 7479, loss = 0.48289506\n",
      "Iteration 7480, loss = 0.48351058\n",
      "Iteration 7481, loss = 0.48417604\n",
      "Iteration 7482, loss = 0.48494243\n",
      "Iteration 7483, loss = 0.48482817\n",
      "Iteration 7484, loss = 0.48362469\n",
      "Iteration 7485, loss = 0.48348785\n",
      "Iteration 7486, loss = 0.48305003\n",
      "Iteration 7487, loss = 0.48336993\n",
      "Iteration 7488, loss = 0.48319023\n",
      "Iteration 7489, loss = 0.48287933\n",
      "Iteration 7490, loss = 0.48301873\n",
      "Iteration 7491, loss = 0.48411830\n",
      "Iteration 7492, loss = 0.48460187\n",
      "Iteration 7493, loss = 0.48445559\n",
      "Iteration 7494, loss = 0.48360529\n",
      "Iteration 7495, loss = 0.48251418\n",
      "Iteration 7496, loss = 0.48309560\n",
      "Iteration 7497, loss = 0.48599932\n",
      "Iteration 7498, loss = 0.48806169\n",
      "Iteration 7499, loss = 0.48647287\n",
      "Iteration 7500, loss = 0.48403318\n",
      "Iteration 7501, loss = 0.48258202\n",
      "Iteration 7502, loss = 0.48399985\n",
      "Iteration 7503, loss = 0.48512715\n",
      "Iteration 7504, loss = 0.48471515\n",
      "Iteration 7505, loss = 0.48345076\n",
      "Iteration 7506, loss = 0.48326110\n",
      "Iteration 7507, loss = 0.48302439\n",
      "Iteration 7508, loss = 0.48303183\n",
      "Iteration 7509, loss = 0.48303453\n",
      "Iteration 7510, loss = 0.48340906\n",
      "Iteration 7511, loss = 0.48311868\n",
      "Iteration 7512, loss = 0.48282507\n",
      "Iteration 7513, loss = 0.48280879\n",
      "Iteration 7514, loss = 0.48427359\n",
      "Iteration 7515, loss = 0.48717254\n",
      "Iteration 7516, loss = 0.48642170\n",
      "Iteration 7517, loss = 0.48347989\n",
      "Iteration 7518, loss = 0.48359379\n",
      "Iteration 7519, loss = 0.48417430\n",
      "Iteration 7520, loss = 0.48524334\n",
      "Iteration 7521, loss = 0.48543975\n",
      "Iteration 7522, loss = 0.48456208\n",
      "Iteration 7523, loss = 0.48467822\n",
      "Iteration 7524, loss = 0.48313842\n",
      "Iteration 7525, loss = 0.48314822\n",
      "Iteration 7526, loss = 0.48304980\n",
      "Iteration 7527, loss = 0.48302448\n",
      "Iteration 7528, loss = 0.48306186\n",
      "Iteration 7529, loss = 0.48294537\n",
      "Iteration 7530, loss = 0.48299862\n",
      "Iteration 7531, loss = 0.48321149\n",
      "Iteration 7532, loss = 0.48334320\n",
      "Iteration 7533, loss = 0.48314726\n",
      "Iteration 7534, loss = 0.48307606\n",
      "Iteration 7535, loss = 0.48307471\n",
      "Iteration 7536, loss = 0.48288909\n",
      "Iteration 7537, loss = 0.48291793\n",
      "Iteration 7538, loss = 0.48350678\n",
      "Iteration 7539, loss = 0.48457081\n",
      "Iteration 7540, loss = 0.48458638\n",
      "Iteration 7541, loss = 0.48400958\n",
      "Iteration 7542, loss = 0.48344318\n",
      "Iteration 7543, loss = 0.48348461\n",
      "Iteration 7544, loss = 0.48290166\n",
      "Iteration 7545, loss = 0.48295754\n",
      "Iteration 7546, loss = 0.48288817\n",
      "Iteration 7547, loss = 0.48291647\n",
      "Iteration 7548, loss = 0.48292523\n",
      "Iteration 7549, loss = 0.48318820\n",
      "Iteration 7550, loss = 0.48312013\n",
      "Iteration 7551, loss = 0.48288135\n",
      "Iteration 7552, loss = 0.48301500\n",
      "Iteration 7553, loss = 0.48289258\n",
      "Iteration 7554, loss = 0.48279603\n",
      "Iteration 7555, loss = 0.48292157\n",
      "Iteration 7556, loss = 0.48353248\n",
      "Iteration 7557, loss = 0.48405776\n",
      "Iteration 7558, loss = 0.48398204\n",
      "Iteration 7559, loss = 0.48379023\n",
      "Iteration 7560, loss = 0.48354615\n",
      "Iteration 7561, loss = 0.48277331\n",
      "Iteration 7562, loss = 0.48357127\n",
      "Iteration 7563, loss = 0.48458728\n",
      "Iteration 7564, loss = 0.48577891\n",
      "Iteration 7565, loss = 0.48622688\n",
      "Iteration 7566, loss = 0.48552615\n",
      "Iteration 7567, loss = 0.48392918\n",
      "Iteration 7568, loss = 0.48281056\n",
      "Iteration 7569, loss = 0.48261987\n",
      "Iteration 7570, loss = 0.48522199\n",
      "Iteration 7571, loss = 0.48641370\n",
      "Iteration 7572, loss = 0.48580501\n",
      "Iteration 7573, loss = 0.48498983\n",
      "Iteration 7574, loss = 0.48306365\n",
      "Iteration 7575, loss = 0.48285420\n",
      "Iteration 7576, loss = 0.48312262\n",
      "Iteration 7577, loss = 0.48354849\n",
      "Iteration 7578, loss = 0.48400118\n",
      "Iteration 7579, loss = 0.48427280\n",
      "Iteration 7580, loss = 0.48405704\n",
      "Iteration 7581, loss = 0.48324447\n",
      "Iteration 7582, loss = 0.48254319\n",
      "Iteration 7583, loss = 0.48463150\n",
      "Iteration 7584, loss = 0.48559415\n",
      "Iteration 7585, loss = 0.48617248\n",
      "Iteration 7586, loss = 0.48523274\n",
      "Iteration 7587, loss = 0.48460643\n",
      "Iteration 7588, loss = 0.48321510\n",
      "Iteration 7589, loss = 0.48307466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7590, loss = 0.48301262\n",
      "Iteration 7591, loss = 0.48299157\n",
      "Iteration 7592, loss = 0.48300582\n",
      "Iteration 7593, loss = 0.48409863\n",
      "Iteration 7594, loss = 0.48338403\n",
      "Iteration 7595, loss = 0.48290060\n",
      "Iteration 7596, loss = 0.48357929\n",
      "Iteration 7597, loss = 0.48349185\n",
      "Iteration 7598, loss = 0.48342372\n",
      "Iteration 7599, loss = 0.48421593\n",
      "Iteration 7600, loss = 0.48561605\n",
      "Iteration 7601, loss = 0.48634834\n",
      "Iteration 7602, loss = 0.48564182\n",
      "Iteration 7603, loss = 0.48419679\n",
      "Iteration 7604, loss = 0.48359642\n",
      "Iteration 7605, loss = 0.48305888\n",
      "Iteration 7606, loss = 0.48325376\n",
      "Iteration 7607, loss = 0.48329617\n",
      "Iteration 7608, loss = 0.48318751\n",
      "Iteration 7609, loss = 0.48324636\n",
      "Iteration 7610, loss = 0.48305870\n",
      "Iteration 7611, loss = 0.48307336\n",
      "Iteration 7612, loss = 0.48310465\n",
      "Iteration 7613, loss = 0.48321644\n",
      "Iteration 7614, loss = 0.48301725\n",
      "Iteration 7615, loss = 0.48311754\n",
      "Iteration 7616, loss = 0.48314365\n",
      "Iteration 7617, loss = 0.48300188\n",
      "Iteration 7618, loss = 0.48300583\n",
      "Iteration 7619, loss = 0.48296434\n",
      "Iteration 7620, loss = 0.48294385\n",
      "Iteration 7621, loss = 0.48328548\n",
      "Iteration 7622, loss = 0.48299390\n",
      "Iteration 7623, loss = 0.48297173\n",
      "Iteration 7624, loss = 0.48310573\n",
      "Iteration 7625, loss = 0.48351594\n",
      "Iteration 7626, loss = 0.48347552\n",
      "Iteration 7627, loss = 0.48275832\n",
      "Iteration 7628, loss = 0.48244689\n",
      "Iteration 7629, loss = 0.48461621\n",
      "Iteration 7630, loss = 0.48810828\n",
      "Iteration 7631, loss = 0.49175423\n",
      "Iteration 7632, loss = 0.49295351\n",
      "Iteration 7633, loss = 0.49109335\n",
      "Iteration 7634, loss = 0.48745253\n",
      "Iteration 7635, loss = 0.48503998\n",
      "Iteration 7636, loss = 0.48314685\n",
      "Iteration 7637, loss = 0.48275161\n",
      "Iteration 7638, loss = 0.48372149\n",
      "Iteration 7639, loss = 0.48491919\n",
      "Iteration 7640, loss = 0.48532771\n",
      "Iteration 7641, loss = 0.48448124\n",
      "Iteration 7642, loss = 0.48335775\n",
      "Iteration 7643, loss = 0.48275378\n",
      "Iteration 7644, loss = 0.48343348\n",
      "Iteration 7645, loss = 0.48412299\n",
      "Iteration 7646, loss = 0.48368152\n",
      "Iteration 7647, loss = 0.48346470\n",
      "Iteration 7648, loss = 0.48292526\n",
      "Iteration 7649, loss = 0.48298451\n",
      "Iteration 7650, loss = 0.48292742\n",
      "Iteration 7651, loss = 0.48295109\n",
      "Iteration 7652, loss = 0.48290540\n",
      "Iteration 7653, loss = 0.48289937\n",
      "Iteration 7654, loss = 0.48300816\n",
      "Iteration 7655, loss = 0.48318886\n",
      "Iteration 7656, loss = 0.48339577\n",
      "Iteration 7657, loss = 0.48387800\n",
      "Iteration 7658, loss = 0.48362479\n",
      "Iteration 7659, loss = 0.48337597\n",
      "Iteration 7660, loss = 0.48290820\n",
      "Iteration 7661, loss = 0.48320745\n",
      "Iteration 7662, loss = 0.48326474\n",
      "Iteration 7663, loss = 0.48311443\n",
      "Iteration 7664, loss = 0.48301522\n",
      "Iteration 7665, loss = 0.48290934\n",
      "Iteration 7666, loss = 0.48310409\n",
      "Iteration 7667, loss = 0.48317620\n",
      "Iteration 7668, loss = 0.48304484\n",
      "Iteration 7669, loss = 0.48322786\n",
      "Iteration 7670, loss = 0.48326785\n",
      "Iteration 7671, loss = 0.48325703\n",
      "Iteration 7672, loss = 0.48313269\n",
      "Iteration 7673, loss = 0.48301482\n",
      "Iteration 7674, loss = 0.48309795\n",
      "Iteration 7675, loss = 0.48296991\n",
      "Iteration 7676, loss = 0.48330777\n",
      "Iteration 7677, loss = 0.48325092\n",
      "Iteration 7678, loss = 0.48249460\n",
      "Iteration 7679, loss = 0.48334136\n",
      "Iteration 7680, loss = 0.48496603\n",
      "Iteration 7681, loss = 0.48567647\n",
      "Iteration 7682, loss = 0.48511962\n",
      "Iteration 7683, loss = 0.48428337\n",
      "Iteration 7684, loss = 0.48411361\n",
      "Iteration 7685, loss = 0.48335746\n",
      "Iteration 7686, loss = 0.48347097\n",
      "Iteration 7687, loss = 0.48331733\n",
      "Iteration 7688, loss = 0.48327228\n",
      "Iteration 7689, loss = 0.48305582\n",
      "Iteration 7690, loss = 0.48308127\n",
      "Iteration 7691, loss = 0.48339311\n",
      "Iteration 7692, loss = 0.48314272\n",
      "Iteration 7693, loss = 0.48294748\n",
      "Iteration 7694, loss = 0.48288611\n",
      "Iteration 7695, loss = 0.48290778\n",
      "Iteration 7696, loss = 0.48321986\n",
      "Iteration 7697, loss = 0.48285184\n",
      "Iteration 7698, loss = 0.48330430\n",
      "Iteration 7699, loss = 0.48478143\n",
      "Iteration 7700, loss = 0.48625933\n",
      "Iteration 7701, loss = 0.48661138\n",
      "Iteration 7702, loss = 0.48548797\n",
      "Iteration 7703, loss = 0.48413600\n",
      "Iteration 7704, loss = 0.48295607\n",
      "Iteration 7705, loss = 0.48304089\n",
      "Iteration 7706, loss = 0.48321368\n",
      "Iteration 7707, loss = 0.48335351\n",
      "Iteration 7708, loss = 0.48355960\n",
      "Iteration 7709, loss = 0.48316246\n",
      "Iteration 7710, loss = 0.48335843\n",
      "Iteration 7711, loss = 0.48358821\n",
      "Iteration 7712, loss = 0.48383787\n",
      "Iteration 7713, loss = 0.48392829\n",
      "Iteration 7714, loss = 0.48356081\n",
      "Iteration 7715, loss = 0.48297451\n",
      "Iteration 7716, loss = 0.48277071\n",
      "Iteration 7717, loss = 0.48326680\n",
      "Iteration 7718, loss = 0.48438112\n",
      "Iteration 7719, loss = 0.48575844\n",
      "Iteration 7720, loss = 0.48671248\n",
      "Iteration 7721, loss = 0.48586193\n",
      "Iteration 7722, loss = 0.48342133\n",
      "Iteration 7723, loss = 0.48320027\n",
      "Iteration 7724, loss = 0.48346512\n",
      "Iteration 7725, loss = 0.48372102\n",
      "Iteration 7726, loss = 0.48352295\n",
      "Iteration 7727, loss = 0.48322277\n",
      "Iteration 7728, loss = 0.48302180\n",
      "Iteration 7729, loss = 0.48300345\n",
      "Iteration 7730, loss = 0.48295329\n",
      "Iteration 7731, loss = 0.48312215\n",
      "Iteration 7732, loss = 0.48364679\n",
      "Iteration 7733, loss = 0.48420754\n",
      "Iteration 7734, loss = 0.48424786\n",
      "Iteration 7735, loss = 0.48370219\n",
      "Iteration 7736, loss = 0.48225454\n",
      "Iteration 7737, loss = 0.48318348\n",
      "Iteration 7738, loss = 0.48666234\n",
      "Iteration 7739, loss = 0.49202073\n",
      "Iteration 7740, loss = 0.49304557\n",
      "Iteration 7741, loss = 0.48968047\n",
      "Iteration 7742, loss = 0.48667012\n",
      "Iteration 7743, loss = 0.48427171\n",
      "Iteration 7744, loss = 0.48310840\n",
      "Iteration 7745, loss = 0.48285644\n",
      "Iteration 7746, loss = 0.48324656\n",
      "Iteration 7747, loss = 0.48344128\n",
      "Iteration 7748, loss = 0.48369988\n",
      "Iteration 7749, loss = 0.48328540\n",
      "Iteration 7750, loss = 0.48319836\n",
      "Iteration 7751, loss = 0.48301361\n",
      "Iteration 7752, loss = 0.48295983\n",
      "Iteration 7753, loss = 0.48306156\n",
      "Iteration 7754, loss = 0.48291621\n",
      "Iteration 7755, loss = 0.48356970\n",
      "Iteration 7756, loss = 0.48418774\n",
      "Iteration 7757, loss = 0.48403484\n",
      "Iteration 7758, loss = 0.48363876\n",
      "Iteration 7759, loss = 0.48291934\n",
      "Iteration 7760, loss = 0.48278415\n",
      "Iteration 7761, loss = 0.48295679\n",
      "Iteration 7762, loss = 0.48319480\n",
      "Iteration 7763, loss = 0.48379057\n",
      "Iteration 7764, loss = 0.48397263\n",
      "Iteration 7765, loss = 0.48303116\n",
      "Iteration 7766, loss = 0.48233886\n",
      "Iteration 7767, loss = 0.48393985\n",
      "Iteration 7768, loss = 0.48940662\n",
      "Iteration 7769, loss = 0.49291463\n",
      "Iteration 7770, loss = 0.49104291\n",
      "Iteration 7771, loss = 0.48562001\n",
      "Iteration 7772, loss = 0.48297980\n",
      "Iteration 7773, loss = 0.48419123\n",
      "Iteration 7774, loss = 0.48726049\n",
      "Iteration 7775, loss = 0.48897402\n",
      "Iteration 7776, loss = 0.48861449\n",
      "Iteration 7777, loss = 0.48691607\n",
      "Iteration 7778, loss = 0.48449594\n",
      "Iteration 7779, loss = 0.48290540\n",
      "Iteration 7780, loss = 0.48317632\n",
      "Iteration 7781, loss = 0.48515950\n",
      "Iteration 7782, loss = 0.48452325\n",
      "Iteration 7783, loss = 0.48350493\n",
      "Iteration 7784, loss = 0.48307341\n",
      "Iteration 7785, loss = 0.48376187\n",
      "Iteration 7786, loss = 0.48327117\n",
      "Iteration 7787, loss = 0.48272627\n",
      "Iteration 7788, loss = 0.48327419\n",
      "Iteration 7789, loss = 0.48469766\n",
      "Iteration 7790, loss = 0.48493976\n",
      "Iteration 7791, loss = 0.48467224\n",
      "Iteration 7792, loss = 0.48296744\n",
      "Iteration 7793, loss = 0.48315246\n",
      "Iteration 7794, loss = 0.48305998\n",
      "Iteration 7795, loss = 0.48308950\n",
      "Iteration 7796, loss = 0.48369268\n",
      "Iteration 7797, loss = 0.48337221\n",
      "Iteration 7798, loss = 0.48282762\n",
      "Iteration 7799, loss = 0.48370756\n",
      "Iteration 7800, loss = 0.48405497\n",
      "Iteration 7801, loss = 0.48436150\n",
      "Iteration 7802, loss = 0.48457381\n",
      "Iteration 7803, loss = 0.48404430\n",
      "Iteration 7804, loss = 0.48389824\n",
      "Iteration 7805, loss = 0.48305695\n",
      "Iteration 7806, loss = 0.48279607\n",
      "Iteration 7807, loss = 0.48380361\n",
      "Iteration 7808, loss = 0.48476357\n",
      "Iteration 7809, loss = 0.48527783\n",
      "Iteration 7810, loss = 0.48525262\n",
      "Iteration 7811, loss = 0.48471830\n",
      "Iteration 7812, loss = 0.48401532\n",
      "Iteration 7813, loss = 0.48347321\n",
      "Iteration 7814, loss = 0.48305435\n",
      "Iteration 7815, loss = 0.48296403\n",
      "Iteration 7816, loss = 0.48308022\n",
      "Iteration 7817, loss = 0.48293785\n",
      "Iteration 7818, loss = 0.48310416\n",
      "Iteration 7819, loss = 0.48311840\n",
      "Iteration 7820, loss = 0.48335751\n",
      "Iteration 7821, loss = 0.48413999\n",
      "Iteration 7822, loss = 0.48540652\n",
      "Iteration 7823, loss = 0.48786935\n",
      "Iteration 7824, loss = 0.48691857\n",
      "Iteration 7825, loss = 0.48477962\n",
      "Iteration 7826, loss = 0.48247976\n",
      "Iteration 7827, loss = 0.48371721\n",
      "Iteration 7828, loss = 0.48475692\n",
      "Iteration 7829, loss = 0.48624529\n",
      "Iteration 7830, loss = 0.48778030\n",
      "Iteration 7831, loss = 0.48650859\n",
      "Iteration 7832, loss = 0.48366926\n",
      "Iteration 7833, loss = 0.48295699\n",
      "Iteration 7834, loss = 0.48325193\n",
      "Iteration 7835, loss = 0.48427187\n",
      "Iteration 7836, loss = 0.48430275\n",
      "Iteration 7837, loss = 0.48364959\n",
      "Iteration 7838, loss = 0.48274625\n",
      "Iteration 7839, loss = 0.48314729\n",
      "Iteration 7840, loss = 0.48419682\n",
      "Iteration 7841, loss = 0.48445385\n",
      "Iteration 7842, loss = 0.48425327\n",
      "Iteration 7843, loss = 0.48426722\n",
      "Iteration 7844, loss = 0.48414173\n",
      "Iteration 7845, loss = 0.48396738\n",
      "Iteration 7846, loss = 0.48345809\n",
      "Iteration 7847, loss = 0.48329332\n",
      "Iteration 7848, loss = 0.48356267\n",
      "Iteration 7849, loss = 0.48392312\n",
      "Iteration 7850, loss = 0.48332055\n",
      "Iteration 7851, loss = 0.48364101\n",
      "Iteration 7852, loss = 0.48370349\n",
      "Iteration 7853, loss = 0.48523364\n",
      "Iteration 7854, loss = 0.48741131\n",
      "Iteration 7855, loss = 0.48850332\n",
      "Iteration 7856, loss = 0.48689355\n",
      "Iteration 7857, loss = 0.48537999\n",
      "Iteration 7858, loss = 0.48318112\n",
      "Iteration 7859, loss = 0.48325864\n",
      "Iteration 7860, loss = 0.48312063\n",
      "Iteration 7861, loss = 0.48314909\n",
      "Iteration 7862, loss = 0.48320019\n",
      "Iteration 7863, loss = 0.48319706\n",
      "Iteration 7864, loss = 0.48307172\n",
      "Iteration 7865, loss = 0.48327780\n",
      "Iteration 7866, loss = 0.48331705\n",
      "Iteration 7867, loss = 0.48291450\n",
      "Iteration 7868, loss = 0.48351272\n",
      "Iteration 7869, loss = 0.48400839\n",
      "Iteration 7870, loss = 0.48384880\n",
      "Iteration 7871, loss = 0.48394972\n",
      "Iteration 7872, loss = 0.48314170\n",
      "Iteration 7873, loss = 0.48329697\n",
      "Iteration 7874, loss = 0.48304085\n",
      "Iteration 7875, loss = 0.48449898\n",
      "Iteration 7876, loss = 0.48397172\n",
      "Iteration 7877, loss = 0.48332615\n",
      "Iteration 7878, loss = 0.48321172\n",
      "Iteration 7879, loss = 0.48321901\n",
      "Iteration 7880, loss = 0.48316864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7881, loss = 0.48312591\n",
      "Iteration 7882, loss = 0.48309527\n",
      "Iteration 7883, loss = 0.48309381\n",
      "Iteration 7884, loss = 0.48318983\n",
      "Iteration 7885, loss = 0.48322683\n",
      "Iteration 7886, loss = 0.48347867\n",
      "Iteration 7887, loss = 0.48336226\n",
      "Iteration 7888, loss = 0.48352724\n",
      "Iteration 7889, loss = 0.48298693\n",
      "Iteration 7890, loss = 0.48305312\n",
      "Iteration 7891, loss = 0.48325095\n",
      "Iteration 7892, loss = 0.48294266\n",
      "Iteration 7893, loss = 0.48286474\n",
      "Iteration 7894, loss = 0.48329518\n",
      "Iteration 7895, loss = 0.48593142\n",
      "Iteration 7896, loss = 0.48628325\n",
      "Iteration 7897, loss = 0.48522763\n",
      "Iteration 7898, loss = 0.48486184\n",
      "Iteration 7899, loss = 0.48377168\n",
      "Iteration 7900, loss = 0.48366228\n",
      "Iteration 7901, loss = 0.48469657\n",
      "Iteration 7902, loss = 0.48464545\n",
      "Iteration 7903, loss = 0.48400213\n",
      "Iteration 7904, loss = 0.48339413\n",
      "Iteration 7905, loss = 0.48341776\n",
      "Iteration 7906, loss = 0.48311484\n",
      "Iteration 7907, loss = 0.48308490\n",
      "Iteration 7908, loss = 0.48305066\n",
      "Iteration 7909, loss = 0.48395547\n",
      "Iteration 7910, loss = 0.48417495\n",
      "Iteration 7911, loss = 0.48380399\n",
      "Iteration 7912, loss = 0.48410261\n",
      "Iteration 7913, loss = 0.48469786\n",
      "Iteration 7914, loss = 0.48509814\n",
      "Iteration 7915, loss = 0.48455569\n",
      "Iteration 7916, loss = 0.48339298\n",
      "Iteration 7917, loss = 0.48268839\n",
      "Iteration 7918, loss = 0.48312545\n",
      "Iteration 7919, loss = 0.48495320\n",
      "Iteration 7920, loss = 0.48854733\n",
      "Iteration 7921, loss = 0.49120627\n",
      "Iteration 7922, loss = 0.48982016\n",
      "Iteration 7923, loss = 0.48657631\n",
      "Iteration 7924, loss = 0.48381467\n",
      "Iteration 7925, loss = 0.48349305\n",
      "Iteration 7926, loss = 0.48315074\n",
      "Iteration 7927, loss = 0.48338456\n",
      "Iteration 7928, loss = 0.48369442\n",
      "Iteration 7929, loss = 0.48344334\n",
      "Iteration 7930, loss = 0.48337296\n",
      "Iteration 7931, loss = 0.48295869\n",
      "Iteration 7932, loss = 0.48323101\n",
      "Iteration 7933, loss = 0.48364623\n",
      "Iteration 7934, loss = 0.48285701\n",
      "Iteration 7935, loss = 0.48316310\n",
      "Iteration 7936, loss = 0.48333696\n",
      "Iteration 7937, loss = 0.48356284\n",
      "Iteration 7938, loss = 0.48372257\n",
      "Iteration 7939, loss = 0.48363224\n",
      "Iteration 7940, loss = 0.48334487\n",
      "Iteration 7941, loss = 0.48333073\n",
      "Iteration 7942, loss = 0.48307495\n",
      "Iteration 7943, loss = 0.48316886\n",
      "Iteration 7944, loss = 0.48289614\n",
      "Iteration 7945, loss = 0.48291358\n",
      "Iteration 7946, loss = 0.48289309\n",
      "Iteration 7947, loss = 0.48309099\n",
      "Iteration 7948, loss = 0.48307079\n",
      "Iteration 7949, loss = 0.48300736\n",
      "Iteration 7950, loss = 0.48291535\n",
      "Iteration 7951, loss = 0.48281292\n",
      "Iteration 7952, loss = 0.48369103\n",
      "Iteration 7953, loss = 0.48388405\n",
      "Iteration 7954, loss = 0.48398670\n",
      "Iteration 7955, loss = 0.48467767\n",
      "Iteration 7956, loss = 0.48494982\n",
      "Iteration 7957, loss = 0.48459919\n",
      "Iteration 7958, loss = 0.48361865\n",
      "Iteration 7959, loss = 0.48304141\n",
      "Iteration 7960, loss = 0.48353642\n",
      "Iteration 7961, loss = 0.48279657\n",
      "Iteration 7962, loss = 0.48310411\n",
      "Iteration 7963, loss = 0.48330500\n",
      "Iteration 7964, loss = 0.48348569\n",
      "Iteration 7965, loss = 0.48342970\n",
      "Iteration 7966, loss = 0.48312291\n",
      "Iteration 7967, loss = 0.48264673\n",
      "Iteration 7968, loss = 0.48296442\n",
      "Iteration 7969, loss = 0.48429003\n",
      "Iteration 7970, loss = 0.48578349\n",
      "Iteration 7971, loss = 0.48579462\n",
      "Iteration 7972, loss = 0.48402583\n",
      "Iteration 7973, loss = 0.48381508\n",
      "Iteration 7974, loss = 0.48326636\n",
      "Iteration 7975, loss = 0.48361744\n",
      "Iteration 7976, loss = 0.48381236\n",
      "Iteration 7977, loss = 0.48373317\n",
      "Iteration 7978, loss = 0.48327428\n",
      "Iteration 7979, loss = 0.48276511\n",
      "Iteration 7980, loss = 0.48379412\n",
      "Iteration 7981, loss = 0.48381986\n",
      "Iteration 7982, loss = 0.48393653\n",
      "Iteration 7983, loss = 0.48359820\n",
      "Iteration 7984, loss = 0.48337784\n",
      "Iteration 7985, loss = 0.48295929\n",
      "Iteration 7986, loss = 0.48307495\n",
      "Iteration 7987, loss = 0.48293338\n",
      "Iteration 7988, loss = 0.48290175\n",
      "Iteration 7989, loss = 0.48296555\n",
      "Iteration 7990, loss = 0.48288730\n",
      "Iteration 7991, loss = 0.48400675\n",
      "Iteration 7992, loss = 0.48297200\n",
      "Iteration 7993, loss = 0.48301697\n",
      "Iteration 7994, loss = 0.48333071\n",
      "Iteration 7995, loss = 0.48329744\n",
      "Iteration 7996, loss = 0.48303204\n",
      "Iteration 7997, loss = 0.48287288\n",
      "Iteration 7998, loss = 0.48328281\n",
      "Iteration 7999, loss = 0.48331121\n",
      "Iteration 8000, loss = 0.48306010\n",
      "Iteration 8001, loss = 0.48285134\n",
      "Iteration 8002, loss = 0.48348157\n",
      "Iteration 8003, loss = 0.48293383\n",
      "Iteration 8004, loss = 0.48262772\n",
      "Iteration 8005, loss = 0.48354087\n",
      "Iteration 8006, loss = 0.48444851\n",
      "Iteration 8007, loss = 0.48474609\n",
      "Iteration 8008, loss = 0.48444535\n",
      "Iteration 8009, loss = 0.48374641\n",
      "Iteration 8010, loss = 0.48314078\n",
      "Iteration 8011, loss = 0.48293970\n",
      "Iteration 8012, loss = 0.48344833\n",
      "Iteration 8013, loss = 0.48347748\n",
      "Iteration 8014, loss = 0.48362370\n",
      "Iteration 8015, loss = 0.48291933\n",
      "Iteration 8016, loss = 0.48293141\n",
      "Iteration 8017, loss = 0.48305434\n",
      "Iteration 8018, loss = 0.48344036\n",
      "Iteration 8019, loss = 0.48381094\n",
      "Iteration 8020, loss = 0.48397203\n",
      "Iteration 8021, loss = 0.48460552\n",
      "Iteration 8022, loss = 0.48353371\n",
      "Iteration 8023, loss = 0.48241211\n",
      "Iteration 8024, loss = 0.48310829\n",
      "Iteration 8025, loss = 0.48652845\n",
      "Iteration 8026, loss = 0.48880347\n",
      "Iteration 8027, loss = 0.48791874\n",
      "Iteration 8028, loss = 0.48449030\n",
      "Iteration 8029, loss = 0.48223951\n",
      "Iteration 8030, loss = 0.48485747\n",
      "Iteration 8031, loss = 0.48871437\n",
      "Iteration 8032, loss = 0.49254180\n",
      "Iteration 8033, loss = 0.49196622\n",
      "Iteration 8034, loss = 0.48707462\n",
      "Iteration 8035, loss = 0.48389458\n",
      "Iteration 8036, loss = 0.48366199\n",
      "Iteration 8037, loss = 0.48517634\n",
      "Iteration 8038, loss = 0.48602502\n",
      "Iteration 8039, loss = 0.48604738\n",
      "Iteration 8040, loss = 0.48571506\n",
      "Iteration 8041, loss = 0.48524527\n",
      "Iteration 8042, loss = 0.48385010\n",
      "Iteration 8043, loss = 0.48360230\n",
      "Iteration 8044, loss = 0.48336497\n",
      "Iteration 8045, loss = 0.48365029\n",
      "Iteration 8046, loss = 0.48325455\n",
      "Iteration 8047, loss = 0.48396851\n",
      "Iteration 8048, loss = 0.48320627\n",
      "Iteration 8049, loss = 0.48294793\n",
      "Iteration 8050, loss = 0.48331554\n",
      "Iteration 8051, loss = 0.48360732\n",
      "Iteration 8052, loss = 0.48395568\n",
      "Iteration 8053, loss = 0.48449857\n",
      "Iteration 8054, loss = 0.48451995\n",
      "Iteration 8055, loss = 0.48370567\n",
      "Iteration 8056, loss = 0.48343675\n",
      "Iteration 8057, loss = 0.48301829\n",
      "Iteration 8058, loss = 0.48335618\n",
      "Iteration 8059, loss = 0.48350063\n",
      "Iteration 8060, loss = 0.48352030\n",
      "Iteration 8061, loss = 0.48317949\n",
      "Iteration 8062, loss = 0.48297360\n",
      "Iteration 8063, loss = 0.48417937\n",
      "Iteration 8064, loss = 0.48340321\n",
      "Iteration 8065, loss = 0.48349662\n",
      "Iteration 8066, loss = 0.48296801\n",
      "Iteration 8067, loss = 0.48333700\n",
      "Iteration 8068, loss = 0.48296519\n",
      "Iteration 8069, loss = 0.48390384\n",
      "Iteration 8070, loss = 0.48339809\n",
      "Iteration 8071, loss = 0.48351909\n",
      "Iteration 8072, loss = 0.48370442\n",
      "Iteration 8073, loss = 0.48355433\n",
      "Iteration 8074, loss = 0.48332065\n",
      "Iteration 8075, loss = 0.48298385\n",
      "Iteration 8076, loss = 0.48272916\n",
      "Iteration 8077, loss = 0.48319713\n",
      "Iteration 8078, loss = 0.48401727\n",
      "Iteration 8079, loss = 0.48528128\n",
      "Iteration 8080, loss = 0.48698074\n",
      "Iteration 8081, loss = 0.48631128\n",
      "Iteration 8082, loss = 0.48383761\n",
      "Iteration 8083, loss = 0.48366104\n",
      "Iteration 8084, loss = 0.48296151\n",
      "Iteration 8085, loss = 0.48394851\n",
      "Iteration 8086, loss = 0.48466388\n",
      "Iteration 8087, loss = 0.48496300\n",
      "Iteration 8088, loss = 0.48551916\n",
      "Iteration 8089, loss = 0.48519379\n",
      "Iteration 8090, loss = 0.48416962\n",
      "Iteration 8091, loss = 0.48318673\n",
      "Iteration 8092, loss = 0.48297251\n",
      "Iteration 8093, loss = 0.48311320\n",
      "Iteration 8094, loss = 0.48328071\n",
      "Iteration 8095, loss = 0.48345317\n",
      "Iteration 8096, loss = 0.48280601\n",
      "Iteration 8097, loss = 0.48285750\n",
      "Iteration 8098, loss = 0.48525919\n",
      "Iteration 8099, loss = 0.48588435\n",
      "Iteration 8100, loss = 0.48505286\n",
      "Iteration 8101, loss = 0.48373804\n",
      "Iteration 8102, loss = 0.48387488\n",
      "Iteration 8103, loss = 0.48318515\n",
      "Iteration 8104, loss = 0.48320357\n",
      "Iteration 8105, loss = 0.48316676\n",
      "Iteration 8106, loss = 0.48328299\n",
      "Iteration 8107, loss = 0.48299679\n",
      "Iteration 8108, loss = 0.48308340\n",
      "Iteration 8109, loss = 0.48305158\n",
      "Iteration 8110, loss = 0.48287011\n",
      "Iteration 8111, loss = 0.48305207\n",
      "Iteration 8112, loss = 0.48361481\n",
      "Iteration 8113, loss = 0.48355728\n",
      "Iteration 8114, loss = 0.48306169\n",
      "Iteration 8115, loss = 0.48294848\n",
      "Iteration 8116, loss = 0.48342187\n",
      "Iteration 8117, loss = 0.48299838\n",
      "Iteration 8118, loss = 0.48277780\n",
      "Iteration 8119, loss = 0.48315825\n",
      "Iteration 8120, loss = 0.48334901\n",
      "Iteration 8121, loss = 0.48348475\n",
      "Iteration 8122, loss = 0.48300244\n",
      "Iteration 8123, loss = 0.48315938\n",
      "Iteration 8124, loss = 0.48321889\n",
      "Iteration 8125, loss = 0.48316301\n",
      "Iteration 8126, loss = 0.48289106\n",
      "Iteration 8127, loss = 0.48305205\n",
      "Iteration 8128, loss = 0.48329456\n",
      "Iteration 8129, loss = 0.48363682\n",
      "Iteration 8130, loss = 0.48357672\n",
      "Iteration 8131, loss = 0.48301970\n",
      "Iteration 8132, loss = 0.48237022\n",
      "Iteration 8133, loss = 0.48393044\n",
      "Iteration 8134, loss = 0.48552955\n",
      "Iteration 8135, loss = 0.48676748\n",
      "Iteration 8136, loss = 0.48706873\n",
      "Iteration 8137, loss = 0.48564513\n",
      "Iteration 8138, loss = 0.48381956\n",
      "Iteration 8139, loss = 0.48283349\n",
      "Iteration 8140, loss = 0.48301621\n",
      "Iteration 8141, loss = 0.48620183\n",
      "Iteration 8142, loss = 0.48623882\n",
      "Iteration 8143, loss = 0.48503337\n",
      "Iteration 8144, loss = 0.48338512\n",
      "Iteration 8145, loss = 0.48297914\n",
      "Iteration 8146, loss = 0.48328932\n",
      "Iteration 8147, loss = 0.48289027\n",
      "Iteration 8148, loss = 0.48303709\n",
      "Iteration 8149, loss = 0.48326163\n",
      "Iteration 8150, loss = 0.48419634\n",
      "Iteration 8151, loss = 0.48451314\n",
      "Iteration 8152, loss = 0.48388252\n",
      "Iteration 8153, loss = 0.48297890\n",
      "Iteration 8154, loss = 0.48325025\n",
      "Iteration 8155, loss = 0.48438888\n",
      "Iteration 8156, loss = 0.48399892\n",
      "Iteration 8157, loss = 0.48312714\n",
      "Iteration 8158, loss = 0.48287168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8159, loss = 0.48291190\n",
      "Iteration 8160, loss = 0.48427630\n",
      "Iteration 8161, loss = 0.48494669\n",
      "Iteration 8162, loss = 0.48449973\n",
      "Iteration 8163, loss = 0.48324527\n",
      "Iteration 8164, loss = 0.48268590\n",
      "Iteration 8165, loss = 0.48361294\n",
      "Iteration 8166, loss = 0.48540057\n",
      "Iteration 8167, loss = 0.48522432\n",
      "Iteration 8168, loss = 0.48396309\n",
      "Iteration 8169, loss = 0.48415631\n",
      "Iteration 8170, loss = 0.48317041\n",
      "Iteration 8171, loss = 0.48309187\n",
      "Iteration 8172, loss = 0.48321450\n",
      "Iteration 8173, loss = 0.48303809\n",
      "Iteration 8174, loss = 0.48304940\n",
      "Iteration 8175, loss = 0.48318638\n",
      "Iteration 8176, loss = 0.48313908\n",
      "Iteration 8177, loss = 0.48305707\n",
      "Iteration 8178, loss = 0.48307888\n",
      "Iteration 8179, loss = 0.48328104\n",
      "Iteration 8180, loss = 0.48332550\n",
      "Iteration 8181, loss = 0.48301910\n",
      "Iteration 8182, loss = 0.48297950\n",
      "Iteration 8183, loss = 0.48294678\n",
      "Iteration 8184, loss = 0.48289369\n",
      "Iteration 8185, loss = 0.48330964\n",
      "Iteration 8186, loss = 0.48395036\n",
      "Iteration 8187, loss = 0.48426420\n",
      "Iteration 8188, loss = 0.48447552\n",
      "Iteration 8189, loss = 0.48428505\n",
      "Iteration 8190, loss = 0.48438176\n",
      "Iteration 8191, loss = 0.48428503\n",
      "Iteration 8192, loss = 0.48296480\n",
      "Iteration 8193, loss = 0.48274231\n",
      "Iteration 8194, loss = 0.48338217\n",
      "Iteration 8195, loss = 0.48393334\n",
      "Iteration 8196, loss = 0.48487613\n",
      "Iteration 8197, loss = 0.48468797\n",
      "Iteration 8198, loss = 0.48344348\n",
      "Iteration 8199, loss = 0.48270023\n",
      "Iteration 8200, loss = 0.48329691\n",
      "Iteration 8201, loss = 0.48499910\n",
      "Iteration 8202, loss = 0.48661362\n",
      "Iteration 8203, loss = 0.48481485\n",
      "Iteration 8204, loss = 0.48286909\n",
      "Iteration 8205, loss = 0.48364206\n",
      "Iteration 8206, loss = 0.48577546\n",
      "Iteration 8207, loss = 0.48830452\n",
      "Iteration 8208, loss = 0.48967271\n",
      "Iteration 8209, loss = 0.48939931\n",
      "Iteration 8210, loss = 0.48898150\n",
      "Iteration 8211, loss = 0.48792411\n",
      "Iteration 8212, loss = 0.48658501\n",
      "Iteration 8213, loss = 0.48638077\n",
      "Iteration 8214, loss = 0.48471998\n",
      "Iteration 8215, loss = 0.48384439\n",
      "Iteration 8216, loss = 0.48415122\n",
      "Iteration 8217, loss = 0.48305906\n",
      "Iteration 8218, loss = 0.48306076\n",
      "Iteration 8219, loss = 0.48328719\n",
      "Iteration 8220, loss = 0.48387775\n",
      "Iteration 8221, loss = 0.48354063\n",
      "Iteration 8222, loss = 0.48339429\n",
      "Iteration 8223, loss = 0.48304444\n",
      "Iteration 8224, loss = 0.48302314\n",
      "Iteration 8225, loss = 0.48318716\n",
      "Iteration 8226, loss = 0.48292352\n",
      "Iteration 8227, loss = 0.48309473\n",
      "Iteration 8228, loss = 0.48352480\n",
      "Iteration 8229, loss = 0.48410504\n",
      "Iteration 8230, loss = 0.48414220\n",
      "Iteration 8231, loss = 0.48357553\n",
      "Iteration 8232, loss = 0.48341553\n",
      "Iteration 8233, loss = 0.48307710\n",
      "Iteration 8234, loss = 0.48302386\n",
      "Iteration 8235, loss = 0.48330770\n",
      "Iteration 8236, loss = 0.48342038\n",
      "Iteration 8237, loss = 0.48389018\n",
      "Iteration 8238, loss = 0.48434648\n",
      "Iteration 8239, loss = 0.48487308\n",
      "Iteration 8240, loss = 0.48505307\n",
      "Iteration 8241, loss = 0.48442457\n",
      "Iteration 8242, loss = 0.48367576\n",
      "Iteration 8243, loss = 0.48314673\n",
      "Iteration 8244, loss = 0.48305578\n",
      "Iteration 8245, loss = 0.48303795\n",
      "Iteration 8246, loss = 0.48313904\n",
      "Iteration 8247, loss = 0.48292738\n",
      "Iteration 8248, loss = 0.48308743\n",
      "Iteration 8249, loss = 0.48377071\n",
      "Iteration 8250, loss = 0.48386176\n",
      "Iteration 8251, loss = 0.48363950\n",
      "Iteration 8252, loss = 0.48312267\n",
      "Iteration 8253, loss = 0.48347546\n",
      "Iteration 8254, loss = 0.48335536\n",
      "Iteration 8255, loss = 0.48332234\n",
      "Iteration 8256, loss = 0.48332598\n",
      "Iteration 8257, loss = 0.48315945\n",
      "Iteration 8258, loss = 0.48331548\n",
      "Iteration 8259, loss = 0.48312932\n",
      "Iteration 8260, loss = 0.48323016\n",
      "Iteration 8261, loss = 0.48314749\n",
      "Iteration 8262, loss = 0.48314774\n",
      "Iteration 8263, loss = 0.48328728\n",
      "Iteration 8264, loss = 0.48308227\n",
      "Iteration 8265, loss = 0.48307026\n",
      "Iteration 8266, loss = 0.48313398\n",
      "Iteration 8267, loss = 0.48307462\n",
      "Iteration 8268, loss = 0.48306946\n",
      "Iteration 8269, loss = 0.48318829\n",
      "Iteration 8270, loss = 0.48334801\n",
      "Iteration 8271, loss = 0.48373984\n",
      "Iteration 8272, loss = 0.48372015\n",
      "Iteration 8273, loss = 0.48317436\n",
      "Iteration 8274, loss = 0.48325403\n",
      "Iteration 8275, loss = 0.48330313\n",
      "Iteration 8276, loss = 0.48405673\n",
      "Iteration 8277, loss = 0.48561136\n",
      "Iteration 8278, loss = 0.48569377\n",
      "Iteration 8279, loss = 0.48431817\n",
      "Iteration 8280, loss = 0.48301419\n",
      "Iteration 8281, loss = 0.48437689\n",
      "Iteration 8282, loss = 0.48392698\n",
      "Iteration 8283, loss = 0.48368877\n",
      "Iteration 8284, loss = 0.48309858\n",
      "Iteration 8285, loss = 0.48298739\n",
      "Iteration 8286, loss = 0.48297933\n",
      "Iteration 8287, loss = 0.48372968\n",
      "Iteration 8288, loss = 0.48365950\n",
      "Iteration 8289, loss = 0.48391851\n",
      "Iteration 8290, loss = 0.48308749\n",
      "Iteration 8291, loss = 0.48308859\n",
      "Iteration 8292, loss = 0.48375398\n",
      "Iteration 8293, loss = 0.48334902\n",
      "Iteration 8294, loss = 0.48311594\n",
      "Iteration 8295, loss = 0.48298338\n",
      "Iteration 8296, loss = 0.48323501\n",
      "Iteration 8297, loss = 0.48359316\n",
      "Iteration 8298, loss = 0.48381877\n",
      "Iteration 8299, loss = 0.48389937\n",
      "Iteration 8300, loss = 0.48370532\n",
      "Iteration 8301, loss = 0.48353898\n",
      "Iteration 8302, loss = 0.48305646\n",
      "Iteration 8303, loss = 0.48302789\n",
      "Iteration 8304, loss = 0.48296527\n",
      "Iteration 8305, loss = 0.48296121\n",
      "Iteration 8306, loss = 0.48344589\n",
      "Iteration 8307, loss = 0.48308502\n",
      "Iteration 8308, loss = 0.48307116\n",
      "Iteration 8309, loss = 0.48311314\n",
      "Iteration 8310, loss = 0.48368555\n",
      "Iteration 8311, loss = 0.48457714\n",
      "Iteration 8312, loss = 0.48444633\n",
      "Iteration 8313, loss = 0.48334894\n",
      "Iteration 8314, loss = 0.48269555\n",
      "Iteration 8315, loss = 0.48308214\n",
      "Iteration 8316, loss = 0.48407729\n",
      "Iteration 8317, loss = 0.48537332\n",
      "Iteration 8318, loss = 0.48664602\n",
      "Iteration 8319, loss = 0.48790659\n",
      "Iteration 8320, loss = 0.48832254\n",
      "Iteration 8321, loss = 0.48662672\n",
      "Iteration 8322, loss = 0.48367076\n",
      "Iteration 8323, loss = 0.48336504\n",
      "Iteration 8324, loss = 0.48480581\n",
      "Iteration 8325, loss = 0.48757152\n",
      "Iteration 8326, loss = 0.48724868\n",
      "Iteration 8327, loss = 0.48529445\n",
      "Iteration 8328, loss = 0.48339496\n",
      "Iteration 8329, loss = 0.48291152\n",
      "Iteration 8330, loss = 0.48304869\n",
      "Iteration 8331, loss = 0.48419006\n",
      "Iteration 8332, loss = 0.48379390\n",
      "Iteration 8333, loss = 0.48347197\n",
      "Iteration 8334, loss = 0.48298227\n",
      "Iteration 8335, loss = 0.48338641\n",
      "Iteration 8336, loss = 0.48307593\n",
      "Iteration 8337, loss = 0.48305951\n",
      "Iteration 8338, loss = 0.48291880\n",
      "Iteration 8339, loss = 0.48272456\n",
      "Iteration 8340, loss = 0.48366628\n",
      "Iteration 8341, loss = 0.48381337\n",
      "Iteration 8342, loss = 0.48370712\n",
      "Iteration 8343, loss = 0.48339999\n",
      "Iteration 8344, loss = 0.48298044\n",
      "Iteration 8345, loss = 0.48271628\n",
      "Iteration 8346, loss = 0.48309945\n",
      "Iteration 8347, loss = 0.48405756\n",
      "Iteration 8348, loss = 0.48365636\n",
      "Iteration 8349, loss = 0.48349722\n",
      "Iteration 8350, loss = 0.48283613\n",
      "Iteration 8351, loss = 0.48285889\n",
      "Iteration 8352, loss = 0.48329170\n",
      "Iteration 8353, loss = 0.48292663\n",
      "Iteration 8354, loss = 0.48271473\n",
      "Iteration 8355, loss = 0.48304867\n",
      "Iteration 8356, loss = 0.48367623\n",
      "Iteration 8357, loss = 0.48461012\n",
      "Iteration 8358, loss = 0.48472289\n",
      "Iteration 8359, loss = 0.48357257\n",
      "Iteration 8360, loss = 0.48250107\n",
      "Iteration 8361, loss = 0.48382605\n",
      "Iteration 8362, loss = 0.48487618\n",
      "Iteration 8363, loss = 0.48482884\n",
      "Iteration 8364, loss = 0.48413683\n",
      "Iteration 8365, loss = 0.48291826\n",
      "Iteration 8366, loss = 0.48307648\n",
      "Iteration 8367, loss = 0.48401761\n",
      "Iteration 8368, loss = 0.48397138\n",
      "Iteration 8369, loss = 0.48334242\n",
      "Iteration 8370, loss = 0.48310884\n",
      "Iteration 8371, loss = 0.48300370\n",
      "Iteration 8372, loss = 0.48290451\n",
      "Iteration 8373, loss = 0.48292203\n",
      "Iteration 8374, loss = 0.48306916\n",
      "Iteration 8375, loss = 0.48287656\n",
      "Iteration 8376, loss = 0.48314519\n",
      "Iteration 8377, loss = 0.48321795\n",
      "Iteration 8378, loss = 0.48377210\n",
      "Iteration 8379, loss = 0.48418671\n",
      "Iteration 8380, loss = 0.48386272\n",
      "Iteration 8381, loss = 0.48325251\n",
      "Iteration 8382, loss = 0.48276305\n",
      "Iteration 8383, loss = 0.48288970\n",
      "Iteration 8384, loss = 0.48360242\n",
      "Iteration 8385, loss = 0.48450691\n",
      "Iteration 8386, loss = 0.48470587\n",
      "Iteration 8387, loss = 0.48424523\n",
      "Iteration 8388, loss = 0.48411695\n",
      "Iteration 8389, loss = 0.48367661\n",
      "Iteration 8390, loss = 0.48332711\n",
      "Iteration 8391, loss = 0.48283158\n",
      "Iteration 8392, loss = 0.48328689\n",
      "Iteration 8393, loss = 0.48332504\n",
      "Iteration 8394, loss = 0.48301894\n",
      "Iteration 8395, loss = 0.48282476\n",
      "Iteration 8396, loss = 0.48339035\n",
      "Iteration 8397, loss = 0.48406610\n",
      "Iteration 8398, loss = 0.48456713\n",
      "Iteration 8399, loss = 0.48478508\n",
      "Iteration 8400, loss = 0.48606505\n",
      "Iteration 8401, loss = 0.48568727\n",
      "Iteration 8402, loss = 0.48415256\n",
      "Iteration 8403, loss = 0.48302467\n",
      "Iteration 8404, loss = 0.48305200\n",
      "Iteration 8405, loss = 0.48390049\n",
      "Iteration 8406, loss = 0.48444406\n",
      "Iteration 8407, loss = 0.48337543\n",
      "Iteration 8408, loss = 0.48267746\n",
      "Iteration 8409, loss = 0.48432663\n",
      "Iteration 8410, loss = 0.48593999\n",
      "Iteration 8411, loss = 0.48606499\n",
      "Iteration 8412, loss = 0.48482795\n",
      "Iteration 8413, loss = 0.48301844\n",
      "Iteration 8414, loss = 0.48332717\n",
      "Iteration 8415, loss = 0.48355081\n",
      "Iteration 8416, loss = 0.48505803\n",
      "Iteration 8417, loss = 0.48617099\n",
      "Iteration 8418, loss = 0.48643537\n",
      "Iteration 8419, loss = 0.48616657\n",
      "Iteration 8420, loss = 0.48412506\n",
      "Iteration 8421, loss = 0.48327463\n",
      "Iteration 8422, loss = 0.48267350\n",
      "Iteration 8423, loss = 0.48385804\n",
      "Iteration 8424, loss = 0.48526113\n",
      "Iteration 8425, loss = 0.48562344\n",
      "Iteration 8426, loss = 0.48467914\n",
      "Iteration 8427, loss = 0.48287223\n",
      "Iteration 8428, loss = 0.48259167\n",
      "Iteration 8429, loss = 0.48386611\n",
      "Iteration 8430, loss = 0.48917595\n",
      "Iteration 8431, loss = 0.49574243\n",
      "Iteration 8432, loss = 0.49505466\n",
      "Iteration 8433, loss = 0.48890907\n",
      "Iteration 8434, loss = 0.48228099\n",
      "Iteration 8435, loss = 0.48508039\n",
      "Iteration 8436, loss = 0.48803751\n",
      "Iteration 8437, loss = 0.48857961\n",
      "Iteration 8438, loss = 0.48663711\n",
      "Iteration 8439, loss = 0.48427371\n",
      "Iteration 8440, loss = 0.48237435\n",
      "Iteration 8441, loss = 0.48294697\n",
      "Iteration 8442, loss = 0.48655831\n",
      "Iteration 8443, loss = 0.49248199\n",
      "Iteration 8444, loss = 0.49948447\n",
      "Iteration 8445, loss = 0.49665539\n",
      "Iteration 8446, loss = 0.48883513\n",
      "Iteration 8447, loss = 0.48356718\n",
      "Iteration 8448, loss = 0.48366978\n",
      "Iteration 8449, loss = 0.48574215\n",
      "Iteration 8450, loss = 0.48629902\n",
      "Iteration 8451, loss = 0.48565091\n",
      "Iteration 8452, loss = 0.48419376\n",
      "Iteration 8453, loss = 0.48383050\n",
      "Iteration 8454, loss = 0.48306967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8455, loss = 0.48366007\n",
      "Iteration 8456, loss = 0.48356370\n",
      "Iteration 8457, loss = 0.48346179\n",
      "Iteration 8458, loss = 0.48365732\n",
      "Iteration 8459, loss = 0.48347439\n",
      "Iteration 8460, loss = 0.48380751\n",
      "Iteration 8461, loss = 0.48408788\n",
      "Iteration 8462, loss = 0.48417104\n",
      "Iteration 8463, loss = 0.48381847\n",
      "Iteration 8464, loss = 0.48370889\n",
      "Iteration 8465, loss = 0.48311754\n",
      "Iteration 8466, loss = 0.48299633\n",
      "Iteration 8467, loss = 0.48378675\n",
      "Iteration 8468, loss = 0.48339090\n",
      "Iteration 8469, loss = 0.48322541\n",
      "Iteration 8470, loss = 0.48321473\n",
      "Iteration 8471, loss = 0.48310837\n",
      "Iteration 8472, loss = 0.48313765\n",
      "Iteration 8473, loss = 0.48319983\n",
      "Iteration 8474, loss = 0.48320396\n",
      "Iteration 8475, loss = 0.48318127\n",
      "Iteration 8476, loss = 0.48321418\n",
      "Iteration 8477, loss = 0.48318412\n",
      "Iteration 8478, loss = 0.48301042\n",
      "Iteration 8479, loss = 0.48381182\n",
      "Iteration 8480, loss = 0.48324657\n",
      "Iteration 8481, loss = 0.48313213\n",
      "Iteration 8482, loss = 0.48305375\n",
      "Iteration 8483, loss = 0.48303184\n",
      "Iteration 8484, loss = 0.48343105\n",
      "Iteration 8485, loss = 0.48317096\n",
      "Iteration 8486, loss = 0.48296018\n",
      "Iteration 8487, loss = 0.48307688\n",
      "Iteration 8488, loss = 0.48347445\n",
      "Iteration 8489, loss = 0.48358148\n",
      "Iteration 8490, loss = 0.48342831\n",
      "Iteration 8491, loss = 0.48309530\n",
      "Iteration 8492, loss = 0.48271453\n",
      "Iteration 8493, loss = 0.48393963\n",
      "Iteration 8494, loss = 0.48364872\n",
      "Iteration 8495, loss = 0.48314449\n",
      "Iteration 8496, loss = 0.48247927\n",
      "Iteration 8497, loss = 0.48328795\n",
      "Iteration 8498, loss = 0.48591144\n",
      "Iteration 8499, loss = 0.48757420\n",
      "Iteration 8500, loss = 0.48723879\n",
      "Iteration 8501, loss = 0.48545373\n",
      "Iteration 8502, loss = 0.48408151\n",
      "Iteration 8503, loss = 0.48286985\n",
      "Iteration 8504, loss = 0.48420088\n",
      "Iteration 8505, loss = 0.48322708\n",
      "Iteration 8506, loss = 0.48301082\n",
      "Iteration 8507, loss = 0.48274059\n",
      "Iteration 8508, loss = 0.48467318\n",
      "Iteration 8509, loss = 0.48421702\n",
      "Iteration 8510, loss = 0.48348607\n",
      "Iteration 8511, loss = 0.48306595\n",
      "Iteration 8512, loss = 0.48302531\n",
      "Iteration 8513, loss = 0.48303089\n",
      "Iteration 8514, loss = 0.48308533\n",
      "Iteration 8515, loss = 0.48297721\n",
      "Iteration 8516, loss = 0.48271628\n",
      "Iteration 8517, loss = 0.48343233\n",
      "Iteration 8518, loss = 0.48397929\n",
      "Iteration 8519, loss = 0.48431606\n",
      "Iteration 8520, loss = 0.48391208\n",
      "Iteration 8521, loss = 0.48345602\n",
      "Iteration 8522, loss = 0.48330502\n",
      "Iteration 8523, loss = 0.48300977\n",
      "Iteration 8524, loss = 0.48300188\n",
      "Iteration 8525, loss = 0.48301682\n",
      "Iteration 8526, loss = 0.48332298\n",
      "Iteration 8527, loss = 0.48331471\n",
      "Iteration 8528, loss = 0.48326558\n",
      "Iteration 8529, loss = 0.48307008\n",
      "Iteration 8530, loss = 0.48307613\n",
      "Iteration 8531, loss = 0.48307438\n",
      "Iteration 8532, loss = 0.48291245\n",
      "Iteration 8533, loss = 0.48293217\n",
      "Iteration 8534, loss = 0.48362013\n",
      "Iteration 8535, loss = 0.48419302\n",
      "Iteration 8536, loss = 0.48449003\n",
      "Iteration 8537, loss = 0.48518521\n",
      "Iteration 8538, loss = 0.48473751\n",
      "Iteration 8539, loss = 0.48392064\n",
      "Iteration 8540, loss = 0.48303601\n",
      "Iteration 8541, loss = 0.48322451\n",
      "Iteration 8542, loss = 0.48304011\n",
      "Iteration 8543, loss = 0.48316152\n",
      "Iteration 8544, loss = 0.48295400\n",
      "Iteration 8545, loss = 0.48320657\n",
      "Iteration 8546, loss = 0.48309143\n",
      "Iteration 8547, loss = 0.48294678\n",
      "Iteration 8548, loss = 0.48308619\n",
      "Iteration 8549, loss = 0.48355232\n",
      "Iteration 8550, loss = 0.48367173\n",
      "Iteration 8551, loss = 0.48332262\n",
      "Iteration 8552, loss = 0.48295042\n",
      "Iteration 8553, loss = 0.48293730\n",
      "Iteration 8554, loss = 0.48311724\n",
      "Iteration 8555, loss = 0.48374074\n",
      "Iteration 8556, loss = 0.48548726\n",
      "Iteration 8557, loss = 0.48585568\n",
      "Iteration 8558, loss = 0.48445160\n",
      "Iteration 8559, loss = 0.48266444\n",
      "Iteration 8560, loss = 0.48283630\n",
      "Iteration 8561, loss = 0.48496248\n",
      "Iteration 8562, loss = 0.48851240\n",
      "Iteration 8563, loss = 0.49048470\n",
      "Iteration 8564, loss = 0.48919859\n",
      "Iteration 8565, loss = 0.48611586\n",
      "Iteration 8566, loss = 0.48385062\n",
      "Iteration 8567, loss = 0.48327118\n",
      "Iteration 8568, loss = 0.48316308\n",
      "Iteration 8569, loss = 0.48344287\n",
      "Iteration 8570, loss = 0.48319762\n",
      "Iteration 8571, loss = 0.48281489\n",
      "Iteration 8572, loss = 0.48315834\n",
      "Iteration 8573, loss = 0.48304204\n",
      "Iteration 8574, loss = 0.48277199\n",
      "Iteration 8575, loss = 0.48255210\n",
      "Iteration 8576, loss = 0.48467411\n",
      "Iteration 8577, loss = 0.48569238\n",
      "Iteration 8578, loss = 0.48536388\n",
      "Iteration 8579, loss = 0.48412271\n",
      "Iteration 8580, loss = 0.48313888\n",
      "Iteration 8581, loss = 0.48290738\n",
      "Iteration 8582, loss = 0.48346737\n",
      "Iteration 8583, loss = 0.48336713\n",
      "Iteration 8584, loss = 0.48349513\n",
      "Iteration 8585, loss = 0.48292584\n",
      "Iteration 8586, loss = 0.48343799\n",
      "Iteration 8587, loss = 0.48341395\n",
      "Iteration 8588, loss = 0.48306334\n",
      "Iteration 8589, loss = 0.48291790\n",
      "Iteration 8590, loss = 0.48291598\n",
      "Iteration 8591, loss = 0.48303915\n",
      "Iteration 8592, loss = 0.48302309\n",
      "Iteration 8593, loss = 0.48291722\n",
      "Iteration 8594, loss = 0.48324987\n",
      "Iteration 8595, loss = 0.48286411\n",
      "Iteration 8596, loss = 0.48311066\n",
      "Iteration 8597, loss = 0.48306406\n",
      "Iteration 8598, loss = 0.48287657\n",
      "Iteration 8599, loss = 0.48293349\n",
      "Iteration 8600, loss = 0.48300617\n",
      "Iteration 8601, loss = 0.48305955\n",
      "Iteration 8602, loss = 0.48317053\n",
      "Iteration 8603, loss = 0.48307163\n",
      "Iteration 8604, loss = 0.48268173\n",
      "Iteration 8605, loss = 0.48329275\n",
      "Iteration 8606, loss = 0.48417530\n",
      "Iteration 8607, loss = 0.48648688\n",
      "Iteration 8608, loss = 0.48775823\n",
      "Iteration 8609, loss = 0.48758480\n",
      "Iteration 8610, loss = 0.48706670\n",
      "Iteration 8611, loss = 0.48526268\n",
      "Iteration 8612, loss = 0.48362352\n",
      "Iteration 8613, loss = 0.48272002\n",
      "Iteration 8614, loss = 0.48405706\n",
      "Iteration 8615, loss = 0.48519793\n",
      "Iteration 8616, loss = 0.48465265\n",
      "Iteration 8617, loss = 0.48315434\n",
      "Iteration 8618, loss = 0.48277434\n",
      "Iteration 8619, loss = 0.48322493\n",
      "Iteration 8620, loss = 0.48487681\n",
      "Iteration 8621, loss = 0.48647856\n",
      "Iteration 8622, loss = 0.48743224\n",
      "Iteration 8623, loss = 0.48662537\n",
      "Iteration 8624, loss = 0.48469889\n",
      "Iteration 8625, loss = 0.48336736\n",
      "Iteration 8626, loss = 0.48274881\n",
      "Iteration 8627, loss = 0.48312327\n",
      "Iteration 8628, loss = 0.48397559\n",
      "Iteration 8629, loss = 0.48419773\n",
      "Iteration 8630, loss = 0.48326398\n",
      "Iteration 8631, loss = 0.48300473\n",
      "Iteration 8632, loss = 0.48370927\n",
      "Iteration 8633, loss = 0.48418743\n",
      "Iteration 8634, loss = 0.48448785\n",
      "Iteration 8635, loss = 0.48473388\n",
      "Iteration 8636, loss = 0.48441688\n",
      "Iteration 8637, loss = 0.48436109\n",
      "Iteration 8638, loss = 0.48337251\n",
      "Iteration 8639, loss = 0.48309359\n",
      "Iteration 8640, loss = 0.48256977\n",
      "Iteration 8641, loss = 0.48259422\n",
      "Iteration 8642, loss = 0.48474378\n",
      "Iteration 8643, loss = 0.48946569\n",
      "Iteration 8644, loss = 0.49001781\n",
      "Iteration 8645, loss = 0.48684220\n",
      "Iteration 8646, loss = 0.48456452\n",
      "Iteration 8647, loss = 0.48294218\n",
      "Iteration 8648, loss = 0.48344869\n",
      "Iteration 8649, loss = 0.48333479\n",
      "Iteration 8650, loss = 0.48319540\n",
      "Iteration 8651, loss = 0.48308664\n",
      "Iteration 8652, loss = 0.48296945\n",
      "Iteration 8653, loss = 0.48369297\n",
      "Iteration 8654, loss = 0.48403750\n",
      "Iteration 8655, loss = 0.48358875\n",
      "Iteration 8656, loss = 0.48431298\n",
      "Iteration 8657, loss = 0.48455129\n",
      "Iteration 8658, loss = 0.48402122\n",
      "Iteration 8659, loss = 0.48304071\n",
      "Iteration 8660, loss = 0.48330045\n",
      "Iteration 8661, loss = 0.48310072\n",
      "Iteration 8662, loss = 0.48328195\n",
      "Iteration 8663, loss = 0.48387986\n",
      "Iteration 8664, loss = 0.48409517\n",
      "Iteration 8665, loss = 0.48371177\n",
      "Iteration 8666, loss = 0.48311339\n",
      "Iteration 8667, loss = 0.48275836\n",
      "Iteration 8668, loss = 0.48310187\n",
      "Iteration 8669, loss = 0.48351115\n",
      "Iteration 8670, loss = 0.48384523\n",
      "Iteration 8671, loss = 0.48369303\n",
      "Iteration 8672, loss = 0.48334197\n",
      "Iteration 8673, loss = 0.48305101\n",
      "Iteration 8674, loss = 0.48320222\n",
      "Iteration 8675, loss = 0.48355624\n",
      "Iteration 8676, loss = 0.48302852\n",
      "Iteration 8677, loss = 0.48301515\n",
      "Iteration 8678, loss = 0.48273030\n",
      "Iteration 8679, loss = 0.48276169\n",
      "Iteration 8680, loss = 0.48405840\n",
      "Iteration 8681, loss = 0.48545232\n",
      "Iteration 8682, loss = 0.48482426\n",
      "Iteration 8683, loss = 0.48355202\n",
      "Iteration 8684, loss = 0.48390240\n",
      "Iteration 8685, loss = 0.48392481\n",
      "Iteration 8686, loss = 0.48358051\n",
      "Iteration 8687, loss = 0.48272269\n",
      "Iteration 8688, loss = 0.48241802\n",
      "Iteration 8689, loss = 0.48494328\n",
      "Iteration 8690, loss = 0.48770220\n",
      "Iteration 8691, loss = 0.48681683\n",
      "Iteration 8692, loss = 0.48452552\n",
      "Iteration 8693, loss = 0.48319622\n",
      "Iteration 8694, loss = 0.48306523\n",
      "Iteration 8695, loss = 0.48400575\n",
      "Iteration 8696, loss = 0.48471962\n",
      "Iteration 8697, loss = 0.48527077\n",
      "Iteration 8698, loss = 0.48565148\n",
      "Iteration 8699, loss = 0.48477990\n",
      "Iteration 8700, loss = 0.48338831\n",
      "Iteration 8701, loss = 0.48258302\n",
      "Iteration 8702, loss = 0.48385265\n",
      "Iteration 8703, loss = 0.48499746\n",
      "Iteration 8704, loss = 0.48597450\n",
      "Iteration 8705, loss = 0.48555206\n",
      "Iteration 8706, loss = 0.48352676\n",
      "Iteration 8707, loss = 0.48311450\n",
      "Iteration 8708, loss = 0.48342984\n",
      "Iteration 8709, loss = 0.48528427\n",
      "Iteration 8710, loss = 0.48711099\n",
      "Iteration 8711, loss = 0.48777298\n",
      "Iteration 8712, loss = 0.48714338\n",
      "Iteration 8713, loss = 0.48538706\n",
      "Iteration 8714, loss = 0.48428497\n",
      "Iteration 8715, loss = 0.48338218\n",
      "Iteration 8716, loss = 0.48302997\n",
      "Iteration 8717, loss = 0.48282934\n",
      "Iteration 8718, loss = 0.48315449\n",
      "Iteration 8719, loss = 0.48377620\n",
      "Iteration 8720, loss = 0.48393698\n",
      "Iteration 8721, loss = 0.48378475\n",
      "Iteration 8722, loss = 0.48319873\n",
      "Iteration 8723, loss = 0.48308738\n",
      "Iteration 8724, loss = 0.48286214\n",
      "Iteration 8725, loss = 0.48293275\n",
      "Iteration 8726, loss = 0.48355497\n",
      "Iteration 8727, loss = 0.48352461\n",
      "Iteration 8728, loss = 0.48331406\n",
      "Iteration 8729, loss = 0.48336305\n",
      "Iteration 8730, loss = 0.48378118\n",
      "Iteration 8731, loss = 0.48370400\n",
      "Iteration 8732, loss = 0.48449308\n",
      "Iteration 8733, loss = 0.48493071\n",
      "Iteration 8734, loss = 0.48474368\n",
      "Iteration 8735, loss = 0.48353636\n",
      "Iteration 8736, loss = 0.48297371\n",
      "Iteration 8737, loss = 0.48428137\n",
      "Iteration 8738, loss = 0.48416153\n",
      "Iteration 8739, loss = 0.48367027\n",
      "Iteration 8740, loss = 0.48312490\n",
      "Iteration 8741, loss = 0.48312296\n",
      "Iteration 8742, loss = 0.48307997\n",
      "Iteration 8743, loss = 0.48387913\n",
      "Iteration 8744, loss = 0.48304391\n",
      "Iteration 8745, loss = 0.48397940\n",
      "Iteration 8746, loss = 0.48576034\n",
      "Iteration 8747, loss = 0.48648092\n",
      "Iteration 8748, loss = 0.48548718\n",
      "Iteration 8749, loss = 0.48325904\n",
      "Iteration 8750, loss = 0.48287586\n",
      "Iteration 8751, loss = 0.48376705\n",
      "Iteration 8752, loss = 0.48471837\n",
      "Iteration 8753, loss = 0.48513284\n",
      "Iteration 8754, loss = 0.48497971\n",
      "Iteration 8755, loss = 0.48537879\n",
      "Iteration 8756, loss = 0.48439326\n",
      "Iteration 8757, loss = 0.48416686\n",
      "Iteration 8758, loss = 0.48297777\n",
      "Iteration 8759, loss = 0.48296403\n",
      "Iteration 8760, loss = 0.48297783\n",
      "Iteration 8761, loss = 0.48301644\n",
      "Iteration 8762, loss = 0.48295834\n",
      "Iteration 8763, loss = 0.48296962\n",
      "Iteration 8764, loss = 0.48367787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8765, loss = 0.48349696\n",
      "Iteration 8766, loss = 0.48318565\n",
      "Iteration 8767, loss = 0.48326393\n",
      "Iteration 8768, loss = 0.48303419\n",
      "Iteration 8769, loss = 0.48293147\n",
      "Iteration 8770, loss = 0.48292069\n",
      "Iteration 8771, loss = 0.48295946\n",
      "Iteration 8772, loss = 0.48314779\n",
      "Iteration 8773, loss = 0.48290830\n",
      "Iteration 8774, loss = 0.48292037\n",
      "Iteration 8775, loss = 0.48319489\n",
      "Iteration 8776, loss = 0.48310020\n",
      "Iteration 8777, loss = 0.48311172\n",
      "Iteration 8778, loss = 0.48291163\n",
      "Iteration 8779, loss = 0.48280927\n",
      "Iteration 8780, loss = 0.48316372\n",
      "Iteration 8781, loss = 0.48338236\n",
      "Iteration 8782, loss = 0.48331689\n",
      "Iteration 8783, loss = 0.48301173\n",
      "Iteration 8784, loss = 0.48281646\n",
      "Iteration 8785, loss = 0.48282233\n",
      "Iteration 8786, loss = 0.48352083\n",
      "Iteration 8787, loss = 0.48355208\n",
      "Iteration 8788, loss = 0.48297188\n",
      "Iteration 8789, loss = 0.48260039\n",
      "Iteration 8790, loss = 0.48353676\n",
      "Iteration 8791, loss = 0.48484156\n",
      "Iteration 8792, loss = 0.48629923\n",
      "Iteration 8793, loss = 0.48731150\n",
      "Iteration 8794, loss = 0.48710989\n",
      "Iteration 8795, loss = 0.48588517\n",
      "Iteration 8796, loss = 0.48431920\n",
      "Iteration 8797, loss = 0.48253380\n",
      "Iteration 8798, loss = 0.48278686\n",
      "Iteration 8799, loss = 0.48663799\n",
      "Iteration 8800, loss = 0.48829656\n",
      "Iteration 8801, loss = 0.48837192\n",
      "Iteration 8802, loss = 0.48610918\n",
      "Iteration 8803, loss = 0.48293263\n",
      "Iteration 8804, loss = 0.48335885\n",
      "Iteration 8805, loss = 0.48582027\n",
      "Iteration 8806, loss = 0.48625378\n",
      "Iteration 8807, loss = 0.48482269\n",
      "Iteration 8808, loss = 0.48362868\n",
      "Iteration 8809, loss = 0.48282428\n",
      "Iteration 8810, loss = 0.48291378\n",
      "Iteration 8811, loss = 0.48372231\n",
      "Iteration 8812, loss = 0.48532003\n",
      "Iteration 8813, loss = 0.48501283\n",
      "Iteration 8814, loss = 0.48401620\n",
      "Iteration 8815, loss = 0.48329554\n",
      "Iteration 8816, loss = 0.48293806\n",
      "Iteration 8817, loss = 0.48313186\n",
      "Iteration 8818, loss = 0.48302109\n",
      "Iteration 8819, loss = 0.48276639\n",
      "Iteration 8820, loss = 0.48309094\n",
      "Iteration 8821, loss = 0.48420087\n",
      "Iteration 8822, loss = 0.48494294\n",
      "Iteration 8823, loss = 0.48456092\n",
      "Iteration 8824, loss = 0.48339007\n",
      "Iteration 8825, loss = 0.48313845\n",
      "Iteration 8826, loss = 0.48371469\n",
      "Iteration 8827, loss = 0.48397447\n",
      "Iteration 8828, loss = 0.48351486\n",
      "Iteration 8829, loss = 0.48304687\n",
      "Iteration 8830, loss = 0.48293211\n",
      "Iteration 8831, loss = 0.48330729\n",
      "Iteration 8832, loss = 0.48364285\n",
      "Iteration 8833, loss = 0.48372736\n",
      "Iteration 8834, loss = 0.48372427\n",
      "Iteration 8835, loss = 0.48355767\n",
      "Iteration 8836, loss = 0.48359659\n",
      "Iteration 8837, loss = 0.48353716\n",
      "Iteration 8838, loss = 0.48331035\n",
      "Iteration 8839, loss = 0.48293849\n",
      "Iteration 8840, loss = 0.48270697\n",
      "Iteration 8841, loss = 0.48432757\n",
      "Iteration 8842, loss = 0.49057419\n",
      "Iteration 8843, loss = 0.49201652\n",
      "Iteration 8844, loss = 0.48917182\n",
      "Iteration 8845, loss = 0.48567228\n",
      "Iteration 8846, loss = 0.48453793\n",
      "Iteration 8847, loss = 0.48298278\n",
      "Iteration 8848, loss = 0.48380948\n",
      "Iteration 8849, loss = 0.48506812\n",
      "Iteration 8850, loss = 0.48578250\n",
      "Iteration 8851, loss = 0.48570222\n",
      "Iteration 8852, loss = 0.48500161\n",
      "Iteration 8853, loss = 0.48402422\n",
      "Iteration 8854, loss = 0.48283903\n",
      "Iteration 8855, loss = 0.48344699\n",
      "Iteration 8856, loss = 0.48520502\n",
      "Iteration 8857, loss = 0.48571546\n",
      "Iteration 8858, loss = 0.48463862\n",
      "Iteration 8859, loss = 0.48376007\n",
      "Iteration 8860, loss = 0.48277451\n",
      "Iteration 8861, loss = 0.48308556\n",
      "Iteration 8862, loss = 0.48568323\n",
      "Iteration 8863, loss = 0.48531204\n",
      "Iteration 8864, loss = 0.48427175\n",
      "Iteration 8865, loss = 0.48371801\n",
      "Iteration 8866, loss = 0.48269839\n",
      "Iteration 8867, loss = 0.48425678\n",
      "Iteration 8868, loss = 0.48457503\n",
      "Iteration 8869, loss = 0.48490218\n",
      "Iteration 8870, loss = 0.48408365\n",
      "Iteration 8871, loss = 0.48443580\n",
      "Iteration 8872, loss = 0.48451074\n",
      "Iteration 8873, loss = 0.48397587\n",
      "Iteration 8874, loss = 0.48296607\n",
      "Iteration 8875, loss = 0.48351451\n",
      "Iteration 8876, loss = 0.48341699\n",
      "Iteration 8877, loss = 0.48363123\n",
      "Iteration 8878, loss = 0.48385449\n",
      "Iteration 8879, loss = 0.48415239\n",
      "Iteration 8880, loss = 0.48393627\n",
      "Iteration 8881, loss = 0.48324425\n",
      "Iteration 8882, loss = 0.48326000\n",
      "Iteration 8883, loss = 0.48329564\n",
      "Iteration 8884, loss = 0.48384872\n",
      "Iteration 8885, loss = 0.48369032\n",
      "Iteration 8886, loss = 0.48315750\n",
      "Iteration 8887, loss = 0.48299821\n",
      "Iteration 8888, loss = 0.48307624\n",
      "Iteration 8889, loss = 0.48360201\n",
      "Iteration 8890, loss = 0.48490798\n",
      "Iteration 8891, loss = 0.48509588\n",
      "Iteration 8892, loss = 0.48424325\n",
      "Iteration 8893, loss = 0.48299011\n",
      "Iteration 8894, loss = 0.48398399\n",
      "Iteration 8895, loss = 0.48393386\n",
      "Iteration 8896, loss = 0.48344793\n",
      "Iteration 8897, loss = 0.48298245\n",
      "Iteration 8898, loss = 0.48310746\n",
      "Iteration 8899, loss = 0.48363509\n",
      "Iteration 8900, loss = 0.48454130\n",
      "Iteration 8901, loss = 0.48527866\n",
      "Iteration 8902, loss = 0.48563437\n",
      "Iteration 8903, loss = 0.48572510\n",
      "Iteration 8904, loss = 0.48518579\n",
      "Iteration 8905, loss = 0.48469229\n",
      "Iteration 8906, loss = 0.48359414\n",
      "Iteration 8907, loss = 0.48335280\n",
      "Iteration 8908, loss = 0.48325151\n",
      "Iteration 8909, loss = 0.48334161\n",
      "Iteration 8910, loss = 0.48307005\n",
      "Iteration 8911, loss = 0.48317983\n",
      "Iteration 8912, loss = 0.48308244\n",
      "Iteration 8913, loss = 0.48322202\n",
      "Iteration 8914, loss = 0.48287283\n",
      "Iteration 8915, loss = 0.48289121\n",
      "Iteration 8916, loss = 0.48379070\n",
      "Iteration 8917, loss = 0.48418328\n",
      "Iteration 8918, loss = 0.48365708\n",
      "Iteration 8919, loss = 0.48277985\n",
      "Iteration 8920, loss = 0.48345206\n",
      "Iteration 8921, loss = 0.48372033\n",
      "Iteration 8922, loss = 0.48409313\n",
      "Iteration 8923, loss = 0.48409601\n",
      "Iteration 8924, loss = 0.48343857\n",
      "Iteration 8925, loss = 0.48305944\n",
      "Iteration 8926, loss = 0.48286078\n",
      "Iteration 8927, loss = 0.48352085\n",
      "Iteration 8928, loss = 0.48452272\n",
      "Iteration 8929, loss = 0.48576652\n",
      "Iteration 8930, loss = 0.48573022\n",
      "Iteration 8931, loss = 0.48458092\n",
      "Iteration 8932, loss = 0.48412212\n",
      "Iteration 8933, loss = 0.48342490\n",
      "Iteration 8934, loss = 0.48308296\n",
      "Iteration 8935, loss = 0.48311867\n",
      "Iteration 8936, loss = 0.48307963\n",
      "Iteration 8937, loss = 0.48327032\n",
      "Iteration 8938, loss = 0.48290583\n",
      "Iteration 8939, loss = 0.48304431\n",
      "Iteration 8940, loss = 0.48311111\n",
      "Iteration 8941, loss = 0.48414814\n",
      "Iteration 8942, loss = 0.48420131\n",
      "Iteration 8943, loss = 0.48389493\n",
      "Iteration 8944, loss = 0.48374289\n",
      "Iteration 8945, loss = 0.48311746\n",
      "Iteration 8946, loss = 0.48308525\n",
      "Iteration 8947, loss = 0.48335823\n",
      "Iteration 8948, loss = 0.48287068\n",
      "Iteration 8949, loss = 0.48268892\n",
      "Iteration 8950, loss = 0.48330730\n",
      "Iteration 8951, loss = 0.48435363\n",
      "Iteration 8952, loss = 0.48448977\n",
      "Iteration 8953, loss = 0.48367610\n",
      "Iteration 8954, loss = 0.48271390\n",
      "Iteration 8955, loss = 0.48342725\n",
      "Iteration 8956, loss = 0.48424393\n",
      "Iteration 8957, loss = 0.48431074\n",
      "Iteration 8958, loss = 0.48403574\n",
      "Iteration 8959, loss = 0.48335709\n",
      "Iteration 8960, loss = 0.48307180\n",
      "Iteration 8961, loss = 0.48278641\n",
      "Iteration 8962, loss = 0.48363904\n",
      "Iteration 8963, loss = 0.48349139\n",
      "Iteration 8964, loss = 0.48352430\n",
      "Iteration 8965, loss = 0.48410519\n",
      "Iteration 8966, loss = 0.48382207\n",
      "Iteration 8967, loss = 0.48304018\n",
      "Iteration 8968, loss = 0.48281842\n",
      "Iteration 8969, loss = 0.48369414\n",
      "Iteration 8970, loss = 0.48528006\n",
      "Iteration 8971, loss = 0.48396115\n",
      "Iteration 8972, loss = 0.48283333\n",
      "Iteration 8973, loss = 0.48377869\n",
      "Iteration 8974, loss = 0.48409699\n",
      "Iteration 8975, loss = 0.48415126\n",
      "Iteration 8976, loss = 0.48375511\n",
      "Iteration 8977, loss = 0.48336014\n",
      "Iteration 8978, loss = 0.48333657\n",
      "Iteration 8979, loss = 0.48292216\n",
      "Iteration 8980, loss = 0.48303104\n",
      "Iteration 8981, loss = 0.48277943\n",
      "Iteration 8982, loss = 0.48385559\n",
      "Iteration 8983, loss = 0.48453391\n",
      "Iteration 8984, loss = 0.48419113\n",
      "Iteration 8985, loss = 0.48305884\n",
      "Iteration 8986, loss = 0.48240522\n",
      "Iteration 8987, loss = 0.48499234\n",
      "Iteration 8988, loss = 0.48710565\n",
      "Iteration 8989, loss = 0.48632910\n",
      "Iteration 8990, loss = 0.48461127\n",
      "Iteration 8991, loss = 0.48287120\n",
      "Iteration 8992, loss = 0.48323134\n",
      "Iteration 8993, loss = 0.48463941\n",
      "Iteration 8994, loss = 0.48595010\n",
      "Iteration 8995, loss = 0.48682806\n",
      "Iteration 8996, loss = 0.48698393\n",
      "Iteration 8997, loss = 0.48628547\n",
      "Iteration 8998, loss = 0.48499017\n",
      "Iteration 8999, loss = 0.48430072\n",
      "Iteration 9000, loss = 0.48400297\n",
      "Iteration 9001, loss = 0.48367805\n",
      "Iteration 9002, loss = 0.48358116\n",
      "Iteration 9003, loss = 0.48286452\n",
      "Iteration 9004, loss = 0.48365442\n",
      "Iteration 9005, loss = 0.48416079\n",
      "Iteration 9006, loss = 0.48423962\n",
      "Iteration 9007, loss = 0.48381927\n",
      "Iteration 9008, loss = 0.48321997\n",
      "Iteration 9009, loss = 0.48315335\n",
      "Iteration 9010, loss = 0.48301385\n",
      "Iteration 9011, loss = 0.48379057\n",
      "Iteration 9012, loss = 0.48390944\n",
      "Iteration 9013, loss = 0.48373719\n",
      "Iteration 9014, loss = 0.48496671\n",
      "Iteration 9015, loss = 0.48529896\n",
      "Iteration 9016, loss = 0.48470405\n",
      "Iteration 9017, loss = 0.48393976\n",
      "Iteration 9018, loss = 0.48351722\n",
      "Iteration 9019, loss = 0.48313808\n",
      "Iteration 9020, loss = 0.48305779\n",
      "Iteration 9021, loss = 0.48316490\n",
      "Iteration 9022, loss = 0.48313837\n",
      "Iteration 9023, loss = 0.48382929\n",
      "Iteration 9024, loss = 0.48412230\n",
      "Iteration 9025, loss = 0.48310319\n",
      "Iteration 9026, loss = 0.48338099\n",
      "Iteration 9027, loss = 0.48394392\n",
      "Iteration 9028, loss = 0.48440210\n",
      "Iteration 9029, loss = 0.48443924\n",
      "Iteration 9030, loss = 0.48447108\n",
      "Iteration 9031, loss = 0.48391191\n",
      "Iteration 9032, loss = 0.48356228\n",
      "Iteration 9033, loss = 0.48304403\n",
      "Iteration 9034, loss = 0.48259684\n",
      "Iteration 9035, loss = 0.48363810\n",
      "Iteration 9036, loss = 0.48660171\n",
      "Iteration 9037, loss = 0.48634251\n",
      "Iteration 9038, loss = 0.48453943\n",
      "Iteration 9039, loss = 0.48366570\n",
      "Iteration 9040, loss = 0.48358410\n",
      "Iteration 9041, loss = 0.48303327\n",
      "Iteration 9042, loss = 0.48293087\n",
      "Iteration 9043, loss = 0.48292045\n",
      "Iteration 9044, loss = 0.48297342\n",
      "Iteration 9045, loss = 0.48348133\n",
      "Iteration 9046, loss = 0.48382520\n",
      "Iteration 9047, loss = 0.48398655\n",
      "Iteration 9048, loss = 0.48400290\n",
      "Iteration 9049, loss = 0.48354148\n",
      "Iteration 9050, loss = 0.48282466\n",
      "Iteration 9051, loss = 0.48330848\n",
      "Iteration 9052, loss = 0.48331239\n",
      "Iteration 9053, loss = 0.48308418\n",
      "Iteration 9054, loss = 0.48280775\n",
      "Iteration 9055, loss = 0.48302958\n",
      "Iteration 9056, loss = 0.48338620\n",
      "Iteration 9057, loss = 0.48371499\n",
      "Iteration 9058, loss = 0.48404638\n",
      "Iteration 9059, loss = 0.48481396\n",
      "Iteration 9060, loss = 0.48654814\n",
      "Iteration 9061, loss = 0.48704930\n",
      "Iteration 9062, loss = 0.48558101\n",
      "Iteration 9063, loss = 0.48391571\n",
      "Iteration 9064, loss = 0.48370937\n",
      "Iteration 9065, loss = 0.48316500\n",
      "Iteration 9066, loss = 0.48338588\n",
      "Iteration 9067, loss = 0.48319364\n",
      "Iteration 9068, loss = 0.48284808\n",
      "Iteration 9069, loss = 0.48271136\n",
      "Iteration 9070, loss = 0.48324686\n",
      "Iteration 9071, loss = 0.48452751\n",
      "Iteration 9072, loss = 0.48488593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9073, loss = 0.48388930\n",
      "Iteration 9074, loss = 0.48288670\n",
      "Iteration 9075, loss = 0.48322818\n",
      "Iteration 9076, loss = 0.48513936\n",
      "Iteration 9077, loss = 0.48605455\n",
      "Iteration 9078, loss = 0.48541008\n",
      "Iteration 9079, loss = 0.48370090\n",
      "Iteration 9080, loss = 0.48279168\n",
      "Iteration 9081, loss = 0.48312449\n",
      "Iteration 9082, loss = 0.48515504\n",
      "Iteration 9083, loss = 0.48542214\n",
      "Training loss did not improve more than tol=0.000100 for 2000 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.63046817\n",
      "Iteration 2, loss = 0.63016229\n",
      "Iteration 3, loss = 0.62983338\n",
      "Iteration 4, loss = 0.62957536\n",
      "Iteration 5, loss = 0.62941379\n",
      "Iteration 6, loss = 0.62917812\n",
      "Iteration 7, loss = 0.62894204\n",
      "Iteration 8, loss = 0.62872041\n",
      "Iteration 9, loss = 0.62853029\n",
      "Iteration 10, loss = 0.62837267\n",
      "Iteration 11, loss = 0.62821490\n",
      "Iteration 12, loss = 0.62804703\n",
      "Iteration 13, loss = 0.62785229\n",
      "Iteration 14, loss = 0.62768400\n",
      "Iteration 15, loss = 0.62751244\n",
      "Iteration 16, loss = 0.62730662\n",
      "Iteration 17, loss = 0.62713777\n",
      "Iteration 18, loss = 0.62696627\n",
      "Iteration 19, loss = 0.62680945\n",
      "Iteration 20, loss = 0.62663014\n",
      "Iteration 21, loss = 0.62648167\n",
      "Iteration 22, loss = 0.62632498\n",
      "Iteration 23, loss = 0.62616787\n",
      "Iteration 24, loss = 0.62602920\n",
      "Iteration 25, loss = 0.62586937\n",
      "Iteration 26, loss = 0.62571953\n",
      "Iteration 27, loss = 0.62557499\n",
      "Iteration 28, loss = 0.62543726\n",
      "Iteration 29, loss = 0.62530200\n",
      "Iteration 30, loss = 0.62515619\n",
      "Iteration 31, loss = 0.62502498\n",
      "Iteration 32, loss = 0.62487945\n",
      "Iteration 33, loss = 0.62469380\n",
      "Iteration 34, loss = 0.62456136\n",
      "Iteration 35, loss = 0.62444285\n",
      "Iteration 36, loss = 0.62432293\n",
      "Iteration 37, loss = 0.62417903\n",
      "Iteration 38, loss = 0.62404840\n",
      "Iteration 39, loss = 0.62391574\n",
      "Iteration 40, loss = 0.62377722\n",
      "Iteration 41, loss = 0.62367847\n",
      "Iteration 42, loss = 0.62354067\n",
      "Iteration 43, loss = 0.62342326\n",
      "Iteration 44, loss = 0.62333657\n",
      "Iteration 45, loss = 0.62321008\n",
      "Iteration 46, loss = 0.62308369\n",
      "Iteration 47, loss = 0.62297716\n",
      "Iteration 48, loss = 0.62284780\n",
      "Iteration 49, loss = 0.62274080\n",
      "Iteration 50, loss = 0.62262648\n",
      "Iteration 51, loss = 0.62250899\n",
      "Iteration 52, loss = 0.62240228\n",
      "Iteration 53, loss = 0.62229652\n",
      "Iteration 54, loss = 0.62218221\n",
      "Iteration 55, loss = 0.62206480\n",
      "Iteration 56, loss = 0.62194389\n",
      "Iteration 57, loss = 0.62184108\n",
      "Iteration 58, loss = 0.62174201\n",
      "Iteration 59, loss = 0.62163412\n",
      "Iteration 60, loss = 0.62152647\n",
      "Iteration 61, loss = 0.62143429\n",
      "Iteration 62, loss = 0.62133421\n",
      "Iteration 63, loss = 0.62121976\n",
      "Iteration 64, loss = 0.62110039\n",
      "Iteration 65, loss = 0.62100188\n",
      "Iteration 66, loss = 0.62088962\n",
      "Iteration 67, loss = 0.62080206\n",
      "Iteration 68, loss = 0.62069989\n",
      "Iteration 69, loss = 0.62057904\n",
      "Iteration 70, loss = 0.62046863\n",
      "Iteration 71, loss = 0.62037781\n",
      "Iteration 72, loss = 0.62026623\n",
      "Iteration 73, loss = 0.62015694\n",
      "Iteration 74, loss = 0.62014252\n",
      "Iteration 75, loss = 0.61994656\n",
      "Iteration 76, loss = 0.61984422\n",
      "Iteration 77, loss = 0.61978538\n",
      "Iteration 78, loss = 0.61968063\n",
      "Iteration 79, loss = 0.61956878\n",
      "Iteration 80, loss = 0.61948121\n",
      "Iteration 81, loss = 0.61940115\n",
      "Iteration 82, loss = 0.61927839\n",
      "Iteration 83, loss = 0.61913699\n",
      "Iteration 84, loss = 0.61902760\n",
      "Iteration 85, loss = 0.61895004\n",
      "Iteration 86, loss = 0.61882422\n",
      "Iteration 87, loss = 0.61870644\n",
      "Iteration 88, loss = 0.61862250\n",
      "Iteration 89, loss = 0.61846225\n",
      "Iteration 90, loss = 0.61838474\n",
      "Iteration 91, loss = 0.61823583\n",
      "Iteration 92, loss = 0.61808723\n",
      "Iteration 93, loss = 0.61799614\n",
      "Iteration 94, loss = 0.61784475\n",
      "Iteration 95, loss = 0.61769580\n",
      "Iteration 96, loss = 0.61756021\n",
      "Iteration 97, loss = 0.61743533\n",
      "Iteration 98, loss = 0.61728020\n",
      "Iteration 99, loss = 0.61711949\n",
      "Iteration 100, loss = 0.61696796\n",
      "Iteration 101, loss = 0.61687055\n",
      "Iteration 102, loss = 0.61665422\n",
      "Iteration 103, loss = 0.61650717\n",
      "Iteration 104, loss = 0.61644954\n",
      "Iteration 105, loss = 0.61629430\n",
      "Iteration 106, loss = 0.61590984\n",
      "Iteration 107, loss = 0.61575587\n",
      "Iteration 108, loss = 0.61564350\n",
      "Iteration 109, loss = 0.61546426\n",
      "Iteration 110, loss = 0.61531235\n",
      "Iteration 111, loss = 0.61488710\n",
      "Iteration 112, loss = 0.61484321\n",
      "Iteration 113, loss = 0.61475450\n",
      "Iteration 114, loss = 0.61425545\n",
      "Iteration 115, loss = 0.61407051\n",
      "Iteration 116, loss = 0.61391873\n",
      "Iteration 117, loss = 0.61379769\n",
      "Iteration 118, loss = 0.61361931\n",
      "Iteration 119, loss = 0.61338470\n",
      "Iteration 120, loss = 0.61298016\n",
      "Iteration 121, loss = 0.61253070\n",
      "Iteration 122, loss = 0.61240051\n",
      "Iteration 123, loss = 0.61206895\n",
      "Iteration 124, loss = 0.61153779\n",
      "Iteration 125, loss = 0.61130270\n",
      "Iteration 126, loss = 0.61098277\n",
      "Iteration 127, loss = 0.61061579\n",
      "Iteration 128, loss = 0.61014143\n",
      "Iteration 129, loss = 0.60972957\n",
      "Iteration 130, loss = 0.60929815\n",
      "Iteration 131, loss = 0.60885964\n",
      "Iteration 132, loss = 0.60846450\n",
      "Iteration 133, loss = 0.60808198\n",
      "Iteration 134, loss = 0.60757424\n",
      "Iteration 135, loss = 0.60695004\n",
      "Iteration 136, loss = 0.60668350\n",
      "Iteration 137, loss = 0.60610432\n",
      "Iteration 138, loss = 0.60551819\n",
      "Iteration 139, loss = 0.60503248\n",
      "Iteration 140, loss = 0.60454458\n",
      "Iteration 141, loss = 0.60380198\n",
      "Iteration 142, loss = 0.60339977\n",
      "Iteration 143, loss = 0.60290107\n",
      "Iteration 144, loss = 0.60214231\n",
      "Iteration 145, loss = 0.60154214\n",
      "Iteration 146, loss = 0.60096097\n",
      "Iteration 147, loss = 0.60036166\n",
      "Iteration 148, loss = 0.59975661\n",
      "Iteration 149, loss = 0.59893804\n",
      "Iteration 150, loss = 0.59839614\n",
      "Iteration 151, loss = 0.59762376\n",
      "Iteration 152, loss = 0.59689995\n",
      "Iteration 153, loss = 0.59615849\n",
      "Iteration 154, loss = 0.59546862\n",
      "Iteration 155, loss = 0.59475956\n",
      "Iteration 156, loss = 0.59407767\n",
      "Iteration 157, loss = 0.59329570\n",
      "Iteration 158, loss = 0.59302500\n",
      "Iteration 159, loss = 0.59185714\n",
      "Iteration 160, loss = 0.59127991\n",
      "Iteration 161, loss = 0.59058916\n",
      "Iteration 162, loss = 0.58983244\n",
      "Iteration 163, loss = 0.58907891\n",
      "Iteration 164, loss = 0.58891639\n",
      "Iteration 165, loss = 0.58784042\n",
      "Iteration 166, loss = 0.58710666\n",
      "Iteration 167, loss = 0.58654828\n",
      "Iteration 168, loss = 0.58590643\n",
      "Iteration 169, loss = 0.58543986\n",
      "Iteration 170, loss = 0.58440251\n",
      "Iteration 171, loss = 0.58456030\n",
      "Iteration 172, loss = 0.58396302\n",
      "Iteration 173, loss = 0.58338473\n",
      "Iteration 174, loss = 0.58254576\n",
      "Iteration 175, loss = 0.58159938\n",
      "Iteration 176, loss = 0.58138497\n",
      "Iteration 177, loss = 0.58056809\n",
      "Iteration 178, loss = 0.58024954\n",
      "Iteration 179, loss = 0.57956128\n",
      "Iteration 180, loss = 0.57881712\n",
      "Iteration 181, loss = 0.57871382\n",
      "Iteration 182, loss = 0.57833954\n",
      "Iteration 183, loss = 0.57743237\n",
      "Iteration 184, loss = 0.57693743\n",
      "Iteration 185, loss = 0.57668864\n",
      "Iteration 186, loss = 0.57590709\n",
      "Iteration 187, loss = 0.57585689\n",
      "Iteration 188, loss = 0.57528829\n",
      "Iteration 189, loss = 0.57520001\n",
      "Iteration 190, loss = 0.57463455\n",
      "Iteration 191, loss = 0.57415122\n",
      "Iteration 192, loss = 0.57318114\n",
      "Iteration 193, loss = 0.57254482\n",
      "Iteration 194, loss = 0.57236856\n",
      "Iteration 195, loss = 0.57175265\n",
      "Iteration 196, loss = 0.57126496\n",
      "Iteration 197, loss = 0.57113517\n",
      "Iteration 198, loss = 0.57162937\n",
      "Iteration 199, loss = 0.56987757\n",
      "Iteration 200, loss = 0.56977211\n",
      "Iteration 201, loss = 0.56916170\n",
      "Iteration 202, loss = 0.56853277\n",
      "Iteration 203, loss = 0.56829175\n",
      "Iteration 204, loss = 0.56795286\n",
      "Iteration 205, loss = 0.56719821\n",
      "Iteration 206, loss = 0.56715831\n",
      "Iteration 207, loss = 0.56664314\n",
      "Iteration 208, loss = 0.56592288\n",
      "Iteration 209, loss = 0.56566485\n",
      "Iteration 210, loss = 0.56540250\n",
      "Iteration 211, loss = 0.56463624\n",
      "Iteration 212, loss = 0.56406626\n",
      "Iteration 213, loss = 0.56370473\n",
      "Iteration 214, loss = 0.56308664\n",
      "Iteration 215, loss = 0.56265184\n",
      "Iteration 216, loss = 0.56238232\n",
      "Iteration 217, loss = 0.56211334\n",
      "Iteration 218, loss = 0.56148827\n",
      "Iteration 219, loss = 0.56087926\n",
      "Iteration 220, loss = 0.56022382\n",
      "Iteration 221, loss = 0.55978161\n",
      "Iteration 222, loss = 0.55928222\n",
      "Iteration 223, loss = 0.55850863\n",
      "Iteration 224, loss = 0.55817367\n",
      "Iteration 225, loss = 0.55750327\n",
      "Iteration 226, loss = 0.55692529\n",
      "Iteration 227, loss = 0.55652568\n",
      "Iteration 228, loss = 0.55594528\n",
      "Iteration 229, loss = 0.55526571\n",
      "Iteration 230, loss = 0.55523650\n",
      "Iteration 231, loss = 0.55434324\n",
      "Iteration 232, loss = 0.55361992\n",
      "Iteration 233, loss = 0.55306428\n",
      "Iteration 234, loss = 0.55254778\n",
      "Iteration 235, loss = 0.55197246\n",
      "Iteration 236, loss = 0.55136743\n",
      "Iteration 237, loss = 0.55071671\n",
      "Iteration 238, loss = 0.55003009\n",
      "Iteration 239, loss = 0.54987870\n",
      "Iteration 240, loss = 0.54924713\n",
      "Iteration 241, loss = 0.54838981\n",
      "Iteration 242, loss = 0.54765743\n",
      "Iteration 243, loss = 0.54701424\n",
      "Iteration 244, loss = 0.54665569\n",
      "Iteration 245, loss = 0.54588346\n",
      "Iteration 246, loss = 0.54523868\n",
      "Iteration 247, loss = 0.54433661\n",
      "Iteration 248, loss = 0.54396289\n",
      "Iteration 249, loss = 0.54326306\n",
      "Iteration 250, loss = 0.54227784\n",
      "Iteration 251, loss = 0.54194955\n",
      "Iteration 252, loss = 0.54178595\n",
      "Iteration 253, loss = 0.54066534\n",
      "Iteration 254, loss = 0.53946648\n",
      "Iteration 255, loss = 0.53919081\n",
      "Iteration 256, loss = 0.53869618\n",
      "Iteration 257, loss = 0.53747419\n",
      "Iteration 258, loss = 0.53643030\n",
      "Iteration 259, loss = 0.53647878\n",
      "Iteration 260, loss = 0.53618404\n",
      "Iteration 261, loss = 0.53452137\n",
      "Iteration 262, loss = 0.53346495\n",
      "Iteration 263, loss = 0.53271209\n",
      "Iteration 264, loss = 0.53212955\n",
      "Iteration 265, loss = 0.53122514\n",
      "Iteration 266, loss = 0.53086272\n",
      "Iteration 267, loss = 0.52968736\n",
      "Iteration 268, loss = 0.52883390\n",
      "Iteration 269, loss = 0.52804699\n",
      "Iteration 270, loss = 0.52785036\n",
      "Iteration 271, loss = 0.52628164\n",
      "Iteration 272, loss = 0.52622063\n",
      "Iteration 273, loss = 0.52568233\n",
      "Iteration 274, loss = 0.52407404\n",
      "Iteration 275, loss = 0.52349104\n",
      "Iteration 276, loss = 0.52318525\n",
      "Iteration 277, loss = 0.52200097\n",
      "Iteration 278, loss = 0.52082125\n",
      "Iteration 279, loss = 0.52098710\n",
      "Iteration 280, loss = 0.52051716\n",
      "Iteration 281, loss = 0.52030587\n",
      "Iteration 282, loss = 0.51832318\n",
      "Iteration 283, loss = 0.51721382\n",
      "Iteration 284, loss = 0.51655331\n",
      "Iteration 285, loss = 0.51712490\n",
      "Iteration 286, loss = 0.51639587\n",
      "Iteration 287, loss = 0.51496383\n",
      "Iteration 288, loss = 0.51357004\n",
      "Iteration 289, loss = 0.51387200\n",
      "Iteration 290, loss = 0.51300875\n",
      "Iteration 291, loss = 0.51141076\n",
      "Iteration 292, loss = 0.51101233\n",
      "Iteration 293, loss = 0.51005210\n",
      "Iteration 294, loss = 0.50946333\n",
      "Iteration 295, loss = 0.50920823\n",
      "Iteration 296, loss = 0.50800951\n",
      "Iteration 297, loss = 0.50730701\n",
      "Iteration 298, loss = 0.50670289\n",
      "Iteration 299, loss = 0.50590048\n",
      "Iteration 300, loss = 0.50539883\n",
      "Iteration 301, loss = 0.50462514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 302, loss = 0.50415888\n",
      "Iteration 303, loss = 0.50367181\n",
      "Iteration 304, loss = 0.50283140\n",
      "Iteration 305, loss = 0.50217163\n",
      "Iteration 306, loss = 0.50200833\n",
      "Iteration 307, loss = 0.50103771\n",
      "Iteration 308, loss = 0.50036750\n",
      "Iteration 309, loss = 0.50030620\n",
      "Iteration 310, loss = 0.49996386\n",
      "Iteration 311, loss = 0.49883786\n",
      "Iteration 312, loss = 0.49833877\n",
      "Iteration 313, loss = 0.49793178\n",
      "Iteration 314, loss = 0.49756586\n",
      "Iteration 315, loss = 0.49704594\n",
      "Iteration 316, loss = 0.49711124\n",
      "Iteration 317, loss = 0.49637303\n",
      "Iteration 318, loss = 0.49551970\n",
      "Iteration 319, loss = 0.49571409\n",
      "Iteration 320, loss = 0.49565868\n",
      "Iteration 321, loss = 0.49474150\n",
      "Iteration 322, loss = 0.49409590\n",
      "Iteration 323, loss = 0.49485669\n",
      "Iteration 324, loss = 0.49394885\n",
      "Iteration 325, loss = 0.49306370\n",
      "Iteration 326, loss = 0.49369809\n",
      "Iteration 327, loss = 0.49413780\n",
      "Iteration 328, loss = 0.49171122\n",
      "Iteration 329, loss = 0.49239047\n",
      "Iteration 330, loss = 0.49410413\n",
      "Iteration 331, loss = 0.49268583\n",
      "Iteration 332, loss = 0.49054392\n",
      "Iteration 333, loss = 0.49135136\n",
      "Iteration 334, loss = 0.49148381\n",
      "Iteration 335, loss = 0.49073291\n",
      "Iteration 336, loss = 0.48975310\n",
      "Iteration 337, loss = 0.48972523\n",
      "Iteration 338, loss = 0.49101175\n",
      "Iteration 339, loss = 0.49056297\n",
      "Iteration 340, loss = 0.48907444\n",
      "Iteration 341, loss = 0.48924898\n",
      "Iteration 342, loss = 0.48873089\n",
      "Iteration 343, loss = 0.48876714\n",
      "Iteration 344, loss = 0.48812216\n",
      "Iteration 345, loss = 0.48812492\n",
      "Iteration 346, loss = 0.48827735\n",
      "Iteration 347, loss = 0.48770428\n",
      "Iteration 348, loss = 0.48775877\n",
      "Iteration 349, loss = 0.48768421\n",
      "Iteration 350, loss = 0.48737647\n",
      "Iteration 351, loss = 0.48725945\n",
      "Iteration 352, loss = 0.48685236\n",
      "Iteration 353, loss = 0.48741909\n",
      "Iteration 354, loss = 0.48780884\n",
      "Iteration 355, loss = 0.48859259\n",
      "Iteration 356, loss = 0.48752485\n",
      "Iteration 357, loss = 0.48771857\n",
      "Iteration 358, loss = 0.48651935\n",
      "Iteration 359, loss = 0.48669880\n",
      "Iteration 360, loss = 0.48597108\n",
      "Iteration 361, loss = 0.48703842\n",
      "Iteration 362, loss = 0.48682548\n",
      "Iteration 363, loss = 0.48613332\n",
      "Iteration 364, loss = 0.48632138\n",
      "Iteration 365, loss = 0.48587075\n",
      "Iteration 366, loss = 0.48561586\n",
      "Iteration 367, loss = 0.48637715\n",
      "Iteration 368, loss = 0.48528604\n",
      "Iteration 369, loss = 0.48522235\n",
      "Iteration 370, loss = 0.48600362\n",
      "Iteration 371, loss = 0.48643258\n",
      "Iteration 372, loss = 0.48545240\n",
      "Iteration 373, loss = 0.48498566\n",
      "Iteration 374, loss = 0.48564494\n",
      "Iteration 375, loss = 0.48540334\n",
      "Iteration 376, loss = 0.48465419\n",
      "Iteration 377, loss = 0.48516995\n",
      "Iteration 378, loss = 0.48636382\n",
      "Iteration 379, loss = 0.48574211\n",
      "Iteration 380, loss = 0.48602157\n",
      "Iteration 381, loss = 0.48485918\n",
      "Iteration 382, loss = 0.48621347\n",
      "Iteration 383, loss = 0.48581596\n",
      "Iteration 384, loss = 0.48496459\n",
      "Iteration 385, loss = 0.48513511\n",
      "Iteration 386, loss = 0.48490902\n",
      "Iteration 387, loss = 0.48470036\n",
      "Iteration 388, loss = 0.48441267\n",
      "Iteration 389, loss = 0.48452909\n",
      "Iteration 390, loss = 0.48471311\n",
      "Iteration 391, loss = 0.48440476\n",
      "Iteration 392, loss = 0.48430244\n",
      "Iteration 393, loss = 0.48420558\n",
      "Iteration 394, loss = 0.48415836\n",
      "Iteration 395, loss = 0.48414636\n",
      "Iteration 396, loss = 0.48458217\n",
      "Iteration 397, loss = 0.48453110\n",
      "Iteration 398, loss = 0.48447998\n",
      "Iteration 399, loss = 0.48443007\n",
      "Iteration 400, loss = 0.48402126\n",
      "Iteration 401, loss = 0.48444283\n",
      "Iteration 402, loss = 0.48395399\n",
      "Iteration 403, loss = 0.48477766\n",
      "Iteration 404, loss = 0.48474661\n",
      "Iteration 405, loss = 0.48417794\n",
      "Iteration 406, loss = 0.48410856\n",
      "Iteration 407, loss = 0.48447940\n",
      "Iteration 408, loss = 0.48495412\n",
      "Iteration 409, loss = 0.48394836\n",
      "Iteration 410, loss = 0.48408677\n",
      "Iteration 411, loss = 0.48376164\n",
      "Iteration 412, loss = 0.48413521\n",
      "Iteration 413, loss = 0.48462062\n",
      "Iteration 414, loss = 0.48408197\n",
      "Iteration 415, loss = 0.48370183\n",
      "Iteration 416, loss = 0.48366841\n",
      "Iteration 417, loss = 0.48383063\n",
      "Iteration 418, loss = 0.48482874\n",
      "Iteration 419, loss = 0.48469485\n",
      "Iteration 420, loss = 0.48381441\n",
      "Iteration 421, loss = 0.48396993\n",
      "Iteration 422, loss = 0.48427698\n",
      "Iteration 423, loss = 0.48398448\n",
      "Iteration 424, loss = 0.48376329\n",
      "Iteration 425, loss = 0.48393298\n",
      "Iteration 426, loss = 0.48385583\n",
      "Iteration 427, loss = 0.48382936\n",
      "Iteration 428, loss = 0.48393133\n",
      "Iteration 429, loss = 0.48358315\n",
      "Iteration 430, loss = 0.48353649\n",
      "Iteration 431, loss = 0.48391741\n",
      "Iteration 432, loss = 0.48349877\n",
      "Iteration 433, loss = 0.48369991\n",
      "Iteration 434, loss = 0.48414787\n",
      "Iteration 435, loss = 0.48438447\n",
      "Iteration 436, loss = 0.48371375\n",
      "Iteration 437, loss = 0.48365763\n",
      "Iteration 438, loss = 0.48358474\n",
      "Iteration 439, loss = 0.48360618\n",
      "Iteration 440, loss = 0.48371743\n",
      "Iteration 441, loss = 0.48362941\n",
      "Iteration 442, loss = 0.48339321\n",
      "Iteration 443, loss = 0.48312759\n",
      "Iteration 444, loss = 0.48399863\n",
      "Iteration 445, loss = 0.48474635\n",
      "Iteration 446, loss = 0.48434719\n",
      "Iteration 447, loss = 0.48376911\n",
      "Iteration 448, loss = 0.48340770\n",
      "Iteration 449, loss = 0.48379852\n",
      "Iteration 450, loss = 0.48337466\n",
      "Iteration 451, loss = 0.48401160\n",
      "Iteration 452, loss = 0.48342259\n",
      "Iteration 453, loss = 0.48488847\n",
      "Iteration 454, loss = 0.48341877\n",
      "Iteration 455, loss = 0.48329984\n",
      "Iteration 456, loss = 0.48404680\n",
      "Iteration 457, loss = 0.48439005\n",
      "Iteration 458, loss = 0.48360738\n",
      "Iteration 459, loss = 0.48332519\n",
      "Iteration 460, loss = 0.48381096\n",
      "Iteration 461, loss = 0.48484053\n",
      "Iteration 462, loss = 0.48345134\n",
      "Iteration 463, loss = 0.48347318\n",
      "Iteration 464, loss = 0.48365149\n",
      "Iteration 465, loss = 0.48378092\n",
      "Iteration 466, loss = 0.48486109\n",
      "Iteration 467, loss = 0.48324319\n",
      "Iteration 468, loss = 0.48315526\n",
      "Iteration 469, loss = 0.48333544\n",
      "Iteration 470, loss = 0.48326996\n",
      "Iteration 471, loss = 0.48326352\n",
      "Iteration 472, loss = 0.48320936\n",
      "Iteration 473, loss = 0.48316160\n",
      "Iteration 474, loss = 0.48311758\n",
      "Iteration 475, loss = 0.48333057\n",
      "Iteration 476, loss = 0.48320605\n",
      "Iteration 477, loss = 0.48317911\n",
      "Iteration 478, loss = 0.48316272\n",
      "Iteration 479, loss = 0.48310075\n",
      "Iteration 480, loss = 0.48401740\n",
      "Iteration 481, loss = 0.48331269\n",
      "Iteration 482, loss = 0.48315331\n",
      "Iteration 483, loss = 0.48321538\n",
      "Iteration 484, loss = 0.48313518\n",
      "Iteration 485, loss = 0.48303401\n",
      "Iteration 486, loss = 0.48302358\n",
      "Iteration 487, loss = 0.48325247\n",
      "Iteration 488, loss = 0.48353825\n",
      "Iteration 489, loss = 0.48382150\n",
      "Iteration 490, loss = 0.48313544\n",
      "Iteration 491, loss = 0.48347401\n",
      "Iteration 492, loss = 0.48316038\n",
      "Iteration 493, loss = 0.48305583\n",
      "Iteration 494, loss = 0.48307267\n",
      "Iteration 495, loss = 0.48323149\n",
      "Iteration 496, loss = 0.48299415\n",
      "Iteration 497, loss = 0.48308755\n",
      "Iteration 498, loss = 0.48339011\n",
      "Iteration 499, loss = 0.48371765\n",
      "Iteration 500, loss = 0.48353636\n",
      "Iteration 501, loss = 0.48510667\n",
      "Iteration 502, loss = 0.48430698\n",
      "Iteration 503, loss = 0.48312174\n",
      "Iteration 504, loss = 0.48365062\n",
      "Iteration 505, loss = 0.48536362\n",
      "Iteration 506, loss = 0.48475004\n",
      "Iteration 507, loss = 0.48289586\n",
      "Iteration 508, loss = 0.48330061\n",
      "Iteration 509, loss = 0.48453497\n",
      "Iteration 510, loss = 0.48454225\n",
      "Iteration 511, loss = 0.48284254\n",
      "Iteration 512, loss = 0.48369584\n",
      "Iteration 513, loss = 0.48544409\n",
      "Iteration 514, loss = 0.48450790\n",
      "Iteration 515, loss = 0.48346393\n",
      "Iteration 516, loss = 0.48317848\n",
      "Iteration 517, loss = 0.48382686\n",
      "Iteration 518, loss = 0.48368083\n",
      "Iteration 519, loss = 0.48278837\n",
      "Iteration 520, loss = 0.48306625\n",
      "Iteration 521, loss = 0.48376305\n",
      "Iteration 522, loss = 0.48342721\n",
      "Iteration 523, loss = 0.48312641\n",
      "Iteration 524, loss = 0.48408751\n",
      "Iteration 525, loss = 0.48428275\n",
      "Iteration 526, loss = 0.48368401\n",
      "Iteration 527, loss = 0.48263449\n",
      "Iteration 528, loss = 0.48519893\n",
      "Iteration 529, loss = 0.48375803\n",
      "Iteration 530, loss = 0.48365431\n",
      "Iteration 531, loss = 0.48294811\n",
      "Iteration 532, loss = 0.48368673\n",
      "Iteration 533, loss = 0.48319995\n",
      "Iteration 534, loss = 0.48340753\n",
      "Iteration 535, loss = 0.48423309\n",
      "Iteration 536, loss = 0.48284106\n",
      "Iteration 537, loss = 0.48284545\n",
      "Iteration 538, loss = 0.48282464\n",
      "Iteration 539, loss = 0.48288816\n",
      "Iteration 540, loss = 0.48288867\n",
      "Iteration 541, loss = 0.48280994\n",
      "Iteration 542, loss = 0.48316154\n",
      "Iteration 543, loss = 0.48286764\n",
      "Iteration 544, loss = 0.48283506\n",
      "Iteration 545, loss = 0.48319712\n",
      "Iteration 546, loss = 0.48337200\n",
      "Iteration 547, loss = 0.48298007\n",
      "Iteration 548, loss = 0.48286204\n",
      "Iteration 549, loss = 0.48289514\n",
      "Iteration 550, loss = 0.48285925\n",
      "Iteration 551, loss = 0.48308412\n",
      "Iteration 552, loss = 0.48280288\n",
      "Iteration 553, loss = 0.48289889\n",
      "Iteration 554, loss = 0.48278298\n",
      "Iteration 555, loss = 0.48283331\n",
      "Iteration 556, loss = 0.48278122\n",
      "Iteration 557, loss = 0.48367168\n",
      "Iteration 558, loss = 0.48359545\n",
      "Iteration 559, loss = 0.48273169\n",
      "Iteration 560, loss = 0.48281227\n",
      "Iteration 561, loss = 0.48307185\n",
      "Iteration 562, loss = 0.48340879\n",
      "Iteration 563, loss = 0.48305491\n",
      "Iteration 564, loss = 0.48272829\n",
      "Iteration 565, loss = 0.48284755\n",
      "Iteration 566, loss = 0.48369931\n",
      "Iteration 567, loss = 0.48433773\n",
      "Iteration 568, loss = 0.48341082\n",
      "Iteration 569, loss = 0.48327154\n",
      "Iteration 570, loss = 0.48352880\n",
      "Iteration 571, loss = 0.48391273\n",
      "Iteration 572, loss = 0.48329434\n",
      "Iteration 573, loss = 0.48292258\n",
      "Iteration 574, loss = 0.48288440\n",
      "Iteration 575, loss = 0.48297501\n",
      "Iteration 576, loss = 0.48353056\n",
      "Iteration 577, loss = 0.48443712\n",
      "Iteration 578, loss = 0.48275672\n",
      "Iteration 579, loss = 0.48270667\n",
      "Iteration 580, loss = 0.48274804\n",
      "Iteration 581, loss = 0.48286629\n",
      "Iteration 582, loss = 0.48272022\n",
      "Iteration 583, loss = 0.48262874\n",
      "Iteration 584, loss = 0.48282072\n",
      "Iteration 585, loss = 0.48306150\n",
      "Iteration 586, loss = 0.48352581\n",
      "Iteration 587, loss = 0.48367759\n",
      "Iteration 588, loss = 0.48291392\n",
      "Iteration 589, loss = 0.48240685\n",
      "Iteration 590, loss = 0.48312994\n",
      "Iteration 591, loss = 0.48375376\n",
      "Iteration 592, loss = 0.48400748\n",
      "Iteration 593, loss = 0.48305216\n",
      "Iteration 594, loss = 0.48301399\n",
      "Iteration 595, loss = 0.48288153\n",
      "Iteration 596, loss = 0.48268103\n",
      "Iteration 597, loss = 0.48261116\n",
      "Iteration 598, loss = 0.48266545\n",
      "Iteration 599, loss = 0.48282383\n",
      "Iteration 600, loss = 0.48295170\n",
      "Iteration 601, loss = 0.48259395\n",
      "Iteration 602, loss = 0.48286000\n",
      "Iteration 603, loss = 0.48294684\n",
      "Iteration 604, loss = 0.48333817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 605, loss = 0.48274591\n",
      "Iteration 606, loss = 0.48268765\n",
      "Iteration 607, loss = 0.48262679\n",
      "Iteration 608, loss = 0.48259438\n",
      "Iteration 609, loss = 0.48267062\n",
      "Iteration 610, loss = 0.48264859\n",
      "Iteration 611, loss = 0.48279695\n",
      "Iteration 612, loss = 0.48253882\n",
      "Iteration 613, loss = 0.48304577\n",
      "Iteration 614, loss = 0.48288013\n",
      "Iteration 615, loss = 0.48283047\n",
      "Iteration 616, loss = 0.48246842\n",
      "Iteration 617, loss = 0.48265690\n",
      "Iteration 618, loss = 0.48354854\n",
      "Iteration 619, loss = 0.48306787\n",
      "Iteration 620, loss = 0.48247049\n",
      "Iteration 621, loss = 0.48314642\n",
      "Iteration 622, loss = 0.48325832\n",
      "Iteration 623, loss = 0.48285004\n",
      "Iteration 624, loss = 0.48327784\n",
      "Iteration 625, loss = 0.48302758\n",
      "Iteration 626, loss = 0.48378479\n",
      "Iteration 627, loss = 0.48284222\n",
      "Iteration 628, loss = 0.48278383\n",
      "Iteration 629, loss = 0.48256845\n",
      "Iteration 630, loss = 0.48253165\n",
      "Iteration 631, loss = 0.48262963\n",
      "Iteration 632, loss = 0.48262262\n",
      "Iteration 633, loss = 0.48285676\n",
      "Iteration 634, loss = 0.48250722\n",
      "Iteration 635, loss = 0.48348004\n",
      "Iteration 636, loss = 0.48233203\n",
      "Iteration 637, loss = 0.48246442\n",
      "Iteration 638, loss = 0.48404253\n",
      "Iteration 639, loss = 0.48390269\n",
      "Iteration 640, loss = 0.48278942\n",
      "Iteration 641, loss = 0.48233044\n",
      "Iteration 642, loss = 0.48282599\n",
      "Iteration 643, loss = 0.48358008\n",
      "Iteration 644, loss = 0.48379494\n",
      "Iteration 645, loss = 0.48278331\n",
      "Iteration 646, loss = 0.48241501\n",
      "Iteration 647, loss = 0.48269484\n",
      "Iteration 648, loss = 0.48336410\n",
      "Iteration 649, loss = 0.48321424\n",
      "Iteration 650, loss = 0.48343510\n",
      "Iteration 651, loss = 0.48253315\n",
      "Iteration 652, loss = 0.48251558\n",
      "Iteration 653, loss = 0.48255849\n",
      "Iteration 654, loss = 0.48240941\n",
      "Iteration 655, loss = 0.48247803\n",
      "Iteration 656, loss = 0.48272728\n",
      "Iteration 657, loss = 0.48280142\n",
      "Iteration 658, loss = 0.48311541\n",
      "Iteration 659, loss = 0.48269990\n",
      "Iteration 660, loss = 0.48248978\n",
      "Iteration 661, loss = 0.48223577\n",
      "Iteration 662, loss = 0.48260502\n",
      "Iteration 663, loss = 0.48293932\n",
      "Iteration 664, loss = 0.48302513\n",
      "Iteration 665, loss = 0.48272498\n",
      "Iteration 666, loss = 0.48272122\n",
      "Iteration 667, loss = 0.48235146\n",
      "Iteration 668, loss = 0.48238418\n",
      "Iteration 669, loss = 0.48242153\n",
      "Iteration 670, loss = 0.48257740\n",
      "Iteration 671, loss = 0.48271350\n",
      "Iteration 672, loss = 0.48273746\n",
      "Iteration 673, loss = 0.48251978\n",
      "Iteration 674, loss = 0.48232022\n",
      "Iteration 675, loss = 0.48258996\n",
      "Iteration 676, loss = 0.48318006\n",
      "Iteration 677, loss = 0.48217079\n",
      "Iteration 678, loss = 0.48312138\n",
      "Iteration 679, loss = 0.48337687\n",
      "Iteration 680, loss = 0.48318176\n",
      "Iteration 681, loss = 0.48286939\n",
      "Iteration 682, loss = 0.48285332\n",
      "Iteration 683, loss = 0.48274650\n",
      "Iteration 684, loss = 0.48254118\n",
      "Iteration 685, loss = 0.48246587\n",
      "Iteration 686, loss = 0.48262615\n",
      "Iteration 687, loss = 0.48290946\n",
      "Iteration 688, loss = 0.48230049\n",
      "Iteration 689, loss = 0.48244403\n",
      "Iteration 690, loss = 0.48230269\n",
      "Iteration 691, loss = 0.48231821\n",
      "Iteration 692, loss = 0.48232928\n",
      "Iteration 693, loss = 0.48228894\n",
      "Iteration 694, loss = 0.48233843\n",
      "Iteration 695, loss = 0.48240003\n",
      "Iteration 696, loss = 0.48297025\n",
      "Iteration 697, loss = 0.48271784\n",
      "Iteration 698, loss = 0.48286547\n",
      "Iteration 699, loss = 0.48253072\n",
      "Iteration 700, loss = 0.48226842\n",
      "Iteration 701, loss = 0.48242986\n",
      "Iteration 702, loss = 0.48270263\n",
      "Iteration 703, loss = 0.48287566\n",
      "Iteration 704, loss = 0.48264991\n",
      "Iteration 705, loss = 0.48250272\n",
      "Iteration 706, loss = 0.48226235\n",
      "Iteration 707, loss = 0.48220894\n",
      "Iteration 708, loss = 0.48219531\n",
      "Iteration 709, loss = 0.48244812\n",
      "Iteration 710, loss = 0.48222996\n",
      "Iteration 711, loss = 0.48247494\n",
      "Iteration 712, loss = 0.48238815\n",
      "Iteration 713, loss = 0.48195484\n",
      "Iteration 714, loss = 0.48273259\n",
      "Iteration 715, loss = 0.48305384\n",
      "Iteration 716, loss = 0.48234856\n",
      "Iteration 717, loss = 0.48220810\n",
      "Iteration 718, loss = 0.48268984\n",
      "Iteration 719, loss = 0.48328184\n",
      "Iteration 720, loss = 0.48317887\n",
      "Iteration 721, loss = 0.48257032\n",
      "Iteration 722, loss = 0.48213577\n",
      "Iteration 723, loss = 0.48222578\n",
      "Iteration 724, loss = 0.48243757\n",
      "Iteration 725, loss = 0.48262446\n",
      "Iteration 726, loss = 0.48254285\n",
      "Iteration 727, loss = 0.48276037\n",
      "Iteration 728, loss = 0.48245570\n",
      "Iteration 729, loss = 0.48234597\n",
      "Iteration 730, loss = 0.48231159\n",
      "Iteration 731, loss = 0.48232049\n",
      "Iteration 732, loss = 0.48231881\n",
      "Iteration 733, loss = 0.48229607\n",
      "Iteration 734, loss = 0.48222116\n",
      "Iteration 735, loss = 0.48236253\n",
      "Iteration 736, loss = 0.48242142\n",
      "Iteration 737, loss = 0.48214294\n",
      "Iteration 738, loss = 0.48256988\n",
      "Iteration 739, loss = 0.48196361\n",
      "Iteration 740, loss = 0.48257091\n",
      "Iteration 741, loss = 0.48307224\n",
      "Iteration 742, loss = 0.48238028\n",
      "Iteration 743, loss = 0.48239009\n",
      "Iteration 744, loss = 0.48206554\n",
      "Iteration 745, loss = 0.48208409\n",
      "Iteration 746, loss = 0.48335845\n",
      "Iteration 747, loss = 0.48205499\n",
      "Iteration 748, loss = 0.48378460\n",
      "Iteration 749, loss = 0.48264884\n",
      "Iteration 750, loss = 0.48242206\n",
      "Iteration 751, loss = 0.48217454\n",
      "Iteration 752, loss = 0.48205669\n",
      "Iteration 753, loss = 0.48243301\n",
      "Iteration 754, loss = 0.48333025\n",
      "Iteration 755, loss = 0.48199173\n",
      "Iteration 756, loss = 0.48208797\n",
      "Iteration 757, loss = 0.48252252\n",
      "Iteration 758, loss = 0.48234380\n",
      "Iteration 759, loss = 0.48243709\n",
      "Iteration 760, loss = 0.48293456\n",
      "Iteration 761, loss = 0.48213156\n",
      "Iteration 762, loss = 0.48302800\n",
      "Iteration 763, loss = 0.48226398\n",
      "Iteration 764, loss = 0.48289069\n",
      "Iteration 765, loss = 0.48255326\n",
      "Iteration 766, loss = 0.48224569\n",
      "Iteration 767, loss = 0.48293829\n",
      "Iteration 768, loss = 0.48242784\n",
      "Iteration 769, loss = 0.48203152\n",
      "Iteration 770, loss = 0.48200982\n",
      "Iteration 771, loss = 0.48193911\n",
      "Iteration 772, loss = 0.48194887\n",
      "Iteration 773, loss = 0.48219309\n",
      "Iteration 774, loss = 0.48204942\n",
      "Iteration 775, loss = 0.48201148\n",
      "Iteration 776, loss = 0.48205768\n",
      "Iteration 777, loss = 0.48179355\n",
      "Iteration 778, loss = 0.48213175\n",
      "Iteration 779, loss = 0.48231547\n",
      "Iteration 780, loss = 0.48247711\n",
      "Iteration 781, loss = 0.48257079\n",
      "Iteration 782, loss = 0.48201695\n",
      "Iteration 783, loss = 0.48280795\n",
      "Iteration 784, loss = 0.48203509\n",
      "Iteration 785, loss = 0.48215774\n",
      "Iteration 786, loss = 0.48176530\n",
      "Iteration 787, loss = 0.48180708\n",
      "Iteration 788, loss = 0.48265495\n",
      "Iteration 789, loss = 0.48290150\n",
      "Iteration 790, loss = 0.48243121\n",
      "Iteration 791, loss = 0.48203610\n",
      "Iteration 792, loss = 0.48220492\n",
      "Iteration 793, loss = 0.48194945\n",
      "Iteration 794, loss = 0.48173807\n",
      "Iteration 795, loss = 0.48237954\n",
      "Iteration 796, loss = 0.48268087\n",
      "Iteration 797, loss = 0.48196998\n",
      "Iteration 798, loss = 0.48188967\n",
      "Iteration 799, loss = 0.48180023\n",
      "Iteration 800, loss = 0.48185967\n",
      "Iteration 801, loss = 0.48241913\n",
      "Iteration 802, loss = 0.48206041\n",
      "Iteration 803, loss = 0.48174842\n",
      "Iteration 804, loss = 0.48192105\n",
      "Iteration 805, loss = 0.48194075\n",
      "Iteration 806, loss = 0.48219245\n",
      "Iteration 807, loss = 0.48180663\n",
      "Iteration 808, loss = 0.48232912\n",
      "Iteration 809, loss = 0.48172984\n",
      "Iteration 810, loss = 0.48291358\n",
      "Iteration 811, loss = 0.48211033\n",
      "Iteration 812, loss = 0.48176382\n",
      "Iteration 813, loss = 0.48204037\n",
      "Iteration 814, loss = 0.48209446\n",
      "Iteration 815, loss = 0.48206210\n",
      "Iteration 816, loss = 0.48173010\n",
      "Iteration 817, loss = 0.48176047\n",
      "Iteration 818, loss = 0.48174455\n",
      "Iteration 819, loss = 0.48162502\n",
      "Iteration 820, loss = 0.48189645\n",
      "Iteration 821, loss = 0.48197948\n",
      "Iteration 822, loss = 0.48169366\n",
      "Iteration 823, loss = 0.48202548\n",
      "Iteration 824, loss = 0.48262021\n",
      "Iteration 825, loss = 0.48243142\n",
      "Iteration 826, loss = 0.48171444\n",
      "Iteration 827, loss = 0.48204624\n",
      "Iteration 828, loss = 0.48169863\n",
      "Iteration 829, loss = 0.48165074\n",
      "Iteration 830, loss = 0.48224804\n",
      "Iteration 831, loss = 0.48153867\n",
      "Iteration 832, loss = 0.48205382\n",
      "Iteration 833, loss = 0.48207962\n",
      "Iteration 834, loss = 0.48197227\n",
      "Iteration 835, loss = 0.48120808\n",
      "Iteration 836, loss = 0.48166993\n",
      "Iteration 837, loss = 0.48404266\n",
      "Iteration 838, loss = 0.48540378\n",
      "Iteration 839, loss = 0.48371223\n",
      "Iteration 840, loss = 0.48194483\n",
      "Iteration 841, loss = 0.48331698\n",
      "Iteration 842, loss = 0.48262084\n",
      "Iteration 843, loss = 0.48259577\n",
      "Iteration 844, loss = 0.48213542\n",
      "Iteration 845, loss = 0.48296415\n",
      "Iteration 846, loss = 0.48174174\n",
      "Iteration 847, loss = 0.48264209\n",
      "Iteration 848, loss = 0.48156244\n",
      "Iteration 849, loss = 0.48163691\n",
      "Iteration 850, loss = 0.48176143\n",
      "Iteration 851, loss = 0.48195908\n",
      "Iteration 852, loss = 0.48186031\n",
      "Iteration 853, loss = 0.48160893\n",
      "Iteration 854, loss = 0.48170090\n",
      "Iteration 855, loss = 0.48182267\n",
      "Iteration 856, loss = 0.48178107\n",
      "Iteration 857, loss = 0.48161252\n",
      "Iteration 858, loss = 0.48138738\n",
      "Iteration 859, loss = 0.48243282\n",
      "Iteration 860, loss = 0.48257980\n",
      "Iteration 861, loss = 0.48208595\n",
      "Iteration 862, loss = 0.48151956\n",
      "Iteration 863, loss = 0.48144411\n",
      "Iteration 864, loss = 0.48208914\n",
      "Iteration 865, loss = 0.48257105\n",
      "Iteration 866, loss = 0.48229547\n",
      "Iteration 867, loss = 0.48203709\n",
      "Iteration 868, loss = 0.48168100\n",
      "Iteration 869, loss = 0.48180057\n",
      "Iteration 870, loss = 0.48199026\n",
      "Iteration 871, loss = 0.48208497\n",
      "Iteration 872, loss = 0.48153785\n",
      "Iteration 873, loss = 0.48122536\n",
      "Iteration 874, loss = 0.48296295\n",
      "Iteration 875, loss = 0.48250635\n",
      "Iteration 876, loss = 0.48168155\n",
      "Iteration 877, loss = 0.48105964\n",
      "Iteration 878, loss = 0.48200845\n",
      "Iteration 879, loss = 0.48303205\n",
      "Iteration 880, loss = 0.48361762\n",
      "Iteration 881, loss = 0.48159486\n",
      "Iteration 882, loss = 0.48148871\n",
      "Iteration 883, loss = 0.48199962\n",
      "Iteration 884, loss = 0.48177781\n",
      "Iteration 885, loss = 0.48169992\n",
      "Iteration 886, loss = 0.48160170\n",
      "Iteration 887, loss = 0.48149330\n",
      "Iteration 888, loss = 0.48139323\n",
      "Iteration 889, loss = 0.48145949\n",
      "Iteration 890, loss = 0.48165940\n",
      "Iteration 891, loss = 0.48173624\n",
      "Iteration 892, loss = 0.48138024\n",
      "Iteration 893, loss = 0.48176268\n",
      "Iteration 894, loss = 0.48120615\n",
      "Iteration 895, loss = 0.48152488\n",
      "Iteration 896, loss = 0.48204984\n",
      "Iteration 897, loss = 0.48275004\n",
      "Iteration 898, loss = 0.48264266\n",
      "Iteration 899, loss = 0.48192648\n",
      "Iteration 900, loss = 0.48129736\n",
      "Iteration 901, loss = 0.48119476\n",
      "Iteration 902, loss = 0.48155817\n",
      "Iteration 903, loss = 0.48332504\n",
      "Iteration 904, loss = 0.48189723\n",
      "Iteration 905, loss = 0.48109708\n",
      "Iteration 906, loss = 0.48140480\n",
      "Iteration 907, loss = 0.48255899\n",
      "Iteration 908, loss = 0.48325513\n",
      "Iteration 909, loss = 0.48295828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 910, loss = 0.48209238\n",
      "Iteration 911, loss = 0.48237516\n",
      "Iteration 912, loss = 0.48132185\n",
      "Iteration 913, loss = 0.48137672\n",
      "Iteration 914, loss = 0.48151322\n",
      "Iteration 915, loss = 0.48151640\n",
      "Iteration 916, loss = 0.48115941\n",
      "Iteration 917, loss = 0.48195805\n",
      "Iteration 918, loss = 0.48152516\n",
      "Iteration 919, loss = 0.48148026\n",
      "Iteration 920, loss = 0.48100474\n",
      "Iteration 921, loss = 0.48130930\n",
      "Iteration 922, loss = 0.48226551\n",
      "Iteration 923, loss = 0.48191349\n",
      "Iteration 924, loss = 0.48066831\n",
      "Iteration 925, loss = 0.48244887\n",
      "Iteration 926, loss = 0.48360295\n",
      "Iteration 927, loss = 0.48255154\n",
      "Iteration 928, loss = 0.48153614\n",
      "Iteration 929, loss = 0.48140943\n",
      "Iteration 930, loss = 0.48185828\n",
      "Iteration 931, loss = 0.48208483\n",
      "Iteration 932, loss = 0.48189775\n",
      "Iteration 933, loss = 0.48141844\n",
      "Iteration 934, loss = 0.48108052\n",
      "Iteration 935, loss = 0.48108148\n",
      "Iteration 936, loss = 0.48134507\n",
      "Iteration 937, loss = 0.48146403\n",
      "Iteration 938, loss = 0.48217660\n",
      "Iteration 939, loss = 0.48165734\n",
      "Iteration 940, loss = 0.48102442\n",
      "Iteration 941, loss = 0.48127389\n",
      "Iteration 942, loss = 0.48165368\n",
      "Iteration 943, loss = 0.48183103\n",
      "Iteration 944, loss = 0.48118536\n",
      "Iteration 945, loss = 0.48236326\n",
      "Iteration 946, loss = 0.48146991\n",
      "Iteration 947, loss = 0.48116413\n",
      "Iteration 948, loss = 0.48221902\n",
      "Iteration 949, loss = 0.48127489\n",
      "Iteration 950, loss = 0.48113557\n",
      "Iteration 951, loss = 0.48139219\n",
      "Iteration 952, loss = 0.48124644\n",
      "Iteration 953, loss = 0.48095003\n",
      "Iteration 954, loss = 0.48113828\n",
      "Iteration 955, loss = 0.48109119\n",
      "Iteration 956, loss = 0.48125141\n",
      "Iteration 957, loss = 0.48100547\n",
      "Iteration 958, loss = 0.48106713\n",
      "Iteration 959, loss = 0.48130082\n",
      "Iteration 960, loss = 0.48129037\n",
      "Iteration 961, loss = 0.48100312\n",
      "Iteration 962, loss = 0.48164788\n",
      "Iteration 963, loss = 0.48172872\n",
      "Iteration 964, loss = 0.48105052\n",
      "Iteration 965, loss = 0.48111349\n",
      "Iteration 966, loss = 0.48128244\n",
      "Iteration 967, loss = 0.48067464\n",
      "Iteration 968, loss = 0.48117964\n",
      "Iteration 969, loss = 0.48200269\n",
      "Iteration 970, loss = 0.48183652\n",
      "Iteration 971, loss = 0.48107079\n",
      "Iteration 972, loss = 0.48062235\n",
      "Iteration 973, loss = 0.48195975\n",
      "Iteration 974, loss = 0.48180510\n",
      "Iteration 975, loss = 0.48152062\n",
      "Iteration 976, loss = 0.48098655\n",
      "Iteration 977, loss = 0.48135673\n",
      "Iteration 978, loss = 0.48127721\n",
      "Iteration 979, loss = 0.48096105\n",
      "Iteration 980, loss = 0.48092406\n",
      "Iteration 981, loss = 0.48140783\n",
      "Iteration 982, loss = 0.48151043\n",
      "Iteration 983, loss = 0.48107923\n",
      "Iteration 984, loss = 0.48077080\n",
      "Iteration 985, loss = 0.48127126\n",
      "Iteration 986, loss = 0.48074072\n",
      "Iteration 987, loss = 0.48120251\n",
      "Iteration 988, loss = 0.48076872\n",
      "Iteration 989, loss = 0.48093907\n",
      "Iteration 990, loss = 0.48081598\n",
      "Iteration 991, loss = 0.48077074\n",
      "Iteration 992, loss = 0.48067183\n",
      "Iteration 993, loss = 0.48098184\n",
      "Iteration 994, loss = 0.48081171\n",
      "Iteration 995, loss = 0.48074468\n",
      "Iteration 996, loss = 0.48065667\n",
      "Iteration 997, loss = 0.48066668\n",
      "Iteration 998, loss = 0.48083614\n",
      "Iteration 999, loss = 0.48099251\n",
      "Iteration 1000, loss = 0.48105532\n",
      "Iteration 1001, loss = 0.48082575\n",
      "Iteration 1002, loss = 0.48065728\n",
      "Iteration 1003, loss = 0.48079626\n",
      "Iteration 1004, loss = 0.48090064\n",
      "Iteration 1005, loss = 0.48091288\n",
      "Iteration 1006, loss = 0.48079355\n",
      "Iteration 1007, loss = 0.48068097\n",
      "Iteration 1008, loss = 0.48061515\n",
      "Iteration 1009, loss = 0.48065082\n",
      "Iteration 1010, loss = 0.48071388\n",
      "Iteration 1011, loss = 0.48075254\n",
      "Iteration 1012, loss = 0.48102005\n",
      "Iteration 1013, loss = 0.48075083\n",
      "Iteration 1014, loss = 0.48107194\n",
      "Iteration 1015, loss = 0.48042956\n",
      "Iteration 1016, loss = 0.48112559\n",
      "Iteration 1017, loss = 0.48129913\n",
      "Iteration 1018, loss = 0.48098474\n",
      "Iteration 1019, loss = 0.48077091\n",
      "Iteration 1020, loss = 0.48141550\n",
      "Iteration 1021, loss = 0.48067532\n",
      "Iteration 1022, loss = 0.48075554\n",
      "Iteration 1023, loss = 0.48062950\n",
      "Iteration 1024, loss = 0.48062442\n",
      "Iteration 1025, loss = 0.48060261\n",
      "Iteration 1026, loss = 0.48065743\n",
      "Iteration 1027, loss = 0.48068981\n",
      "Iteration 1028, loss = 0.48067549\n",
      "Iteration 1029, loss = 0.48049829\n",
      "Iteration 1030, loss = 0.48067090\n",
      "Iteration 1031, loss = 0.48050272\n",
      "Iteration 1032, loss = 0.48051908\n",
      "Iteration 1033, loss = 0.48067324\n",
      "Iteration 1034, loss = 0.48081287\n",
      "Iteration 1035, loss = 0.48024019\n",
      "Iteration 1036, loss = 0.48111836\n",
      "Iteration 1037, loss = 0.48231927\n",
      "Iteration 1038, loss = 0.48254198\n",
      "Iteration 1039, loss = 0.48116608\n",
      "Iteration 1040, loss = 0.48080364\n",
      "Iteration 1041, loss = 0.48038435\n",
      "Iteration 1042, loss = 0.48053995\n",
      "Iteration 1043, loss = 0.48155113\n",
      "Iteration 1044, loss = 0.48119632\n",
      "Iteration 1045, loss = 0.48057555\n",
      "Iteration 1046, loss = 0.48075036\n",
      "Iteration 1047, loss = 0.48043116\n",
      "Iteration 1048, loss = 0.48048607\n",
      "Iteration 1049, loss = 0.48138428\n",
      "Iteration 1050, loss = 0.48056117\n",
      "Iteration 1051, loss = 0.48034039\n",
      "Iteration 1052, loss = 0.48042188\n",
      "Iteration 1053, loss = 0.48032475\n",
      "Iteration 1054, loss = 0.48030613\n",
      "Iteration 1055, loss = 0.48040874\n",
      "Iteration 1056, loss = 0.48039730\n",
      "Iteration 1057, loss = 0.48045042\n",
      "Iteration 1058, loss = 0.48076978\n",
      "Iteration 1059, loss = 0.48099407\n",
      "Iteration 1060, loss = 0.48059115\n",
      "Iteration 1061, loss = 0.48070140\n",
      "Iteration 1062, loss = 0.48018635\n",
      "Iteration 1063, loss = 0.48032520\n",
      "Iteration 1064, loss = 0.48093358\n",
      "Iteration 1065, loss = 0.48166620\n",
      "Iteration 1066, loss = 0.48088894\n",
      "Iteration 1067, loss = 0.48034717\n",
      "Iteration 1068, loss = 0.48083090\n",
      "Iteration 1069, loss = 0.48090663\n",
      "Iteration 1070, loss = 0.48028215\n",
      "Iteration 1071, loss = 0.48057294\n",
      "Iteration 1072, loss = 0.48072831\n",
      "Iteration 1073, loss = 0.48068952\n",
      "Iteration 1074, loss = 0.48115904\n",
      "Iteration 1075, loss = 0.48096531\n",
      "Iteration 1076, loss = 0.48001094\n",
      "Iteration 1077, loss = 0.48051965\n",
      "Iteration 1078, loss = 0.48102889\n",
      "Iteration 1079, loss = 0.48085510\n",
      "Iteration 1080, loss = 0.48104605\n",
      "Iteration 1081, loss = 0.48032093\n",
      "Iteration 1082, loss = 0.48010706\n",
      "Iteration 1083, loss = 0.48006879\n",
      "Iteration 1084, loss = 0.48100240\n",
      "Iteration 1085, loss = 0.48037758\n",
      "Iteration 1086, loss = 0.48030044\n",
      "Iteration 1087, loss = 0.48014582\n",
      "Iteration 1088, loss = 0.48061510\n",
      "Iteration 1089, loss = 0.48033497\n",
      "Iteration 1090, loss = 0.48007886\n",
      "Iteration 1091, loss = 0.48007384\n",
      "Iteration 1092, loss = 0.48018848\n",
      "Iteration 1093, loss = 0.48031096\n",
      "Iteration 1094, loss = 0.48053054\n",
      "Iteration 1095, loss = 0.48023502\n",
      "Iteration 1096, loss = 0.48019062\n",
      "Iteration 1097, loss = 0.48044527\n",
      "Iteration 1098, loss = 0.48009387\n",
      "Iteration 1099, loss = 0.48009785\n",
      "Iteration 1100, loss = 0.48010212\n",
      "Iteration 1101, loss = 0.48002695\n",
      "Iteration 1102, loss = 0.48001747\n",
      "Iteration 1103, loss = 0.48001457\n",
      "Iteration 1104, loss = 0.47999832\n",
      "Iteration 1105, loss = 0.48007461\n",
      "Iteration 1106, loss = 0.48034375\n",
      "Iteration 1107, loss = 0.48034068\n",
      "Iteration 1108, loss = 0.48007222\n",
      "Iteration 1109, loss = 0.47988521\n",
      "Iteration 1110, loss = 0.48012934\n",
      "Iteration 1111, loss = 0.48115849\n",
      "Iteration 1112, loss = 0.48066186\n",
      "Iteration 1113, loss = 0.47993090\n",
      "Iteration 1114, loss = 0.48036396\n",
      "Iteration 1115, loss = 0.48071486\n",
      "Iteration 1116, loss = 0.48109126\n",
      "Iteration 1117, loss = 0.48061617\n",
      "Iteration 1118, loss = 0.48008097\n",
      "Iteration 1119, loss = 0.48016035\n",
      "Iteration 1120, loss = 0.48021705\n",
      "Iteration 1121, loss = 0.47995717\n",
      "Iteration 1122, loss = 0.47981226\n",
      "Iteration 1123, loss = 0.47999169\n",
      "Iteration 1124, loss = 0.48073101\n",
      "Iteration 1125, loss = 0.48090255\n",
      "Iteration 1126, loss = 0.48045039\n",
      "Iteration 1127, loss = 0.47967530\n",
      "Iteration 1128, loss = 0.47967633\n",
      "Iteration 1129, loss = 0.48058518\n",
      "Iteration 1130, loss = 0.48130014\n",
      "Iteration 1131, loss = 0.48054726\n",
      "Iteration 1132, loss = 0.47971192\n",
      "Iteration 1133, loss = 0.48025431\n",
      "Iteration 1134, loss = 0.48088892\n",
      "Iteration 1135, loss = 0.48119043\n",
      "Iteration 1136, loss = 0.47981855\n",
      "Iteration 1137, loss = 0.47974331\n",
      "Iteration 1138, loss = 0.47995914\n",
      "Iteration 1139, loss = 0.48035643\n",
      "Iteration 1140, loss = 0.48038187\n",
      "Iteration 1141, loss = 0.48002515\n",
      "Iteration 1142, loss = 0.47983195\n",
      "Iteration 1143, loss = 0.47977774\n",
      "Iteration 1144, loss = 0.48008084\n",
      "Iteration 1145, loss = 0.48071054\n",
      "Iteration 1146, loss = 0.48007578\n",
      "Iteration 1147, loss = 0.47994907\n",
      "Iteration 1148, loss = 0.47977705\n",
      "Iteration 1149, loss = 0.47970741\n",
      "Iteration 1150, loss = 0.47986529\n",
      "Iteration 1151, loss = 0.48073386\n",
      "Iteration 1152, loss = 0.48009112\n",
      "Iteration 1153, loss = 0.47944572\n",
      "Iteration 1154, loss = 0.48113532\n",
      "Iteration 1155, loss = 0.48104009\n",
      "Iteration 1156, loss = 0.48119418\n",
      "Iteration 1157, loss = 0.48058331\n",
      "Iteration 1158, loss = 0.47977261\n",
      "Iteration 1159, loss = 0.48018255\n",
      "Iteration 1160, loss = 0.48046623\n",
      "Iteration 1161, loss = 0.47989591\n",
      "Iteration 1162, loss = 0.47969415\n",
      "Iteration 1163, loss = 0.47983047\n",
      "Iteration 1164, loss = 0.47968729\n",
      "Iteration 1165, loss = 0.47968446\n",
      "Iteration 1166, loss = 0.47954680\n",
      "Iteration 1167, loss = 0.47960979\n",
      "Iteration 1168, loss = 0.47979221\n",
      "Iteration 1169, loss = 0.47976974\n",
      "Iteration 1170, loss = 0.48000999\n",
      "Iteration 1171, loss = 0.47967511\n",
      "Iteration 1172, loss = 0.47963184\n",
      "Iteration 1173, loss = 0.47951898\n",
      "Iteration 1174, loss = 0.47951729\n",
      "Iteration 1175, loss = 0.47963108\n",
      "Iteration 1176, loss = 0.47998605\n",
      "Iteration 1177, loss = 0.48007768\n",
      "Iteration 1178, loss = 0.47994806\n",
      "Iteration 1179, loss = 0.47944176\n",
      "Iteration 1180, loss = 0.47972324\n",
      "Iteration 1181, loss = 0.48132615\n",
      "Iteration 1182, loss = 0.48007503\n",
      "Iteration 1183, loss = 0.47927161\n",
      "Iteration 1184, loss = 0.48006506\n",
      "Iteration 1185, loss = 0.48052413\n",
      "Iteration 1186, loss = 0.48034176\n",
      "Iteration 1187, loss = 0.48007792\n",
      "Iteration 1188, loss = 0.47943835\n",
      "Iteration 1189, loss = 0.47951080\n",
      "Iteration 1190, loss = 0.47976969\n",
      "Iteration 1191, loss = 0.48010560\n",
      "Iteration 1192, loss = 0.48011778\n",
      "Iteration 1193, loss = 0.48051534\n",
      "Iteration 1194, loss = 0.47961912\n",
      "Iteration 1195, loss = 0.47951854\n",
      "Iteration 1196, loss = 0.48001425\n",
      "Iteration 1197, loss = 0.47957269\n",
      "Iteration 1198, loss = 0.47923484\n",
      "Iteration 1199, loss = 0.47985003\n",
      "Iteration 1200, loss = 0.47993877\n",
      "Iteration 1201, loss = 0.47948674\n",
      "Iteration 1202, loss = 0.47912167\n",
      "Iteration 1203, loss = 0.48018659\n",
      "Iteration 1204, loss = 0.48027516\n",
      "Iteration 1205, loss = 0.47992723\n",
      "Iteration 1206, loss = 0.47959353\n",
      "Iteration 1207, loss = 0.47937957\n",
      "Iteration 1208, loss = 0.47961197\n",
      "Iteration 1209, loss = 0.47957688\n",
      "Iteration 1210, loss = 0.47947495\n",
      "Iteration 1211, loss = 0.47966085\n",
      "Iteration 1212, loss = 0.47979761\n",
      "Iteration 1213, loss = 0.47993762\n",
      "Iteration 1214, loss = 0.48043862\n",
      "Iteration 1215, loss = 0.47989668\n",
      "Iteration 1216, loss = 0.47980072\n",
      "Iteration 1217, loss = 0.47943081\n",
      "Iteration 1218, loss = 0.47969295\n",
      "Iteration 1219, loss = 0.47927404\n",
      "Iteration 1220, loss = 0.47926518\n",
      "Iteration 1221, loss = 0.47942094\n",
      "Iteration 1222, loss = 0.47949460\n",
      "Iteration 1223, loss = 0.47939179\n",
      "Iteration 1224, loss = 0.47959584\n",
      "Iteration 1225, loss = 0.47944645\n",
      "Iteration 1226, loss = 0.47943496\n",
      "Iteration 1227, loss = 0.47939953\n",
      "Iteration 1228, loss = 0.47923469\n",
      "Iteration 1229, loss = 0.47930036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1230, loss = 0.47923090\n",
      "Iteration 1231, loss = 0.47939544\n",
      "Iteration 1232, loss = 0.47921125\n",
      "Iteration 1233, loss = 0.47916834\n",
      "Iteration 1234, loss = 0.47969079\n",
      "Iteration 1235, loss = 0.47908133\n",
      "Iteration 1236, loss = 0.47911619\n",
      "Iteration 1237, loss = 0.47950545\n",
      "Iteration 1238, loss = 0.47960937\n",
      "Iteration 1239, loss = 0.47926182\n",
      "Iteration 1240, loss = 0.47906107\n",
      "Iteration 1241, loss = 0.47920030\n",
      "Iteration 1242, loss = 0.47946815\n",
      "Iteration 1243, loss = 0.47955018\n",
      "Iteration 1244, loss = 0.47915408\n",
      "Iteration 1245, loss = 0.47904448\n",
      "Iteration 1246, loss = 0.47970462\n",
      "Iteration 1247, loss = 0.47946156\n",
      "Iteration 1248, loss = 0.47922178\n",
      "Iteration 1249, loss = 0.47901281\n",
      "Iteration 1250, loss = 0.47925465\n",
      "Iteration 1251, loss = 0.47952623\n",
      "Iteration 1252, loss = 0.47936934\n",
      "Iteration 1253, loss = 0.47900690\n",
      "Iteration 1254, loss = 0.47898422\n",
      "Iteration 1255, loss = 0.47924446\n",
      "Iteration 1256, loss = 0.47958381\n",
      "Iteration 1257, loss = 0.47959358\n",
      "Iteration 1258, loss = 0.47936292\n",
      "Iteration 1259, loss = 0.47921623\n",
      "Iteration 1260, loss = 0.47905693\n",
      "Iteration 1261, loss = 0.47918668\n",
      "Iteration 1262, loss = 0.47915486\n",
      "Iteration 1263, loss = 0.47897330\n",
      "Iteration 1264, loss = 0.47922641\n",
      "Iteration 1265, loss = 0.47910304\n",
      "Iteration 1266, loss = 0.47949417\n",
      "Iteration 1267, loss = 0.47921009\n",
      "Iteration 1268, loss = 0.47931106\n",
      "Iteration 1269, loss = 0.47921310\n",
      "Iteration 1270, loss = 0.47904240\n",
      "Iteration 1271, loss = 0.47899820\n",
      "Iteration 1272, loss = 0.48025892\n",
      "Iteration 1273, loss = 0.47873163\n",
      "Iteration 1274, loss = 0.47926317\n",
      "Iteration 1275, loss = 0.47992542\n",
      "Iteration 1276, loss = 0.47983557\n",
      "Iteration 1277, loss = 0.47897125\n",
      "Iteration 1278, loss = 0.47888337\n",
      "Iteration 1279, loss = 0.47926964\n",
      "Iteration 1280, loss = 0.47987107\n",
      "Iteration 1281, loss = 0.47990216\n",
      "Iteration 1282, loss = 0.47968764\n",
      "Iteration 1283, loss = 0.47899903\n",
      "Iteration 1284, loss = 0.47886543\n",
      "Iteration 1285, loss = 0.47899462\n",
      "Iteration 1286, loss = 0.47925245\n",
      "Iteration 1287, loss = 0.47908940\n",
      "Iteration 1288, loss = 0.47875363\n",
      "Iteration 1289, loss = 0.47913721\n",
      "Iteration 1290, loss = 0.47984000\n",
      "Iteration 1291, loss = 0.47924854\n",
      "Iteration 1292, loss = 0.47864997\n",
      "Iteration 1293, loss = 0.47891435\n",
      "Iteration 1294, loss = 0.47944480\n",
      "Iteration 1295, loss = 0.47971458\n",
      "Iteration 1296, loss = 0.48092597\n",
      "Iteration 1297, loss = 0.47884419\n",
      "Iteration 1298, loss = 0.47875994\n",
      "Iteration 1299, loss = 0.47908914\n",
      "Iteration 1300, loss = 0.47886710\n",
      "Iteration 1301, loss = 0.47920716\n",
      "Iteration 1302, loss = 0.47895940\n",
      "Iteration 1303, loss = 0.47927147\n",
      "Iteration 1304, loss = 0.47877724\n",
      "Iteration 1305, loss = 0.47891745\n",
      "Iteration 1306, loss = 0.47877816\n",
      "Iteration 1307, loss = 0.47860748\n",
      "Iteration 1308, loss = 0.47876014\n",
      "Iteration 1309, loss = 0.47924205\n",
      "Iteration 1310, loss = 0.47904279\n",
      "Iteration 1311, loss = 0.47861825\n",
      "Iteration 1312, loss = 0.47931778\n",
      "Iteration 1313, loss = 0.47904099\n",
      "Iteration 1314, loss = 0.47888922\n",
      "Iteration 1315, loss = 0.47884567\n",
      "Iteration 1316, loss = 0.47871017\n",
      "Iteration 1317, loss = 0.47883979\n",
      "Iteration 1318, loss = 0.47873647\n",
      "Iteration 1319, loss = 0.47864443\n",
      "Iteration 1320, loss = 0.47880815\n",
      "Iteration 1321, loss = 0.47899463\n",
      "Iteration 1322, loss = 0.47873820\n",
      "Iteration 1323, loss = 0.47852074\n",
      "Iteration 1324, loss = 0.47905708\n",
      "Iteration 1325, loss = 0.47905043\n",
      "Iteration 1326, loss = 0.47866071\n",
      "Iteration 1327, loss = 0.47837873\n",
      "Iteration 1328, loss = 0.47947184\n",
      "Iteration 1329, loss = 0.48020732\n",
      "Iteration 1330, loss = 0.47932328\n",
      "Iteration 1331, loss = 0.47987780\n",
      "Iteration 1332, loss = 0.47873552\n",
      "Iteration 1333, loss = 0.47911141\n",
      "Iteration 1334, loss = 0.47939823\n",
      "Iteration 1335, loss = 0.47963431\n",
      "Iteration 1336, loss = 0.47849224\n",
      "Iteration 1337, loss = 0.47858216\n",
      "Iteration 1338, loss = 0.47872121\n",
      "Iteration 1339, loss = 0.47905505\n",
      "Iteration 1340, loss = 0.47884739\n",
      "Iteration 1341, loss = 0.47878477\n",
      "Iteration 1342, loss = 0.47872770\n",
      "Iteration 1343, loss = 0.47843781\n",
      "Iteration 1344, loss = 0.47850970\n",
      "Iteration 1345, loss = 0.47868729\n",
      "Iteration 1346, loss = 0.47847096\n",
      "Iteration 1347, loss = 0.47848349\n",
      "Iteration 1348, loss = 0.47846932\n",
      "Iteration 1349, loss = 0.47842231\n",
      "Iteration 1350, loss = 0.47855570\n",
      "Iteration 1351, loss = 0.47846757\n",
      "Iteration 1352, loss = 0.47874931\n",
      "Iteration 1353, loss = 0.47894366\n",
      "Iteration 1354, loss = 0.47955742\n",
      "Iteration 1355, loss = 0.47854021\n",
      "Iteration 1356, loss = 0.47841577\n",
      "Iteration 1357, loss = 0.47859604\n",
      "Iteration 1358, loss = 0.47846070\n",
      "Iteration 1359, loss = 0.47833195\n",
      "Iteration 1360, loss = 0.47836989\n",
      "Iteration 1361, loss = 0.47887818\n",
      "Iteration 1362, loss = 0.47857506\n",
      "Iteration 1363, loss = 0.47832296\n",
      "Iteration 1364, loss = 0.47833820\n",
      "Iteration 1365, loss = 0.47896579\n",
      "Iteration 1366, loss = 0.47947839\n",
      "Iteration 1367, loss = 0.47841543\n",
      "Iteration 1368, loss = 0.47837943\n",
      "Iteration 1369, loss = 0.47829087\n",
      "Iteration 1370, loss = 0.47829983\n",
      "Iteration 1371, loss = 0.47846706\n",
      "Iteration 1372, loss = 0.47872602\n",
      "Iteration 1373, loss = 0.47856527\n",
      "Iteration 1374, loss = 0.47844309\n",
      "Iteration 1375, loss = 0.47852337\n",
      "Iteration 1376, loss = 0.47853129\n",
      "Iteration 1377, loss = 0.47822303\n",
      "Iteration 1378, loss = 0.47830281\n",
      "Iteration 1379, loss = 0.47882735\n",
      "Iteration 1380, loss = 0.47922728\n",
      "Iteration 1381, loss = 0.47909730\n",
      "Iteration 1382, loss = 0.47832793\n",
      "Iteration 1383, loss = 0.47798115\n",
      "Iteration 1384, loss = 0.47908705\n",
      "Iteration 1385, loss = 0.47944349\n",
      "Iteration 1386, loss = 0.47928849\n",
      "Iteration 1387, loss = 0.47830661\n",
      "Iteration 1388, loss = 0.47953664\n",
      "Iteration 1389, loss = 0.47883489\n",
      "Iteration 1390, loss = 0.47824262\n",
      "Iteration 1391, loss = 0.47841886\n",
      "Iteration 1392, loss = 0.47873349\n",
      "Iteration 1393, loss = 0.47868261\n",
      "Iteration 1394, loss = 0.47844876\n",
      "Iteration 1395, loss = 0.47815352\n",
      "Iteration 1396, loss = 0.47815334\n",
      "Iteration 1397, loss = 0.47825695\n",
      "Iteration 1398, loss = 0.47835318\n",
      "Iteration 1399, loss = 0.47832693\n",
      "Iteration 1400, loss = 0.47822137\n",
      "Iteration 1401, loss = 0.47841106\n",
      "Iteration 1402, loss = 0.47837433\n",
      "Iteration 1403, loss = 0.47854602\n",
      "Iteration 1404, loss = 0.47802042\n",
      "Iteration 1405, loss = 0.47885883\n",
      "Iteration 1406, loss = 0.47869879\n",
      "Iteration 1407, loss = 0.47849760\n",
      "Iteration 1408, loss = 0.47862785\n",
      "Iteration 1409, loss = 0.47849873\n",
      "Iteration 1410, loss = 0.47800261\n",
      "Iteration 1411, loss = 0.47857114\n",
      "Iteration 1412, loss = 0.47837053\n",
      "Iteration 1413, loss = 0.47824254\n",
      "Iteration 1414, loss = 0.47826117\n",
      "Iteration 1415, loss = 0.47809273\n",
      "Iteration 1416, loss = 0.47822108\n",
      "Iteration 1417, loss = 0.47817944\n",
      "Iteration 1418, loss = 0.47809189\n",
      "Iteration 1419, loss = 0.47825325\n",
      "Iteration 1420, loss = 0.47806656\n",
      "Iteration 1421, loss = 0.47811663\n",
      "Iteration 1422, loss = 0.47828759\n",
      "Iteration 1423, loss = 0.47817817\n",
      "Iteration 1424, loss = 0.47780889\n",
      "Iteration 1425, loss = 0.47845188\n",
      "Iteration 1426, loss = 0.47862905\n",
      "Iteration 1427, loss = 0.47852792\n",
      "Iteration 1428, loss = 0.47810288\n",
      "Iteration 1429, loss = 0.47827910\n",
      "Iteration 1430, loss = 0.47832702\n",
      "Iteration 1431, loss = 0.47823297\n",
      "Iteration 1432, loss = 0.47794914\n",
      "Iteration 1433, loss = 0.47822209\n",
      "Iteration 1434, loss = 0.47818391\n",
      "Iteration 1435, loss = 0.47840657\n",
      "Iteration 1436, loss = 0.47814836\n",
      "Iteration 1437, loss = 0.47779667\n",
      "Iteration 1438, loss = 0.47821953\n",
      "Iteration 1439, loss = 0.47876611\n",
      "Iteration 1440, loss = 0.47897296\n",
      "Iteration 1441, loss = 0.47844080\n",
      "Iteration 1442, loss = 0.47826729\n",
      "Iteration 1443, loss = 0.47796627\n",
      "Iteration 1444, loss = 0.47776862\n",
      "Iteration 1445, loss = 0.47799350\n",
      "Iteration 1446, loss = 0.47864541\n",
      "Iteration 1447, loss = 0.47865795\n",
      "Iteration 1448, loss = 0.47812533\n",
      "Iteration 1449, loss = 0.47791597\n",
      "Iteration 1450, loss = 0.47807439\n",
      "Iteration 1451, loss = 0.47799650\n",
      "Iteration 1452, loss = 0.47819890\n",
      "Iteration 1453, loss = 0.47785736\n",
      "Iteration 1454, loss = 0.47791257\n",
      "Iteration 1455, loss = 0.47783903\n",
      "Iteration 1456, loss = 0.47805822\n",
      "Iteration 1457, loss = 0.47801178\n",
      "Iteration 1458, loss = 0.47781664\n",
      "Iteration 1459, loss = 0.47789972\n",
      "Iteration 1460, loss = 0.47795270\n",
      "Iteration 1461, loss = 0.47818423\n",
      "Iteration 1462, loss = 0.47791181\n",
      "Iteration 1463, loss = 0.47798548\n",
      "Iteration 1464, loss = 0.47822083\n",
      "Iteration 1465, loss = 0.47855400\n",
      "Iteration 1466, loss = 0.47816095\n",
      "Iteration 1467, loss = 0.47821765\n",
      "Iteration 1468, loss = 0.47809273\n",
      "Iteration 1469, loss = 0.47824055\n",
      "Iteration 1470, loss = 0.47778366\n",
      "Iteration 1471, loss = 0.47775498\n",
      "Iteration 1472, loss = 0.47777682\n",
      "Iteration 1473, loss = 0.47777953\n",
      "Iteration 1474, loss = 0.47789960\n",
      "Iteration 1475, loss = 0.47788734\n",
      "Iteration 1476, loss = 0.47818303\n",
      "Iteration 1477, loss = 0.47829278\n",
      "Iteration 1478, loss = 0.47805735\n",
      "Iteration 1479, loss = 0.47766734\n",
      "Iteration 1480, loss = 0.47811051\n",
      "Iteration 1481, loss = 0.47824040\n",
      "Iteration 1482, loss = 0.47812051\n",
      "Iteration 1483, loss = 0.47770099\n",
      "Iteration 1484, loss = 0.47917045\n",
      "Iteration 1485, loss = 0.47834113\n",
      "Iteration 1486, loss = 0.47829922\n",
      "Iteration 1487, loss = 0.47766976\n",
      "Iteration 1488, loss = 0.47824490\n",
      "Iteration 1489, loss = 0.47843613\n",
      "Iteration 1490, loss = 0.47776937\n",
      "Iteration 1491, loss = 0.47758978\n",
      "Iteration 1492, loss = 0.47789685\n",
      "Iteration 1493, loss = 0.47814791\n",
      "Iteration 1494, loss = 0.47822369\n",
      "Iteration 1495, loss = 0.47806781\n",
      "Iteration 1496, loss = 0.47779555\n",
      "Iteration 1497, loss = 0.47796815\n",
      "Iteration 1498, loss = 0.47838404\n",
      "Iteration 1499, loss = 0.47799103\n",
      "Iteration 1500, loss = 0.47793661\n",
      "Iteration 1501, loss = 0.47836183\n",
      "Iteration 1502, loss = 0.47759976\n",
      "Iteration 1503, loss = 0.47789405\n",
      "Iteration 1504, loss = 0.47781592\n",
      "Iteration 1505, loss = 0.47770362\n",
      "Iteration 1506, loss = 0.47747457\n",
      "Iteration 1507, loss = 0.47765533\n",
      "Iteration 1508, loss = 0.47793610\n",
      "Iteration 1509, loss = 0.47791666\n",
      "Iteration 1510, loss = 0.47760341\n",
      "Iteration 1511, loss = 0.47785131\n",
      "Iteration 1512, loss = 0.47771811\n",
      "Iteration 1513, loss = 0.47756106\n",
      "Iteration 1514, loss = 0.47790397\n",
      "Iteration 1515, loss = 0.47774656\n",
      "Iteration 1516, loss = 0.47760524\n",
      "Iteration 1517, loss = 0.47788508\n",
      "Iteration 1518, loss = 0.47781966\n",
      "Iteration 1519, loss = 0.47817132\n",
      "Iteration 1520, loss = 0.47755120\n",
      "Iteration 1521, loss = 0.47752667\n",
      "Iteration 1522, loss = 0.47762940\n",
      "Iteration 1523, loss = 0.47776854\n",
      "Iteration 1524, loss = 0.47761228\n",
      "Iteration 1525, loss = 0.47771163\n",
      "Iteration 1526, loss = 0.47753703\n",
      "Iteration 1527, loss = 0.47746903\n",
      "Iteration 1528, loss = 0.47751837\n",
      "Iteration 1529, loss = 0.47745343\n",
      "Iteration 1530, loss = 0.47747562\n",
      "Iteration 1531, loss = 0.47754912\n",
      "Iteration 1532, loss = 0.47745294\n",
      "Iteration 1533, loss = 0.47768485\n",
      "Iteration 1534, loss = 0.47758099\n",
      "Iteration 1535, loss = 0.47746211\n",
      "Iteration 1536, loss = 0.47760059\n",
      "Iteration 1537, loss = 0.47757342\n",
      "Iteration 1538, loss = 0.47843723\n",
      "Iteration 1539, loss = 0.47872398\n",
      "Iteration 1540, loss = 0.47781827\n",
      "Iteration 1541, loss = 0.47746859\n",
      "Iteration 1542, loss = 0.47813013\n",
      "Iteration 1543, loss = 0.47774442\n",
      "Iteration 1544, loss = 0.47765631\n",
      "Iteration 1545, loss = 0.47751364\n",
      "Iteration 1546, loss = 0.47745018\n",
      "Iteration 1547, loss = 0.47745180\n",
      "Iteration 1548, loss = 0.47748051\n",
      "Iteration 1549, loss = 0.47751232\n",
      "Iteration 1550, loss = 0.47736242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1551, loss = 0.47735784\n",
      "Iteration 1552, loss = 0.47754628\n",
      "Iteration 1553, loss = 0.47773482\n",
      "Iteration 1554, loss = 0.47762942\n",
      "Iteration 1555, loss = 0.47716508\n",
      "Iteration 1556, loss = 0.47757553\n",
      "Iteration 1557, loss = 0.47783881\n",
      "Iteration 1558, loss = 0.47798667\n",
      "Iteration 1559, loss = 0.47754336\n",
      "Iteration 1560, loss = 0.47708840\n",
      "Iteration 1561, loss = 0.47835221\n",
      "Iteration 1562, loss = 0.47805001\n",
      "Iteration 1563, loss = 0.47754121\n",
      "Iteration 1564, loss = 0.47716472\n",
      "Iteration 1565, loss = 0.47744443\n",
      "Iteration 1566, loss = 0.47840229\n",
      "Iteration 1567, loss = 0.47867902\n",
      "Iteration 1568, loss = 0.47740696\n",
      "Iteration 1569, loss = 0.47701458\n",
      "Iteration 1570, loss = 0.47843020\n",
      "Iteration 1571, loss = 0.47908079\n",
      "Iteration 1572, loss = 0.47857029\n",
      "Iteration 1573, loss = 0.47742161\n",
      "Iteration 1574, loss = 0.47739058\n",
      "Iteration 1575, loss = 0.47763004\n",
      "Iteration 1576, loss = 0.47777510\n",
      "Iteration 1577, loss = 0.47718768\n",
      "Iteration 1578, loss = 0.47749865\n",
      "Iteration 1579, loss = 0.47776435\n",
      "Iteration 1580, loss = 0.47796023\n",
      "Iteration 1581, loss = 0.47768038\n",
      "Iteration 1582, loss = 0.47749333\n",
      "Iteration 1583, loss = 0.47721566\n",
      "Iteration 1584, loss = 0.47754511\n",
      "Iteration 1585, loss = 0.47724508\n",
      "Iteration 1586, loss = 0.47717019\n",
      "Iteration 1587, loss = 0.47725562\n",
      "Iteration 1588, loss = 0.47770212\n",
      "Iteration 1589, loss = 0.47745827\n",
      "Iteration 1590, loss = 0.47708909\n",
      "Iteration 1591, loss = 0.47794741\n",
      "Iteration 1592, loss = 0.47746119\n",
      "Iteration 1593, loss = 0.47723307\n",
      "Iteration 1594, loss = 0.47706295\n",
      "Iteration 1595, loss = 0.47787064\n",
      "Iteration 1596, loss = 0.47761132\n",
      "Iteration 1597, loss = 0.47743174\n",
      "Iteration 1598, loss = 0.47719969\n",
      "Iteration 1599, loss = 0.47705440\n",
      "Iteration 1600, loss = 0.47726956\n",
      "Iteration 1601, loss = 0.47744607\n",
      "Iteration 1602, loss = 0.47790225\n",
      "Iteration 1603, loss = 0.47737470\n",
      "Iteration 1604, loss = 0.47746602\n",
      "Iteration 1605, loss = 0.47707669\n",
      "Iteration 1606, loss = 0.47719701\n",
      "Iteration 1607, loss = 0.47746285\n",
      "Iteration 1608, loss = 0.47749000\n",
      "Iteration 1609, loss = 0.47707111\n",
      "Iteration 1610, loss = 0.47720886\n",
      "Iteration 1611, loss = 0.47797292\n",
      "Iteration 1612, loss = 0.47727407\n",
      "Iteration 1613, loss = 0.47692759\n",
      "Iteration 1614, loss = 0.47735205\n",
      "Iteration 1615, loss = 0.47758316\n",
      "Iteration 1616, loss = 0.47746298\n",
      "Iteration 1617, loss = 0.47727197\n",
      "Iteration 1618, loss = 0.47704787\n",
      "Iteration 1619, loss = 0.47730038\n",
      "Iteration 1620, loss = 0.47717411\n",
      "Iteration 1621, loss = 0.47705926\n",
      "Iteration 1622, loss = 0.47728038\n",
      "Iteration 1623, loss = 0.47822048\n",
      "Iteration 1624, loss = 0.47808405\n",
      "Iteration 1625, loss = 0.47720576\n",
      "Iteration 1626, loss = 0.47702235\n",
      "Iteration 1627, loss = 0.47716246\n",
      "Iteration 1628, loss = 0.47743467\n",
      "Iteration 1629, loss = 0.47765870\n",
      "Iteration 1630, loss = 0.47717048\n",
      "Iteration 1631, loss = 0.47749082\n",
      "Iteration 1632, loss = 0.47729581\n",
      "Iteration 1633, loss = 0.47701192\n",
      "Iteration 1634, loss = 0.47686860\n",
      "Iteration 1635, loss = 0.47694448\n",
      "Iteration 1636, loss = 0.47721266\n",
      "Iteration 1637, loss = 0.47717810\n",
      "Iteration 1638, loss = 0.47764952\n",
      "Iteration 1639, loss = 0.47733812\n",
      "Iteration 1640, loss = 0.47743948\n",
      "Iteration 1641, loss = 0.47767379\n",
      "Iteration 1642, loss = 0.47710364\n",
      "Iteration 1643, loss = 0.47707668\n",
      "Iteration 1644, loss = 0.47707131\n",
      "Iteration 1645, loss = 0.47742905\n",
      "Iteration 1646, loss = 0.47711260\n",
      "Iteration 1647, loss = 0.47704765\n",
      "Iteration 1648, loss = 0.47690051\n",
      "Iteration 1649, loss = 0.47703555\n",
      "Iteration 1650, loss = 0.47715552\n",
      "Iteration 1651, loss = 0.47715987\n",
      "Iteration 1652, loss = 0.47685667\n",
      "Iteration 1653, loss = 0.47704736\n",
      "Iteration 1654, loss = 0.47683289\n",
      "Iteration 1655, loss = 0.47739102\n",
      "Iteration 1656, loss = 0.47763737\n",
      "Iteration 1657, loss = 0.47757335\n",
      "Iteration 1658, loss = 0.47685796\n",
      "Iteration 1659, loss = 0.47686257\n",
      "Iteration 1660, loss = 0.47695160\n",
      "Iteration 1661, loss = 0.47713718\n",
      "Iteration 1662, loss = 0.47693186\n",
      "Iteration 1663, loss = 0.47680451\n",
      "Iteration 1664, loss = 0.47727362\n",
      "Iteration 1665, loss = 0.47720343\n",
      "Iteration 1666, loss = 0.47685937\n",
      "Iteration 1667, loss = 0.47738851\n",
      "Iteration 1668, loss = 0.47729633\n",
      "Iteration 1669, loss = 0.47698394\n",
      "Iteration 1670, loss = 0.47706834\n",
      "Iteration 1671, loss = 0.47701437\n",
      "Iteration 1672, loss = 0.47716383\n",
      "Iteration 1673, loss = 0.47687748\n",
      "Iteration 1674, loss = 0.47698361\n",
      "Iteration 1675, loss = 0.47706220\n",
      "Iteration 1676, loss = 0.47669371\n",
      "Iteration 1677, loss = 0.47688731\n",
      "Iteration 1678, loss = 0.47715913\n",
      "Iteration 1679, loss = 0.47742722\n",
      "Iteration 1680, loss = 0.47695468\n",
      "Iteration 1681, loss = 0.47698952\n",
      "Iteration 1682, loss = 0.47673081\n",
      "Iteration 1683, loss = 0.47671539\n",
      "Iteration 1684, loss = 0.47682841\n",
      "Iteration 1685, loss = 0.47708016\n",
      "Iteration 1686, loss = 0.47695327\n",
      "Iteration 1687, loss = 0.47670226\n",
      "Iteration 1688, loss = 0.47681349\n",
      "Iteration 1689, loss = 0.47705278\n",
      "Iteration 1690, loss = 0.47696760\n",
      "Iteration 1691, loss = 0.47671265\n",
      "Iteration 1692, loss = 0.47698890\n",
      "Iteration 1693, loss = 0.47680250\n",
      "Iteration 1694, loss = 0.47658226\n",
      "Iteration 1695, loss = 0.47759104\n",
      "Iteration 1696, loss = 0.47723125\n",
      "Iteration 1697, loss = 0.47694118\n",
      "Iteration 1698, loss = 0.47696013\n",
      "Iteration 1699, loss = 0.47782255\n",
      "Iteration 1700, loss = 0.47767137\n",
      "Iteration 1701, loss = 0.47673632\n",
      "Iteration 1702, loss = 0.47811725\n",
      "Iteration 1703, loss = 0.47692786\n",
      "Iteration 1704, loss = 0.47697787\n",
      "Iteration 1705, loss = 0.47690138\n",
      "Iteration 1706, loss = 0.47653527\n",
      "Iteration 1707, loss = 0.47665442\n",
      "Iteration 1708, loss = 0.47721449\n",
      "Iteration 1709, loss = 0.47800075\n",
      "Iteration 1710, loss = 0.47788501\n",
      "Iteration 1711, loss = 0.47675377\n",
      "Iteration 1712, loss = 0.47660099\n",
      "Iteration 1713, loss = 0.47657924\n",
      "Iteration 1714, loss = 0.47706269\n",
      "Iteration 1715, loss = 0.47720704\n",
      "Iteration 1716, loss = 0.47704198\n",
      "Iteration 1717, loss = 0.47692283\n",
      "Iteration 1718, loss = 0.47666155\n",
      "Iteration 1719, loss = 0.47690684\n",
      "Iteration 1720, loss = 0.47718196\n",
      "Iteration 1721, loss = 0.47711449\n",
      "Iteration 1722, loss = 0.47666013\n",
      "Iteration 1723, loss = 0.47775162\n",
      "Iteration 1724, loss = 0.47653805\n",
      "Iteration 1725, loss = 0.47651670\n",
      "Iteration 1726, loss = 0.47667248\n",
      "Iteration 1727, loss = 0.47668841\n",
      "Iteration 1728, loss = 0.47663420\n",
      "Iteration 1729, loss = 0.47675877\n",
      "Iteration 1730, loss = 0.47701652\n",
      "Iteration 1731, loss = 0.47684731\n",
      "Iteration 1732, loss = 0.47686227\n",
      "Iteration 1733, loss = 0.47661092\n",
      "Iteration 1734, loss = 0.47654576\n",
      "Iteration 1735, loss = 0.47723216\n",
      "Iteration 1736, loss = 0.47658280\n",
      "Iteration 1737, loss = 0.47648747\n",
      "Iteration 1738, loss = 0.47660543\n",
      "Iteration 1739, loss = 0.47657975\n",
      "Iteration 1740, loss = 0.47651427\n",
      "Iteration 1741, loss = 0.47647479\n",
      "Iteration 1742, loss = 0.47661460\n",
      "Iteration 1743, loss = 0.47657206\n",
      "Iteration 1744, loss = 0.47658249\n",
      "Iteration 1745, loss = 0.47647662\n",
      "Iteration 1746, loss = 0.47645166\n",
      "Iteration 1747, loss = 0.47651930\n",
      "Iteration 1748, loss = 0.47688440\n",
      "Iteration 1749, loss = 0.47693366\n",
      "Iteration 1750, loss = 0.47649016\n",
      "Iteration 1751, loss = 0.47646288\n",
      "Iteration 1752, loss = 0.47669568\n",
      "Iteration 1753, loss = 0.47647384\n",
      "Iteration 1754, loss = 0.47657872\n",
      "Iteration 1755, loss = 0.47685083\n",
      "Iteration 1756, loss = 0.47668589\n",
      "Iteration 1757, loss = 0.47663868\n",
      "Iteration 1758, loss = 0.47646584\n",
      "Iteration 1759, loss = 0.47652175\n",
      "Iteration 1760, loss = 0.47646649\n",
      "Iteration 1761, loss = 0.47650968\n",
      "Iteration 1762, loss = 0.47651857\n",
      "Iteration 1763, loss = 0.47650261\n",
      "Iteration 1764, loss = 0.47661513\n",
      "Iteration 1765, loss = 0.47673375\n",
      "Iteration 1766, loss = 0.47666942\n",
      "Iteration 1767, loss = 0.47648172\n",
      "Iteration 1768, loss = 0.47679770\n",
      "Iteration 1769, loss = 0.47646812\n",
      "Iteration 1770, loss = 0.47648876\n",
      "Iteration 1771, loss = 0.47648384\n",
      "Iteration 1772, loss = 0.47658824\n",
      "Iteration 1773, loss = 0.47718033\n",
      "Iteration 1774, loss = 0.47649666\n",
      "Iteration 1775, loss = 0.47666250\n",
      "Iteration 1776, loss = 0.47655436\n",
      "Iteration 1777, loss = 0.47625052\n",
      "Iteration 1778, loss = 0.47632488\n",
      "Iteration 1779, loss = 0.47661273\n",
      "Iteration 1780, loss = 0.47648673\n",
      "Iteration 1781, loss = 0.47615214\n",
      "Iteration 1782, loss = 0.47651229\n",
      "Iteration 1783, loss = 0.47677403\n",
      "Iteration 1784, loss = 0.47684941\n",
      "Iteration 1785, loss = 0.47675190\n",
      "Iteration 1786, loss = 0.47662738\n",
      "Iteration 1787, loss = 0.47643268\n",
      "Iteration 1788, loss = 0.47668141\n",
      "Iteration 1789, loss = 0.47682084\n",
      "Iteration 1790, loss = 0.47682565\n",
      "Iteration 1791, loss = 0.47632457\n",
      "Iteration 1792, loss = 0.47630086\n",
      "Iteration 1793, loss = 0.47643326\n",
      "Iteration 1794, loss = 0.47697917\n",
      "Iteration 1795, loss = 0.47611315\n",
      "Iteration 1796, loss = 0.47645111\n",
      "Iteration 1797, loss = 0.47691024\n",
      "Iteration 1798, loss = 0.47700495\n",
      "Iteration 1799, loss = 0.47667159\n",
      "Iteration 1800, loss = 0.47661610\n",
      "Iteration 1801, loss = 0.47624549\n",
      "Iteration 1802, loss = 0.47651567\n",
      "Iteration 1803, loss = 0.47663392\n",
      "Iteration 1804, loss = 0.47656328\n",
      "Iteration 1805, loss = 0.47635142\n",
      "Iteration 1806, loss = 0.47621532\n",
      "Iteration 1807, loss = 0.47656936\n",
      "Iteration 1808, loss = 0.47631439\n",
      "Iteration 1809, loss = 0.47676604\n",
      "Iteration 1810, loss = 0.47662345\n",
      "Iteration 1811, loss = 0.47658728\n",
      "Iteration 1812, loss = 0.47631724\n",
      "Iteration 1813, loss = 0.47623745\n",
      "Iteration 1814, loss = 0.47798953\n",
      "Iteration 1815, loss = 0.47657936\n",
      "Iteration 1816, loss = 0.47645030\n",
      "Iteration 1817, loss = 0.47645252\n",
      "Iteration 1818, loss = 0.47656147\n",
      "Iteration 1819, loss = 0.47643860\n",
      "Iteration 1820, loss = 0.47611656\n",
      "Iteration 1821, loss = 0.47639634\n",
      "Iteration 1822, loss = 0.47657236\n",
      "Iteration 1823, loss = 0.47660545\n",
      "Iteration 1824, loss = 0.47656809\n",
      "Iteration 1825, loss = 0.47625468\n",
      "Iteration 1826, loss = 0.47615674\n",
      "Iteration 1827, loss = 0.47619733\n",
      "Iteration 1828, loss = 0.47623135\n",
      "Iteration 1829, loss = 0.47634055\n",
      "Iteration 1830, loss = 0.47630153\n",
      "Iteration 1831, loss = 0.47628138\n",
      "Iteration 1832, loss = 0.47608818\n",
      "Iteration 1833, loss = 0.47655749\n",
      "Iteration 1834, loss = 0.47612326\n",
      "Iteration 1835, loss = 0.47664266\n",
      "Iteration 1836, loss = 0.47623703\n",
      "Iteration 1837, loss = 0.47622981\n",
      "Iteration 1838, loss = 0.47628578\n",
      "Iteration 1839, loss = 0.47622080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1840, loss = 0.47617662\n",
      "Iteration 1841, loss = 0.47605115\n",
      "Iteration 1842, loss = 0.47611098\n",
      "Iteration 1843, loss = 0.47641490\n",
      "Iteration 1844, loss = 0.47601456\n",
      "Iteration 1845, loss = 0.47696737\n",
      "Iteration 1846, loss = 0.47659994\n",
      "Iteration 1847, loss = 0.47611780\n",
      "Iteration 1848, loss = 0.47611184\n",
      "Iteration 1849, loss = 0.47628612\n",
      "Iteration 1850, loss = 0.47618988\n",
      "Iteration 1851, loss = 0.47602853\n",
      "Iteration 1852, loss = 0.47594850\n",
      "Iteration 1853, loss = 0.47600234\n",
      "Iteration 1854, loss = 0.47660612\n",
      "Iteration 1855, loss = 0.47665719\n",
      "Iteration 1856, loss = 0.47624654\n",
      "Iteration 1857, loss = 0.47614831\n",
      "Iteration 1858, loss = 0.47651946\n",
      "Iteration 1859, loss = 0.47626387\n",
      "Iteration 1860, loss = 0.47608458\n",
      "Iteration 1861, loss = 0.47603404\n",
      "Iteration 1862, loss = 0.47620041\n",
      "Iteration 1863, loss = 0.47646696\n",
      "Iteration 1864, loss = 0.47628646\n",
      "Iteration 1865, loss = 0.47627318\n",
      "Iteration 1866, loss = 0.47599014\n",
      "Iteration 1867, loss = 0.47582389\n",
      "Iteration 1868, loss = 0.47629655\n",
      "Iteration 1869, loss = 0.47636866\n",
      "Iteration 1870, loss = 0.47642622\n",
      "Iteration 1871, loss = 0.47572481\n",
      "Iteration 1872, loss = 0.47572782\n",
      "Iteration 1873, loss = 0.47705176\n",
      "Iteration 1874, loss = 0.47790856\n",
      "Iteration 1875, loss = 0.47722462\n",
      "Iteration 1876, loss = 0.47637002\n",
      "Iteration 1877, loss = 0.47601143\n",
      "Iteration 1878, loss = 0.47678798\n",
      "Iteration 1879, loss = 0.47700212\n",
      "Iteration 1880, loss = 0.47775013\n",
      "Iteration 1881, loss = 0.47618960\n",
      "Iteration 1882, loss = 0.47613946\n",
      "Iteration 1883, loss = 0.47619263\n",
      "Iteration 1884, loss = 0.47609653\n",
      "Iteration 1885, loss = 0.47609316\n",
      "Iteration 1886, loss = 0.47588672\n",
      "Iteration 1887, loss = 0.47589970\n",
      "Iteration 1888, loss = 0.47600299\n",
      "Iteration 1889, loss = 0.47595620\n",
      "Iteration 1890, loss = 0.47649327\n",
      "Iteration 1891, loss = 0.47618114\n",
      "Iteration 1892, loss = 0.47608106\n",
      "Iteration 1893, loss = 0.47609873\n",
      "Iteration 1894, loss = 0.47615153\n",
      "Iteration 1895, loss = 0.47601001\n",
      "Iteration 1896, loss = 0.47596972\n",
      "Iteration 1897, loss = 0.47593357\n",
      "Iteration 1898, loss = 0.47608938\n",
      "Iteration 1899, loss = 0.47633257\n",
      "Iteration 1900, loss = 0.47588463\n",
      "Iteration 1901, loss = 0.47660277\n",
      "Iteration 1902, loss = 0.47659627\n",
      "Iteration 1903, loss = 0.47613142\n",
      "Iteration 1904, loss = 0.47595147\n",
      "Iteration 1905, loss = 0.47589422\n",
      "Iteration 1906, loss = 0.47587902\n",
      "Iteration 1907, loss = 0.47588027\n",
      "Iteration 1908, loss = 0.47586555\n",
      "Iteration 1909, loss = 0.47586266\n",
      "Iteration 1910, loss = 0.47583159\n",
      "Iteration 1911, loss = 0.47607177\n",
      "Iteration 1912, loss = 0.47606682\n",
      "Iteration 1913, loss = 0.47591779\n",
      "Iteration 1914, loss = 0.47596664\n",
      "Iteration 1915, loss = 0.47580894\n",
      "Iteration 1916, loss = 0.47593176\n",
      "Iteration 1917, loss = 0.47607648\n",
      "Iteration 1918, loss = 0.47597046\n",
      "Iteration 1919, loss = 0.47591450\n",
      "Iteration 1920, loss = 0.47579277\n",
      "Iteration 1921, loss = 0.47601930\n",
      "Iteration 1922, loss = 0.47603657\n",
      "Iteration 1923, loss = 0.47583827\n",
      "Iteration 1924, loss = 0.47610678\n",
      "Iteration 1925, loss = 0.47611394\n",
      "Iteration 1926, loss = 0.47577992\n",
      "Iteration 1927, loss = 0.47587764\n",
      "Iteration 1928, loss = 0.47580623\n",
      "Iteration 1929, loss = 0.47589926\n",
      "Iteration 1930, loss = 0.47577201\n",
      "Iteration 1931, loss = 0.47582523\n",
      "Iteration 1932, loss = 0.47576684\n",
      "Iteration 1933, loss = 0.47594350\n",
      "Iteration 1934, loss = 0.47574481\n",
      "Iteration 1935, loss = 0.47585760\n",
      "Iteration 1936, loss = 0.47605297\n",
      "Iteration 1937, loss = 0.47606424\n",
      "Iteration 1938, loss = 0.47590800\n",
      "Iteration 1939, loss = 0.47572837\n",
      "Iteration 1940, loss = 0.47584524\n",
      "Iteration 1941, loss = 0.47621792\n",
      "Iteration 1942, loss = 0.47593003\n",
      "Iteration 1943, loss = 0.47620133\n",
      "Iteration 1944, loss = 0.47645550\n",
      "Iteration 1945, loss = 0.47542251\n",
      "Iteration 1946, loss = 0.47589007\n",
      "Iteration 1947, loss = 0.47718043\n",
      "Iteration 1948, loss = 0.47733979\n",
      "Iteration 1949, loss = 0.47678991\n",
      "Iteration 1950, loss = 0.47764365\n",
      "Iteration 1951, loss = 0.47595195\n",
      "Iteration 1952, loss = 0.47645417\n",
      "Iteration 1953, loss = 0.47607569\n",
      "Iteration 1954, loss = 0.47621538\n",
      "Iteration 1955, loss = 0.47609637\n",
      "Iteration 1956, loss = 0.47604809\n",
      "Iteration 1957, loss = 0.47593236\n",
      "Iteration 1958, loss = 0.47605705\n",
      "Iteration 1959, loss = 0.47607831\n",
      "Iteration 1960, loss = 0.47596743\n",
      "Iteration 1961, loss = 0.47572609\n",
      "Iteration 1962, loss = 0.47561016\n",
      "Iteration 1963, loss = 0.47570656\n",
      "Iteration 1964, loss = 0.47588648\n",
      "Iteration 1965, loss = 0.47623487\n",
      "Iteration 1966, loss = 0.47583229\n",
      "Iteration 1967, loss = 0.47610113\n",
      "Iteration 1968, loss = 0.47622454\n",
      "Iteration 1969, loss = 0.47577850\n",
      "Iteration 1970, loss = 0.47577603\n",
      "Iteration 1971, loss = 0.47585796\n",
      "Iteration 1972, loss = 0.47645291\n",
      "Iteration 1973, loss = 0.47698499\n",
      "Iteration 1974, loss = 0.47592182\n",
      "Iteration 1975, loss = 0.47590664\n",
      "Iteration 1976, loss = 0.47582723\n",
      "Iteration 1977, loss = 0.47575869\n",
      "Iteration 1978, loss = 0.47554841\n",
      "Iteration 1979, loss = 0.47666758\n",
      "Iteration 1980, loss = 0.47573190\n",
      "Iteration 1981, loss = 0.47638245\n",
      "Iteration 1982, loss = 0.47572875\n",
      "Iteration 1983, loss = 0.47568284\n",
      "Iteration 1984, loss = 0.47567160\n",
      "Iteration 1985, loss = 0.47579803\n",
      "Iteration 1986, loss = 0.47567389\n",
      "Iteration 1987, loss = 0.47557692\n",
      "Iteration 1988, loss = 0.47553954\n",
      "Iteration 1989, loss = 0.47589289\n",
      "Iteration 1990, loss = 0.47566192\n",
      "Iteration 1991, loss = 0.47587524\n",
      "Iteration 1992, loss = 0.47581596\n",
      "Iteration 1993, loss = 0.47562198\n",
      "Iteration 1994, loss = 0.47554943\n",
      "Iteration 1995, loss = 0.47572804\n",
      "Iteration 1996, loss = 0.47567914\n",
      "Iteration 1997, loss = 0.47553611\n",
      "Iteration 1998, loss = 0.47550619\n",
      "Iteration 1999, loss = 0.47630520\n",
      "Iteration 2000, loss = 0.47578549\n",
      "Iteration 2001, loss = 0.47601726\n",
      "Iteration 2002, loss = 0.47559152\n",
      "Iteration 2003, loss = 0.47575502\n",
      "Iteration 2004, loss = 0.47573454\n",
      "Iteration 2005, loss = 0.47575158\n",
      "Iteration 2006, loss = 0.47571114\n",
      "Iteration 2007, loss = 0.47553157\n",
      "Iteration 2008, loss = 0.47556444\n",
      "Iteration 2009, loss = 0.47557486\n",
      "Iteration 2010, loss = 0.47558423\n",
      "Iteration 2011, loss = 0.47558641\n",
      "Iteration 2012, loss = 0.47566976\n",
      "Iteration 2013, loss = 0.47556822\n",
      "Iteration 2014, loss = 0.47556129\n",
      "Iteration 2015, loss = 0.47574514\n",
      "Iteration 2016, loss = 0.47594272\n",
      "Iteration 2017, loss = 0.47587063\n",
      "Iteration 2018, loss = 0.47567106\n",
      "Iteration 2019, loss = 0.47608032\n",
      "Iteration 2020, loss = 0.47597498\n",
      "Iteration 2021, loss = 0.47557871\n",
      "Iteration 2022, loss = 0.47554050\n",
      "Iteration 2023, loss = 0.47548634\n",
      "Iteration 2024, loss = 0.47549781\n",
      "Iteration 2025, loss = 0.47550980\n",
      "Iteration 2026, loss = 0.47551865\n",
      "Iteration 2027, loss = 0.47551704\n",
      "Iteration 2028, loss = 0.47545050\n",
      "Iteration 2029, loss = 0.47570606\n",
      "Iteration 2030, loss = 0.47576255\n",
      "Iteration 2031, loss = 0.47567671\n",
      "Iteration 2032, loss = 0.47562004\n",
      "Iteration 2033, loss = 0.47548272\n",
      "Iteration 2034, loss = 0.47548847\n",
      "Iteration 2035, loss = 0.47543187\n",
      "Iteration 2036, loss = 0.47543332\n",
      "Iteration 2037, loss = 0.47546043\n",
      "Iteration 2038, loss = 0.47547390\n",
      "Iteration 2039, loss = 0.47544049\n",
      "Iteration 2040, loss = 0.47541347\n",
      "Iteration 2041, loss = 0.47566285\n",
      "Iteration 2042, loss = 0.47548434\n",
      "Iteration 2043, loss = 0.47551554\n",
      "Iteration 2044, loss = 0.47542731\n",
      "Iteration 2045, loss = 0.47542861\n",
      "Iteration 2046, loss = 0.47554408\n",
      "Iteration 2047, loss = 0.47566246\n",
      "Iteration 2048, loss = 0.47560633\n",
      "Iteration 2049, loss = 0.47584500\n",
      "Iteration 2050, loss = 0.47547483\n",
      "Iteration 2051, loss = 0.47548592\n",
      "Iteration 2052, loss = 0.47543601\n",
      "Iteration 2053, loss = 0.47543165\n",
      "Iteration 2054, loss = 0.47540137\n",
      "Iteration 2055, loss = 0.47560779\n",
      "Iteration 2056, loss = 0.47542244\n",
      "Iteration 2057, loss = 0.47546770\n",
      "Iteration 2058, loss = 0.47558768\n",
      "Iteration 2059, loss = 0.47607023\n",
      "Iteration 2060, loss = 0.47635237\n",
      "Iteration 2061, loss = 0.47570731\n",
      "Iteration 2062, loss = 0.47549146\n",
      "Iteration 2063, loss = 0.47627888\n",
      "Iteration 2064, loss = 0.47680629\n",
      "Iteration 2065, loss = 0.47644891\n",
      "Iteration 2066, loss = 0.47608568\n",
      "Iteration 2067, loss = 0.47546231\n",
      "Iteration 2068, loss = 0.47549174\n",
      "Iteration 2069, loss = 0.47599327\n",
      "Iteration 2070, loss = 0.47685397\n",
      "Iteration 2071, loss = 0.47588805\n",
      "Iteration 2072, loss = 0.47550911\n",
      "Iteration 2073, loss = 0.47533788\n",
      "Iteration 2074, loss = 0.47601376\n",
      "Iteration 2075, loss = 0.47538910\n",
      "Iteration 2076, loss = 0.47534418\n",
      "Iteration 2077, loss = 0.47539953\n",
      "Iteration 2078, loss = 0.47583137\n",
      "Iteration 2079, loss = 0.47547676\n",
      "Iteration 2080, loss = 0.47574403\n",
      "Iteration 2081, loss = 0.47568773\n",
      "Iteration 2082, loss = 0.47548108\n",
      "Iteration 2083, loss = 0.47558351\n",
      "Iteration 2084, loss = 0.47537586\n",
      "Iteration 2085, loss = 0.47546520\n",
      "Iteration 2086, loss = 0.47560157\n",
      "Iteration 2087, loss = 0.47538225\n",
      "Iteration 2088, loss = 0.47528004\n",
      "Iteration 2089, loss = 0.47530254\n",
      "Iteration 2090, loss = 0.47538120\n",
      "Iteration 2091, loss = 0.47552301\n",
      "Iteration 2092, loss = 0.47531210\n",
      "Iteration 2093, loss = 0.47534676\n",
      "Iteration 2094, loss = 0.47540737\n",
      "Iteration 2095, loss = 0.47538950\n",
      "Iteration 2096, loss = 0.47541106\n",
      "Iteration 2097, loss = 0.47554242\n",
      "Iteration 2098, loss = 0.47534608\n",
      "Iteration 2099, loss = 0.47540495\n",
      "Iteration 2100, loss = 0.47636035\n",
      "Iteration 2101, loss = 0.47562347\n",
      "Iteration 2102, loss = 0.47529428\n",
      "Iteration 2103, loss = 0.47597705\n",
      "Iteration 2104, loss = 0.47578728\n",
      "Iteration 2105, loss = 0.47558143\n",
      "Iteration 2106, loss = 0.47498280\n",
      "Iteration 2107, loss = 0.47611710\n",
      "Iteration 2108, loss = 0.47635533\n",
      "Iteration 2109, loss = 0.47640605\n",
      "Iteration 2110, loss = 0.47574118\n",
      "Iteration 2111, loss = 0.47530917\n",
      "Iteration 2112, loss = 0.47558951\n",
      "Iteration 2113, loss = 0.47538549\n",
      "Iteration 2114, loss = 0.47536785\n",
      "Iteration 2115, loss = 0.47557747\n",
      "Iteration 2116, loss = 0.47549302\n",
      "Iteration 2117, loss = 0.47542232\n",
      "Iteration 2118, loss = 0.47544502\n",
      "Iteration 2119, loss = 0.47549552\n",
      "Iteration 2120, loss = 0.47561154\n",
      "Iteration 2121, loss = 0.47520173\n",
      "Iteration 2122, loss = 0.47515704\n",
      "Iteration 2123, loss = 0.47630402\n",
      "Iteration 2124, loss = 0.47591042\n",
      "Iteration 2125, loss = 0.47521736\n",
      "Iteration 2126, loss = 0.47544269\n",
      "Iteration 2127, loss = 0.47546810\n",
      "Iteration 2128, loss = 0.47571399\n",
      "Iteration 2129, loss = 0.47554893\n",
      "Iteration 2130, loss = 0.47567758\n",
      "Iteration 2131, loss = 0.47514254\n",
      "Iteration 2132, loss = 0.47529595\n",
      "Iteration 2133, loss = 0.47565276\n",
      "Iteration 2134, loss = 0.47578713\n",
      "Iteration 2135, loss = 0.47561846\n",
      "Iteration 2136, loss = 0.47516765\n",
      "Iteration 2137, loss = 0.47530139\n",
      "Iteration 2138, loss = 0.47537261\n",
      "Iteration 2139, loss = 0.47549473\n",
      "Iteration 2140, loss = 0.47533736\n",
      "Iteration 2141, loss = 0.47520859\n",
      "Iteration 2142, loss = 0.47527494\n",
      "Iteration 2143, loss = 0.47517933\n",
      "Iteration 2144, loss = 0.47522098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2145, loss = 0.47553615\n",
      "Iteration 2146, loss = 0.47521593\n",
      "Iteration 2147, loss = 0.47534636\n",
      "Iteration 2148, loss = 0.47532658\n",
      "Iteration 2149, loss = 0.47538108\n",
      "Iteration 2150, loss = 0.47523606\n",
      "Iteration 2151, loss = 0.47508184\n",
      "Iteration 2152, loss = 0.47515908\n",
      "Iteration 2153, loss = 0.47535944\n",
      "Iteration 2154, loss = 0.47537887\n",
      "Iteration 2155, loss = 0.47522705\n",
      "Iteration 2156, loss = 0.47500322\n",
      "Iteration 2157, loss = 0.47537018\n",
      "Iteration 2158, loss = 0.47570661\n",
      "Iteration 2159, loss = 0.47562573\n",
      "Iteration 2160, loss = 0.47509309\n",
      "Iteration 2161, loss = 0.47547199\n",
      "Iteration 2162, loss = 0.47552511\n",
      "Iteration 2163, loss = 0.47519337\n",
      "Iteration 2164, loss = 0.47530121\n",
      "Iteration 2165, loss = 0.47527341\n",
      "Iteration 2166, loss = 0.47530127\n",
      "Iteration 2167, loss = 0.47534811\n",
      "Iteration 2168, loss = 0.47543112\n",
      "Iteration 2169, loss = 0.47533989\n",
      "Iteration 2170, loss = 0.47515419\n",
      "Iteration 2171, loss = 0.47505322\n",
      "Iteration 2172, loss = 0.47560607\n",
      "Iteration 2173, loss = 0.47545512\n",
      "Iteration 2174, loss = 0.47534168\n",
      "Iteration 2175, loss = 0.47561698\n",
      "Iteration 2176, loss = 0.47510763\n",
      "Iteration 2177, loss = 0.47510033\n",
      "Iteration 2178, loss = 0.47532961\n",
      "Iteration 2179, loss = 0.47572753\n",
      "Iteration 2180, loss = 0.47564732\n",
      "Iteration 2181, loss = 0.47527921\n",
      "Iteration 2182, loss = 0.47499788\n",
      "Iteration 2183, loss = 0.47507775\n",
      "Iteration 2184, loss = 0.47554343\n",
      "Iteration 2185, loss = 0.47579099\n",
      "Iteration 2186, loss = 0.47558209\n",
      "Iteration 2187, loss = 0.47503971\n",
      "Iteration 2188, loss = 0.47489758\n",
      "Iteration 2189, loss = 0.47524461\n",
      "Iteration 2190, loss = 0.47587409\n",
      "Iteration 2191, loss = 0.47575931\n",
      "Iteration 2192, loss = 0.47518026\n",
      "Iteration 2193, loss = 0.47495716\n",
      "Iteration 2194, loss = 0.47513752\n",
      "Iteration 2195, loss = 0.47529919\n",
      "Iteration 2196, loss = 0.47541231\n",
      "Iteration 2197, loss = 0.47557062\n",
      "Iteration 2198, loss = 0.47501720\n",
      "Iteration 2199, loss = 0.47599716\n",
      "Iteration 2200, loss = 0.47554267\n",
      "Iteration 2201, loss = 0.47567046\n",
      "Iteration 2202, loss = 0.47559535\n",
      "Iteration 2203, loss = 0.47526202\n",
      "Iteration 2204, loss = 0.47519850\n",
      "Iteration 2205, loss = 0.47498579\n",
      "Iteration 2206, loss = 0.47536506\n",
      "Iteration 2207, loss = 0.47504413\n",
      "Iteration 2208, loss = 0.47490341\n",
      "Iteration 2209, loss = 0.47512271\n",
      "Iteration 2210, loss = 0.47551431\n",
      "Iteration 2211, loss = 0.47572552\n",
      "Iteration 2212, loss = 0.47516949\n",
      "Iteration 2213, loss = 0.47488448\n",
      "Iteration 2214, loss = 0.47631177\n",
      "Iteration 2215, loss = 0.47573587\n",
      "Iteration 2216, loss = 0.47511505\n",
      "Iteration 2217, loss = 0.47473011\n",
      "Iteration 2218, loss = 0.47515157\n",
      "Iteration 2219, loss = 0.47616740\n",
      "Iteration 2220, loss = 0.47618687\n",
      "Iteration 2221, loss = 0.47542228\n",
      "Iteration 2222, loss = 0.47515833\n",
      "Iteration 2223, loss = 0.47557419\n",
      "Iteration 2224, loss = 0.47623566\n",
      "Iteration 2225, loss = 0.47595690\n",
      "Iteration 2226, loss = 0.47541883\n",
      "Iteration 2227, loss = 0.47492717\n",
      "Iteration 2228, loss = 0.47519814\n",
      "Iteration 2229, loss = 0.47572648\n",
      "Iteration 2230, loss = 0.47574169\n",
      "Iteration 2231, loss = 0.47623501\n",
      "Iteration 2232, loss = 0.47575537\n",
      "Iteration 2233, loss = 0.47489559\n",
      "Iteration 2234, loss = 0.47516396\n",
      "Iteration 2235, loss = 0.47562729\n",
      "Iteration 2236, loss = 0.47595610\n",
      "Iteration 2237, loss = 0.47554103\n",
      "Iteration 2238, loss = 0.47509624\n",
      "Iteration 2239, loss = 0.47492093\n",
      "Iteration 2240, loss = 0.47483619\n",
      "Iteration 2241, loss = 0.47622905\n",
      "Iteration 2242, loss = 0.47581972\n",
      "Iteration 2243, loss = 0.47493385\n",
      "Iteration 2244, loss = 0.47454133\n",
      "Iteration 2245, loss = 0.47520209\n",
      "Iteration 2246, loss = 0.47684063\n",
      "Iteration 2247, loss = 0.47715406\n",
      "Iteration 2248, loss = 0.47621556\n",
      "Iteration 2249, loss = 0.47626214\n",
      "Iteration 2250, loss = 0.47502980\n",
      "Iteration 2251, loss = 0.47495413\n",
      "Iteration 2252, loss = 0.47491068\n",
      "Iteration 2253, loss = 0.47502199\n",
      "Iteration 2254, loss = 0.47493507\n",
      "Iteration 2255, loss = 0.47488726\n",
      "Iteration 2256, loss = 0.47489071\n",
      "Iteration 2257, loss = 0.47516814\n",
      "Iteration 2258, loss = 0.47500124\n",
      "Iteration 2259, loss = 0.47484765\n",
      "Iteration 2260, loss = 0.47494612\n",
      "Iteration 2261, loss = 0.47493227\n",
      "Iteration 2262, loss = 0.47532054\n",
      "Iteration 2263, loss = 0.47485519\n",
      "Iteration 2264, loss = 0.47483813\n",
      "Iteration 2265, loss = 0.47479608\n",
      "Iteration 2266, loss = 0.47480173\n",
      "Iteration 2267, loss = 0.47527058\n",
      "Iteration 2268, loss = 0.47506825\n",
      "Iteration 2269, loss = 0.47549819\n",
      "Iteration 2270, loss = 0.47483840\n",
      "Iteration 2271, loss = 0.47483594\n",
      "Iteration 2272, loss = 0.47482804\n",
      "Iteration 2273, loss = 0.47481047\n",
      "Iteration 2274, loss = 0.47479562\n",
      "Iteration 2275, loss = 0.47486949\n",
      "Iteration 2276, loss = 0.47503164\n",
      "Iteration 2277, loss = 0.47488938\n",
      "Iteration 2278, loss = 0.47485874\n",
      "Iteration 2279, loss = 0.47491401\n",
      "Iteration 2280, loss = 0.47521960\n",
      "Iteration 2281, loss = 0.47561485\n",
      "Iteration 2282, loss = 0.47526319\n",
      "Iteration 2283, loss = 0.47477678\n",
      "Iteration 2284, loss = 0.47489400\n",
      "Iteration 2285, loss = 0.47576049\n",
      "Iteration 2286, loss = 0.47505908\n",
      "Iteration 2287, loss = 0.47464767\n",
      "Iteration 2288, loss = 0.47493166\n",
      "Iteration 2289, loss = 0.47550247\n",
      "Iteration 2290, loss = 0.47553408\n",
      "Iteration 2291, loss = 0.47508045\n",
      "Iteration 2292, loss = 0.47477402\n",
      "Iteration 2293, loss = 0.47473223\n",
      "Iteration 2294, loss = 0.47578142\n",
      "Iteration 2295, loss = 0.47514466\n",
      "Iteration 2296, loss = 0.47473294\n",
      "Iteration 2297, loss = 0.47477511\n",
      "Iteration 2298, loss = 0.47498571\n",
      "Iteration 2299, loss = 0.47545700\n",
      "Iteration 2300, loss = 0.47577907\n",
      "Iteration 2301, loss = 0.47475510\n",
      "Iteration 2302, loss = 0.47487185\n",
      "Iteration 2303, loss = 0.47503675\n",
      "Iteration 2304, loss = 0.47503234\n",
      "Iteration 2305, loss = 0.47515603\n",
      "Iteration 2306, loss = 0.47487199\n",
      "Iteration 2307, loss = 0.47475468\n",
      "Iteration 2308, loss = 0.47484031\n",
      "Iteration 2309, loss = 0.47501247\n",
      "Iteration 2310, loss = 0.47493307\n",
      "Iteration 2311, loss = 0.47487813\n",
      "Iteration 2312, loss = 0.47566885\n",
      "Iteration 2313, loss = 0.47470806\n",
      "Iteration 2314, loss = 0.47477737\n",
      "Iteration 2315, loss = 0.47507876\n",
      "Iteration 2316, loss = 0.47476956\n",
      "Iteration 2317, loss = 0.47500612\n",
      "Iteration 2318, loss = 0.47485085\n",
      "Iteration 2319, loss = 0.47497158\n",
      "Iteration 2320, loss = 0.47480227\n",
      "Iteration 2321, loss = 0.47463915\n",
      "Iteration 2322, loss = 0.47487135\n",
      "Iteration 2323, loss = 0.47564826\n",
      "Iteration 2324, loss = 0.47587627\n",
      "Iteration 2325, loss = 0.47594956\n",
      "Iteration 2326, loss = 0.47471917\n",
      "Iteration 2327, loss = 0.47505139\n",
      "Iteration 2328, loss = 0.47484620\n",
      "Iteration 2329, loss = 0.47470037\n",
      "Iteration 2330, loss = 0.47456241\n",
      "Iteration 2331, loss = 0.47487923\n",
      "Iteration 2332, loss = 0.47496084\n",
      "Iteration 2333, loss = 0.47489765\n",
      "Iteration 2334, loss = 0.47473125\n",
      "Iteration 2335, loss = 0.47456573\n",
      "Iteration 2336, loss = 0.47465201\n",
      "Iteration 2337, loss = 0.47491504\n",
      "Iteration 2338, loss = 0.47534433\n",
      "Iteration 2339, loss = 0.47507942\n",
      "Iteration 2340, loss = 0.47490529\n",
      "Iteration 2341, loss = 0.47466415\n",
      "Iteration 2342, loss = 0.47471941\n",
      "Iteration 2343, loss = 0.47491743\n",
      "Iteration 2344, loss = 0.47507024\n",
      "Iteration 2345, loss = 0.47521251\n",
      "Iteration 2346, loss = 0.47482231\n",
      "Iteration 2347, loss = 0.47509324\n",
      "Iteration 2348, loss = 0.47475978\n",
      "Iteration 2349, loss = 0.47465674\n",
      "Iteration 2350, loss = 0.47510665\n",
      "Iteration 2351, loss = 0.47469494\n",
      "Iteration 2352, loss = 0.47553733\n",
      "Iteration 2353, loss = 0.47525815\n",
      "Iteration 2354, loss = 0.47469082\n",
      "Iteration 2355, loss = 0.47490778\n",
      "Iteration 2356, loss = 0.47524770\n",
      "Iteration 2357, loss = 0.47537090\n",
      "Iteration 2358, loss = 0.47503304\n",
      "Iteration 2359, loss = 0.47542715\n",
      "Iteration 2360, loss = 0.47469673\n",
      "Iteration 2361, loss = 0.47476419\n",
      "Iteration 2362, loss = 0.47478550\n",
      "Iteration 2363, loss = 0.47475341\n",
      "Iteration 2364, loss = 0.47458908\n",
      "Iteration 2365, loss = 0.47498189\n",
      "Iteration 2366, loss = 0.47466787\n",
      "Iteration 2367, loss = 0.47486447\n",
      "Iteration 2368, loss = 0.47472094\n",
      "Iteration 2369, loss = 0.47492979\n",
      "Iteration 2370, loss = 0.47464940\n",
      "Iteration 2371, loss = 0.47455648\n",
      "Iteration 2372, loss = 0.47463048\n",
      "Iteration 2373, loss = 0.47464987\n",
      "Iteration 2374, loss = 0.47477780\n",
      "Iteration 2375, loss = 0.47469023\n",
      "Iteration 2376, loss = 0.47461949\n",
      "Iteration 2377, loss = 0.47497042\n",
      "Iteration 2378, loss = 0.47481136\n",
      "Iteration 2379, loss = 0.47462406\n",
      "Iteration 2380, loss = 0.47514469\n",
      "Iteration 2381, loss = 0.47459487\n",
      "Iteration 2382, loss = 0.47453121\n",
      "Iteration 2383, loss = 0.47463828\n",
      "Iteration 2384, loss = 0.47482741\n",
      "Iteration 2385, loss = 0.47497664\n",
      "Iteration 2386, loss = 0.47472169\n",
      "Iteration 2387, loss = 0.47467183\n",
      "Iteration 2388, loss = 0.47554345\n",
      "Iteration 2389, loss = 0.47488827\n",
      "Iteration 2390, loss = 0.47468561\n",
      "Iteration 2391, loss = 0.47474471\n",
      "Iteration 2392, loss = 0.47474297\n",
      "Iteration 2393, loss = 0.47500150\n",
      "Iteration 2394, loss = 0.47520532\n",
      "Iteration 2395, loss = 0.47531848\n",
      "Iteration 2396, loss = 0.47488332\n",
      "Iteration 2397, loss = 0.47472727\n",
      "Iteration 2398, loss = 0.47460843\n",
      "Iteration 2399, loss = 0.47522931\n",
      "Iteration 2400, loss = 0.47498536\n",
      "Iteration 2401, loss = 0.47496315\n",
      "Iteration 2402, loss = 0.47465212\n",
      "Iteration 2403, loss = 0.47465817\n",
      "Iteration 2404, loss = 0.47446942\n",
      "Iteration 2405, loss = 0.47455189\n",
      "Iteration 2406, loss = 0.47470499\n",
      "Iteration 2407, loss = 0.47485799\n",
      "Iteration 2408, loss = 0.47480997\n",
      "Iteration 2409, loss = 0.47503211\n",
      "Iteration 2410, loss = 0.47489734\n",
      "Iteration 2411, loss = 0.47453204\n",
      "Iteration 2412, loss = 0.47442535\n",
      "Iteration 2413, loss = 0.47551922\n",
      "Iteration 2414, loss = 0.47557484\n",
      "Iteration 2415, loss = 0.47513910\n",
      "Iteration 2416, loss = 0.47530391\n",
      "Iteration 2417, loss = 0.47452916\n",
      "Iteration 2418, loss = 0.47463562\n",
      "Iteration 2419, loss = 0.47454573\n",
      "Iteration 2420, loss = 0.47455297\n",
      "Iteration 2421, loss = 0.47453327\n",
      "Iteration 2422, loss = 0.47454144\n",
      "Iteration 2423, loss = 0.47452940\n",
      "Iteration 2424, loss = 0.47494003\n",
      "Iteration 2425, loss = 0.47469112\n",
      "Iteration 2426, loss = 0.47447716\n",
      "Iteration 2427, loss = 0.47457249\n",
      "Iteration 2428, loss = 0.47478028\n",
      "Iteration 2429, loss = 0.47452215\n",
      "Iteration 2430, loss = 0.47467018\n",
      "Iteration 2431, loss = 0.47468515\n",
      "Iteration 2432, loss = 0.47456599\n",
      "Iteration 2433, loss = 0.47465046\n",
      "Iteration 2434, loss = 0.47450375\n",
      "Iteration 2435, loss = 0.47454253\n",
      "Iteration 2436, loss = 0.47444315\n",
      "Iteration 2437, loss = 0.47458964\n",
      "Iteration 2438, loss = 0.47532547\n",
      "Iteration 2439, loss = 0.47510827\n",
      "Iteration 2440, loss = 0.47448025\n",
      "Iteration 2441, loss = 0.47459191\n",
      "Iteration 2442, loss = 0.47525876\n",
      "Iteration 2443, loss = 0.47504572\n",
      "Iteration 2444, loss = 0.47480732\n",
      "Iteration 2445, loss = 0.47430806\n",
      "Iteration 2446, loss = 0.47473111\n",
      "Iteration 2447, loss = 0.47582332\n",
      "Iteration 2448, loss = 0.47597702\n",
      "Iteration 2449, loss = 0.47498240\n",
      "Iteration 2450, loss = 0.47453811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2451, loss = 0.47453257\n",
      "Iteration 2452, loss = 0.47541255\n",
      "Iteration 2453, loss = 0.47562718\n",
      "Iteration 2454, loss = 0.47500211\n",
      "Iteration 2455, loss = 0.47444302\n",
      "Iteration 2456, loss = 0.47461493\n",
      "Iteration 2457, loss = 0.47491845\n",
      "Iteration 2458, loss = 0.47535165\n",
      "Iteration 2459, loss = 0.47543646\n",
      "Iteration 2460, loss = 0.47490704\n",
      "Iteration 2461, loss = 0.47449730\n",
      "Iteration 2462, loss = 0.47428130\n",
      "Iteration 2463, loss = 0.47442427\n",
      "Iteration 2464, loss = 0.47542675\n",
      "Iteration 2465, loss = 0.47547585\n",
      "Iteration 2466, loss = 0.47494332\n",
      "Iteration 2467, loss = 0.47468920\n",
      "Iteration 2468, loss = 0.47448200\n",
      "Iteration 2469, loss = 0.47472976\n",
      "Iteration 2470, loss = 0.47481042\n",
      "Iteration 2471, loss = 0.47461702\n",
      "Iteration 2472, loss = 0.47430715\n",
      "Iteration 2473, loss = 0.47454816\n",
      "Iteration 2474, loss = 0.47459405\n",
      "Iteration 2475, loss = 0.47461922\n",
      "Iteration 2476, loss = 0.47449539\n",
      "Iteration 2477, loss = 0.47432734\n",
      "Iteration 2478, loss = 0.47441182\n",
      "Iteration 2479, loss = 0.47461182\n",
      "Iteration 2480, loss = 0.47446238\n",
      "Iteration 2481, loss = 0.47422918\n",
      "Iteration 2482, loss = 0.47434222\n",
      "Iteration 2483, loss = 0.47485698\n",
      "Iteration 2484, loss = 0.47494049\n",
      "Iteration 2485, loss = 0.47467221\n",
      "Iteration 2486, loss = 0.47442856\n",
      "Iteration 2487, loss = 0.47427136\n",
      "Iteration 2488, loss = 0.47490567\n",
      "Iteration 2489, loss = 0.47578213\n",
      "Iteration 2490, loss = 0.47621077\n",
      "Iteration 2491, loss = 0.47545966\n",
      "Iteration 2492, loss = 0.47410058\n",
      "Iteration 2493, loss = 0.47517071\n",
      "Iteration 2494, loss = 0.47588086\n",
      "Iteration 2495, loss = 0.47576448\n",
      "Iteration 2496, loss = 0.47484951\n",
      "Iteration 2497, loss = 0.47433268\n",
      "Iteration 2498, loss = 0.47567382\n",
      "Iteration 2499, loss = 0.47475071\n",
      "Iteration 2500, loss = 0.47458306\n",
      "Iteration 2501, loss = 0.47458722\n",
      "Iteration 2502, loss = 0.47441235\n",
      "Iteration 2503, loss = 0.47450906\n",
      "Iteration 2504, loss = 0.47452008\n",
      "Iteration 2505, loss = 0.47441872\n",
      "Iteration 2506, loss = 0.47440215\n",
      "Iteration 2507, loss = 0.47442072\n",
      "Iteration 2508, loss = 0.47451031\n",
      "Iteration 2509, loss = 0.47445221\n",
      "Iteration 2510, loss = 0.47416216\n",
      "Iteration 2511, loss = 0.47422749\n",
      "Iteration 2512, loss = 0.47468024\n",
      "Iteration 2513, loss = 0.47504097\n",
      "Iteration 2514, loss = 0.47499249\n",
      "Iteration 2515, loss = 0.47510929\n",
      "Iteration 2516, loss = 0.47471094\n",
      "Iteration 2517, loss = 0.47434696\n",
      "Iteration 2518, loss = 0.47437916\n",
      "Iteration 2519, loss = 0.47440089\n",
      "Iteration 2520, loss = 0.47435376\n",
      "Iteration 2521, loss = 0.47435478\n",
      "Iteration 2522, loss = 0.47455518\n",
      "Iteration 2523, loss = 0.47446334\n",
      "Iteration 2524, loss = 0.47439003\n",
      "Iteration 2525, loss = 0.47454290\n",
      "Iteration 2526, loss = 0.47465017\n",
      "Iteration 2527, loss = 0.47419874\n",
      "Iteration 2528, loss = 0.47431211\n",
      "Iteration 2529, loss = 0.47461405\n",
      "Iteration 2530, loss = 0.47467465\n",
      "Iteration 2531, loss = 0.47466121\n",
      "Iteration 2532, loss = 0.47442645\n",
      "Iteration 2533, loss = 0.47418211\n",
      "Iteration 2534, loss = 0.47448366\n",
      "Iteration 2535, loss = 0.47450416\n",
      "Iteration 2536, loss = 0.47422041\n",
      "Iteration 2537, loss = 0.47439624\n",
      "Iteration 2538, loss = 0.47433464\n",
      "Iteration 2539, loss = 0.47450492\n",
      "Iteration 2540, loss = 0.47452631\n",
      "Iteration 2541, loss = 0.47464262\n",
      "Iteration 2542, loss = 0.47425592\n",
      "Iteration 2543, loss = 0.47426417\n",
      "Iteration 2544, loss = 0.47435267\n",
      "Iteration 2545, loss = 0.47493140\n",
      "Iteration 2546, loss = 0.47430015\n",
      "Iteration 2547, loss = 0.47440176\n",
      "Iteration 2548, loss = 0.47435648\n",
      "Iteration 2549, loss = 0.47415977\n",
      "Iteration 2550, loss = 0.47417450\n",
      "Iteration 2551, loss = 0.47422380\n",
      "Iteration 2552, loss = 0.47416701\n",
      "Iteration 2553, loss = 0.47413106\n",
      "Iteration 2554, loss = 0.47444767\n",
      "Iteration 2555, loss = 0.47436912\n",
      "Iteration 2556, loss = 0.47418471\n",
      "Iteration 2557, loss = 0.47436595\n",
      "Iteration 2558, loss = 0.47495565\n",
      "Iteration 2559, loss = 0.47429671\n",
      "Iteration 2560, loss = 0.47427468\n",
      "Iteration 2561, loss = 0.47445253\n",
      "Iteration 2562, loss = 0.47426948\n",
      "Iteration 2563, loss = 0.47420920\n",
      "Iteration 2564, loss = 0.47408252\n",
      "Iteration 2565, loss = 0.47448809\n",
      "Iteration 2566, loss = 0.47425394\n",
      "Iteration 2567, loss = 0.47427907\n",
      "Iteration 2568, loss = 0.47410479\n",
      "Iteration 2569, loss = 0.47405359\n",
      "Iteration 2570, loss = 0.47413676\n",
      "Iteration 2571, loss = 0.47441450\n",
      "Iteration 2572, loss = 0.47418547\n",
      "Iteration 2573, loss = 0.47411779\n",
      "Iteration 2574, loss = 0.47425187\n",
      "Iteration 2575, loss = 0.47420168\n",
      "Iteration 2576, loss = 0.47441246\n",
      "Iteration 2577, loss = 0.47445471\n",
      "Iteration 2578, loss = 0.47441251\n",
      "Iteration 2579, loss = 0.47406614\n",
      "Iteration 2580, loss = 0.47414663\n",
      "Iteration 2581, loss = 0.47403953\n",
      "Iteration 2582, loss = 0.47413109\n",
      "Iteration 2583, loss = 0.47413974\n",
      "Iteration 2584, loss = 0.47426570\n",
      "Iteration 2585, loss = 0.47435171\n",
      "Iteration 2586, loss = 0.47420017\n",
      "Iteration 2587, loss = 0.47441249\n",
      "Iteration 2588, loss = 0.47420916\n",
      "Iteration 2589, loss = 0.47409004\n",
      "Iteration 2590, loss = 0.47416841\n",
      "Iteration 2591, loss = 0.47402689\n",
      "Iteration 2592, loss = 0.47411501\n",
      "Iteration 2593, loss = 0.47412403\n",
      "Iteration 2594, loss = 0.47418782\n",
      "Iteration 2595, loss = 0.47414989\n",
      "Iteration 2596, loss = 0.47453983\n",
      "Iteration 2597, loss = 0.47383811\n",
      "Iteration 2598, loss = 0.47394167\n",
      "Iteration 2599, loss = 0.47450392\n",
      "Iteration 2600, loss = 0.47537327\n",
      "Iteration 2601, loss = 0.47453335\n",
      "Iteration 2602, loss = 0.47450900\n",
      "Iteration 2603, loss = 0.47442360\n",
      "Iteration 2604, loss = 0.47450833\n",
      "Iteration 2605, loss = 0.47439168\n",
      "Iteration 2606, loss = 0.47426430\n",
      "Iteration 2607, loss = 0.47399370\n",
      "Iteration 2608, loss = 0.47441525\n",
      "Iteration 2609, loss = 0.47438375\n",
      "Iteration 2610, loss = 0.47467725\n",
      "Iteration 2611, loss = 0.47432404\n",
      "Iteration 2612, loss = 0.47398484\n",
      "Iteration 2613, loss = 0.47403415\n",
      "Iteration 2614, loss = 0.47417033\n",
      "Iteration 2615, loss = 0.47428563\n",
      "Iteration 2616, loss = 0.47424825\n",
      "Iteration 2617, loss = 0.47451644\n",
      "Iteration 2618, loss = 0.47395494\n",
      "Iteration 2619, loss = 0.47436096\n",
      "Iteration 2620, loss = 0.47405211\n",
      "Iteration 2621, loss = 0.47395479\n",
      "Iteration 2622, loss = 0.47390999\n",
      "Iteration 2623, loss = 0.47417288\n",
      "Iteration 2624, loss = 0.47415945\n",
      "Iteration 2625, loss = 0.47392386\n",
      "Iteration 2626, loss = 0.47397789\n",
      "Iteration 2627, loss = 0.47455339\n",
      "Iteration 2628, loss = 0.47445398\n",
      "Iteration 2629, loss = 0.47393368\n",
      "Iteration 2630, loss = 0.47388861\n",
      "Iteration 2631, loss = 0.47398014\n",
      "Iteration 2632, loss = 0.47454100\n",
      "Iteration 2633, loss = 0.47424593\n",
      "Iteration 2634, loss = 0.47387888\n",
      "Iteration 2635, loss = 0.47384783\n",
      "Iteration 2636, loss = 0.47423155\n",
      "Iteration 2637, loss = 0.47445994\n",
      "Iteration 2638, loss = 0.47440296\n",
      "Iteration 2639, loss = 0.47403678\n",
      "Iteration 2640, loss = 0.47400776\n",
      "Iteration 2641, loss = 0.47390933\n",
      "Iteration 2642, loss = 0.47400213\n",
      "Iteration 2643, loss = 0.47395456\n",
      "Iteration 2644, loss = 0.47384616\n",
      "Iteration 2645, loss = 0.47380240\n",
      "Iteration 2646, loss = 0.47398737\n",
      "Iteration 2647, loss = 0.47411694\n",
      "Iteration 2648, loss = 0.47420396\n",
      "Iteration 2649, loss = 0.47403305\n",
      "Iteration 2650, loss = 0.47393978\n",
      "Iteration 2651, loss = 0.47383012\n",
      "Iteration 2652, loss = 0.47384618\n",
      "Iteration 2653, loss = 0.47503258\n",
      "Iteration 2654, loss = 0.47417721\n",
      "Iteration 2655, loss = 0.47385087\n",
      "Iteration 2656, loss = 0.47387837\n",
      "Iteration 2657, loss = 0.47412851\n",
      "Iteration 2658, loss = 0.47422127\n",
      "Iteration 2659, loss = 0.47405621\n",
      "Iteration 2660, loss = 0.47398256\n",
      "Iteration 2661, loss = 0.47397789\n",
      "Iteration 2662, loss = 0.47390325\n",
      "Iteration 2663, loss = 0.47395892\n",
      "Iteration 2664, loss = 0.47412490\n",
      "Iteration 2665, loss = 0.47389254\n",
      "Iteration 2666, loss = 0.47388584\n",
      "Iteration 2667, loss = 0.47385278\n",
      "Iteration 2668, loss = 0.47387306\n",
      "Iteration 2669, loss = 0.47408789\n",
      "Iteration 2670, loss = 0.47379469\n",
      "Iteration 2671, loss = 0.47425647\n",
      "Iteration 2672, loss = 0.47459730\n",
      "Iteration 2673, loss = 0.47439123\n",
      "Iteration 2674, loss = 0.47402237\n",
      "Iteration 2675, loss = 0.47381232\n",
      "Iteration 2676, loss = 0.47474637\n",
      "Iteration 2677, loss = 0.47429504\n",
      "Iteration 2678, loss = 0.47386953\n",
      "Iteration 2679, loss = 0.47368254\n",
      "Iteration 2680, loss = 0.47408548\n",
      "Iteration 2681, loss = 0.47418114\n",
      "Iteration 2682, loss = 0.47412199\n",
      "Iteration 2683, loss = 0.47385980\n",
      "Iteration 2684, loss = 0.47361792\n",
      "Iteration 2685, loss = 0.47421716\n",
      "Iteration 2686, loss = 0.47438549\n",
      "Iteration 2687, loss = 0.47420850\n",
      "Iteration 2688, loss = 0.47404179\n",
      "Iteration 2689, loss = 0.47418950\n",
      "Iteration 2690, loss = 0.47399385\n",
      "Iteration 2691, loss = 0.47387059\n",
      "Iteration 2692, loss = 0.47375965\n",
      "Iteration 2693, loss = 0.47380713\n",
      "Iteration 2694, loss = 0.47389141\n",
      "Iteration 2695, loss = 0.47380807\n",
      "Iteration 2696, loss = 0.47381648\n",
      "Iteration 2697, loss = 0.47378795\n",
      "Iteration 2698, loss = 0.47387367\n",
      "Iteration 2699, loss = 0.47392075\n",
      "Iteration 2700, loss = 0.47380638\n",
      "Iteration 2701, loss = 0.47373133\n",
      "Iteration 2702, loss = 0.47379938\n",
      "Iteration 2703, loss = 0.47399087\n",
      "Iteration 2704, loss = 0.47413242\n",
      "Iteration 2705, loss = 0.47410790\n",
      "Iteration 2706, loss = 0.47416618\n",
      "Iteration 2707, loss = 0.47376627\n",
      "Iteration 2708, loss = 0.47373329\n",
      "Iteration 2709, loss = 0.47378139\n",
      "Iteration 2710, loss = 0.47441322\n",
      "Iteration 2711, loss = 0.47422112\n",
      "Iteration 2712, loss = 0.47387703\n",
      "Iteration 2713, loss = 0.47444149\n",
      "Iteration 2714, loss = 0.47463417\n",
      "Iteration 2715, loss = 0.47402722\n",
      "Iteration 2716, loss = 0.47416104\n",
      "Iteration 2717, loss = 0.47388984\n",
      "Iteration 2718, loss = 0.47384831\n",
      "Iteration 2719, loss = 0.47399766\n",
      "Iteration 2720, loss = 0.47404657\n",
      "Iteration 2721, loss = 0.47381275\n",
      "Iteration 2722, loss = 0.47351281\n",
      "Iteration 2723, loss = 0.47431291\n",
      "Iteration 2724, loss = 0.47535652\n",
      "Iteration 2725, loss = 0.47514561\n",
      "Iteration 2726, loss = 0.47484133\n",
      "Iteration 2727, loss = 0.47441077\n",
      "Iteration 2728, loss = 0.47396090\n",
      "Iteration 2729, loss = 0.47371558\n",
      "Iteration 2730, loss = 0.47377380\n",
      "Iteration 2731, loss = 0.47374300\n",
      "Iteration 2732, loss = 0.47418478\n",
      "Iteration 2733, loss = 0.47372049\n",
      "Iteration 2734, loss = 0.47400984\n",
      "Iteration 2735, loss = 0.47393611\n",
      "Iteration 2736, loss = 0.47445242\n",
      "Iteration 2737, loss = 0.47383906\n",
      "Iteration 2738, loss = 0.47389082\n",
      "Iteration 2739, loss = 0.47363482\n",
      "Iteration 2740, loss = 0.47368965\n",
      "Iteration 2741, loss = 0.47461287\n",
      "Iteration 2742, loss = 0.47434403\n",
      "Iteration 2743, loss = 0.47385074\n",
      "Iteration 2744, loss = 0.47414872\n",
      "Iteration 2745, loss = 0.47376283\n",
      "Iteration 2746, loss = 0.47380130\n",
      "Iteration 2747, loss = 0.47387807\n",
      "Iteration 2748, loss = 0.47370625\n",
      "Iteration 2749, loss = 0.47381788\n",
      "Iteration 2750, loss = 0.47396000\n",
      "Iteration 2751, loss = 0.47405063\n",
      "Iteration 2752, loss = 0.47386332\n",
      "Iteration 2753, loss = 0.47383532\n",
      "Iteration 2754, loss = 0.47361745\n",
      "Iteration 2755, loss = 0.47372790\n",
      "Iteration 2756, loss = 0.47372460\n",
      "Iteration 2757, loss = 0.47370455\n",
      "Iteration 2758, loss = 0.47380810\n",
      "Iteration 2759, loss = 0.47413235\n",
      "Iteration 2760, loss = 0.47409240\n",
      "Iteration 2761, loss = 0.47411512\n",
      "Iteration 2762, loss = 0.47401384\n",
      "Iteration 2763, loss = 0.47385273\n",
      "Iteration 2764, loss = 0.47349824\n",
      "Iteration 2765, loss = 0.47393219\n",
      "Iteration 2766, loss = 0.47385681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2767, loss = 0.47392817\n",
      "Iteration 2768, loss = 0.47396125\n",
      "Iteration 2769, loss = 0.47370258\n",
      "Iteration 2770, loss = 0.47358885\n",
      "Iteration 2771, loss = 0.47359334\n",
      "Iteration 2772, loss = 0.47382608\n",
      "Iteration 2773, loss = 0.47411979\n",
      "Iteration 2774, loss = 0.47431657\n",
      "Iteration 2775, loss = 0.47360154\n",
      "Iteration 2776, loss = 0.47382676\n",
      "Iteration 2777, loss = 0.47366709\n",
      "Iteration 2778, loss = 0.47373396\n",
      "Iteration 2779, loss = 0.47420161\n",
      "Iteration 2780, loss = 0.47360741\n",
      "Iteration 2781, loss = 0.47374083\n",
      "Iteration 2782, loss = 0.47363155\n",
      "Iteration 2783, loss = 0.47380972\n",
      "Iteration 2784, loss = 0.47389876\n",
      "Iteration 2785, loss = 0.47366230\n",
      "Iteration 2786, loss = 0.47368478\n",
      "Iteration 2787, loss = 0.47361090\n",
      "Iteration 2788, loss = 0.47363063\n",
      "Iteration 2789, loss = 0.47384039\n",
      "Iteration 2790, loss = 0.47361959\n",
      "Iteration 2791, loss = 0.47367721\n",
      "Iteration 2792, loss = 0.47374451\n",
      "Iteration 2793, loss = 0.47376199\n",
      "Iteration 2794, loss = 0.47371481\n",
      "Iteration 2795, loss = 0.47361263\n",
      "Iteration 2796, loss = 0.47359298\n",
      "Iteration 2797, loss = 0.47388529\n",
      "Iteration 2798, loss = 0.47377535\n",
      "Iteration 2799, loss = 0.47366656\n",
      "Iteration 2800, loss = 0.47360100\n",
      "Iteration 2801, loss = 0.47381792\n",
      "Iteration 2802, loss = 0.47397818\n",
      "Iteration 2803, loss = 0.47384933\n",
      "Iteration 2804, loss = 0.47357008\n",
      "Iteration 2805, loss = 0.47423844\n",
      "Iteration 2806, loss = 0.47363578\n",
      "Iteration 2807, loss = 0.47374509\n",
      "Iteration 2808, loss = 0.47351748\n",
      "Iteration 2809, loss = 0.47352760\n",
      "Iteration 2810, loss = 0.47350490\n",
      "Iteration 2811, loss = 0.47377969\n",
      "Iteration 2812, loss = 0.47369622\n",
      "Iteration 2813, loss = 0.47361627\n",
      "Iteration 2814, loss = 0.47389099\n",
      "Iteration 2815, loss = 0.47350472\n",
      "Iteration 2816, loss = 0.47388682\n",
      "Iteration 2817, loss = 0.47442278\n",
      "Iteration 2818, loss = 0.47416178\n",
      "Iteration 2819, loss = 0.47362820\n",
      "Iteration 2820, loss = 0.47384292\n",
      "Iteration 2821, loss = 0.47373100\n",
      "Iteration 2822, loss = 0.47375526\n",
      "Iteration 2823, loss = 0.47359431\n",
      "Iteration 2824, loss = 0.47363881\n",
      "Iteration 2825, loss = 0.47375436\n",
      "Iteration 2826, loss = 0.47376077\n",
      "Iteration 2827, loss = 0.47390418\n",
      "Iteration 2828, loss = 0.47388389\n",
      "Iteration 2829, loss = 0.47429621\n",
      "Iteration 2830, loss = 0.47360055\n",
      "Iteration 2831, loss = 0.47393669\n",
      "Iteration 2832, loss = 0.47416572\n",
      "Iteration 2833, loss = 0.47359884\n",
      "Iteration 2834, loss = 0.47360018\n",
      "Iteration 2835, loss = 0.47368067\n",
      "Iteration 2836, loss = 0.47351703\n",
      "Iteration 2837, loss = 0.47335718\n",
      "Iteration 2838, loss = 0.47362391\n",
      "Iteration 2839, loss = 0.47368481\n",
      "Iteration 2840, loss = 0.47371194\n",
      "Iteration 2841, loss = 0.47348943\n",
      "Iteration 2842, loss = 0.47346958\n",
      "Iteration 2843, loss = 0.47358330\n",
      "Iteration 2844, loss = 0.47360583\n",
      "Iteration 2845, loss = 0.47405456\n",
      "Iteration 2846, loss = 0.47348952\n",
      "Iteration 2847, loss = 0.47349554\n",
      "Iteration 2848, loss = 0.47354685\n",
      "Iteration 2849, loss = 0.47349679\n",
      "Iteration 2850, loss = 0.47342903\n",
      "Iteration 2851, loss = 0.47358514\n",
      "Iteration 2852, loss = 0.47338251\n",
      "Iteration 2853, loss = 0.47338141\n",
      "Iteration 2854, loss = 0.47346631\n",
      "Iteration 2855, loss = 0.47349907\n",
      "Iteration 2856, loss = 0.47343055\n",
      "Iteration 2857, loss = 0.47330640\n",
      "Iteration 2858, loss = 0.47525673\n",
      "Iteration 2859, loss = 0.47428391\n",
      "Iteration 2860, loss = 0.47381762\n",
      "Iteration 2861, loss = 0.47341642\n",
      "Iteration 2862, loss = 0.47347749\n",
      "Iteration 2863, loss = 0.47370520\n",
      "Iteration 2864, loss = 0.47355002\n",
      "Iteration 2865, loss = 0.47348080\n",
      "Iteration 2866, loss = 0.47356344\n",
      "Iteration 2867, loss = 0.47363647\n",
      "Iteration 2868, loss = 0.47343868\n",
      "Iteration 2869, loss = 0.47345004\n",
      "Iteration 2870, loss = 0.47361018\n",
      "Iteration 2871, loss = 0.47349065\n",
      "Iteration 2872, loss = 0.47346671\n",
      "Iteration 2873, loss = 0.47377515\n",
      "Iteration 2874, loss = 0.47394920\n",
      "Iteration 2875, loss = 0.47429230\n",
      "Iteration 2876, loss = 0.47399193\n",
      "Iteration 2877, loss = 0.47361949\n",
      "Iteration 2878, loss = 0.47347145\n",
      "Iteration 2879, loss = 0.47361586\n",
      "Iteration 2880, loss = 0.47376170\n",
      "Iteration 2881, loss = 0.47369514\n",
      "Iteration 2882, loss = 0.47346692\n",
      "Iteration 2883, loss = 0.47334309\n",
      "Iteration 2884, loss = 0.47381393\n",
      "Iteration 2885, loss = 0.47364543\n",
      "Iteration 2886, loss = 0.47320086\n",
      "Iteration 2887, loss = 0.47345824\n",
      "Iteration 2888, loss = 0.47407642\n",
      "Iteration 2889, loss = 0.47428475\n",
      "Iteration 2890, loss = 0.47390467\n",
      "Iteration 2891, loss = 0.47347191\n",
      "Iteration 2892, loss = 0.47341617\n",
      "Iteration 2893, loss = 0.47397092\n",
      "Iteration 2894, loss = 0.47394740\n",
      "Iteration 2895, loss = 0.47351472\n",
      "Iteration 2896, loss = 0.47324123\n",
      "Iteration 2897, loss = 0.47349350\n",
      "Iteration 2898, loss = 0.47399253\n",
      "Iteration 2899, loss = 0.47406534\n",
      "Iteration 2900, loss = 0.47390376\n",
      "Iteration 2901, loss = 0.47330566\n",
      "Iteration 2902, loss = 0.47427650\n",
      "Iteration 2903, loss = 0.47376989\n",
      "Iteration 2904, loss = 0.47350056\n",
      "Iteration 2905, loss = 0.47336651\n",
      "Iteration 2906, loss = 0.47357373\n",
      "Iteration 2907, loss = 0.47337864\n",
      "Iteration 2908, loss = 0.47341993\n",
      "Iteration 2909, loss = 0.47334817\n",
      "Iteration 2910, loss = 0.47357348\n",
      "Iteration 2911, loss = 0.47318920\n",
      "Iteration 2912, loss = 0.47347415\n",
      "Iteration 2913, loss = 0.47388353\n",
      "Iteration 2914, loss = 0.47415568\n",
      "Iteration 2915, loss = 0.47361270\n",
      "Iteration 2916, loss = 0.47383982\n",
      "Iteration 2917, loss = 0.47367885\n",
      "Iteration 2918, loss = 0.47391189\n",
      "Iteration 2919, loss = 0.47313200\n",
      "Iteration 2920, loss = 0.47330218\n",
      "Iteration 2921, loss = 0.47405727\n",
      "Iteration 2922, loss = 0.47434421\n",
      "Iteration 2923, loss = 0.47386884\n",
      "Iteration 2924, loss = 0.47353899\n",
      "Iteration 2925, loss = 0.47339133\n",
      "Iteration 2926, loss = 0.47328767\n",
      "Iteration 2927, loss = 0.47365520\n",
      "Iteration 2928, loss = 0.47428948\n",
      "Iteration 2929, loss = 0.47384372\n",
      "Iteration 2930, loss = 0.47432406\n",
      "Iteration 2931, loss = 0.47337372\n",
      "Iteration 2932, loss = 0.47362748\n",
      "Iteration 2933, loss = 0.47337754\n",
      "Iteration 2934, loss = 0.47314271\n",
      "Iteration 2935, loss = 0.47339994\n",
      "Iteration 2936, loss = 0.47377700\n",
      "Iteration 2937, loss = 0.47403793\n",
      "Iteration 2938, loss = 0.47377960\n",
      "Iteration 2939, loss = 0.47346062\n",
      "Iteration 2940, loss = 0.47333001\n",
      "Iteration 2941, loss = 0.47351863\n",
      "Iteration 2942, loss = 0.47380729\n",
      "Iteration 2943, loss = 0.47417575\n",
      "Iteration 2944, loss = 0.47372261\n",
      "Iteration 2945, loss = 0.47338949\n",
      "Iteration 2946, loss = 0.47359589\n",
      "Iteration 2947, loss = 0.47383966\n",
      "Iteration 2948, loss = 0.47333737\n",
      "Iteration 2949, loss = 0.47354702\n",
      "Iteration 2950, loss = 0.47349325\n",
      "Iteration 2951, loss = 0.47415087\n",
      "Iteration 2952, loss = 0.47378789\n",
      "Iteration 2953, loss = 0.47324480\n",
      "Iteration 2954, loss = 0.47341685\n",
      "Iteration 2955, loss = 0.47376916\n",
      "Iteration 2956, loss = 0.47421980\n",
      "Iteration 2957, loss = 0.47354691\n",
      "Iteration 2958, loss = 0.47340881\n",
      "Iteration 2959, loss = 0.47340727\n",
      "Iteration 2960, loss = 0.47408247\n",
      "Iteration 2961, loss = 0.47422986\n",
      "Iteration 2962, loss = 0.47403360\n",
      "Iteration 2963, loss = 0.47332733\n",
      "Iteration 2964, loss = 0.47347242\n",
      "Iteration 2965, loss = 0.47335420\n",
      "Iteration 2966, loss = 0.47330512\n",
      "Iteration 2967, loss = 0.47325663\n",
      "Iteration 2968, loss = 0.47385207\n",
      "Iteration 2969, loss = 0.47320796\n",
      "Iteration 2970, loss = 0.47314642\n",
      "Iteration 2971, loss = 0.47429185\n",
      "Iteration 2972, loss = 0.47387656\n",
      "Iteration 2973, loss = 0.47353824\n",
      "Iteration 2974, loss = 0.47319372\n",
      "Iteration 2975, loss = 0.47401624\n",
      "Iteration 2976, loss = 0.47366657\n",
      "Iteration 2977, loss = 0.47335254\n",
      "Iteration 2978, loss = 0.47333920\n",
      "Iteration 2979, loss = 0.47336900\n",
      "Iteration 2980, loss = 0.47339966\n",
      "Iteration 2981, loss = 0.47334593\n",
      "Iteration 2982, loss = 0.47333091\n",
      "Iteration 2983, loss = 0.47372892\n",
      "Iteration 2984, loss = 0.47345395\n",
      "Iteration 2985, loss = 0.47363364\n",
      "Iteration 2986, loss = 0.47365009\n",
      "Iteration 2987, loss = 0.47363525\n",
      "Iteration 2988, loss = 0.47339626\n",
      "Iteration 2989, loss = 0.47393657\n",
      "Iteration 2990, loss = 0.47340769\n",
      "Iteration 2991, loss = 0.47335988\n",
      "Iteration 2992, loss = 0.47330250\n",
      "Iteration 2993, loss = 0.47329875\n",
      "Iteration 2994, loss = 0.47319085\n",
      "Iteration 2995, loss = 0.47318563\n",
      "Iteration 2996, loss = 0.47324989\n",
      "Iteration 2997, loss = 0.47360164\n",
      "Iteration 2998, loss = 0.47364819\n",
      "Iteration 2999, loss = 0.47391672\n",
      "Iteration 3000, loss = 0.47319487\n",
      "Iteration 3001, loss = 0.47305004\n",
      "Iteration 3002, loss = 0.47417103\n",
      "Iteration 3003, loss = 0.47478624\n",
      "Iteration 3004, loss = 0.47447600\n",
      "Iteration 3005, loss = 0.47341904\n",
      "Iteration 3006, loss = 0.47354069\n",
      "Iteration 3007, loss = 0.47353623\n",
      "Iteration 3008, loss = 0.47357427\n",
      "Iteration 3009, loss = 0.47336903\n",
      "Iteration 3010, loss = 0.47333445\n",
      "Iteration 3011, loss = 0.47325509\n",
      "Iteration 3012, loss = 0.47317444\n",
      "Iteration 3013, loss = 0.47319427\n",
      "Iteration 3014, loss = 0.47323592\n",
      "Iteration 3015, loss = 0.47317578\n",
      "Iteration 3016, loss = 0.47333925\n",
      "Iteration 3017, loss = 0.47323000\n",
      "Iteration 3018, loss = 0.47324452\n",
      "Iteration 3019, loss = 0.47317841\n",
      "Iteration 3020, loss = 0.47351967\n",
      "Iteration 3021, loss = 0.47318171\n",
      "Iteration 3022, loss = 0.47330947\n",
      "Iteration 3023, loss = 0.47321880\n",
      "Iteration 3024, loss = 0.47361195\n",
      "Iteration 3025, loss = 0.47326079\n",
      "Iteration 3026, loss = 0.47311481\n",
      "Iteration 3027, loss = 0.47313438\n",
      "Iteration 3028, loss = 0.47328996\n",
      "Iteration 3029, loss = 0.47347103\n",
      "Iteration 3030, loss = 0.47353197\n",
      "Iteration 3031, loss = 0.47324039\n",
      "Iteration 3032, loss = 0.47324924\n",
      "Iteration 3033, loss = 0.47329802\n",
      "Iteration 3034, loss = 0.47338737\n",
      "Iteration 3035, loss = 0.47344809\n",
      "Iteration 3036, loss = 0.47316925\n",
      "Iteration 3037, loss = 0.47318886\n",
      "Iteration 3038, loss = 0.47315181\n",
      "Iteration 3039, loss = 0.47305089\n",
      "Iteration 3040, loss = 0.47364742\n",
      "Iteration 3041, loss = 0.47324951\n",
      "Iteration 3042, loss = 0.47313905\n",
      "Iteration 3043, loss = 0.47318016\n",
      "Iteration 3044, loss = 0.47332653\n",
      "Iteration 3045, loss = 0.47326993\n",
      "Iteration 3046, loss = 0.47341153\n",
      "Iteration 3047, loss = 0.47329629\n",
      "Iteration 3048, loss = 0.47305589\n",
      "Iteration 3049, loss = 0.47307672\n",
      "Iteration 3050, loss = 0.47328640\n",
      "Iteration 3051, loss = 0.47347376\n",
      "Iteration 3052, loss = 0.47350483\n",
      "Iteration 3053, loss = 0.47337165\n",
      "Iteration 3054, loss = 0.47337936\n",
      "Iteration 3055, loss = 0.47299612\n",
      "Iteration 3056, loss = 0.47307046\n",
      "Iteration 3057, loss = 0.47381051\n",
      "Iteration 3058, loss = 0.47431288\n",
      "Iteration 3059, loss = 0.47441201\n",
      "Iteration 3060, loss = 0.47337305\n",
      "Iteration 3061, loss = 0.47341794\n",
      "Iteration 3062, loss = 0.47327823\n",
      "Iteration 3063, loss = 0.47367474\n",
      "Iteration 3064, loss = 0.47304375\n",
      "Iteration 3065, loss = 0.47311748\n",
      "Iteration 3066, loss = 0.47398786\n",
      "Iteration 3067, loss = 0.47359496\n",
      "Iteration 3068, loss = 0.47304713\n",
      "Iteration 3069, loss = 0.47318915\n",
      "Iteration 3070, loss = 0.47340877\n",
      "Iteration 3071, loss = 0.47393938\n",
      "Iteration 3072, loss = 0.47417822\n",
      "Iteration 3073, loss = 0.47358211\n",
      "Iteration 3074, loss = 0.47288758\n",
      "Iteration 3075, loss = 0.47353806\n",
      "Iteration 3076, loss = 0.47406459\n",
      "Iteration 3077, loss = 0.47385043\n",
      "Iteration 3078, loss = 0.47315109\n",
      "Iteration 3079, loss = 0.47413818\n",
      "Iteration 3080, loss = 0.47327204\n",
      "Iteration 3081, loss = 0.47316169\n",
      "Iteration 3082, loss = 0.47317179\n",
      "Iteration 3083, loss = 0.47314572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3084, loss = 0.47311514\n",
      "Iteration 3085, loss = 0.47311938\n",
      "Iteration 3086, loss = 0.47326111\n",
      "Iteration 3087, loss = 0.47301441\n",
      "Iteration 3088, loss = 0.47320495\n",
      "Iteration 3089, loss = 0.47353457\n",
      "Iteration 3090, loss = 0.47326296\n",
      "Iteration 3091, loss = 0.47300305\n",
      "Iteration 3092, loss = 0.47292963\n",
      "Iteration 3093, loss = 0.47386863\n",
      "Iteration 3094, loss = 0.47367124\n",
      "Iteration 3095, loss = 0.47317068\n",
      "Iteration 3096, loss = 0.47305166\n",
      "Iteration 3097, loss = 0.47308800\n",
      "Iteration 3098, loss = 0.47316248\n",
      "Iteration 3099, loss = 0.47324784\n",
      "Iteration 3100, loss = 0.47315601\n",
      "Iteration 3101, loss = 0.47307296\n",
      "Iteration 3102, loss = 0.47313147\n",
      "Iteration 3103, loss = 0.47300662\n",
      "Iteration 3104, loss = 0.47297003\n",
      "Iteration 3105, loss = 0.47298190\n",
      "Iteration 3106, loss = 0.47321883\n",
      "Iteration 3107, loss = 0.47345839\n",
      "Iteration 3108, loss = 0.47367447\n",
      "Iteration 3109, loss = 0.47385603\n",
      "Iteration 3110, loss = 0.47297958\n",
      "Iteration 3111, loss = 0.47314313\n",
      "Iteration 3112, loss = 0.47344030\n",
      "Iteration 3113, loss = 0.47410220\n",
      "Iteration 3114, loss = 0.47447972\n",
      "Iteration 3115, loss = 0.47426535\n",
      "Iteration 3116, loss = 0.47381891\n",
      "Iteration 3117, loss = 0.47324840\n",
      "Iteration 3118, loss = 0.47302433\n",
      "Iteration 3119, loss = 0.47326750\n",
      "Iteration 3120, loss = 0.47364503\n",
      "Iteration 3121, loss = 0.47299541\n",
      "Iteration 3122, loss = 0.47323352\n",
      "Iteration 3123, loss = 0.47328434\n",
      "Iteration 3124, loss = 0.47363994\n",
      "Iteration 3125, loss = 0.47391331\n",
      "Iteration 3126, loss = 0.47308235\n",
      "Iteration 3127, loss = 0.47304980\n",
      "Iteration 3128, loss = 0.47353346\n",
      "Iteration 3129, loss = 0.47314925\n",
      "Iteration 3130, loss = 0.47303220\n",
      "Iteration 3131, loss = 0.47306129\n",
      "Iteration 3132, loss = 0.47303088\n",
      "Iteration 3133, loss = 0.47318326\n",
      "Iteration 3134, loss = 0.47299377\n",
      "Iteration 3135, loss = 0.47303139\n",
      "Iteration 3136, loss = 0.47317526\n",
      "Iteration 3137, loss = 0.47310862\n",
      "Iteration 3138, loss = 0.47349974\n",
      "Iteration 3139, loss = 0.47310336\n",
      "Iteration 3140, loss = 0.47313353\n",
      "Iteration 3141, loss = 0.47344778\n",
      "Iteration 3142, loss = 0.47307601\n",
      "Iteration 3143, loss = 0.47314974\n",
      "Iteration 3144, loss = 0.47323392\n",
      "Iteration 3145, loss = 0.47330793\n",
      "Iteration 3146, loss = 0.47298008\n",
      "Iteration 3147, loss = 0.47298740\n",
      "Iteration 3148, loss = 0.47307029\n",
      "Iteration 3149, loss = 0.47300650\n",
      "Iteration 3150, loss = 0.47323580\n",
      "Iteration 3151, loss = 0.47305997\n",
      "Iteration 3152, loss = 0.47305223\n",
      "Iteration 3153, loss = 0.47298613\n",
      "Iteration 3154, loss = 0.47298090\n",
      "Iteration 3155, loss = 0.47327616\n",
      "Iteration 3156, loss = 0.47303893\n",
      "Iteration 3157, loss = 0.47351715\n",
      "Iteration 3158, loss = 0.47342333\n",
      "Iteration 3159, loss = 0.47314354\n",
      "Iteration 3160, loss = 0.47293581\n",
      "Iteration 3161, loss = 0.47281180\n",
      "Iteration 3162, loss = 0.47343641\n",
      "Iteration 3163, loss = 0.47332786\n",
      "Iteration 3164, loss = 0.47335928\n",
      "Iteration 3165, loss = 0.47317889\n",
      "Iteration 3166, loss = 0.47325175\n",
      "Iteration 3167, loss = 0.47310103\n",
      "Iteration 3168, loss = 0.47351248\n",
      "Iteration 3169, loss = 0.47308780\n",
      "Iteration 3170, loss = 0.47276991\n",
      "Iteration 3171, loss = 0.47335959\n",
      "Iteration 3172, loss = 0.47362691\n",
      "Iteration 3173, loss = 0.47386534\n",
      "Iteration 3174, loss = 0.47370514\n",
      "Iteration 3175, loss = 0.47317085\n",
      "Iteration 3176, loss = 0.47370052\n",
      "Iteration 3177, loss = 0.47305696\n",
      "Iteration 3178, loss = 0.47308239\n",
      "Iteration 3179, loss = 0.47318359\n",
      "Iteration 3180, loss = 0.47306457\n",
      "Iteration 3181, loss = 0.47309003\n",
      "Iteration 3182, loss = 0.47308286\n",
      "Iteration 3183, loss = 0.47315369\n",
      "Iteration 3184, loss = 0.47330421\n",
      "Iteration 3185, loss = 0.47331501\n",
      "Iteration 3186, loss = 0.47337068\n",
      "Iteration 3187, loss = 0.47299212\n",
      "Iteration 3188, loss = 0.47304422\n",
      "Iteration 3189, loss = 0.47310654\n",
      "Iteration 3190, loss = 0.47304232\n",
      "Iteration 3191, loss = 0.47294161\n",
      "Iteration 3192, loss = 0.47285251\n",
      "Iteration 3193, loss = 0.47305685\n",
      "Iteration 3194, loss = 0.47328203\n",
      "Iteration 3195, loss = 0.47296574\n",
      "Iteration 3196, loss = 0.47264964\n",
      "Iteration 3197, loss = 0.47382951\n",
      "Iteration 3198, loss = 0.47389052\n",
      "Iteration 3199, loss = 0.47373399\n",
      "Iteration 3200, loss = 0.47311611\n",
      "Iteration 3201, loss = 0.47286762\n",
      "Iteration 3202, loss = 0.47290390\n",
      "Iteration 3203, loss = 0.47344463\n",
      "Iteration 3204, loss = 0.47367504\n",
      "Iteration 3205, loss = 0.47382672\n",
      "Iteration 3206, loss = 0.47366739\n",
      "Iteration 3207, loss = 0.47337834\n",
      "Iteration 3208, loss = 0.47308303\n",
      "Iteration 3209, loss = 0.47314472\n",
      "Iteration 3210, loss = 0.47313499\n",
      "Iteration 3211, loss = 0.47312927\n",
      "Iteration 3212, loss = 0.47301280\n",
      "Iteration 3213, loss = 0.47307336\n",
      "Iteration 3214, loss = 0.47296603\n",
      "Iteration 3215, loss = 0.47291139\n",
      "Iteration 3216, loss = 0.47310840\n",
      "Iteration 3217, loss = 0.47280586\n",
      "Iteration 3218, loss = 0.47347115\n",
      "Iteration 3219, loss = 0.47349024\n",
      "Iteration 3220, loss = 0.47321778\n",
      "Iteration 3221, loss = 0.47279570\n",
      "Iteration 3222, loss = 0.47266426\n",
      "Iteration 3223, loss = 0.47375971\n",
      "Iteration 3224, loss = 0.47375033\n",
      "Iteration 3225, loss = 0.47327849\n",
      "Iteration 3226, loss = 0.47304182\n",
      "Iteration 3227, loss = 0.47309231\n",
      "Iteration 3228, loss = 0.47323417\n",
      "Iteration 3229, loss = 0.47315086\n",
      "Iteration 3230, loss = 0.47330808\n",
      "Iteration 3231, loss = 0.47301875\n",
      "Iteration 3232, loss = 0.47315455\n",
      "Iteration 3233, loss = 0.47317397\n",
      "Iteration 3234, loss = 0.47299061\n",
      "Iteration 3235, loss = 0.47287472\n",
      "Iteration 3236, loss = 0.47339268\n",
      "Iteration 3237, loss = 0.47310770\n",
      "Iteration 3238, loss = 0.47357542\n",
      "Iteration 3239, loss = 0.47335535\n",
      "Iteration 3240, loss = 0.47323497\n",
      "Iteration 3241, loss = 0.47294820\n",
      "Iteration 3242, loss = 0.47279313\n",
      "Iteration 3243, loss = 0.47303154\n",
      "Iteration 3244, loss = 0.47349191\n",
      "Iteration 3245, loss = 0.47402219\n",
      "Iteration 3246, loss = 0.47355671\n",
      "Iteration 3247, loss = 0.47339560\n",
      "Iteration 3248, loss = 0.47317913\n",
      "Iteration 3249, loss = 0.47266182\n",
      "Iteration 3250, loss = 0.47330569\n",
      "Iteration 3251, loss = 0.47352772\n",
      "Iteration 3252, loss = 0.47343359\n",
      "Iteration 3253, loss = 0.47336117\n",
      "Iteration 3254, loss = 0.47292396\n",
      "Iteration 3255, loss = 0.47288163\n",
      "Iteration 3256, loss = 0.47293960\n",
      "Iteration 3257, loss = 0.47346497\n",
      "Iteration 3258, loss = 0.47347772\n",
      "Iteration 3259, loss = 0.47318137\n",
      "Iteration 3260, loss = 0.47299860\n",
      "Iteration 3261, loss = 0.47302618\n",
      "Iteration 3262, loss = 0.47282474\n",
      "Iteration 3263, loss = 0.47305120\n",
      "Iteration 3264, loss = 0.47295631\n",
      "Iteration 3265, loss = 0.47285772\n",
      "Iteration 3266, loss = 0.47296536\n",
      "Iteration 3267, loss = 0.47297993\n",
      "Iteration 3268, loss = 0.47289446\n",
      "Iteration 3269, loss = 0.47287125\n",
      "Iteration 3270, loss = 0.47315302\n",
      "Iteration 3271, loss = 0.47308540\n",
      "Iteration 3272, loss = 0.47301096\n",
      "Iteration 3273, loss = 0.47360977\n",
      "Iteration 3274, loss = 0.47298928\n",
      "Iteration 3275, loss = 0.47315860\n",
      "Iteration 3276, loss = 0.47288881\n",
      "Iteration 3277, loss = 0.47276368\n",
      "Iteration 3278, loss = 0.47286860\n",
      "Iteration 3279, loss = 0.47311635\n",
      "Iteration 3280, loss = 0.47322600\n",
      "Iteration 3281, loss = 0.47305859\n",
      "Iteration 3282, loss = 0.47286973\n",
      "Iteration 3283, loss = 0.47271207\n",
      "Iteration 3284, loss = 0.47356143\n",
      "Iteration 3285, loss = 0.47312390\n",
      "Iteration 3286, loss = 0.47278712\n",
      "Iteration 3287, loss = 0.47268220\n",
      "Iteration 3288, loss = 0.47293630\n",
      "Iteration 3289, loss = 0.47316054\n",
      "Iteration 3290, loss = 0.47320118\n",
      "Iteration 3291, loss = 0.47290781\n",
      "Iteration 3292, loss = 0.47330830\n",
      "Iteration 3293, loss = 0.47291248\n",
      "Iteration 3294, loss = 0.47298030\n",
      "Iteration 3295, loss = 0.47324897\n",
      "Iteration 3296, loss = 0.47297011\n",
      "Iteration 3297, loss = 0.47281937\n",
      "Iteration 3298, loss = 0.47280489\n",
      "Iteration 3299, loss = 0.47382765\n",
      "Iteration 3300, loss = 0.47337421\n",
      "Iteration 3301, loss = 0.47313528\n",
      "Iteration 3302, loss = 0.47320207\n",
      "Iteration 3303, loss = 0.47295721\n",
      "Iteration 3304, loss = 0.47318317\n",
      "Iteration 3305, loss = 0.47313339\n",
      "Iteration 3306, loss = 0.47284822\n",
      "Iteration 3307, loss = 0.47276834\n",
      "Iteration 3308, loss = 0.47295613\n",
      "Iteration 3309, loss = 0.47328131\n",
      "Iteration 3310, loss = 0.47284196\n",
      "Iteration 3311, loss = 0.47360143\n",
      "Iteration 3312, loss = 0.47318445\n",
      "Iteration 3313, loss = 0.47281732\n",
      "Iteration 3314, loss = 0.47306656\n",
      "Iteration 3315, loss = 0.47280818\n",
      "Iteration 3316, loss = 0.47285746\n",
      "Iteration 3317, loss = 0.47282083\n",
      "Iteration 3318, loss = 0.47286744\n",
      "Iteration 3319, loss = 0.47296471\n",
      "Iteration 3320, loss = 0.47297849\n",
      "Iteration 3321, loss = 0.47296117\n",
      "Iteration 3322, loss = 0.47283232\n",
      "Iteration 3323, loss = 0.47284932\n",
      "Iteration 3324, loss = 0.47282046\n",
      "Iteration 3325, loss = 0.47292825\n",
      "Iteration 3326, loss = 0.47283006\n",
      "Iteration 3327, loss = 0.47319713\n",
      "Iteration 3328, loss = 0.47266660\n",
      "Iteration 3329, loss = 0.47289161\n",
      "Iteration 3330, loss = 0.47328995\n",
      "Iteration 3331, loss = 0.47329727\n",
      "Iteration 3332, loss = 0.47307425\n",
      "Iteration 3333, loss = 0.47278678\n",
      "Iteration 3334, loss = 0.47311897\n",
      "Iteration 3335, loss = 0.47331373\n",
      "Iteration 3336, loss = 0.47284402\n",
      "Iteration 3337, loss = 0.47281478\n",
      "Iteration 3338, loss = 0.47312030\n",
      "Iteration 3339, loss = 0.47313157\n",
      "Iteration 3340, loss = 0.47289021\n",
      "Iteration 3341, loss = 0.47277638\n",
      "Iteration 3342, loss = 0.47303526\n",
      "Iteration 3343, loss = 0.47314732\n",
      "Iteration 3344, loss = 0.47282570\n",
      "Iteration 3345, loss = 0.47304562\n",
      "Iteration 3346, loss = 0.47324289\n",
      "Iteration 3347, loss = 0.47337531\n",
      "Iteration 3348, loss = 0.47297693\n",
      "Iteration 3349, loss = 0.47268161\n",
      "Iteration 3350, loss = 0.47257175\n",
      "Iteration 3351, loss = 0.47302823\n",
      "Iteration 3352, loss = 0.47321688\n",
      "Iteration 3353, loss = 0.47308659\n",
      "Iteration 3354, loss = 0.47304253\n",
      "Iteration 3355, loss = 0.47272516\n",
      "Iteration 3356, loss = 0.47284990\n",
      "Iteration 3357, loss = 0.47300743\n",
      "Iteration 3358, loss = 0.47311235\n",
      "Iteration 3359, loss = 0.47277195\n",
      "Iteration 3360, loss = 0.47267575\n",
      "Iteration 3361, loss = 0.47265253\n",
      "Iteration 3362, loss = 0.47272405\n",
      "Iteration 3363, loss = 0.47277051\n",
      "Iteration 3364, loss = 0.47275112\n",
      "Iteration 3365, loss = 0.47272732\n",
      "Iteration 3366, loss = 0.47277148\n",
      "Iteration 3367, loss = 0.47302810\n",
      "Iteration 3368, loss = 0.47354324\n",
      "Iteration 3369, loss = 0.47333527\n",
      "Iteration 3370, loss = 0.47321182\n",
      "Iteration 3371, loss = 0.47259968\n",
      "Iteration 3372, loss = 0.47320892\n",
      "Iteration 3373, loss = 0.47342959\n",
      "Iteration 3374, loss = 0.47314935\n",
      "Iteration 3375, loss = 0.47264641\n",
      "Iteration 3376, loss = 0.47289766\n",
      "Iteration 3377, loss = 0.47299114\n",
      "Iteration 3378, loss = 0.47332833\n",
      "Iteration 3379, loss = 0.47316689\n",
      "Iteration 3380, loss = 0.47335294\n",
      "Iteration 3381, loss = 0.47295023\n",
      "Iteration 3382, loss = 0.47295968\n",
      "Iteration 3383, loss = 0.47271837\n",
      "Iteration 3384, loss = 0.47270410\n",
      "Iteration 3385, loss = 0.47281384\n",
      "Iteration 3386, loss = 0.47307366\n",
      "Iteration 3387, loss = 0.47356220\n",
      "Iteration 3388, loss = 0.47417814\n",
      "Iteration 3389, loss = 0.47319573\n",
      "Iteration 3390, loss = 0.47293631\n",
      "Iteration 3391, loss = 0.47257249\n",
      "Iteration 3392, loss = 0.47248184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3393, loss = 0.47409119\n",
      "Iteration 3394, loss = 0.47410365\n",
      "Iteration 3395, loss = 0.47346223\n",
      "Iteration 3396, loss = 0.47251282\n",
      "Iteration 3397, loss = 0.47275312\n",
      "Iteration 3398, loss = 0.47345228\n",
      "Iteration 3399, loss = 0.47363930\n",
      "Iteration 3400, loss = 0.47307830\n",
      "Iteration 3401, loss = 0.47296325\n",
      "Iteration 3402, loss = 0.47295832\n",
      "Iteration 3403, loss = 0.47311423\n",
      "Iteration 3404, loss = 0.47298799\n",
      "Iteration 3405, loss = 0.47265846\n",
      "Iteration 3406, loss = 0.47266003\n",
      "Iteration 3407, loss = 0.47283175\n",
      "Iteration 3408, loss = 0.47318767\n",
      "Iteration 3409, loss = 0.47348649\n",
      "Iteration 3410, loss = 0.47306376\n",
      "Iteration 3411, loss = 0.47299651\n",
      "Iteration 3412, loss = 0.47282392\n",
      "Iteration 3413, loss = 0.47293985\n",
      "Iteration 3414, loss = 0.47266986\n",
      "Iteration 3415, loss = 0.47275778\n",
      "Iteration 3416, loss = 0.47272911\n",
      "Iteration 3417, loss = 0.47265346\n",
      "Iteration 3418, loss = 0.47266972\n",
      "Iteration 3419, loss = 0.47279666\n",
      "Iteration 3420, loss = 0.47295333\n",
      "Iteration 3421, loss = 0.47323263\n",
      "Iteration 3422, loss = 0.47297443\n",
      "Iteration 3423, loss = 0.47282117\n",
      "Iteration 3424, loss = 0.47265301\n",
      "Iteration 3425, loss = 0.47270467\n",
      "Iteration 3426, loss = 0.47288187\n",
      "Iteration 3427, loss = 0.47322040\n",
      "Iteration 3428, loss = 0.47279645\n",
      "Iteration 3429, loss = 0.47259957\n",
      "Iteration 3430, loss = 0.47290324\n",
      "Iteration 3431, loss = 0.47438477\n",
      "Iteration 3432, loss = 0.47374139\n",
      "Iteration 3433, loss = 0.47337170\n",
      "Iteration 3434, loss = 0.47257407\n",
      "Iteration 3435, loss = 0.47271623\n",
      "Iteration 3436, loss = 0.47309455\n",
      "Iteration 3437, loss = 0.47344294\n",
      "Iteration 3438, loss = 0.47308875\n",
      "Iteration 3439, loss = 0.47300852\n",
      "Iteration 3440, loss = 0.47280145\n",
      "Iteration 3441, loss = 0.47269769\n",
      "Iteration 3442, loss = 0.47264876\n",
      "Iteration 3443, loss = 0.47265111\n",
      "Iteration 3444, loss = 0.47265452\n",
      "Iteration 3445, loss = 0.47275225\n",
      "Iteration 3446, loss = 0.47262618\n",
      "Iteration 3447, loss = 0.47274331\n",
      "Iteration 3448, loss = 0.47271928\n",
      "Iteration 3449, loss = 0.47274358\n",
      "Iteration 3450, loss = 0.47264690\n",
      "Iteration 3451, loss = 0.47262763\n",
      "Iteration 3452, loss = 0.47271180\n",
      "Iteration 3453, loss = 0.47263772\n",
      "Iteration 3454, loss = 0.47264687\n",
      "Iteration 3455, loss = 0.47268062\n",
      "Iteration 3456, loss = 0.47276700\n",
      "Iteration 3457, loss = 0.47259749\n",
      "Iteration 3458, loss = 0.47260120\n",
      "Iteration 3459, loss = 0.47260009\n",
      "Iteration 3460, loss = 0.47266023\n",
      "Iteration 3461, loss = 0.47266068\n",
      "Iteration 3462, loss = 0.47258984\n",
      "Iteration 3463, loss = 0.47259713\n",
      "Iteration 3464, loss = 0.47257618\n",
      "Iteration 3465, loss = 0.47283605\n",
      "Iteration 3466, loss = 0.47293689\n",
      "Iteration 3467, loss = 0.47268010\n",
      "Iteration 3468, loss = 0.47275477\n",
      "Iteration 3469, loss = 0.47266023\n",
      "Iteration 3470, loss = 0.47295135\n",
      "Iteration 3471, loss = 0.47287050\n",
      "Iteration 3472, loss = 0.47264810\n",
      "Iteration 3473, loss = 0.47254457\n",
      "Iteration 3474, loss = 0.47264279\n",
      "Iteration 3475, loss = 0.47287513\n",
      "Iteration 3476, loss = 0.47304602\n",
      "Iteration 3477, loss = 0.47318199\n",
      "Iteration 3478, loss = 0.47296127\n",
      "Iteration 3479, loss = 0.47308278\n",
      "Iteration 3480, loss = 0.47289898\n",
      "Iteration 3481, loss = 0.47295617\n",
      "Iteration 3482, loss = 0.47285099\n",
      "Iteration 3483, loss = 0.47268134\n",
      "Iteration 3484, loss = 0.47262388\n",
      "Iteration 3485, loss = 0.47262069\n",
      "Iteration 3486, loss = 0.47283511\n",
      "Iteration 3487, loss = 0.47253468\n",
      "Iteration 3488, loss = 0.47275907\n",
      "Iteration 3489, loss = 0.47295749\n",
      "Iteration 3490, loss = 0.47312737\n",
      "Iteration 3491, loss = 0.47291684\n",
      "Iteration 3492, loss = 0.47286128\n",
      "Iteration 3493, loss = 0.47288040\n",
      "Iteration 3494, loss = 0.47273178\n",
      "Iteration 3495, loss = 0.47264553\n",
      "Iteration 3496, loss = 0.47265973\n",
      "Iteration 3497, loss = 0.47275268\n",
      "Iteration 3498, loss = 0.47269379\n",
      "Iteration 3499, loss = 0.47267994\n",
      "Iteration 3500, loss = 0.47268230\n",
      "Iteration 3501, loss = 0.47292674\n",
      "Iteration 3502, loss = 0.47289400\n",
      "Iteration 3503, loss = 0.47267801\n",
      "Iteration 3504, loss = 0.47265163\n",
      "Iteration 3505, loss = 0.47263056\n",
      "Iteration 3506, loss = 0.47275302\n",
      "Iteration 3507, loss = 0.47287927\n",
      "Iteration 3508, loss = 0.47281777\n",
      "Iteration 3509, loss = 0.47275541\n",
      "Iteration 3510, loss = 0.47267141\n",
      "Iteration 3511, loss = 0.47263399\n",
      "Iteration 3512, loss = 0.47260146\n",
      "Iteration 3513, loss = 0.47269121\n",
      "Iteration 3514, loss = 0.47289994\n",
      "Iteration 3515, loss = 0.47289578\n",
      "Iteration 3516, loss = 0.47269157\n",
      "Iteration 3517, loss = 0.47251424\n",
      "Iteration 3518, loss = 0.47269542\n",
      "Iteration 3519, loss = 0.47284738\n",
      "Iteration 3520, loss = 0.47276476\n",
      "Iteration 3521, loss = 0.47281688\n",
      "Iteration 3522, loss = 0.47283639\n",
      "Iteration 3523, loss = 0.47289190\n",
      "Iteration 3524, loss = 0.47266308\n",
      "Iteration 3525, loss = 0.47259498\n",
      "Iteration 3526, loss = 0.47261179\n",
      "Iteration 3527, loss = 0.47271680\n",
      "Iteration 3528, loss = 0.47267379\n",
      "Iteration 3529, loss = 0.47266217\n",
      "Iteration 3530, loss = 0.47255916\n",
      "Iteration 3531, loss = 0.47287099\n",
      "Iteration 3532, loss = 0.47320344\n",
      "Iteration 3533, loss = 0.47319836\n",
      "Iteration 3534, loss = 0.47288175\n",
      "Iteration 3535, loss = 0.47260369\n",
      "Iteration 3536, loss = 0.47336541\n",
      "Iteration 3537, loss = 0.47263567\n",
      "Iteration 3538, loss = 0.47257010\n",
      "Iteration 3539, loss = 0.47272075\n",
      "Iteration 3540, loss = 0.47260494\n",
      "Iteration 3541, loss = 0.47257545\n",
      "Iteration 3542, loss = 0.47253838\n",
      "Iteration 3543, loss = 0.47261099\n",
      "Iteration 3544, loss = 0.47297339\n",
      "Iteration 3545, loss = 0.47264391\n",
      "Iteration 3546, loss = 0.47257463\n",
      "Iteration 3547, loss = 0.47309591\n",
      "Iteration 3548, loss = 0.47301094\n",
      "Iteration 3549, loss = 0.47326584\n",
      "Iteration 3550, loss = 0.47274775\n",
      "Iteration 3551, loss = 0.47261813\n",
      "Iteration 3552, loss = 0.47257242\n",
      "Iteration 3553, loss = 0.47252979\n",
      "Iteration 3554, loss = 0.47254919\n",
      "Iteration 3555, loss = 0.47299476\n",
      "Iteration 3556, loss = 0.47272333\n",
      "Iteration 3557, loss = 0.47278856\n",
      "Iteration 3558, loss = 0.47245490\n",
      "Iteration 3559, loss = 0.47320362\n",
      "Iteration 3560, loss = 0.47304923\n",
      "Iteration 3561, loss = 0.47278536\n",
      "Iteration 3562, loss = 0.47252748\n",
      "Iteration 3563, loss = 0.47285566\n",
      "Iteration 3564, loss = 0.47271936\n",
      "Iteration 3565, loss = 0.47283651\n",
      "Iteration 3566, loss = 0.47248065\n",
      "Iteration 3567, loss = 0.47247448\n",
      "Iteration 3568, loss = 0.47267279\n",
      "Iteration 3569, loss = 0.47261418\n",
      "Iteration 3570, loss = 0.47255023\n",
      "Iteration 3571, loss = 0.47255829\n",
      "Iteration 3572, loss = 0.47255940\n",
      "Iteration 3573, loss = 0.47272677\n",
      "Iteration 3574, loss = 0.47261402\n",
      "Iteration 3575, loss = 0.47236154\n",
      "Iteration 3576, loss = 0.47329297\n",
      "Iteration 3577, loss = 0.47339984\n",
      "Iteration 3578, loss = 0.47313704\n",
      "Iteration 3579, loss = 0.47302527\n",
      "Iteration 3580, loss = 0.47326788\n",
      "Iteration 3581, loss = 0.47270712\n",
      "Iteration 3582, loss = 0.47269308\n",
      "Iteration 3583, loss = 0.47322125\n",
      "Iteration 3584, loss = 0.47315410\n",
      "Iteration 3585, loss = 0.47260091\n",
      "Iteration 3586, loss = 0.47254253\n",
      "Iteration 3587, loss = 0.47255024\n",
      "Iteration 3588, loss = 0.47269019\n",
      "Iteration 3589, loss = 0.47293136\n",
      "Iteration 3590, loss = 0.47276741\n",
      "Iteration 3591, loss = 0.47329597\n",
      "Iteration 3592, loss = 0.47263642\n",
      "Iteration 3593, loss = 0.47259698\n",
      "Iteration 3594, loss = 0.47257644\n",
      "Iteration 3595, loss = 0.47237882\n",
      "Iteration 3596, loss = 0.47324051\n",
      "Iteration 3597, loss = 0.47276842\n",
      "Iteration 3598, loss = 0.47264480\n",
      "Iteration 3599, loss = 0.47314427\n",
      "Iteration 3600, loss = 0.47238980\n",
      "Iteration 3601, loss = 0.47272455\n",
      "Iteration 3602, loss = 0.47284992\n",
      "Iteration 3603, loss = 0.47277204\n",
      "Iteration 3604, loss = 0.47288448\n",
      "Iteration 3605, loss = 0.47243934\n",
      "Iteration 3606, loss = 0.47260043\n",
      "Iteration 3607, loss = 0.47288728\n",
      "Iteration 3608, loss = 0.47284866\n",
      "Iteration 3609, loss = 0.47271746\n",
      "Iteration 3610, loss = 0.47257348\n",
      "Iteration 3611, loss = 0.47243152\n",
      "Iteration 3612, loss = 0.47246252\n",
      "Iteration 3613, loss = 0.47273644\n",
      "Iteration 3614, loss = 0.47331035\n",
      "Iteration 3615, loss = 0.47275317\n",
      "Iteration 3616, loss = 0.47325956\n",
      "Iteration 3617, loss = 0.47274462\n",
      "Iteration 3618, loss = 0.47288652\n",
      "Iteration 3619, loss = 0.47272077\n",
      "Iteration 3620, loss = 0.47288890\n",
      "Iteration 3621, loss = 0.47310602\n",
      "Iteration 3622, loss = 0.47258702\n",
      "Iteration 3623, loss = 0.47373749\n",
      "Iteration 3624, loss = 0.47340313\n",
      "Iteration 3625, loss = 0.47307185\n",
      "Iteration 3626, loss = 0.47285269\n",
      "Iteration 3627, loss = 0.47292148\n",
      "Iteration 3628, loss = 0.47289709\n",
      "Iteration 3629, loss = 0.47263202\n",
      "Iteration 3630, loss = 0.47251890\n",
      "Iteration 3631, loss = 0.47242835\n",
      "Iteration 3632, loss = 0.47248227\n",
      "Iteration 3633, loss = 0.47266790\n",
      "Iteration 3634, loss = 0.47308105\n",
      "Iteration 3635, loss = 0.47306381\n",
      "Iteration 3636, loss = 0.47272255\n",
      "Iteration 3637, loss = 0.47254147\n",
      "Iteration 3638, loss = 0.47254544\n",
      "Iteration 3639, loss = 0.47251041\n",
      "Iteration 3640, loss = 0.47243081\n",
      "Iteration 3641, loss = 0.47283781\n",
      "Iteration 3642, loss = 0.47256519\n",
      "Iteration 3643, loss = 0.47243741\n",
      "Iteration 3644, loss = 0.47225856\n",
      "Iteration 3645, loss = 0.47272160\n",
      "Iteration 3646, loss = 0.47308186\n",
      "Iteration 3647, loss = 0.47324427\n",
      "Iteration 3648, loss = 0.47270232\n",
      "Iteration 3649, loss = 0.47242895\n",
      "Iteration 3650, loss = 0.47282986\n",
      "Iteration 3651, loss = 0.47281891\n",
      "Iteration 3652, loss = 0.47252546\n",
      "Iteration 3653, loss = 0.47239798\n",
      "Iteration 3654, loss = 0.47253458\n",
      "Iteration 3655, loss = 0.47271918\n",
      "Iteration 3656, loss = 0.47332741\n",
      "Iteration 3657, loss = 0.47251699\n",
      "Iteration 3658, loss = 0.47264404\n",
      "Iteration 3659, loss = 0.47277712\n",
      "Iteration 3660, loss = 0.47248473\n",
      "Iteration 3661, loss = 0.47260755\n",
      "Iteration 3662, loss = 0.47253248\n",
      "Iteration 3663, loss = 0.47251266\n",
      "Iteration 3664, loss = 0.47255737\n",
      "Iteration 3665, loss = 0.47270158\n",
      "Iteration 3666, loss = 0.47259872\n",
      "Iteration 3667, loss = 0.47247746\n",
      "Iteration 3668, loss = 0.47263576\n",
      "Iteration 3669, loss = 0.47262866\n",
      "Iteration 3670, loss = 0.47267596\n",
      "Iteration 3671, loss = 0.47242444\n",
      "Iteration 3672, loss = 0.47250641\n",
      "Iteration 3673, loss = 0.47272682\n",
      "Iteration 3674, loss = 0.47257449\n",
      "Iteration 3675, loss = 0.47247820\n",
      "Iteration 3676, loss = 0.47250746\n",
      "Iteration 3677, loss = 0.47238513\n",
      "Iteration 3678, loss = 0.47237529\n",
      "Iteration 3679, loss = 0.47240793\n",
      "Iteration 3680, loss = 0.47245192\n",
      "Iteration 3681, loss = 0.47260321\n",
      "Iteration 3682, loss = 0.47254762\n",
      "Iteration 3683, loss = 0.47257712\n",
      "Iteration 3684, loss = 0.47240178\n",
      "Iteration 3685, loss = 0.47277870\n",
      "Iteration 3686, loss = 0.47259008\n",
      "Iteration 3687, loss = 0.47246903\n",
      "Iteration 3688, loss = 0.47240333\n",
      "Iteration 3689, loss = 0.47249515\n",
      "Iteration 3690, loss = 0.47264185\n",
      "Iteration 3691, loss = 0.47272022\n",
      "Iteration 3692, loss = 0.47282625\n",
      "Iteration 3693, loss = 0.47251077\n",
      "Iteration 3694, loss = 0.47236998\n",
      "Iteration 3695, loss = 0.47232233\n",
      "Iteration 3696, loss = 0.47279535\n",
      "Iteration 3697, loss = 0.47291215\n",
      "Iteration 3698, loss = 0.47261666\n",
      "Iteration 3699, loss = 0.47291793\n",
      "Iteration 3700, loss = 0.47261925\n",
      "Iteration 3701, loss = 0.47276336\n",
      "Iteration 3702, loss = 0.47263838\n",
      "Iteration 3703, loss = 0.47250891\n",
      "Iteration 3704, loss = 0.47266673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3705, loss = 0.47284850\n",
      "Iteration 3706, loss = 0.47336875\n",
      "Iteration 3707, loss = 0.47349565\n",
      "Iteration 3708, loss = 0.47304671\n",
      "Iteration 3709, loss = 0.47254520\n",
      "Iteration 3710, loss = 0.47265421\n",
      "Iteration 3711, loss = 0.47313173\n",
      "Iteration 3712, loss = 0.47290200\n",
      "Iteration 3713, loss = 0.47299235\n",
      "Iteration 3714, loss = 0.47259169\n",
      "Iteration 3715, loss = 0.47276156\n",
      "Iteration 3716, loss = 0.47254040\n",
      "Iteration 3717, loss = 0.47256182\n",
      "Iteration 3718, loss = 0.47251572\n",
      "Iteration 3719, loss = 0.47250278\n",
      "Iteration 3720, loss = 0.47248964\n",
      "Iteration 3721, loss = 0.47284835\n",
      "Iteration 3722, loss = 0.47239691\n",
      "Iteration 3723, loss = 0.47261101\n",
      "Iteration 3724, loss = 0.47276686\n",
      "Iteration 3725, loss = 0.47287861\n",
      "Iteration 3726, loss = 0.47319521\n",
      "Iteration 3727, loss = 0.47310995\n",
      "Iteration 3728, loss = 0.47309301\n",
      "Iteration 3729, loss = 0.47260608\n",
      "Iteration 3730, loss = 0.47263682\n",
      "Iteration 3731, loss = 0.47323334\n",
      "Iteration 3732, loss = 0.47276056\n",
      "Iteration 3733, loss = 0.47245359\n",
      "Iteration 3734, loss = 0.47251843\n",
      "Iteration 3735, loss = 0.47257484\n",
      "Iteration 3736, loss = 0.47293461\n",
      "Iteration 3737, loss = 0.47262681\n",
      "Iteration 3738, loss = 0.47276676\n",
      "Iteration 3739, loss = 0.47363513\n",
      "Iteration 3740, loss = 0.47343420\n",
      "Iteration 3741, loss = 0.47260453\n",
      "Iteration 3742, loss = 0.47237727\n",
      "Iteration 3743, loss = 0.47302846\n",
      "Iteration 3744, loss = 0.47326962\n",
      "Iteration 3745, loss = 0.47301787\n",
      "Iteration 3746, loss = 0.47262462\n",
      "Iteration 3747, loss = 0.47252730\n",
      "Iteration 3748, loss = 0.47263156\n",
      "Iteration 3749, loss = 0.47273840\n",
      "Iteration 3750, loss = 0.47258704\n",
      "Iteration 3751, loss = 0.47247759\n",
      "Iteration 3752, loss = 0.47230771\n",
      "Iteration 3753, loss = 0.47242896\n",
      "Iteration 3754, loss = 0.47286097\n",
      "Iteration 3755, loss = 0.47332204\n",
      "Iteration 3756, loss = 0.47275433\n",
      "Iteration 3757, loss = 0.47299136\n",
      "Iteration 3758, loss = 0.47284856\n",
      "Iteration 3759, loss = 0.47283170\n",
      "Iteration 3760, loss = 0.47231872\n",
      "Iteration 3761, loss = 0.47216713\n",
      "Iteration 3762, loss = 0.47268515\n",
      "Iteration 3763, loss = 0.47361040\n",
      "Iteration 3764, loss = 0.47394503\n",
      "Iteration 3765, loss = 0.47375826\n",
      "Iteration 3766, loss = 0.47269402\n",
      "Iteration 3767, loss = 0.47278999\n",
      "Iteration 3768, loss = 0.47252224\n",
      "Iteration 3769, loss = 0.47340986\n",
      "Iteration 3770, loss = 0.47262811\n",
      "Iteration 3771, loss = 0.47249749\n",
      "Iteration 3772, loss = 0.47236157\n",
      "Iteration 3773, loss = 0.47276217\n",
      "Iteration 3774, loss = 0.47309588\n",
      "Iteration 3775, loss = 0.47304914\n",
      "Iteration 3776, loss = 0.47282185\n",
      "Iteration 3777, loss = 0.47236289\n",
      "Iteration 3778, loss = 0.47229146\n",
      "Iteration 3779, loss = 0.47246665\n",
      "Iteration 3780, loss = 0.47267427\n",
      "Iteration 3781, loss = 0.47280804\n",
      "Iteration 3782, loss = 0.47269079\n",
      "Iteration 3783, loss = 0.47239329\n",
      "Iteration 3784, loss = 0.47248069\n",
      "Iteration 3785, loss = 0.47262870\n",
      "Iteration 3786, loss = 0.47276919\n",
      "Iteration 3787, loss = 0.47252555\n",
      "Iteration 3788, loss = 0.47287896\n",
      "Iteration 3789, loss = 0.47248388\n",
      "Iteration 3790, loss = 0.47252750\n",
      "Iteration 3791, loss = 0.47277605\n",
      "Iteration 3792, loss = 0.47257473\n",
      "Iteration 3793, loss = 0.47224636\n",
      "Iteration 3794, loss = 0.47236865\n",
      "Iteration 3795, loss = 0.47275438\n",
      "Iteration 3796, loss = 0.47293584\n",
      "Iteration 3797, loss = 0.47307517\n",
      "Iteration 3798, loss = 0.47243988\n",
      "Iteration 3799, loss = 0.47230437\n",
      "Iteration 3800, loss = 0.47222636\n",
      "Iteration 3801, loss = 0.47270145\n",
      "Iteration 3802, loss = 0.47260512\n",
      "Iteration 3803, loss = 0.47257568\n",
      "Iteration 3804, loss = 0.47255137\n",
      "Iteration 3805, loss = 0.47250640\n",
      "Iteration 3806, loss = 0.47253951\n",
      "Iteration 3807, loss = 0.47253913\n",
      "Iteration 3808, loss = 0.47239645\n",
      "Iteration 3809, loss = 0.47240136\n",
      "Iteration 3810, loss = 0.47258915\n",
      "Iteration 3811, loss = 0.47253607\n",
      "Iteration 3812, loss = 0.47281418\n",
      "Iteration 3813, loss = 0.47234032\n",
      "Iteration 3814, loss = 0.47231070\n",
      "Iteration 3815, loss = 0.47235059\n",
      "Iteration 3816, loss = 0.47253123\n",
      "Iteration 3817, loss = 0.47277462\n",
      "Iteration 3818, loss = 0.47274105\n",
      "Iteration 3819, loss = 0.47230970\n",
      "Iteration 3820, loss = 0.47225279\n",
      "Iteration 3821, loss = 0.47249650\n",
      "Iteration 3822, loss = 0.47387816\n",
      "Iteration 3823, loss = 0.47329462\n",
      "Iteration 3824, loss = 0.47235038\n",
      "Iteration 3825, loss = 0.47251345\n",
      "Iteration 3826, loss = 0.47283625\n",
      "Iteration 3827, loss = 0.47267109\n",
      "Iteration 3828, loss = 0.47246853\n",
      "Iteration 3829, loss = 0.47240322\n",
      "Iteration 3830, loss = 0.47239056\n",
      "Iteration 3831, loss = 0.47257896\n",
      "Iteration 3832, loss = 0.47253744\n",
      "Iteration 3833, loss = 0.47246045\n",
      "Iteration 3834, loss = 0.47231319\n",
      "Iteration 3835, loss = 0.47240824\n",
      "Iteration 3836, loss = 0.47259262\n",
      "Iteration 3837, loss = 0.47246820\n",
      "Iteration 3838, loss = 0.47292875\n",
      "Iteration 3839, loss = 0.47231856\n",
      "Iteration 3840, loss = 0.47230694\n",
      "Iteration 3841, loss = 0.47250504\n",
      "Iteration 3842, loss = 0.47238156\n",
      "Iteration 3843, loss = 0.47227814\n",
      "Iteration 3844, loss = 0.47281394\n",
      "Iteration 3845, loss = 0.47266260\n",
      "Iteration 3846, loss = 0.47242506\n",
      "Iteration 3847, loss = 0.47230212\n",
      "Iteration 3848, loss = 0.47268357\n",
      "Iteration 3849, loss = 0.47241147\n",
      "Iteration 3850, loss = 0.47219587\n",
      "Iteration 3851, loss = 0.47278299\n",
      "Iteration 3852, loss = 0.47278896\n",
      "Iteration 3853, loss = 0.47268115\n",
      "Iteration 3854, loss = 0.47227994\n",
      "Iteration 3855, loss = 0.47225395\n",
      "Iteration 3856, loss = 0.47310205\n",
      "Iteration 3857, loss = 0.47299571\n",
      "Iteration 3858, loss = 0.47277347\n",
      "Iteration 3859, loss = 0.47240981\n",
      "Iteration 3860, loss = 0.47242037\n",
      "Iteration 3861, loss = 0.47274898\n",
      "Iteration 3862, loss = 0.47257611\n",
      "Iteration 3863, loss = 0.47233219\n",
      "Iteration 3864, loss = 0.47232212\n",
      "Iteration 3865, loss = 0.47222341\n",
      "Iteration 3866, loss = 0.47245886\n",
      "Iteration 3867, loss = 0.47273009\n",
      "Iteration 3868, loss = 0.47308996\n",
      "Iteration 3869, loss = 0.47305318\n",
      "Iteration 3870, loss = 0.47318689\n",
      "Iteration 3871, loss = 0.47281976\n",
      "Iteration 3872, loss = 0.47244767\n",
      "Iteration 3873, loss = 0.47269772\n",
      "Iteration 3874, loss = 0.47286569\n",
      "Iteration 3875, loss = 0.47305053\n",
      "Iteration 3876, loss = 0.47280321\n",
      "Iteration 3877, loss = 0.47261180\n",
      "Iteration 3878, loss = 0.47278235\n",
      "Iteration 3879, loss = 0.47280286\n",
      "Iteration 3880, loss = 0.47241672\n",
      "Iteration 3881, loss = 0.47253461\n",
      "Iteration 3882, loss = 0.47255216\n",
      "Iteration 3883, loss = 0.47272076\n",
      "Iteration 3884, loss = 0.47260833\n",
      "Iteration 3885, loss = 0.47244129\n",
      "Iteration 3886, loss = 0.47277801\n",
      "Iteration 3887, loss = 0.47230270\n",
      "Iteration 3888, loss = 0.47201948\n",
      "Iteration 3889, loss = 0.47300603\n",
      "Iteration 3890, loss = 0.47299774\n",
      "Iteration 3891, loss = 0.47273668\n",
      "Iteration 3892, loss = 0.47209367\n",
      "Iteration 3893, loss = 0.47238656\n",
      "Iteration 3894, loss = 0.47289130\n",
      "Iteration 3895, loss = 0.47349807\n",
      "Iteration 3896, loss = 0.47367210\n",
      "Iteration 3897, loss = 0.47296158\n",
      "Iteration 3898, loss = 0.47303558\n",
      "Iteration 3899, loss = 0.47229232\n",
      "Iteration 3900, loss = 0.47248447\n",
      "Iteration 3901, loss = 0.47287622\n",
      "Iteration 3902, loss = 0.47260091\n",
      "Iteration 3903, loss = 0.47247452\n",
      "Iteration 3904, loss = 0.47240007\n",
      "Iteration 3905, loss = 0.47238508\n",
      "Iteration 3906, loss = 0.47261494\n",
      "Iteration 3907, loss = 0.47237339\n",
      "Iteration 3908, loss = 0.47233184\n",
      "Iteration 3909, loss = 0.47316239\n",
      "Iteration 3910, loss = 0.47236545\n",
      "Iteration 3911, loss = 0.47226212\n",
      "Iteration 3912, loss = 0.47288624\n",
      "Iteration 3913, loss = 0.47268739\n",
      "Iteration 3914, loss = 0.47232632\n",
      "Iteration 3915, loss = 0.47241423\n",
      "Iteration 3916, loss = 0.47249774\n",
      "Iteration 3917, loss = 0.47240177\n",
      "Iteration 3918, loss = 0.47227122\n",
      "Iteration 3919, loss = 0.47222852\n",
      "Iteration 3920, loss = 0.47231709\n",
      "Iteration 3921, loss = 0.47291889\n",
      "Iteration 3922, loss = 0.47256841\n",
      "Iteration 3923, loss = 0.47241653\n",
      "Iteration 3924, loss = 0.47246526\n",
      "Iteration 3925, loss = 0.47233543\n",
      "Iteration 3926, loss = 0.47243536\n",
      "Iteration 3927, loss = 0.47243984\n",
      "Iteration 3928, loss = 0.47256119\n",
      "Iteration 3929, loss = 0.47229155\n",
      "Iteration 3930, loss = 0.47225452\n",
      "Iteration 3931, loss = 0.47224005\n",
      "Iteration 3932, loss = 0.47233478\n",
      "Iteration 3933, loss = 0.47242721\n",
      "Iteration 3934, loss = 0.47237156\n",
      "Iteration 3935, loss = 0.47234453\n",
      "Iteration 3936, loss = 0.47231002\n",
      "Iteration 3937, loss = 0.47225662\n",
      "Iteration 3938, loss = 0.47239166\n",
      "Iteration 3939, loss = 0.47218903\n",
      "Iteration 3940, loss = 0.47238995\n",
      "Iteration 3941, loss = 0.47259111\n",
      "Iteration 3942, loss = 0.47235179\n",
      "Iteration 3943, loss = 0.47227930\n",
      "Iteration 3944, loss = 0.47231574\n",
      "Iteration 3945, loss = 0.47237494\n",
      "Iteration 3946, loss = 0.47253233\n",
      "Iteration 3947, loss = 0.47248716\n",
      "Iteration 3948, loss = 0.47215403\n",
      "Iteration 3949, loss = 0.47288183\n",
      "Iteration 3950, loss = 0.47239856\n",
      "Iteration 3951, loss = 0.47263236\n",
      "Iteration 3952, loss = 0.47253902\n",
      "Iteration 3953, loss = 0.47233569\n",
      "Iteration 3954, loss = 0.47218428\n",
      "Iteration 3955, loss = 0.47225519\n",
      "Iteration 3956, loss = 0.47235350\n",
      "Iteration 3957, loss = 0.47228103\n",
      "Iteration 3958, loss = 0.47221426\n",
      "Iteration 3959, loss = 0.47221390\n",
      "Iteration 3960, loss = 0.47215861\n",
      "Iteration 3961, loss = 0.47232927\n",
      "Iteration 3962, loss = 0.47256647\n",
      "Iteration 3963, loss = 0.47296678\n",
      "Iteration 3964, loss = 0.47308705\n",
      "Iteration 3965, loss = 0.47257369\n",
      "Iteration 3966, loss = 0.47237944\n",
      "Iteration 3967, loss = 0.47244136\n",
      "Iteration 3968, loss = 0.47246097\n",
      "Iteration 3969, loss = 0.47245186\n",
      "Iteration 3970, loss = 0.47281946\n",
      "Iteration 3971, loss = 0.47271231\n",
      "Iteration 3972, loss = 0.47259966\n",
      "Iteration 3973, loss = 0.47246654\n",
      "Iteration 3974, loss = 0.47246093\n",
      "Iteration 3975, loss = 0.47245103\n",
      "Iteration 3976, loss = 0.47235094\n",
      "Iteration 3977, loss = 0.47227889\n",
      "Iteration 3978, loss = 0.47248865\n",
      "Iteration 3979, loss = 0.47256347\n",
      "Iteration 3980, loss = 0.47248421\n",
      "Iteration 3981, loss = 0.47223838\n",
      "Iteration 3982, loss = 0.47227396\n",
      "Iteration 3983, loss = 0.47241631\n",
      "Iteration 3984, loss = 0.47239719\n",
      "Iteration 3985, loss = 0.47273349\n",
      "Iteration 3986, loss = 0.47281831\n",
      "Iteration 3987, loss = 0.47224392\n",
      "Iteration 3988, loss = 0.47322092\n",
      "Iteration 3989, loss = 0.47297045\n",
      "Iteration 3990, loss = 0.47252341\n",
      "Iteration 3991, loss = 0.47245117\n",
      "Iteration 3992, loss = 0.47228753\n",
      "Iteration 3993, loss = 0.47251027\n",
      "Iteration 3994, loss = 0.47252727\n",
      "Iteration 3995, loss = 0.47219831\n",
      "Iteration 3996, loss = 0.47214261\n",
      "Iteration 3997, loss = 0.47229141\n",
      "Iteration 3998, loss = 0.47285679\n",
      "Iteration 3999, loss = 0.47278462\n",
      "Iteration 4000, loss = 0.47224496\n",
      "Iteration 4001, loss = 0.47210026\n",
      "Iteration 4002, loss = 0.47217117\n",
      "Iteration 4003, loss = 0.47243097\n",
      "Iteration 4004, loss = 0.47254137\n",
      "Iteration 4005, loss = 0.47236739\n",
      "Iteration 4006, loss = 0.47229735\n",
      "Iteration 4007, loss = 0.47224601\n",
      "Iteration 4008, loss = 0.47214105\n",
      "Iteration 4009, loss = 0.47245127\n",
      "Iteration 4010, loss = 0.47218451\n",
      "Iteration 4011, loss = 0.47220611\n",
      "Iteration 4012, loss = 0.47218730\n",
      "Iteration 4013, loss = 0.47219804\n",
      "Iteration 4014, loss = 0.47211793\n",
      "Iteration 4015, loss = 0.47226592\n",
      "Iteration 4016, loss = 0.47262278\n",
      "Iteration 4017, loss = 0.47306648\n",
      "Iteration 4018, loss = 0.47236810\n",
      "Iteration 4019, loss = 0.47255640\n",
      "Iteration 4020, loss = 0.47263299\n",
      "Iteration 4021, loss = 0.47220611\n",
      "Iteration 4022, loss = 0.47262058\n",
      "Iteration 4023, loss = 0.47292648\n",
      "Iteration 4024, loss = 0.47267639\n",
      "Iteration 4025, loss = 0.47248469\n",
      "Iteration 4026, loss = 0.47227254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4027, loss = 0.47210483\n",
      "Iteration 4028, loss = 0.47220450\n",
      "Iteration 4029, loss = 0.47234686\n",
      "Iteration 4030, loss = 0.47301921\n",
      "Iteration 4031, loss = 0.47218344\n",
      "Iteration 4032, loss = 0.47237801\n",
      "Iteration 4033, loss = 0.47245516\n",
      "Iteration 4034, loss = 0.47219772\n",
      "Iteration 4035, loss = 0.47228268\n",
      "Iteration 4036, loss = 0.47218145\n",
      "Iteration 4037, loss = 0.47229740\n",
      "Iteration 4038, loss = 0.47243014\n",
      "Iteration 4039, loss = 0.47237263\n",
      "Iteration 4040, loss = 0.47236278\n",
      "Iteration 4041, loss = 0.47221783\n",
      "Iteration 4042, loss = 0.47211763\n",
      "Iteration 4043, loss = 0.47217869\n",
      "Iteration 4044, loss = 0.47240487\n",
      "Iteration 4045, loss = 0.47266055\n",
      "Iteration 4046, loss = 0.47267472\n",
      "Iteration 4047, loss = 0.47238638\n",
      "Iteration 4048, loss = 0.47212535\n",
      "Iteration 4049, loss = 0.47238790\n",
      "Iteration 4050, loss = 0.47268837\n",
      "Iteration 4051, loss = 0.47297896\n",
      "Iteration 4052, loss = 0.47265289\n",
      "Iteration 4053, loss = 0.47247696\n",
      "Iteration 4054, loss = 0.47212862\n",
      "Iteration 4055, loss = 0.47224040\n",
      "Iteration 4056, loss = 0.47236983\n",
      "Iteration 4057, loss = 0.47218410\n",
      "Iteration 4058, loss = 0.47183676\n",
      "Iteration 4059, loss = 0.47303462\n",
      "Iteration 4060, loss = 0.47320279\n",
      "Iteration 4061, loss = 0.47284741\n",
      "Iteration 4062, loss = 0.47250400\n",
      "Iteration 4063, loss = 0.47233900\n",
      "Iteration 4064, loss = 0.47221376\n",
      "Iteration 4065, loss = 0.47224216\n",
      "Iteration 4066, loss = 0.47238917\n",
      "Iteration 4067, loss = 0.47275304\n",
      "Iteration 4068, loss = 0.47231455\n",
      "Iteration 4069, loss = 0.47235620\n",
      "Iteration 4070, loss = 0.47220040\n",
      "Iteration 4071, loss = 0.47232781\n",
      "Iteration 4072, loss = 0.47343135\n",
      "Iteration 4073, loss = 0.47285019\n",
      "Iteration 4074, loss = 0.47271904\n",
      "Iteration 4075, loss = 0.47261678\n",
      "Iteration 4076, loss = 0.47219251\n",
      "Iteration 4077, loss = 0.47213571\n",
      "Iteration 4078, loss = 0.47223207\n",
      "Iteration 4079, loss = 0.47237566\n",
      "Iteration 4080, loss = 0.47244847\n",
      "Iteration 4081, loss = 0.47231472\n",
      "Iteration 4082, loss = 0.47290638\n",
      "Iteration 4083, loss = 0.47321875\n",
      "Iteration 4084, loss = 0.47239496\n",
      "Iteration 4085, loss = 0.47231936\n",
      "Iteration 4086, loss = 0.47310305\n",
      "Iteration 4087, loss = 0.47233260\n",
      "Iteration 4088, loss = 0.47254149\n",
      "Iteration 4089, loss = 0.47216915\n",
      "Iteration 4090, loss = 0.47236980\n",
      "Iteration 4091, loss = 0.47222683\n",
      "Iteration 4092, loss = 0.47268300\n",
      "Iteration 4093, loss = 0.47251559\n",
      "Iteration 4094, loss = 0.47213990\n",
      "Iteration 4095, loss = 0.47208980\n",
      "Iteration 4096, loss = 0.47253032\n",
      "Iteration 4097, loss = 0.47255545\n",
      "Iteration 4098, loss = 0.47248434\n",
      "Iteration 4099, loss = 0.47273430\n",
      "Iteration 4100, loss = 0.47253086\n",
      "Iteration 4101, loss = 0.47228483\n",
      "Iteration 4102, loss = 0.47209562\n",
      "Iteration 4103, loss = 0.47222956\n",
      "Iteration 4104, loss = 0.47230164\n",
      "Iteration 4105, loss = 0.47253691\n",
      "Iteration 4106, loss = 0.47271860\n",
      "Iteration 4107, loss = 0.47258444\n",
      "Iteration 4108, loss = 0.47273143\n",
      "Iteration 4109, loss = 0.47234538\n",
      "Iteration 4110, loss = 0.47225835\n",
      "Iteration 4111, loss = 0.47241848\n",
      "Iteration 4112, loss = 0.47224939\n",
      "Iteration 4113, loss = 0.47230044\n",
      "Iteration 4114, loss = 0.47212179\n",
      "Iteration 4115, loss = 0.47265110\n",
      "Iteration 4116, loss = 0.47259853\n",
      "Iteration 4117, loss = 0.47232476\n",
      "Iteration 4118, loss = 0.47279123\n",
      "Iteration 4119, loss = 0.47223413\n",
      "Iteration 4120, loss = 0.47221960\n",
      "Iteration 4121, loss = 0.47230293\n",
      "Iteration 4122, loss = 0.47235817\n",
      "Iteration 4123, loss = 0.47239256\n",
      "Iteration 4124, loss = 0.47213134\n",
      "Iteration 4125, loss = 0.47163294\n",
      "Iteration 4126, loss = 0.47276134\n",
      "Iteration 4127, loss = 0.47378226\n",
      "Iteration 4128, loss = 0.47390991\n",
      "Iteration 4129, loss = 0.47326332\n",
      "Iteration 4130, loss = 0.47247649\n",
      "Iteration 4131, loss = 0.47299051\n",
      "Iteration 4132, loss = 0.47241500\n",
      "Iteration 4133, loss = 0.47241461\n",
      "Iteration 4134, loss = 0.47206570\n",
      "Iteration 4135, loss = 0.47225308\n",
      "Iteration 4136, loss = 0.47249558\n",
      "Iteration 4137, loss = 0.47213652\n",
      "Iteration 4138, loss = 0.47211790\n",
      "Iteration 4139, loss = 0.47224678\n",
      "Iteration 4140, loss = 0.47229635\n",
      "Iteration 4141, loss = 0.47224230\n",
      "Iteration 4142, loss = 0.47228310\n",
      "Iteration 4143, loss = 0.47212486\n",
      "Iteration 4144, loss = 0.47230653\n",
      "Iteration 4145, loss = 0.47235045\n",
      "Iteration 4146, loss = 0.47281046\n",
      "Iteration 4147, loss = 0.47301027\n",
      "Iteration 4148, loss = 0.47250427\n",
      "Iteration 4149, loss = 0.47207591\n",
      "Iteration 4150, loss = 0.47217028\n",
      "Iteration 4151, loss = 0.47279547\n",
      "Iteration 4152, loss = 0.47270846\n",
      "Iteration 4153, loss = 0.47225543\n",
      "Iteration 4154, loss = 0.47219999\n",
      "Iteration 4155, loss = 0.47208782\n",
      "Iteration 4156, loss = 0.47218421\n",
      "Iteration 4157, loss = 0.47215090\n",
      "Iteration 4158, loss = 0.47213429\n",
      "Iteration 4159, loss = 0.47223880\n",
      "Iteration 4160, loss = 0.47221599\n",
      "Iteration 4161, loss = 0.47241685\n",
      "Iteration 4162, loss = 0.47216432\n",
      "Iteration 4163, loss = 0.47223245\n",
      "Iteration 4164, loss = 0.47220961\n",
      "Iteration 4165, loss = 0.47252802\n",
      "Iteration 4166, loss = 0.47242157\n",
      "Iteration 4167, loss = 0.47282084\n",
      "Iteration 4168, loss = 0.47316169\n",
      "Iteration 4169, loss = 0.47236422\n",
      "Iteration 4170, loss = 0.47224687\n",
      "Iteration 4171, loss = 0.47250027\n",
      "Iteration 4172, loss = 0.47228379\n",
      "Iteration 4173, loss = 0.47217314\n",
      "Iteration 4174, loss = 0.47206576\n",
      "Iteration 4175, loss = 0.47211802\n",
      "Iteration 4176, loss = 0.47231793\n",
      "Iteration 4177, loss = 0.47219067\n",
      "Iteration 4178, loss = 0.47229062\n",
      "Iteration 4179, loss = 0.47209657\n",
      "Iteration 4180, loss = 0.47214083\n",
      "Iteration 4181, loss = 0.47235002\n",
      "Iteration 4182, loss = 0.47210976\n",
      "Iteration 4183, loss = 0.47205247\n",
      "Iteration 4184, loss = 0.47230524\n",
      "Iteration 4185, loss = 0.47239091\n",
      "Iteration 4186, loss = 0.47229387\n",
      "Iteration 4187, loss = 0.47223759\n",
      "Iteration 4188, loss = 0.47238992\n",
      "Iteration 4189, loss = 0.47226925\n",
      "Iteration 4190, loss = 0.47231178\n",
      "Iteration 4191, loss = 0.47225429\n",
      "Iteration 4192, loss = 0.47223808\n",
      "Iteration 4193, loss = 0.47189728\n",
      "Iteration 4194, loss = 0.47254411\n",
      "Iteration 4195, loss = 0.47289681\n",
      "Iteration 4196, loss = 0.47283272\n",
      "Iteration 4197, loss = 0.47212868\n",
      "Iteration 4198, loss = 0.47212439\n",
      "Iteration 4199, loss = 0.47207394\n",
      "Iteration 4200, loss = 0.47214445\n",
      "Iteration 4201, loss = 0.47228702\n",
      "Iteration 4202, loss = 0.47222934\n",
      "Iteration 4203, loss = 0.47238199\n",
      "Iteration 4204, loss = 0.47248749\n",
      "Iteration 4205, loss = 0.47236569\n",
      "Iteration 4206, loss = 0.47239457\n",
      "Iteration 4207, loss = 0.47214235\n",
      "Iteration 4208, loss = 0.47208606\n",
      "Iteration 4209, loss = 0.47222594\n",
      "Iteration 4210, loss = 0.47213993\n",
      "Iteration 4211, loss = 0.47218928\n",
      "Iteration 4212, loss = 0.47229593\n",
      "Iteration 4213, loss = 0.47209257\n",
      "Iteration 4214, loss = 0.47198563\n",
      "Iteration 4215, loss = 0.47245157\n",
      "Iteration 4216, loss = 0.47225217\n",
      "Iteration 4217, loss = 0.47217180\n",
      "Iteration 4218, loss = 0.47211291\n",
      "Iteration 4219, loss = 0.47224259\n",
      "Iteration 4220, loss = 0.47235710\n",
      "Iteration 4221, loss = 0.47204474\n",
      "Iteration 4222, loss = 0.47276643\n",
      "Iteration 4223, loss = 0.47203629\n",
      "Iteration 4224, loss = 0.47198736\n",
      "Iteration 4225, loss = 0.47241237\n",
      "Iteration 4226, loss = 0.47242281\n",
      "Iteration 4227, loss = 0.47223577\n",
      "Iteration 4228, loss = 0.47247176\n",
      "Iteration 4229, loss = 0.47245841\n",
      "Iteration 4230, loss = 0.47199974\n",
      "Iteration 4231, loss = 0.47227519\n",
      "Iteration 4232, loss = 0.47276654\n",
      "Iteration 4233, loss = 0.47235420\n",
      "Iteration 4234, loss = 0.47235655\n",
      "Iteration 4235, loss = 0.47261889\n",
      "Iteration 4236, loss = 0.47197785\n",
      "Iteration 4237, loss = 0.47253662\n",
      "Iteration 4238, loss = 0.47240764\n",
      "Iteration 4239, loss = 0.47249652\n",
      "Iteration 4240, loss = 0.47220331\n",
      "Iteration 4241, loss = 0.47194758\n",
      "Iteration 4242, loss = 0.47194232\n",
      "Iteration 4243, loss = 0.47224035\n",
      "Iteration 4244, loss = 0.47258431\n",
      "Iteration 4245, loss = 0.47236755\n",
      "Iteration 4246, loss = 0.47226142\n",
      "Iteration 4247, loss = 0.47251659\n",
      "Iteration 4248, loss = 0.47223135\n",
      "Iteration 4249, loss = 0.47210160\n",
      "Iteration 4250, loss = 0.47187978\n",
      "Iteration 4251, loss = 0.47235166\n",
      "Iteration 4252, loss = 0.47252967\n",
      "Iteration 4253, loss = 0.47237973\n",
      "Iteration 4254, loss = 0.47202113\n",
      "Iteration 4255, loss = 0.47211468\n",
      "Iteration 4256, loss = 0.47221407\n",
      "Iteration 4257, loss = 0.47242267\n",
      "Iteration 4258, loss = 0.47219760\n",
      "Iteration 4259, loss = 0.47207058\n",
      "Iteration 4260, loss = 0.47252324\n",
      "Iteration 4261, loss = 0.47225714\n",
      "Iteration 4262, loss = 0.47214776\n",
      "Iteration 4263, loss = 0.47213026\n",
      "Iteration 4264, loss = 0.47230625\n",
      "Iteration 4265, loss = 0.47222359\n",
      "Iteration 4266, loss = 0.47201495\n",
      "Iteration 4267, loss = 0.47328728\n",
      "Iteration 4268, loss = 0.47195191\n",
      "Iteration 4269, loss = 0.47247767\n",
      "Iteration 4270, loss = 0.47324349\n",
      "Iteration 4271, loss = 0.47279134\n",
      "Iteration 4272, loss = 0.47264591\n",
      "Iteration 4273, loss = 0.47229653\n",
      "Iteration 4274, loss = 0.47221652\n",
      "Iteration 4275, loss = 0.47265771\n",
      "Iteration 4276, loss = 0.47266763\n",
      "Iteration 4277, loss = 0.47227337\n",
      "Iteration 4278, loss = 0.47269511\n",
      "Iteration 4279, loss = 0.47216435\n",
      "Iteration 4280, loss = 0.47205784\n",
      "Iteration 4281, loss = 0.47206340\n",
      "Iteration 4282, loss = 0.47197748\n",
      "Iteration 4283, loss = 0.47228262\n",
      "Iteration 4284, loss = 0.47215850\n",
      "Iteration 4285, loss = 0.47189432\n",
      "Iteration 4286, loss = 0.47197957\n",
      "Iteration 4287, loss = 0.47227351\n",
      "Iteration 4288, loss = 0.47242710\n",
      "Iteration 4289, loss = 0.47297036\n",
      "Iteration 4290, loss = 0.47297929\n",
      "Iteration 4291, loss = 0.47272920\n",
      "Iteration 4292, loss = 0.47252205\n",
      "Iteration 4293, loss = 0.47191353\n",
      "Iteration 4294, loss = 0.47269713\n",
      "Iteration 4295, loss = 0.47309622\n",
      "Iteration 4296, loss = 0.47292766\n",
      "Iteration 4297, loss = 0.47239323\n",
      "Iteration 4298, loss = 0.47194454\n",
      "Iteration 4299, loss = 0.47260719\n",
      "Iteration 4300, loss = 0.47261019\n",
      "Iteration 4301, loss = 0.47249566\n",
      "Iteration 4302, loss = 0.47228303\n",
      "Iteration 4303, loss = 0.47196940\n",
      "Iteration 4304, loss = 0.47199482\n",
      "Iteration 4305, loss = 0.47233213\n",
      "Iteration 4306, loss = 0.47227255\n",
      "Iteration 4307, loss = 0.47211955\n",
      "Iteration 4308, loss = 0.47206992\n",
      "Iteration 4309, loss = 0.47208433\n",
      "Iteration 4310, loss = 0.47226511\n",
      "Iteration 4311, loss = 0.47225881\n",
      "Iteration 4312, loss = 0.47248162\n",
      "Iteration 4313, loss = 0.47206625\n",
      "Iteration 4314, loss = 0.47212445\n",
      "Iteration 4315, loss = 0.47202670\n",
      "Iteration 4316, loss = 0.47207744\n",
      "Iteration 4317, loss = 0.47219395\n",
      "Iteration 4318, loss = 0.47202951\n",
      "Iteration 4319, loss = 0.47214637\n",
      "Iteration 4320, loss = 0.47220482\n",
      "Iteration 4321, loss = 0.47221745\n",
      "Iteration 4322, loss = 0.47213180\n",
      "Iteration 4323, loss = 0.47252443\n",
      "Iteration 4324, loss = 0.47198160\n",
      "Iteration 4325, loss = 0.47194548\n",
      "Iteration 4326, loss = 0.47190284\n",
      "Iteration 4327, loss = 0.47219797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4328, loss = 0.47291491\n",
      "Iteration 4329, loss = 0.47234722\n",
      "Iteration 4330, loss = 0.47257793\n",
      "Iteration 4331, loss = 0.47247105\n",
      "Iteration 4332, loss = 0.47245157\n",
      "Iteration 4333, loss = 0.47200842\n",
      "Iteration 4334, loss = 0.47237152\n",
      "Iteration 4335, loss = 0.47263956\n",
      "Iteration 4336, loss = 0.47267369\n",
      "Iteration 4337, loss = 0.47220953\n",
      "Iteration 4338, loss = 0.47230483\n",
      "Iteration 4339, loss = 0.47223078\n",
      "Iteration 4340, loss = 0.47238811\n",
      "Iteration 4341, loss = 0.47250242\n",
      "Iteration 4342, loss = 0.47276685\n",
      "Iteration 4343, loss = 0.47254417\n",
      "Iteration 4344, loss = 0.47254526\n",
      "Iteration 4345, loss = 0.47234797\n",
      "Iteration 4346, loss = 0.47238173\n",
      "Iteration 4347, loss = 0.47221278\n",
      "Iteration 4348, loss = 0.47347705\n",
      "Iteration 4349, loss = 0.47336789\n",
      "Iteration 4350, loss = 0.47257405\n",
      "Iteration 4351, loss = 0.47217429\n",
      "Iteration 4352, loss = 0.47214441\n",
      "Iteration 4353, loss = 0.47216128\n",
      "Iteration 4354, loss = 0.47217194\n",
      "Iteration 4355, loss = 0.47224757\n",
      "Iteration 4356, loss = 0.47217221\n",
      "Iteration 4357, loss = 0.47206238\n",
      "Iteration 4358, loss = 0.47214450\n",
      "Iteration 4359, loss = 0.47213377\n",
      "Iteration 4360, loss = 0.47192824\n",
      "Iteration 4361, loss = 0.47193183\n",
      "Iteration 4362, loss = 0.47231281\n",
      "Iteration 4363, loss = 0.47206879\n",
      "Iteration 4364, loss = 0.47252789\n",
      "Iteration 4365, loss = 0.47225788\n",
      "Iteration 4366, loss = 0.47236820\n",
      "Iteration 4367, loss = 0.47224266\n",
      "Iteration 4368, loss = 0.47205630\n",
      "Iteration 4369, loss = 0.47192278\n",
      "Iteration 4370, loss = 0.47199519\n",
      "Iteration 4371, loss = 0.47223516\n",
      "Iteration 4372, loss = 0.47227906\n",
      "Iteration 4373, loss = 0.47207192\n",
      "Iteration 4374, loss = 0.47217918\n",
      "Iteration 4375, loss = 0.47210720\n",
      "Iteration 4376, loss = 0.47202847\n",
      "Iteration 4377, loss = 0.47203539\n",
      "Iteration 4378, loss = 0.47205508\n",
      "Iteration 4379, loss = 0.47273703\n",
      "Iteration 4380, loss = 0.47226200\n",
      "Iteration 4381, loss = 0.47205553\n",
      "Iteration 4382, loss = 0.47219014\n",
      "Iteration 4383, loss = 0.47206544\n",
      "Iteration 4384, loss = 0.47202612\n",
      "Iteration 4385, loss = 0.47200585\n",
      "Iteration 4386, loss = 0.47210939\n",
      "Iteration 4387, loss = 0.47195241\n",
      "Iteration 4388, loss = 0.47246607\n",
      "Iteration 4389, loss = 0.47292777\n",
      "Iteration 4390, loss = 0.47217957\n",
      "Iteration 4391, loss = 0.47215314\n",
      "Iteration 4392, loss = 0.47177003\n",
      "Iteration 4393, loss = 0.47234672\n",
      "Iteration 4394, loss = 0.47253056\n",
      "Iteration 4395, loss = 0.47251079\n",
      "Iteration 4396, loss = 0.47201676\n",
      "Iteration 4397, loss = 0.47220063\n",
      "Iteration 4398, loss = 0.47209326\n",
      "Iteration 4399, loss = 0.47200337\n",
      "Iteration 4400, loss = 0.47203477\n",
      "Iteration 4401, loss = 0.47209413\n",
      "Iteration 4402, loss = 0.47218796\n",
      "Iteration 4403, loss = 0.47244088\n",
      "Iteration 4404, loss = 0.47185503\n",
      "Iteration 4405, loss = 0.47240018\n",
      "Iteration 4406, loss = 0.47225118\n",
      "Iteration 4407, loss = 0.47207000\n",
      "Iteration 4408, loss = 0.47186405\n",
      "Iteration 4409, loss = 0.47189195\n",
      "Iteration 4410, loss = 0.47215818\n",
      "Iteration 4411, loss = 0.47266148\n",
      "Iteration 4412, loss = 0.47253967\n",
      "Iteration 4413, loss = 0.47216218\n",
      "Iteration 4414, loss = 0.47181305\n",
      "Iteration 4415, loss = 0.47188805\n",
      "Iteration 4416, loss = 0.47313309\n",
      "Iteration 4417, loss = 0.47300471\n",
      "Iteration 4418, loss = 0.47241799\n",
      "Iteration 4419, loss = 0.47224048\n",
      "Iteration 4420, loss = 0.47238376\n",
      "Iteration 4421, loss = 0.47239904\n",
      "Iteration 4422, loss = 0.47234678\n",
      "Iteration 4423, loss = 0.47210767\n",
      "Iteration 4424, loss = 0.47188806\n",
      "Iteration 4425, loss = 0.47212513\n",
      "Iteration 4426, loss = 0.47241082\n",
      "Iteration 4427, loss = 0.47248200\n",
      "Iteration 4428, loss = 0.47210573\n",
      "Iteration 4429, loss = 0.47182661\n",
      "Iteration 4430, loss = 0.47184806\n",
      "Iteration 4431, loss = 0.47249873\n",
      "Iteration 4432, loss = 0.47288545\n",
      "Iteration 4433, loss = 0.47265384\n",
      "Iteration 4434, loss = 0.47216772\n",
      "Iteration 4435, loss = 0.47173304\n",
      "Iteration 4436, loss = 0.47168018\n",
      "Iteration 4437, loss = 0.47372872\n",
      "Iteration 4438, loss = 0.47425212\n",
      "Iteration 4439, loss = 0.47348236\n",
      "Iteration 4440, loss = 0.47275590\n",
      "Iteration 4441, loss = 0.47248733\n",
      "Iteration 4442, loss = 0.47187830\n",
      "Iteration 4443, loss = 0.47259932\n",
      "Iteration 4444, loss = 0.47247328\n",
      "Iteration 4445, loss = 0.47193435\n",
      "Iteration 4446, loss = 0.47172135\n",
      "Iteration 4447, loss = 0.47260510\n",
      "Iteration 4448, loss = 0.47269503\n",
      "Iteration 4449, loss = 0.47248046\n",
      "Iteration 4450, loss = 0.47219100\n",
      "Iteration 4451, loss = 0.47186787\n",
      "Iteration 4452, loss = 0.47202767\n",
      "Iteration 4453, loss = 0.47250459\n",
      "Iteration 4454, loss = 0.47260650\n",
      "Iteration 4455, loss = 0.47222243\n",
      "Iteration 4456, loss = 0.47198956\n",
      "Iteration 4457, loss = 0.47198541\n",
      "Iteration 4458, loss = 0.47226949\n",
      "Iteration 4459, loss = 0.47197880\n",
      "Iteration 4460, loss = 0.47225448\n",
      "Iteration 4461, loss = 0.47199588\n",
      "Iteration 4462, loss = 0.47204832\n",
      "Iteration 4463, loss = 0.47195180\n",
      "Iteration 4464, loss = 0.47193993\n",
      "Iteration 4465, loss = 0.47190248\n",
      "Iteration 4466, loss = 0.47191188\n",
      "Iteration 4467, loss = 0.47190616\n",
      "Iteration 4468, loss = 0.47196909\n",
      "Iteration 4469, loss = 0.47206914\n",
      "Iteration 4470, loss = 0.47210632\n",
      "Iteration 4471, loss = 0.47201076\n",
      "Iteration 4472, loss = 0.47215458\n",
      "Iteration 4473, loss = 0.47234556\n",
      "Iteration 4474, loss = 0.47196426\n",
      "Iteration 4475, loss = 0.47171918\n",
      "Iteration 4476, loss = 0.47196151\n",
      "Iteration 4477, loss = 0.47253994\n",
      "Iteration 4478, loss = 0.47277391\n",
      "Iteration 4479, loss = 0.47268754\n",
      "Iteration 4480, loss = 0.47233941\n",
      "Iteration 4481, loss = 0.47214559\n",
      "Iteration 4482, loss = 0.47195494\n",
      "Iteration 4483, loss = 0.47195857\n",
      "Iteration 4484, loss = 0.47253371\n",
      "Iteration 4485, loss = 0.47262755\n",
      "Iteration 4486, loss = 0.47281105\n",
      "Iteration 4487, loss = 0.47199159\n",
      "Iteration 4488, loss = 0.47189510\n",
      "Iteration 4489, loss = 0.47196625\n",
      "Iteration 4490, loss = 0.47220382\n",
      "Iteration 4491, loss = 0.47220650\n",
      "Iteration 4492, loss = 0.47204356\n",
      "Iteration 4493, loss = 0.47191230\n",
      "Iteration 4494, loss = 0.47198605\n",
      "Iteration 4495, loss = 0.47199474\n",
      "Iteration 4496, loss = 0.47213821\n",
      "Iteration 4497, loss = 0.47208520\n",
      "Iteration 4498, loss = 0.47186317\n",
      "Iteration 4499, loss = 0.47164610\n",
      "Iteration 4500, loss = 0.47259553\n",
      "Iteration 4501, loss = 0.47313622\n",
      "Iteration 4502, loss = 0.47306988\n",
      "Iteration 4503, loss = 0.47236789\n",
      "Iteration 4504, loss = 0.47170873\n",
      "Iteration 4505, loss = 0.47175223\n",
      "Iteration 4506, loss = 0.47247376\n",
      "Iteration 4507, loss = 0.47368271\n",
      "Iteration 4508, loss = 0.47350478\n",
      "Iteration 4509, loss = 0.47286385\n",
      "Iteration 4510, loss = 0.47196523\n",
      "Iteration 4511, loss = 0.47182014\n",
      "Iteration 4512, loss = 0.47244738\n",
      "Iteration 4513, loss = 0.47282060\n",
      "Iteration 4514, loss = 0.47301484\n",
      "Iteration 4515, loss = 0.47276535\n",
      "Iteration 4516, loss = 0.47236822\n",
      "Iteration 4517, loss = 0.47181718\n",
      "Iteration 4518, loss = 0.47287291\n",
      "Iteration 4519, loss = 0.47279336\n",
      "Iteration 4520, loss = 0.47247778\n",
      "Iteration 4521, loss = 0.47228533\n",
      "Iteration 4522, loss = 0.47219828\n",
      "Iteration 4523, loss = 0.47209719\n",
      "Iteration 4524, loss = 0.47205386\n",
      "Iteration 4525, loss = 0.47206302\n",
      "Iteration 4526, loss = 0.47192128\n",
      "Iteration 4527, loss = 0.47206681\n",
      "Iteration 4528, loss = 0.47220136\n",
      "Iteration 4529, loss = 0.47225077\n",
      "Iteration 4530, loss = 0.47243809\n",
      "Iteration 4531, loss = 0.47193737\n",
      "Iteration 4532, loss = 0.47199353\n",
      "Iteration 4533, loss = 0.47199073\n",
      "Iteration 4534, loss = 0.47186478\n",
      "Iteration 4535, loss = 0.47290242\n",
      "Iteration 4536, loss = 0.47220560\n",
      "Iteration 4537, loss = 0.47203395\n",
      "Iteration 4538, loss = 0.47201248\n",
      "Iteration 4539, loss = 0.47190021\n",
      "Iteration 4540, loss = 0.47186967\n",
      "Iteration 4541, loss = 0.47208768\n",
      "Iteration 4542, loss = 0.47217745\n",
      "Iteration 4543, loss = 0.47194640\n",
      "Iteration 4544, loss = 0.47219023\n",
      "Iteration 4545, loss = 0.47204235\n",
      "Iteration 4546, loss = 0.47222187\n",
      "Iteration 4547, loss = 0.47196454\n",
      "Iteration 4548, loss = 0.47205920\n",
      "Iteration 4549, loss = 0.47187161\n",
      "Iteration 4550, loss = 0.47194063\n",
      "Iteration 4551, loss = 0.47219142\n",
      "Iteration 4552, loss = 0.47234055\n",
      "Iteration 4553, loss = 0.47239735\n",
      "Iteration 4554, loss = 0.47237156\n",
      "Iteration 4555, loss = 0.47187875\n",
      "Iteration 4556, loss = 0.47174746\n",
      "Iteration 4557, loss = 0.47238994\n",
      "Iteration 4558, loss = 0.47316362\n",
      "Iteration 4559, loss = 0.47313534\n",
      "Iteration 4560, loss = 0.47247044\n",
      "Iteration 4561, loss = 0.47185904\n",
      "Iteration 4562, loss = 0.47200192\n",
      "Iteration 4563, loss = 0.47230336\n",
      "Iteration 4564, loss = 0.47273444\n",
      "Iteration 4565, loss = 0.47258167\n",
      "Iteration 4566, loss = 0.47270161\n",
      "Iteration 4567, loss = 0.47218248\n",
      "Iteration 4568, loss = 0.47197066\n",
      "Iteration 4569, loss = 0.47180016\n",
      "Iteration 4570, loss = 0.47197119\n",
      "Iteration 4571, loss = 0.47210298\n",
      "Iteration 4572, loss = 0.47205033\n",
      "Iteration 4573, loss = 0.47252842\n",
      "Iteration 4574, loss = 0.47203374\n",
      "Iteration 4575, loss = 0.47244118\n",
      "Iteration 4576, loss = 0.47209715\n",
      "Iteration 4577, loss = 0.47195049\n",
      "Iteration 4578, loss = 0.47189721\n",
      "Iteration 4579, loss = 0.47197967\n",
      "Iteration 4580, loss = 0.47190922\n",
      "Iteration 4581, loss = 0.47187387\n",
      "Iteration 4582, loss = 0.47212230\n",
      "Iteration 4583, loss = 0.47221625\n",
      "Iteration 4584, loss = 0.47212540\n",
      "Iteration 4585, loss = 0.47195181\n",
      "Iteration 4586, loss = 0.47220434\n",
      "Iteration 4587, loss = 0.47222659\n",
      "Iteration 4588, loss = 0.47184846\n",
      "Iteration 4589, loss = 0.47209563\n",
      "Iteration 4590, loss = 0.47208067\n",
      "Iteration 4591, loss = 0.47196930\n",
      "Iteration 4592, loss = 0.47195155\n",
      "Iteration 4593, loss = 0.47195102\n",
      "Iteration 4594, loss = 0.47190513\n",
      "Iteration 4595, loss = 0.47193476\n",
      "Iteration 4596, loss = 0.47193569\n",
      "Iteration 4597, loss = 0.47195488\n",
      "Iteration 4598, loss = 0.47195135\n",
      "Iteration 4599, loss = 0.47205272\n",
      "Iteration 4600, loss = 0.47204173\n",
      "Iteration 4601, loss = 0.47253584\n",
      "Iteration 4602, loss = 0.47207201\n",
      "Iteration 4603, loss = 0.47291162\n",
      "Iteration 4604, loss = 0.47232041\n",
      "Iteration 4605, loss = 0.47216946\n",
      "Iteration 4606, loss = 0.47216494\n",
      "Iteration 4607, loss = 0.47217232\n",
      "Iteration 4608, loss = 0.47209332\n",
      "Iteration 4609, loss = 0.47202493\n",
      "Iteration 4610, loss = 0.47213168\n",
      "Iteration 4611, loss = 0.47196297\n",
      "Iteration 4612, loss = 0.47195295\n",
      "Iteration 4613, loss = 0.47191139\n",
      "Iteration 4614, loss = 0.47192752\n",
      "Iteration 4615, loss = 0.47273401\n",
      "Iteration 4616, loss = 0.47226571\n",
      "Iteration 4617, loss = 0.47194604\n",
      "Iteration 4618, loss = 0.47185114\n",
      "Iteration 4619, loss = 0.47181385\n",
      "Iteration 4620, loss = 0.47202343\n",
      "Iteration 4621, loss = 0.47187529\n",
      "Iteration 4622, loss = 0.47197111\n",
      "Iteration 4623, loss = 0.47195176\n",
      "Iteration 4624, loss = 0.47183556\n",
      "Iteration 4625, loss = 0.47188108\n",
      "Iteration 4626, loss = 0.47190132\n",
      "Iteration 4627, loss = 0.47240395\n",
      "Iteration 4628, loss = 0.47184752\n",
      "Iteration 4629, loss = 0.47190715\n",
      "Iteration 4630, loss = 0.47207355\n",
      "Iteration 4631, loss = 0.47217312\n",
      "Iteration 4632, loss = 0.47227669\n",
      "Iteration 4633, loss = 0.47208616\n",
      "Iteration 4634, loss = 0.47236242\n",
      "Iteration 4635, loss = 0.47200557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4636, loss = 0.47238335\n",
      "Iteration 4637, loss = 0.47266155\n",
      "Iteration 4638, loss = 0.47221566\n",
      "Iteration 4639, loss = 0.47203915\n",
      "Iteration 4640, loss = 0.47306064\n",
      "Iteration 4641, loss = 0.47196085\n",
      "Iteration 4642, loss = 0.47184372\n",
      "Iteration 4643, loss = 0.47198928\n",
      "Iteration 4644, loss = 0.47206524\n",
      "Iteration 4645, loss = 0.47206274\n",
      "Iteration 4646, loss = 0.47239650\n",
      "Iteration 4647, loss = 0.47194842\n",
      "Iteration 4648, loss = 0.47208209\n",
      "Iteration 4649, loss = 0.47192337\n",
      "Iteration 4650, loss = 0.47181758\n",
      "Iteration 4651, loss = 0.47174785\n",
      "Iteration 4652, loss = 0.47196563\n",
      "Iteration 4653, loss = 0.47228630\n",
      "Iteration 4654, loss = 0.47242832\n",
      "Iteration 4655, loss = 0.47249981\n",
      "Iteration 4656, loss = 0.47226117\n",
      "Iteration 4657, loss = 0.47171266\n",
      "Iteration 4658, loss = 0.47230404\n",
      "Iteration 4659, loss = 0.47253726\n",
      "Iteration 4660, loss = 0.47238781\n",
      "Iteration 4661, loss = 0.47257387\n",
      "Iteration 4662, loss = 0.47200391\n",
      "Iteration 4663, loss = 0.47224772\n",
      "Iteration 4664, loss = 0.47313396\n",
      "Iteration 4665, loss = 0.47186986\n",
      "Iteration 4666, loss = 0.47216461\n",
      "Iteration 4667, loss = 0.47204958\n",
      "Iteration 4668, loss = 0.47191784\n",
      "Iteration 4669, loss = 0.47193261\n",
      "Iteration 4670, loss = 0.47195869\n",
      "Iteration 4671, loss = 0.47188171\n",
      "Iteration 4672, loss = 0.47186920\n",
      "Iteration 4673, loss = 0.47192701\n",
      "Iteration 4674, loss = 0.47195891\n",
      "Iteration 4675, loss = 0.47214791\n",
      "Iteration 4676, loss = 0.47194194\n",
      "Iteration 4677, loss = 0.47217061\n",
      "Iteration 4678, loss = 0.47214607\n",
      "Iteration 4679, loss = 0.47204472\n",
      "Iteration 4680, loss = 0.47189326\n",
      "Iteration 4681, loss = 0.47169480\n",
      "Iteration 4682, loss = 0.47214466\n",
      "Iteration 4683, loss = 0.47246870\n",
      "Iteration 4684, loss = 0.47265575\n",
      "Iteration 4685, loss = 0.47242085\n",
      "Iteration 4686, loss = 0.47211355\n",
      "Iteration 4687, loss = 0.47175109\n",
      "Iteration 4688, loss = 0.47345039\n",
      "Iteration 4689, loss = 0.47250464\n",
      "Iteration 4690, loss = 0.47213806\n",
      "Iteration 4691, loss = 0.47219301\n",
      "Iteration 4692, loss = 0.47188163\n",
      "Iteration 4693, loss = 0.47194131\n",
      "Iteration 4694, loss = 0.47218206\n",
      "Iteration 4695, loss = 0.47241390\n",
      "Iteration 4696, loss = 0.47236071\n",
      "Iteration 4697, loss = 0.47190491\n",
      "Iteration 4698, loss = 0.47160052\n",
      "Iteration 4699, loss = 0.47193393\n",
      "Iteration 4700, loss = 0.47274909\n",
      "Iteration 4701, loss = 0.47335251\n",
      "Iteration 4702, loss = 0.47268475\n",
      "Iteration 4703, loss = 0.47209147\n",
      "Iteration 4704, loss = 0.47199733\n",
      "Iteration 4705, loss = 0.47227391\n",
      "Iteration 4706, loss = 0.47234199\n",
      "Iteration 4707, loss = 0.47209388\n",
      "Iteration 4708, loss = 0.47177979\n",
      "Iteration 4709, loss = 0.47175263\n",
      "Iteration 4710, loss = 0.47265581\n",
      "Iteration 4711, loss = 0.47266003\n",
      "Iteration 4712, loss = 0.47199019\n",
      "Iteration 4713, loss = 0.47184824\n",
      "Iteration 4714, loss = 0.47193093\n",
      "Iteration 4715, loss = 0.47167370\n",
      "Iteration 4716, loss = 0.47196702\n",
      "Iteration 4717, loss = 0.47262073\n",
      "Iteration 4718, loss = 0.47287327\n",
      "Iteration 4719, loss = 0.47222672\n",
      "Iteration 4720, loss = 0.47198926\n",
      "Iteration 4721, loss = 0.47225028\n",
      "Iteration 4722, loss = 0.47194357\n",
      "Iteration 4723, loss = 0.47222520\n",
      "Iteration 4724, loss = 0.47201650\n",
      "Iteration 4725, loss = 0.47204218\n",
      "Iteration 4726, loss = 0.47205032\n",
      "Iteration 4727, loss = 0.47182038\n",
      "Iteration 4728, loss = 0.47209271\n",
      "Iteration 4729, loss = 0.47193966\n",
      "Iteration 4730, loss = 0.47209659\n",
      "Iteration 4731, loss = 0.47188422\n",
      "Iteration 4732, loss = 0.47196338\n",
      "Iteration 4733, loss = 0.47203869\n",
      "Iteration 4734, loss = 0.47225050\n",
      "Iteration 4735, loss = 0.47236937\n",
      "Iteration 4736, loss = 0.47172890\n",
      "Iteration 4737, loss = 0.47164837\n",
      "Iteration 4738, loss = 0.47291115\n",
      "Iteration 4739, loss = 0.47350331\n",
      "Iteration 4740, loss = 0.47326544\n",
      "Iteration 4741, loss = 0.47285217\n",
      "Iteration 4742, loss = 0.47184065\n",
      "Iteration 4743, loss = 0.47204505\n",
      "Iteration 4744, loss = 0.47230979\n",
      "Iteration 4745, loss = 0.47243245\n",
      "Iteration 4746, loss = 0.47257074\n",
      "Iteration 4747, loss = 0.47218714\n",
      "Iteration 4748, loss = 0.47216391\n",
      "Iteration 4749, loss = 0.47222877\n",
      "Iteration 4750, loss = 0.47205017\n",
      "Iteration 4751, loss = 0.47213809\n",
      "Iteration 4752, loss = 0.47186126\n",
      "Iteration 4753, loss = 0.47195805\n",
      "Iteration 4754, loss = 0.47185472\n",
      "Iteration 4755, loss = 0.47186675\n",
      "Iteration 4756, loss = 0.47211307\n",
      "Iteration 4757, loss = 0.47182429\n",
      "Iteration 4758, loss = 0.47244870\n",
      "Iteration 4759, loss = 0.47213189\n",
      "Iteration 4760, loss = 0.47187784\n",
      "Iteration 4761, loss = 0.47305005\n",
      "Iteration 4762, loss = 0.47192632\n",
      "Iteration 4763, loss = 0.47210382\n",
      "Iteration 4764, loss = 0.47180656\n",
      "Iteration 4765, loss = 0.47180834\n",
      "Iteration 4766, loss = 0.47183638\n",
      "Iteration 4767, loss = 0.47194784\n",
      "Iteration 4768, loss = 0.47189864\n",
      "Iteration 4769, loss = 0.47189059\n",
      "Iteration 4770, loss = 0.47191858\n",
      "Iteration 4771, loss = 0.47183578\n",
      "Iteration 4772, loss = 0.47240582\n",
      "Iteration 4773, loss = 0.47240217\n",
      "Iteration 4774, loss = 0.47185575\n",
      "Iteration 4775, loss = 0.47182355\n",
      "Iteration 4776, loss = 0.47182469\n",
      "Iteration 4777, loss = 0.47185207\n",
      "Iteration 4778, loss = 0.47178450\n",
      "Iteration 4779, loss = 0.47179909\n",
      "Iteration 4780, loss = 0.47204548\n",
      "Iteration 4781, loss = 0.47202328\n",
      "Iteration 4782, loss = 0.47227186\n",
      "Iteration 4783, loss = 0.47184570\n",
      "Iteration 4784, loss = 0.47175388\n",
      "Iteration 4785, loss = 0.47191111\n",
      "Iteration 4786, loss = 0.47201688\n",
      "Iteration 4787, loss = 0.47184756\n",
      "Iteration 4788, loss = 0.47188727\n",
      "Iteration 4789, loss = 0.47186636\n",
      "Iteration 4790, loss = 0.47200519\n",
      "Iteration 4791, loss = 0.47208266\n",
      "Iteration 4792, loss = 0.47195459\n",
      "Iteration 4793, loss = 0.47183409\n",
      "Iteration 4794, loss = 0.47199917\n",
      "Iteration 4795, loss = 0.47218968\n",
      "Iteration 4796, loss = 0.47218158\n",
      "Iteration 4797, loss = 0.47223695\n",
      "Iteration 4798, loss = 0.47221402\n",
      "Iteration 4799, loss = 0.47198986\n",
      "Iteration 4800, loss = 0.47186812\n",
      "Iteration 4801, loss = 0.47195332\n",
      "Iteration 4802, loss = 0.47180409\n",
      "Iteration 4803, loss = 0.47190762\n",
      "Iteration 4804, loss = 0.47201881\n",
      "Iteration 4805, loss = 0.47224869\n",
      "Iteration 4806, loss = 0.47209238\n",
      "Iteration 4807, loss = 0.47275361\n",
      "Iteration 4808, loss = 0.47201549\n",
      "Iteration 4809, loss = 0.47214509\n",
      "Iteration 4810, loss = 0.47216883\n",
      "Iteration 4811, loss = 0.47214505\n",
      "Iteration 4812, loss = 0.47209609\n",
      "Iteration 4813, loss = 0.47214212\n",
      "Iteration 4814, loss = 0.47212315\n",
      "Iteration 4815, loss = 0.47224668\n",
      "Iteration 4816, loss = 0.47292784\n",
      "Iteration 4817, loss = 0.47233092\n",
      "Iteration 4818, loss = 0.47217362\n",
      "Iteration 4819, loss = 0.47188085\n",
      "Iteration 4820, loss = 0.47175962\n",
      "Iteration 4821, loss = 0.47194884\n",
      "Iteration 4822, loss = 0.47207792\n",
      "Iteration 4823, loss = 0.47210042\n",
      "Iteration 4824, loss = 0.47189947\n",
      "Iteration 4825, loss = 0.47187407\n",
      "Iteration 4826, loss = 0.47174563\n",
      "Iteration 4827, loss = 0.47202055\n",
      "Iteration 4828, loss = 0.47206143\n",
      "Iteration 4829, loss = 0.47197902\n",
      "Iteration 4830, loss = 0.47193283\n",
      "Iteration 4831, loss = 0.47174246\n",
      "Iteration 4832, loss = 0.47179044\n",
      "Iteration 4833, loss = 0.47194428\n",
      "Iteration 4834, loss = 0.47213242\n",
      "Iteration 4835, loss = 0.47214001\n",
      "Iteration 4836, loss = 0.47187233\n",
      "Iteration 4837, loss = 0.47179457\n",
      "Iteration 4838, loss = 0.47188082\n",
      "Iteration 4839, loss = 0.47190127\n",
      "Iteration 4840, loss = 0.47179653\n",
      "Iteration 4841, loss = 0.47202020\n",
      "Iteration 4842, loss = 0.47183160\n",
      "Iteration 4843, loss = 0.47214498\n",
      "Iteration 4844, loss = 0.47195202\n",
      "Iteration 4845, loss = 0.47188040\n",
      "Iteration 4846, loss = 0.47213396\n",
      "Iteration 4847, loss = 0.47187469\n",
      "Iteration 4848, loss = 0.47187615\n",
      "Iteration 4849, loss = 0.47217241\n",
      "Iteration 4850, loss = 0.47199080\n",
      "Iteration 4851, loss = 0.47173797\n",
      "Iteration 4852, loss = 0.47180273\n",
      "Iteration 4853, loss = 0.47201299\n",
      "Iteration 4854, loss = 0.47207427\n",
      "Iteration 4855, loss = 0.47213183\n",
      "Iteration 4856, loss = 0.47204261\n",
      "Iteration 4857, loss = 0.47190622\n",
      "Iteration 4858, loss = 0.47173602\n",
      "Iteration 4859, loss = 0.47193514\n",
      "Iteration 4860, loss = 0.47214890\n",
      "Iteration 4861, loss = 0.47223948\n",
      "Iteration 4862, loss = 0.47197985\n",
      "Iteration 4863, loss = 0.47171919\n",
      "Iteration 4864, loss = 0.47178141\n",
      "Iteration 4865, loss = 0.47190917\n",
      "Iteration 4866, loss = 0.47215851\n",
      "Iteration 4867, loss = 0.47188287\n",
      "Iteration 4868, loss = 0.47166924\n",
      "Iteration 4869, loss = 0.47434492\n",
      "Iteration 4870, loss = 0.47219233\n",
      "Iteration 4871, loss = 0.47185652\n",
      "Iteration 4872, loss = 0.47174959\n",
      "Iteration 4873, loss = 0.47189786\n",
      "Iteration 4874, loss = 0.47238330\n",
      "Iteration 4875, loss = 0.47257918\n",
      "Iteration 4876, loss = 0.47179765\n",
      "Iteration 4877, loss = 0.47221207\n",
      "Iteration 4878, loss = 0.47181452\n",
      "Iteration 4879, loss = 0.47179114\n",
      "Iteration 4880, loss = 0.47174341\n",
      "Iteration 4881, loss = 0.47186774\n",
      "Iteration 4882, loss = 0.47204516\n",
      "Iteration 4883, loss = 0.47236497\n",
      "Iteration 4884, loss = 0.47235832\n",
      "Iteration 4885, loss = 0.47205271\n",
      "Iteration 4886, loss = 0.47270088\n",
      "Iteration 4887, loss = 0.47178196\n",
      "Iteration 4888, loss = 0.47180096\n",
      "Iteration 4889, loss = 0.47197696\n",
      "Iteration 4890, loss = 0.47174395\n",
      "Iteration 4891, loss = 0.47181010\n",
      "Iteration 4892, loss = 0.47260885\n",
      "Iteration 4893, loss = 0.47192964\n",
      "Iteration 4894, loss = 0.47195206\n",
      "Iteration 4895, loss = 0.47193870\n",
      "Iteration 4896, loss = 0.47253042\n",
      "Iteration 4897, loss = 0.47326335\n",
      "Iteration 4898, loss = 0.47320128\n",
      "Iteration 4899, loss = 0.47298781\n",
      "Iteration 4900, loss = 0.47168674\n",
      "Iteration 4901, loss = 0.47181442\n",
      "Iteration 4902, loss = 0.47207358\n",
      "Iteration 4903, loss = 0.47255711\n",
      "Iteration 4904, loss = 0.47267510\n",
      "Iteration 4905, loss = 0.47256487\n",
      "Iteration 4906, loss = 0.47246382\n",
      "Iteration 4907, loss = 0.47189475\n",
      "Iteration 4908, loss = 0.47170746\n",
      "Iteration 4909, loss = 0.47160539\n",
      "Iteration 4910, loss = 0.47310854\n",
      "Iteration 4911, loss = 0.47274819\n",
      "Iteration 4912, loss = 0.47207190\n",
      "Iteration 4913, loss = 0.47244503\n",
      "Iteration 4914, loss = 0.47202937\n",
      "Iteration 4915, loss = 0.47212791\n",
      "Iteration 4916, loss = 0.47195382\n",
      "Iteration 4917, loss = 0.47290160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4918, loss = 0.47183427\n",
      "Iteration 4919, loss = 0.47188931\n",
      "Iteration 4920, loss = 0.47202862\n",
      "Iteration 4921, loss = 0.47187318\n",
      "Iteration 4922, loss = 0.47185752\n",
      "Iteration 4923, loss = 0.47178145\n",
      "Iteration 4924, loss = 0.47181308\n",
      "Iteration 4925, loss = 0.47177788\n",
      "Iteration 4926, loss = 0.47179777\n",
      "Iteration 4927, loss = 0.47179007\n",
      "Iteration 4928, loss = 0.47187530\n",
      "Iteration 4929, loss = 0.47198177\n",
      "Iteration 4930, loss = 0.47176803\n",
      "Iteration 4931, loss = 0.47182146\n",
      "Iteration 4932, loss = 0.47192760\n",
      "Iteration 4933, loss = 0.47231306\n",
      "Iteration 4934, loss = 0.47205952\n",
      "Iteration 4935, loss = 0.47177882\n",
      "Iteration 4936, loss = 0.47238857\n",
      "Iteration 4937, loss = 0.47186380\n",
      "Iteration 4938, loss = 0.47195936\n",
      "Iteration 4939, loss = 0.47180592\n",
      "Iteration 4940, loss = 0.47180406\n",
      "Iteration 4941, loss = 0.47183528\n",
      "Iteration 4942, loss = 0.47193346\n",
      "Iteration 4943, loss = 0.47177662\n",
      "Iteration 4944, loss = 0.47180390\n",
      "Iteration 4945, loss = 0.47177128\n",
      "Iteration 4946, loss = 0.47189364\n",
      "Iteration 4947, loss = 0.47190837\n",
      "Iteration 4948, loss = 0.47220997\n",
      "Iteration 4949, loss = 0.47186335\n",
      "Iteration 4950, loss = 0.47181553\n",
      "Iteration 4951, loss = 0.47178109\n",
      "Iteration 4952, loss = 0.47211434\n",
      "Iteration 4953, loss = 0.47193058\n",
      "Iteration 4954, loss = 0.47212707\n",
      "Iteration 4955, loss = 0.47188576\n",
      "Iteration 4956, loss = 0.47167997\n",
      "Iteration 4957, loss = 0.47282496\n",
      "Iteration 4958, loss = 0.47200622\n",
      "Iteration 4959, loss = 0.47170161\n",
      "Iteration 4960, loss = 0.47197853\n",
      "Iteration 4961, loss = 0.47190092\n",
      "Iteration 4962, loss = 0.47199620\n",
      "Iteration 4963, loss = 0.47197700\n",
      "Iteration 4964, loss = 0.47205397\n",
      "Iteration 4965, loss = 0.47190654\n",
      "Iteration 4966, loss = 0.47181577\n",
      "Iteration 4967, loss = 0.47171434\n",
      "Iteration 4968, loss = 0.47172404\n",
      "Iteration 4969, loss = 0.47217602\n",
      "Iteration 4970, loss = 0.47231731\n",
      "Iteration 4971, loss = 0.47211098\n",
      "Iteration 4972, loss = 0.47203809\n",
      "Iteration 4973, loss = 0.47188763\n",
      "Iteration 4974, loss = 0.47200068\n",
      "Iteration 4975, loss = 0.47177468\n",
      "Iteration 4976, loss = 0.47254917\n",
      "Iteration 4977, loss = 0.47205342\n",
      "Iteration 4978, loss = 0.47173352\n",
      "Iteration 4979, loss = 0.47183553\n",
      "Iteration 4980, loss = 0.47188535\n",
      "Iteration 4981, loss = 0.47201056\n",
      "Iteration 4982, loss = 0.47199016\n",
      "Iteration 4983, loss = 0.47197998\n",
      "Iteration 4984, loss = 0.47179419\n",
      "Iteration 4985, loss = 0.47183270\n",
      "Iteration 4986, loss = 0.47176853\n",
      "Iteration 4987, loss = 0.47181154\n",
      "Iteration 4988, loss = 0.47189324\n",
      "Iteration 4989, loss = 0.47184566\n",
      "Iteration 4990, loss = 0.47177275\n",
      "Iteration 4991, loss = 0.47179696\n",
      "Iteration 4992, loss = 0.47173298\n",
      "Iteration 4993, loss = 0.47176300\n",
      "Iteration 4994, loss = 0.47176683\n",
      "Iteration 4995, loss = 0.47175325\n",
      "Iteration 4996, loss = 0.47167010\n",
      "Iteration 4997, loss = 0.47164539\n",
      "Iteration 4998, loss = 0.47190237\n",
      "Iteration 4999, loss = 0.47207783\n",
      "Iteration 5000, loss = 0.47278936\n",
      "Iteration 5001, loss = 0.47217353\n",
      "Iteration 5002, loss = 0.47213776\n",
      "Iteration 5003, loss = 0.47187880\n",
      "Iteration 5004, loss = 0.47209180\n",
      "Iteration 5005, loss = 0.47226575\n",
      "Iteration 5006, loss = 0.47231330\n",
      "Iteration 5007, loss = 0.47197794\n",
      "Iteration 5008, loss = 0.47176945\n",
      "Iteration 5009, loss = 0.47172758\n",
      "Iteration 5010, loss = 0.47241503\n",
      "Iteration 5011, loss = 0.47316081\n",
      "Iteration 5012, loss = 0.47190917\n",
      "Iteration 5013, loss = 0.47213971\n",
      "Iteration 5014, loss = 0.47210958\n",
      "Iteration 5015, loss = 0.47187253\n",
      "Iteration 5016, loss = 0.47254245\n",
      "Iteration 5017, loss = 0.47176490\n",
      "Iteration 5018, loss = 0.47185079\n",
      "Iteration 5019, loss = 0.47177958\n",
      "Iteration 5020, loss = 0.47179394\n",
      "Iteration 5021, loss = 0.47165288\n",
      "Iteration 5022, loss = 0.47179270\n",
      "Iteration 5023, loss = 0.47195850\n",
      "Iteration 5024, loss = 0.47200943\n",
      "Iteration 5025, loss = 0.47183309\n",
      "Iteration 5026, loss = 0.47182149\n",
      "Iteration 5027, loss = 0.47200364\n",
      "Iteration 5028, loss = 0.47190580\n",
      "Iteration 5029, loss = 0.47195915\n",
      "Iteration 5030, loss = 0.47167055\n",
      "Iteration 5031, loss = 0.47167145\n",
      "Iteration 5032, loss = 0.47174123\n",
      "Iteration 5033, loss = 0.47181294\n",
      "Iteration 5034, loss = 0.47174091\n",
      "Iteration 5035, loss = 0.47171142\n",
      "Iteration 5036, loss = 0.47180775\n",
      "Iteration 5037, loss = 0.47176505\n",
      "Iteration 5038, loss = 0.47173351\n",
      "Iteration 5039, loss = 0.47177265\n",
      "Iteration 5040, loss = 0.47225786\n",
      "Iteration 5041, loss = 0.47186781\n",
      "Iteration 5042, loss = 0.47170316\n",
      "Iteration 5043, loss = 0.47190371\n",
      "Iteration 5044, loss = 0.47208087\n",
      "Iteration 5045, loss = 0.47201396\n",
      "Iteration 5046, loss = 0.47181350\n",
      "Iteration 5047, loss = 0.47183375\n",
      "Iteration 5048, loss = 0.47184326\n",
      "Iteration 5049, loss = 0.47181644\n",
      "Iteration 5050, loss = 0.47203281\n",
      "Iteration 5051, loss = 0.47194000\n",
      "Iteration 5052, loss = 0.47179769\n",
      "Iteration 5053, loss = 0.47198414\n",
      "Iteration 5054, loss = 0.47178706\n",
      "Iteration 5055, loss = 0.47165451\n",
      "Iteration 5056, loss = 0.47167604\n",
      "Iteration 5057, loss = 0.47218183\n",
      "Iteration 5058, loss = 0.47202063\n",
      "Iteration 5059, loss = 0.47255510\n",
      "Iteration 5060, loss = 0.47176432\n",
      "Iteration 5061, loss = 0.47167622\n",
      "Iteration 5062, loss = 0.47175485\n",
      "Iteration 5063, loss = 0.47184709\n",
      "Iteration 5064, loss = 0.47191968\n",
      "Iteration 5065, loss = 0.47218798\n",
      "Iteration 5066, loss = 0.47177655\n",
      "Iteration 5067, loss = 0.47184669\n",
      "Iteration 5068, loss = 0.47173933\n",
      "Iteration 5069, loss = 0.47175050\n",
      "Iteration 5070, loss = 0.47169146\n",
      "Iteration 5071, loss = 0.47188333\n",
      "Iteration 5072, loss = 0.47179984\n",
      "Iteration 5073, loss = 0.47163132\n",
      "Iteration 5074, loss = 0.47205505\n",
      "Iteration 5075, loss = 0.47190639\n",
      "Iteration 5076, loss = 0.47167807\n",
      "Iteration 5077, loss = 0.47150045\n",
      "Iteration 5078, loss = 0.47204852\n",
      "Iteration 5079, loss = 0.47248485\n",
      "Iteration 5080, loss = 0.47241878\n",
      "Iteration 5081, loss = 0.47187393\n",
      "Iteration 5082, loss = 0.47206564\n",
      "Iteration 5083, loss = 0.47174721\n",
      "Iteration 5084, loss = 0.47171962\n",
      "Iteration 5085, loss = 0.47169935\n",
      "Iteration 5086, loss = 0.47207545\n",
      "Iteration 5087, loss = 0.47166897\n",
      "Iteration 5088, loss = 0.47198792\n",
      "Iteration 5089, loss = 0.47193079\n",
      "Iteration 5090, loss = 0.47189646\n",
      "Iteration 5091, loss = 0.47200083\n",
      "Iteration 5092, loss = 0.47170438\n",
      "Iteration 5093, loss = 0.47168371\n",
      "Iteration 5094, loss = 0.47186819\n",
      "Iteration 5095, loss = 0.47184503\n",
      "Iteration 5096, loss = 0.47180045\n",
      "Iteration 5097, loss = 0.47176394\n",
      "Iteration 5098, loss = 0.47184964\n",
      "Iteration 5099, loss = 0.47174849\n",
      "Iteration 5100, loss = 0.47178789\n",
      "Iteration 5101, loss = 0.47166546\n",
      "Iteration 5102, loss = 0.47163329\n",
      "Iteration 5103, loss = 0.47167708\n",
      "Iteration 5104, loss = 0.47177428\n",
      "Iteration 5105, loss = 0.47200929\n",
      "Iteration 5106, loss = 0.47174010\n",
      "Iteration 5107, loss = 0.47196377\n",
      "Iteration 5108, loss = 0.47165456\n",
      "Iteration 5109, loss = 0.47168019\n",
      "Iteration 5110, loss = 0.47172831\n",
      "Iteration 5111, loss = 0.47163097\n",
      "Iteration 5112, loss = 0.47159267\n",
      "Iteration 5113, loss = 0.47186860\n",
      "Iteration 5114, loss = 0.47182808\n",
      "Iteration 5115, loss = 0.47212497\n",
      "Iteration 5116, loss = 0.47227518\n",
      "Iteration 5117, loss = 0.47237477\n",
      "Iteration 5118, loss = 0.47158621\n",
      "Iteration 5119, loss = 0.47221411\n",
      "Iteration 5120, loss = 0.47239918\n",
      "Iteration 5121, loss = 0.47216814\n",
      "Iteration 5122, loss = 0.47160417\n",
      "Iteration 5123, loss = 0.47269218\n",
      "Iteration 5124, loss = 0.47192035\n",
      "Iteration 5125, loss = 0.47187531\n",
      "Iteration 5126, loss = 0.47160259\n",
      "Iteration 5127, loss = 0.47162176\n",
      "Iteration 5128, loss = 0.47178383\n",
      "Iteration 5129, loss = 0.47174488\n",
      "Iteration 5130, loss = 0.47171526\n",
      "Iteration 5131, loss = 0.47168022\n",
      "Iteration 5132, loss = 0.47158121\n",
      "Iteration 5133, loss = 0.47180551\n",
      "Iteration 5134, loss = 0.47189188\n",
      "Iteration 5135, loss = 0.47197880\n",
      "Iteration 5136, loss = 0.47173984\n",
      "Iteration 5137, loss = 0.47194985\n",
      "Iteration 5138, loss = 0.47192518\n",
      "Iteration 5139, loss = 0.47212105\n",
      "Iteration 5140, loss = 0.47179693\n",
      "Iteration 5141, loss = 0.47197135\n",
      "Iteration 5142, loss = 0.47162709\n",
      "Iteration 5143, loss = 0.47186985\n",
      "Iteration 5144, loss = 0.47172455\n",
      "Iteration 5145, loss = 0.47178809\n",
      "Iteration 5146, loss = 0.47173421\n",
      "Iteration 5147, loss = 0.47165723\n",
      "Iteration 5148, loss = 0.47254061\n",
      "Iteration 5149, loss = 0.47181358\n",
      "Iteration 5150, loss = 0.47172680\n",
      "Iteration 5151, loss = 0.47148568\n",
      "Iteration 5152, loss = 0.47206659\n",
      "Iteration 5153, loss = 0.47299703\n",
      "Iteration 5154, loss = 0.47298900\n",
      "Iteration 5155, loss = 0.47220065\n",
      "Iteration 5156, loss = 0.47214350\n",
      "Iteration 5157, loss = 0.47159362\n",
      "Iteration 5158, loss = 0.47165347\n",
      "Iteration 5159, loss = 0.47207720\n",
      "Iteration 5160, loss = 0.47245928\n",
      "Iteration 5161, loss = 0.47168711\n",
      "Iteration 5162, loss = 0.47168026\n",
      "Iteration 5163, loss = 0.47170578\n",
      "Iteration 5164, loss = 0.47168538\n",
      "Iteration 5165, loss = 0.47191089\n",
      "Iteration 5166, loss = 0.47218054\n",
      "Iteration 5167, loss = 0.47225179\n",
      "Iteration 5168, loss = 0.47256619\n",
      "Iteration 5169, loss = 0.47196363\n",
      "Iteration 5170, loss = 0.47176891\n",
      "Iteration 5171, loss = 0.47191339\n",
      "Iteration 5172, loss = 0.47165042\n",
      "Iteration 5173, loss = 0.47183312\n",
      "Iteration 5174, loss = 0.47178505\n",
      "Iteration 5175, loss = 0.47165011\n",
      "Iteration 5176, loss = 0.47207093\n",
      "Iteration 5177, loss = 0.47205138\n",
      "Iteration 5178, loss = 0.47170960\n",
      "Iteration 5179, loss = 0.47207405\n",
      "Iteration 5180, loss = 0.47177532\n",
      "Iteration 5181, loss = 0.47145331\n",
      "Iteration 5182, loss = 0.47221690\n",
      "Iteration 5183, loss = 0.47220413\n",
      "Iteration 5184, loss = 0.47200071\n",
      "Iteration 5185, loss = 0.47200651\n",
      "Iteration 5186, loss = 0.47172056\n",
      "Iteration 5187, loss = 0.47196341\n",
      "Iteration 5188, loss = 0.47239759\n",
      "Iteration 5189, loss = 0.47169600\n",
      "Iteration 5190, loss = 0.47197229\n",
      "Iteration 5191, loss = 0.47155537\n",
      "Iteration 5192, loss = 0.47163958\n",
      "Iteration 5193, loss = 0.47214442\n",
      "Iteration 5194, loss = 0.47251474\n",
      "Iteration 5195, loss = 0.47238540\n",
      "Iteration 5196, loss = 0.47185497\n",
      "Iteration 5197, loss = 0.47221102\n",
      "Iteration 5198, loss = 0.47178438\n",
      "Iteration 5199, loss = 0.47194388\n",
      "Iteration 5200, loss = 0.47181096\n",
      "Iteration 5201, loss = 0.47184660\n",
      "Iteration 5202, loss = 0.47211133\n",
      "Iteration 5203, loss = 0.47228375\n",
      "Iteration 5204, loss = 0.47178901\n",
      "Iteration 5205, loss = 0.47171805\n",
      "Iteration 5206, loss = 0.47191484\n",
      "Iteration 5207, loss = 0.47194350\n",
      "Iteration 5208, loss = 0.47198836\n",
      "Iteration 5209, loss = 0.47211725\n",
      "Iteration 5210, loss = 0.47189778\n",
      "Iteration 5211, loss = 0.47173255\n",
      "Iteration 5212, loss = 0.47167395\n",
      "Iteration 5213, loss = 0.47161887\n",
      "Iteration 5214, loss = 0.47241286\n",
      "Iteration 5215, loss = 0.47160242\n",
      "Iteration 5216, loss = 0.47157165\n",
      "Iteration 5217, loss = 0.47182649\n",
      "Iteration 5218, loss = 0.47206936\n",
      "Iteration 5219, loss = 0.47232865\n",
      "Iteration 5220, loss = 0.47183431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5221, loss = 0.47172950\n",
      "Iteration 5222, loss = 0.47180007\n",
      "Iteration 5223, loss = 0.47212036\n",
      "Iteration 5224, loss = 0.47175610\n",
      "Iteration 5225, loss = 0.47162753\n",
      "Iteration 5226, loss = 0.47187641\n",
      "Iteration 5227, loss = 0.47185629\n",
      "Iteration 5228, loss = 0.47219268\n",
      "Iteration 5229, loss = 0.47192136\n",
      "Iteration 5230, loss = 0.47162442\n",
      "Iteration 5231, loss = 0.47179116\n",
      "Iteration 5232, loss = 0.47201901\n",
      "Iteration 5233, loss = 0.47207121\n",
      "Iteration 5234, loss = 0.47216487\n",
      "Iteration 5235, loss = 0.47179626\n",
      "Iteration 5236, loss = 0.47192079\n",
      "Iteration 5237, loss = 0.47181686\n",
      "Iteration 5238, loss = 0.47182977\n",
      "Iteration 5239, loss = 0.47173865\n",
      "Iteration 5240, loss = 0.47155955\n",
      "Iteration 5241, loss = 0.47193663\n",
      "Iteration 5242, loss = 0.47177177\n",
      "Iteration 5243, loss = 0.47193679\n",
      "Iteration 5244, loss = 0.47174353\n",
      "Iteration 5245, loss = 0.47166292\n",
      "Iteration 5246, loss = 0.47173434\n",
      "Iteration 5247, loss = 0.47158202\n",
      "Iteration 5248, loss = 0.47197602\n",
      "Iteration 5249, loss = 0.47171814\n",
      "Iteration 5250, loss = 0.47175582\n",
      "Iteration 5251, loss = 0.47164986\n",
      "Iteration 5252, loss = 0.47192699\n",
      "Iteration 5253, loss = 0.47176966\n",
      "Iteration 5254, loss = 0.47166368\n",
      "Iteration 5255, loss = 0.47170170\n",
      "Iteration 5256, loss = 0.47178165\n",
      "Iteration 5257, loss = 0.47169022\n",
      "Iteration 5258, loss = 0.47175199\n",
      "Iteration 5259, loss = 0.47170598\n",
      "Iteration 5260, loss = 0.47172173\n",
      "Iteration 5261, loss = 0.47200147\n",
      "Iteration 5262, loss = 0.47194712\n",
      "Iteration 5263, loss = 0.47199672\n",
      "Iteration 5264, loss = 0.47216245\n",
      "Iteration 5265, loss = 0.47163166\n",
      "Iteration 5266, loss = 0.47150562\n",
      "Iteration 5267, loss = 0.47199033\n",
      "Iteration 5268, loss = 0.47187585\n",
      "Iteration 5269, loss = 0.47169016\n",
      "Iteration 5270, loss = 0.47147886\n",
      "Iteration 5271, loss = 0.47191694\n",
      "Iteration 5272, loss = 0.47197402\n",
      "Iteration 5273, loss = 0.47205479\n",
      "Iteration 5274, loss = 0.47182833\n",
      "Iteration 5275, loss = 0.47195894\n",
      "Iteration 5276, loss = 0.47180996\n",
      "Iteration 5277, loss = 0.47176315\n",
      "Iteration 5278, loss = 0.47169883\n",
      "Iteration 5279, loss = 0.47164286\n",
      "Iteration 5280, loss = 0.47197853\n",
      "Iteration 5281, loss = 0.47196543\n",
      "Iteration 5282, loss = 0.47169042\n",
      "Iteration 5283, loss = 0.47155655\n",
      "Iteration 5284, loss = 0.47158031\n",
      "Iteration 5285, loss = 0.47192725\n",
      "Iteration 5286, loss = 0.47188921\n",
      "Iteration 5287, loss = 0.47174559\n",
      "Iteration 5288, loss = 0.47136499\n",
      "Iteration 5289, loss = 0.47240525\n",
      "Iteration 5290, loss = 0.47251552\n",
      "Iteration 5291, loss = 0.47255889\n",
      "Iteration 5292, loss = 0.47194367\n",
      "Iteration 5293, loss = 0.47165219\n",
      "Iteration 5294, loss = 0.47162614\n",
      "Iteration 5295, loss = 0.47172969\n",
      "Iteration 5296, loss = 0.47169681\n",
      "Iteration 5297, loss = 0.47162940\n",
      "Iteration 5298, loss = 0.47163758\n",
      "Iteration 5299, loss = 0.47165699\n",
      "Iteration 5300, loss = 0.47174205\n",
      "Iteration 5301, loss = 0.47166331\n",
      "Iteration 5302, loss = 0.47160547\n",
      "Iteration 5303, loss = 0.47227502\n",
      "Iteration 5304, loss = 0.47192758\n",
      "Iteration 5305, loss = 0.47152119\n",
      "Iteration 5306, loss = 0.47179584\n",
      "Iteration 5307, loss = 0.47212644\n",
      "Iteration 5308, loss = 0.47210958\n",
      "Iteration 5309, loss = 0.47264654\n",
      "Iteration 5310, loss = 0.47175285\n",
      "Iteration 5311, loss = 0.47181607\n",
      "Iteration 5312, loss = 0.47179927\n",
      "Iteration 5313, loss = 0.47164030\n",
      "Iteration 5314, loss = 0.47156117\n",
      "Iteration 5315, loss = 0.47163122\n",
      "Iteration 5316, loss = 0.47206020\n",
      "Iteration 5317, loss = 0.47220505\n",
      "Iteration 5318, loss = 0.47227058\n",
      "Iteration 5319, loss = 0.47170009\n",
      "Iteration 5320, loss = 0.47134459\n",
      "Iteration 5321, loss = 0.47260531\n",
      "Iteration 5322, loss = 0.47248088\n",
      "Iteration 5323, loss = 0.47221117\n",
      "Iteration 5324, loss = 0.47182510\n",
      "Iteration 5325, loss = 0.47180561\n",
      "Iteration 5326, loss = 0.47171028\n",
      "Iteration 5327, loss = 0.47183958\n",
      "Iteration 5328, loss = 0.47172634\n",
      "Iteration 5329, loss = 0.47160622\n",
      "Iteration 5330, loss = 0.47167680\n",
      "Iteration 5331, loss = 0.47163202\n",
      "Iteration 5332, loss = 0.47168613\n",
      "Iteration 5333, loss = 0.47172834\n",
      "Iteration 5334, loss = 0.47157902\n",
      "Iteration 5335, loss = 0.47160920\n",
      "Iteration 5336, loss = 0.47173576\n",
      "Iteration 5337, loss = 0.47172423\n",
      "Iteration 5338, loss = 0.47161353\n",
      "Iteration 5339, loss = 0.47166478\n",
      "Iteration 5340, loss = 0.47168997\n",
      "Iteration 5341, loss = 0.47164628\n",
      "Iteration 5342, loss = 0.47180582\n",
      "Iteration 5343, loss = 0.47217243\n",
      "Iteration 5344, loss = 0.47200002\n",
      "Iteration 5345, loss = 0.47163887\n",
      "Iteration 5346, loss = 0.47173273\n",
      "Iteration 5347, loss = 0.47171826\n",
      "Iteration 5348, loss = 0.47169133\n",
      "Iteration 5349, loss = 0.47173210\n",
      "Iteration 5350, loss = 0.47183808\n",
      "Iteration 5351, loss = 0.47165833\n",
      "Iteration 5352, loss = 0.47164729\n",
      "Iteration 5353, loss = 0.47163168\n",
      "Iteration 5354, loss = 0.47164913\n",
      "Iteration 5355, loss = 0.47173527\n",
      "Iteration 5356, loss = 0.47204402\n",
      "Iteration 5357, loss = 0.47173968\n",
      "Iteration 5358, loss = 0.47147215\n",
      "Iteration 5359, loss = 0.47160183\n",
      "Iteration 5360, loss = 0.47196618\n",
      "Iteration 5361, loss = 0.47228451\n",
      "Iteration 5362, loss = 0.47191823\n",
      "Iteration 5363, loss = 0.47192639\n",
      "Iteration 5364, loss = 0.47179389\n",
      "Iteration 5365, loss = 0.47187661\n",
      "Iteration 5366, loss = 0.47179056\n",
      "Iteration 5367, loss = 0.47174204\n",
      "Iteration 5368, loss = 0.47157290\n",
      "Iteration 5369, loss = 0.47168555\n",
      "Iteration 5370, loss = 0.47168884\n",
      "Iteration 5371, loss = 0.47178736\n",
      "Iteration 5372, loss = 0.47215883\n",
      "Iteration 5373, loss = 0.47198456\n",
      "Iteration 5374, loss = 0.47201563\n",
      "Iteration 5375, loss = 0.47178599\n",
      "Iteration 5376, loss = 0.47169925\n",
      "Iteration 5377, loss = 0.47167600\n",
      "Iteration 5378, loss = 0.47176671\n",
      "Iteration 5379, loss = 0.47185251\n",
      "Iteration 5380, loss = 0.47176102\n",
      "Iteration 5381, loss = 0.47177389\n",
      "Iteration 5382, loss = 0.47179794\n",
      "Iteration 5383, loss = 0.47176783\n",
      "Iteration 5384, loss = 0.47163563\n",
      "Iteration 5385, loss = 0.47155758\n",
      "Iteration 5386, loss = 0.47156709\n",
      "Iteration 5387, loss = 0.47197522\n",
      "Iteration 5388, loss = 0.47172750\n",
      "Iteration 5389, loss = 0.47159728\n",
      "Iteration 5390, loss = 0.47162879\n",
      "Iteration 5391, loss = 0.47175018\n",
      "Iteration 5392, loss = 0.47165088\n",
      "Iteration 5393, loss = 0.47187512\n",
      "Iteration 5394, loss = 0.47155367\n",
      "Iteration 5395, loss = 0.47202652\n",
      "Iteration 5396, loss = 0.47163460\n",
      "Iteration 5397, loss = 0.47234442\n",
      "Iteration 5398, loss = 0.47184210\n",
      "Iteration 5399, loss = 0.47189267\n",
      "Iteration 5400, loss = 0.47188667\n",
      "Iteration 5401, loss = 0.47176341\n",
      "Iteration 5402, loss = 0.47176433\n",
      "Iteration 5403, loss = 0.47170677\n",
      "Iteration 5404, loss = 0.47180419\n",
      "Iteration 5405, loss = 0.47164873\n",
      "Iteration 5406, loss = 0.47186751\n",
      "Iteration 5407, loss = 0.47186672\n",
      "Iteration 5408, loss = 0.47159763\n",
      "Iteration 5409, loss = 0.47164091\n",
      "Iteration 5410, loss = 0.47159011\n",
      "Iteration 5411, loss = 0.47153764\n",
      "Iteration 5412, loss = 0.47178850\n",
      "Iteration 5413, loss = 0.47205266\n",
      "Iteration 5414, loss = 0.47180050\n",
      "Iteration 5415, loss = 0.47206177\n",
      "Iteration 5416, loss = 0.47170995\n",
      "Iteration 5417, loss = 0.47179019\n",
      "Iteration 5418, loss = 0.47168018\n",
      "Iteration 5419, loss = 0.47181584\n",
      "Iteration 5420, loss = 0.47163161\n",
      "Iteration 5421, loss = 0.47174932\n",
      "Iteration 5422, loss = 0.47166036\n",
      "Iteration 5423, loss = 0.47168770\n",
      "Iteration 5424, loss = 0.47202628\n",
      "Iteration 5425, loss = 0.47135626\n",
      "Iteration 5426, loss = 0.47154500\n",
      "Iteration 5427, loss = 0.47250900\n",
      "Iteration 5428, loss = 0.47215398\n",
      "Iteration 5429, loss = 0.47166405\n",
      "Iteration 5430, loss = 0.47130354\n",
      "Iteration 5431, loss = 0.47258066\n",
      "Iteration 5432, loss = 0.47268970\n",
      "Iteration 5433, loss = 0.47253800\n",
      "Iteration 5434, loss = 0.47175490\n",
      "Iteration 5435, loss = 0.47172595\n",
      "Iteration 5436, loss = 0.47176875\n",
      "Iteration 5437, loss = 0.47332821\n",
      "Iteration 5438, loss = 0.47240188\n",
      "Iteration 5439, loss = 0.47189667\n",
      "Iteration 5440, loss = 0.47129294\n",
      "Iteration 5441, loss = 0.47186235\n",
      "Iteration 5442, loss = 0.47270724\n",
      "Iteration 5443, loss = 0.47342137\n",
      "Iteration 5444, loss = 0.47312677\n",
      "Iteration 5445, loss = 0.47268271\n",
      "Iteration 5446, loss = 0.47227762\n",
      "Iteration 5447, loss = 0.47159399\n",
      "Iteration 5448, loss = 0.47150851\n",
      "Iteration 5449, loss = 0.47179481\n",
      "Iteration 5450, loss = 0.47215342\n",
      "Iteration 5451, loss = 0.47233588\n",
      "Iteration 5452, loss = 0.47207283\n",
      "Iteration 5453, loss = 0.47194344\n",
      "Iteration 5454, loss = 0.47152469\n",
      "Iteration 5455, loss = 0.47156041\n",
      "Iteration 5456, loss = 0.47214512\n",
      "Iteration 5457, loss = 0.47272067\n",
      "Iteration 5458, loss = 0.47249435\n",
      "Iteration 5459, loss = 0.47205046\n",
      "Iteration 5460, loss = 0.47210845\n",
      "Iteration 5461, loss = 0.47182541\n",
      "Iteration 5462, loss = 0.47194185\n",
      "Iteration 5463, loss = 0.47181304\n",
      "Iteration 5464, loss = 0.47178571\n",
      "Iteration 5465, loss = 0.47213399\n",
      "Iteration 5466, loss = 0.47213276\n",
      "Iteration 5467, loss = 0.47191788\n",
      "Iteration 5468, loss = 0.47171383\n",
      "Iteration 5469, loss = 0.47167872\n",
      "Iteration 5470, loss = 0.47208277\n",
      "Iteration 5471, loss = 0.47214478\n",
      "Iteration 5472, loss = 0.47198694\n",
      "Iteration 5473, loss = 0.47259747\n",
      "Iteration 5474, loss = 0.47163306\n",
      "Iteration 5475, loss = 0.47185110\n",
      "Iteration 5476, loss = 0.47192369\n",
      "Iteration 5477, loss = 0.47172386\n",
      "Iteration 5478, loss = 0.47175374\n",
      "Iteration 5479, loss = 0.47170901\n",
      "Iteration 5480, loss = 0.47161036\n",
      "Iteration 5481, loss = 0.47153799\n",
      "Iteration 5482, loss = 0.47174361\n",
      "Iteration 5483, loss = 0.47185212\n",
      "Iteration 5484, loss = 0.47193563\n",
      "Iteration 5485, loss = 0.47207083\n",
      "Iteration 5486, loss = 0.47156071\n",
      "Iteration 5487, loss = 0.47171597\n",
      "Iteration 5488, loss = 0.47187014\n",
      "Iteration 5489, loss = 0.47237792\n",
      "Iteration 5490, loss = 0.47189622\n",
      "Iteration 5491, loss = 0.47216337\n",
      "Iteration 5492, loss = 0.47204023\n",
      "Iteration 5493, loss = 0.47150457\n",
      "Iteration 5494, loss = 0.47205671\n",
      "Iteration 5495, loss = 0.47221596\n",
      "Iteration 5496, loss = 0.47171585\n",
      "Iteration 5497, loss = 0.47152368\n",
      "Iteration 5498, loss = 0.47187741\n",
      "Iteration 5499, loss = 0.47181226\n",
      "Iteration 5500, loss = 0.47186724\n",
      "Iteration 5501, loss = 0.47175332\n",
      "Iteration 5502, loss = 0.47157156\n",
      "Iteration 5503, loss = 0.47211255\n",
      "Iteration 5504, loss = 0.47161651\n",
      "Iteration 5505, loss = 0.47160334\n",
      "Iteration 5506, loss = 0.47156445\n",
      "Iteration 5507, loss = 0.47157248\n",
      "Iteration 5508, loss = 0.47158260\n",
      "Iteration 5509, loss = 0.47158351\n",
      "Iteration 5510, loss = 0.47162429\n",
      "Iteration 5511, loss = 0.47160829\n",
      "Iteration 5512, loss = 0.47158900\n",
      "Iteration 5513, loss = 0.47147217\n",
      "Iteration 5514, loss = 0.47206164\n",
      "Iteration 5515, loss = 0.47169711\n",
      "Iteration 5516, loss = 0.47167695\n",
      "Iteration 5517, loss = 0.47160587\n",
      "Iteration 5518, loss = 0.47181559\n",
      "Iteration 5519, loss = 0.47181115\n",
      "Iteration 5520, loss = 0.47156212\n",
      "Iteration 5521, loss = 0.47179626\n",
      "Iteration 5522, loss = 0.47176591\n",
      "Iteration 5523, loss = 0.47188231\n",
      "Iteration 5524, loss = 0.47185476\n",
      "Iteration 5525, loss = 0.47180725\n",
      "Iteration 5526, loss = 0.47224887\n",
      "Iteration 5527, loss = 0.47179678\n",
      "Iteration 5528, loss = 0.47179544\n",
      "Iteration 5529, loss = 0.47186871\n",
      "Iteration 5530, loss = 0.47185919\n",
      "Iteration 5531, loss = 0.47189487\n",
      "Iteration 5532, loss = 0.47184334\n",
      "Iteration 5533, loss = 0.47164142\n",
      "Iteration 5534, loss = 0.47162334\n",
      "Iteration 5535, loss = 0.47155893\n",
      "Iteration 5536, loss = 0.47169952\n",
      "Iteration 5537, loss = 0.47164804\n",
      "Iteration 5538, loss = 0.47165802\n",
      "Iteration 5539, loss = 0.47175273\n",
      "Iteration 5540, loss = 0.47244366\n",
      "Iteration 5541, loss = 0.47170853\n",
      "Iteration 5542, loss = 0.47169446\n",
      "Iteration 5543, loss = 0.47172428\n",
      "Iteration 5544, loss = 0.47171075\n",
      "Iteration 5545, loss = 0.47159825\n",
      "Iteration 5546, loss = 0.47176940\n",
      "Iteration 5547, loss = 0.47184405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5548, loss = 0.47180863\n",
      "Iteration 5549, loss = 0.47158448\n",
      "Iteration 5550, loss = 0.47153854\n",
      "Iteration 5551, loss = 0.47183875\n",
      "Iteration 5552, loss = 0.47156426\n",
      "Iteration 5553, loss = 0.47165472\n",
      "Iteration 5554, loss = 0.47189568\n",
      "Iteration 5555, loss = 0.47187721\n",
      "Iteration 5556, loss = 0.47166653\n",
      "Iteration 5557, loss = 0.47151276\n",
      "Iteration 5558, loss = 0.47157063\n",
      "Iteration 5559, loss = 0.47202753\n",
      "Iteration 5560, loss = 0.47244925\n",
      "Iteration 5561, loss = 0.47238238\n",
      "Iteration 5562, loss = 0.47187030\n",
      "Iteration 5563, loss = 0.47194282\n",
      "Iteration 5564, loss = 0.47201325\n",
      "Iteration 5565, loss = 0.47172172\n",
      "Iteration 5566, loss = 0.47159277\n",
      "Iteration 5567, loss = 0.47165905\n",
      "Iteration 5568, loss = 0.47167485\n",
      "Iteration 5569, loss = 0.47193107\n",
      "Iteration 5570, loss = 0.47182319\n",
      "Iteration 5571, loss = 0.47187366\n",
      "Iteration 5572, loss = 0.47202684\n",
      "Iteration 5573, loss = 0.47199762\n",
      "Iteration 5574, loss = 0.47222828\n",
      "Iteration 5575, loss = 0.47192293\n",
      "Iteration 5576, loss = 0.47177131\n",
      "Iteration 5577, loss = 0.47157384\n",
      "Iteration 5578, loss = 0.47186894\n",
      "Iteration 5579, loss = 0.47175873\n",
      "Iteration 5580, loss = 0.47173706\n",
      "Iteration 5581, loss = 0.47169659\n",
      "Iteration 5582, loss = 0.47172794\n",
      "Iteration 5583, loss = 0.47211501\n",
      "Iteration 5584, loss = 0.47176767\n",
      "Iteration 5585, loss = 0.47172221\n",
      "Iteration 5586, loss = 0.47185811\n",
      "Iteration 5587, loss = 0.47155419\n",
      "Iteration 5588, loss = 0.47155058\n",
      "Iteration 5589, loss = 0.47169757\n",
      "Iteration 5590, loss = 0.47148517\n",
      "Iteration 5591, loss = 0.47166158\n",
      "Iteration 5592, loss = 0.47159274\n",
      "Iteration 5593, loss = 0.47170254\n",
      "Iteration 5594, loss = 0.47179314\n",
      "Iteration 5595, loss = 0.47150663\n",
      "Iteration 5596, loss = 0.47141812\n",
      "Iteration 5597, loss = 0.47172370\n",
      "Iteration 5598, loss = 0.47194583\n",
      "Iteration 5599, loss = 0.47203819\n",
      "Iteration 5600, loss = 0.47199393\n",
      "Iteration 5601, loss = 0.47196238\n",
      "Iteration 5602, loss = 0.47169925\n",
      "Iteration 5603, loss = 0.47168196\n",
      "Iteration 5604, loss = 0.47181101\n",
      "Iteration 5605, loss = 0.47198603\n",
      "Iteration 5606, loss = 0.47176942\n",
      "Iteration 5607, loss = 0.47149229\n",
      "Iteration 5608, loss = 0.47145286\n",
      "Iteration 5609, loss = 0.47209204\n",
      "Iteration 5610, loss = 0.47229091\n",
      "Iteration 5611, loss = 0.47194655\n",
      "Iteration 5612, loss = 0.47183901\n",
      "Iteration 5613, loss = 0.47163569\n",
      "Iteration 5614, loss = 0.47180500\n",
      "Iteration 5615, loss = 0.47203788\n",
      "Iteration 5616, loss = 0.47179164\n",
      "Iteration 5617, loss = 0.47132877\n",
      "Iteration 5618, loss = 0.47205135\n",
      "Iteration 5619, loss = 0.47217691\n",
      "Iteration 5620, loss = 0.47221409\n",
      "Iteration 5621, loss = 0.47191677\n",
      "Iteration 5622, loss = 0.47180932\n",
      "Iteration 5623, loss = 0.47168047\n",
      "Iteration 5624, loss = 0.47155141\n",
      "Iteration 5625, loss = 0.47169550\n",
      "Iteration 5626, loss = 0.47151911\n",
      "Iteration 5627, loss = 0.47168766\n",
      "Iteration 5628, loss = 0.47183497\n",
      "Iteration 5629, loss = 0.47157805\n",
      "Iteration 5630, loss = 0.47150628\n",
      "Iteration 5631, loss = 0.47159326\n",
      "Iteration 5632, loss = 0.47171232\n",
      "Iteration 5633, loss = 0.47237907\n",
      "Iteration 5634, loss = 0.47187092\n",
      "Iteration 5635, loss = 0.47231791\n",
      "Iteration 5636, loss = 0.47216792\n",
      "Iteration 5637, loss = 0.47159606\n",
      "Iteration 5638, loss = 0.47239120\n",
      "Iteration 5639, loss = 0.47190590\n",
      "Iteration 5640, loss = 0.47265351\n",
      "Iteration 5641, loss = 0.47204870\n",
      "Iteration 5642, loss = 0.47153960\n",
      "Iteration 5643, loss = 0.47129718\n",
      "Iteration 5644, loss = 0.47200792\n",
      "Iteration 5645, loss = 0.47243055\n",
      "Iteration 5646, loss = 0.47210210\n",
      "Iteration 5647, loss = 0.47236756\n",
      "Iteration 5648, loss = 0.47199985\n",
      "Iteration 5649, loss = 0.47156874\n",
      "Iteration 5650, loss = 0.47166244\n",
      "Iteration 5651, loss = 0.47215525\n",
      "Iteration 5652, loss = 0.47235432\n",
      "Iteration 5653, loss = 0.47225639\n",
      "Iteration 5654, loss = 0.47207624\n",
      "Iteration 5655, loss = 0.47162511\n",
      "Iteration 5656, loss = 0.47154119\n",
      "Iteration 5657, loss = 0.47177838\n",
      "Iteration 5658, loss = 0.47169772\n",
      "Iteration 5659, loss = 0.47226899\n",
      "Iteration 5660, loss = 0.47217020\n",
      "Iteration 5661, loss = 0.47183340\n",
      "Iteration 5662, loss = 0.47128821\n",
      "Iteration 5663, loss = 0.47162243\n",
      "Iteration 5664, loss = 0.47220054\n",
      "Iteration 5665, loss = 0.47250056\n",
      "Iteration 5666, loss = 0.47223344\n",
      "Iteration 5667, loss = 0.47180134\n",
      "Iteration 5668, loss = 0.47135487\n",
      "Iteration 5669, loss = 0.47178438\n",
      "Iteration 5670, loss = 0.47211540\n",
      "Iteration 5671, loss = 0.47186949\n",
      "Iteration 5672, loss = 0.47181891\n",
      "Iteration 5673, loss = 0.47140695\n",
      "Iteration 5674, loss = 0.47270392\n",
      "Iteration 5675, loss = 0.47165809\n",
      "Iteration 5676, loss = 0.47257689\n",
      "Iteration 5677, loss = 0.47165551\n",
      "Iteration 5678, loss = 0.47148166\n",
      "Iteration 5679, loss = 0.47156849\n",
      "Iteration 5680, loss = 0.47166458\n",
      "Iteration 5681, loss = 0.47155738\n",
      "Iteration 5682, loss = 0.47158796\n",
      "Iteration 5683, loss = 0.47154822\n",
      "Iteration 5684, loss = 0.47151511\n",
      "Iteration 5685, loss = 0.47158086\n",
      "Iteration 5686, loss = 0.47160995\n",
      "Iteration 5687, loss = 0.47160997\n",
      "Iteration 5688, loss = 0.47149791\n",
      "Iteration 5689, loss = 0.47156284\n",
      "Iteration 5690, loss = 0.47159710\n",
      "Iteration 5691, loss = 0.47179562\n",
      "Iteration 5692, loss = 0.47171668\n",
      "Iteration 5693, loss = 0.47202181\n",
      "Iteration 5694, loss = 0.47182957\n",
      "Iteration 5695, loss = 0.47169613\n",
      "Iteration 5696, loss = 0.47167417\n",
      "Iteration 5697, loss = 0.47167449\n",
      "Iteration 5698, loss = 0.47152585\n",
      "Iteration 5699, loss = 0.47148592\n",
      "Iteration 5700, loss = 0.47140265\n",
      "Iteration 5701, loss = 0.47172000\n",
      "Iteration 5702, loss = 0.47156383\n",
      "Iteration 5703, loss = 0.47160339\n",
      "Iteration 5704, loss = 0.47170525\n",
      "Iteration 5705, loss = 0.47157752\n",
      "Iteration 5706, loss = 0.47199505\n",
      "Iteration 5707, loss = 0.47185075\n",
      "Iteration 5708, loss = 0.47147228\n",
      "Iteration 5709, loss = 0.47155450\n",
      "Iteration 5710, loss = 0.47155106\n",
      "Iteration 5711, loss = 0.47201197\n",
      "Iteration 5712, loss = 0.47185725\n",
      "Iteration 5713, loss = 0.47156582\n",
      "Iteration 5714, loss = 0.47144182\n",
      "Iteration 5715, loss = 0.47228085\n",
      "Iteration 5716, loss = 0.47168763\n",
      "Iteration 5717, loss = 0.47161094\n",
      "Iteration 5718, loss = 0.47156487\n",
      "Iteration 5719, loss = 0.47167723\n",
      "Iteration 5720, loss = 0.47167414\n",
      "Iteration 5721, loss = 0.47176381\n",
      "Iteration 5722, loss = 0.47149658\n",
      "Iteration 5723, loss = 0.47137412\n",
      "Iteration 5724, loss = 0.47145416\n",
      "Iteration 5725, loss = 0.47158962\n",
      "Iteration 5726, loss = 0.47165356\n",
      "Iteration 5727, loss = 0.47155335\n",
      "Iteration 5728, loss = 0.47139743\n",
      "Iteration 5729, loss = 0.47134304\n",
      "Iteration 5730, loss = 0.47154998\n",
      "Iteration 5731, loss = 0.47197668\n",
      "Iteration 5732, loss = 0.47245455\n",
      "Iteration 5733, loss = 0.47181767\n",
      "Iteration 5734, loss = 0.47126938\n",
      "Iteration 5735, loss = 0.47185405\n",
      "Iteration 5736, loss = 0.47253137\n",
      "Iteration 5737, loss = 0.47228579\n",
      "Iteration 5738, loss = 0.47176145\n",
      "Iteration 5739, loss = 0.47121417\n",
      "Iteration 5740, loss = 0.47178642\n",
      "Iteration 5741, loss = 0.47226000\n",
      "Iteration 5742, loss = 0.47269072\n",
      "Iteration 5743, loss = 0.47234383\n",
      "Iteration 5744, loss = 0.47273497\n",
      "Iteration 5745, loss = 0.47178697\n",
      "Iteration 5746, loss = 0.47213618\n",
      "Iteration 5747, loss = 0.47326675\n",
      "Iteration 5748, loss = 0.47289804\n",
      "Iteration 5749, loss = 0.47180774\n",
      "Iteration 5750, loss = 0.47145219\n",
      "Iteration 5751, loss = 0.47140840\n",
      "Iteration 5752, loss = 0.47319161\n",
      "Iteration 5753, loss = 0.47309383\n",
      "Iteration 5754, loss = 0.47235816\n",
      "Iteration 5755, loss = 0.47166819\n",
      "Iteration 5756, loss = 0.47217948\n",
      "Iteration 5757, loss = 0.47171017\n",
      "Iteration 5758, loss = 0.47192349\n",
      "Iteration 5759, loss = 0.47203109\n",
      "Iteration 5760, loss = 0.47155795\n",
      "Iteration 5761, loss = 0.47162406\n",
      "Iteration 5762, loss = 0.47153520\n",
      "Iteration 5763, loss = 0.47158074\n",
      "Iteration 5764, loss = 0.47149715\n",
      "Iteration 5765, loss = 0.47144900\n",
      "Iteration 5766, loss = 0.47149322\n",
      "Iteration 5767, loss = 0.47166546\n",
      "Iteration 5768, loss = 0.47162068\n",
      "Iteration 5769, loss = 0.47156327\n",
      "Iteration 5770, loss = 0.47160631\n",
      "Iteration 5771, loss = 0.47151820\n",
      "Iteration 5772, loss = 0.47171438\n",
      "Iteration 5773, loss = 0.47147652\n",
      "Iteration 5774, loss = 0.47172652\n",
      "Iteration 5775, loss = 0.47151531\n",
      "Iteration 5776, loss = 0.47139632\n",
      "Iteration 5777, loss = 0.47141281\n",
      "Iteration 5778, loss = 0.47151493\n",
      "Iteration 5779, loss = 0.47165108\n",
      "Iteration 5780, loss = 0.47183367\n",
      "Iteration 5781, loss = 0.47190698\n",
      "Iteration 5782, loss = 0.47152929\n",
      "Iteration 5783, loss = 0.47164085\n",
      "Iteration 5784, loss = 0.47175733\n",
      "Iteration 5785, loss = 0.47145552\n",
      "Iteration 5786, loss = 0.47134655\n",
      "Iteration 5787, loss = 0.47168298\n",
      "Iteration 5788, loss = 0.47189418\n",
      "Iteration 5789, loss = 0.47191671\n",
      "Iteration 5790, loss = 0.47154328\n",
      "Iteration 5791, loss = 0.47156537\n",
      "Iteration 5792, loss = 0.47173760\n",
      "Iteration 5793, loss = 0.47165734\n",
      "Iteration 5794, loss = 0.47181390\n",
      "Iteration 5795, loss = 0.47181327\n",
      "Iteration 5796, loss = 0.47215133\n",
      "Iteration 5797, loss = 0.47154530\n",
      "Iteration 5798, loss = 0.47163247\n",
      "Iteration 5799, loss = 0.47154937\n",
      "Iteration 5800, loss = 0.47156989\n",
      "Iteration 5801, loss = 0.47171288\n",
      "Iteration 5802, loss = 0.47148241\n",
      "Iteration 5803, loss = 0.47148506\n",
      "Iteration 5804, loss = 0.47166350\n",
      "Iteration 5805, loss = 0.47180643\n",
      "Iteration 5806, loss = 0.47136635\n",
      "Iteration 5807, loss = 0.47182782\n",
      "Iteration 5808, loss = 0.47176020\n",
      "Iteration 5809, loss = 0.47144762\n",
      "Iteration 5810, loss = 0.47153909\n",
      "Iteration 5811, loss = 0.47159536\n",
      "Iteration 5812, loss = 0.47171747\n",
      "Iteration 5813, loss = 0.47151127\n",
      "Iteration 5814, loss = 0.47149164\n",
      "Iteration 5815, loss = 0.47168204\n",
      "Iteration 5816, loss = 0.47187518\n",
      "Iteration 5817, loss = 0.47190900\n",
      "Iteration 5818, loss = 0.47239768\n",
      "Iteration 5819, loss = 0.47180900\n",
      "Iteration 5820, loss = 0.47179633\n",
      "Iteration 5821, loss = 0.47205459\n",
      "Iteration 5822, loss = 0.47165775\n",
      "Iteration 5823, loss = 0.47166954\n",
      "Iteration 5824, loss = 0.47152016\n",
      "Iteration 5825, loss = 0.47189704\n",
      "Iteration 5826, loss = 0.47172044\n",
      "Iteration 5827, loss = 0.47166114\n",
      "Iteration 5828, loss = 0.47176608\n",
      "Iteration 5829, loss = 0.47164079\n",
      "Iteration 5830, loss = 0.47144875\n",
      "Iteration 5831, loss = 0.47158457\n",
      "Iteration 5832, loss = 0.47130214\n",
      "Iteration 5833, loss = 0.47193439\n",
      "Iteration 5834, loss = 0.47209454\n",
      "Iteration 5835, loss = 0.47170018\n",
      "Iteration 5836, loss = 0.47174405\n",
      "Iteration 5837, loss = 0.47137877\n",
      "Iteration 5838, loss = 0.47166101\n",
      "Iteration 5839, loss = 0.47146760\n",
      "Iteration 5840, loss = 0.47140401\n",
      "Iteration 5841, loss = 0.47146458\n",
      "Iteration 5842, loss = 0.47154581\n",
      "Iteration 5843, loss = 0.47145406\n",
      "Iteration 5844, loss = 0.47160821\n",
      "Iteration 5845, loss = 0.47162550\n",
      "Iteration 5846, loss = 0.47144946\n",
      "Iteration 5847, loss = 0.47134229\n",
      "Iteration 5848, loss = 0.47152970\n",
      "Iteration 5849, loss = 0.47168828\n",
      "Iteration 5850, loss = 0.47162244\n",
      "Iteration 5851, loss = 0.47139827\n",
      "Iteration 5852, loss = 0.47173032\n",
      "Iteration 5853, loss = 0.47154800\n",
      "Iteration 5854, loss = 0.47163765\n",
      "Iteration 5855, loss = 0.47159545\n",
      "Iteration 5856, loss = 0.47160285\n",
      "Iteration 5857, loss = 0.47155388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5858, loss = 0.47148611\n",
      "Iteration 5859, loss = 0.47139820\n",
      "Iteration 5860, loss = 0.47144024\n",
      "Iteration 5861, loss = 0.47146231\n",
      "Iteration 5862, loss = 0.47154982\n",
      "Iteration 5863, loss = 0.47157877\n",
      "Iteration 5864, loss = 0.47144141\n",
      "Iteration 5865, loss = 0.47139842\n",
      "Iteration 5866, loss = 0.47144725\n",
      "Iteration 5867, loss = 0.47148799\n",
      "Iteration 5868, loss = 0.47143398\n",
      "Iteration 5869, loss = 0.47138820\n",
      "Iteration 5870, loss = 0.47138211\n",
      "Iteration 5871, loss = 0.47172637\n",
      "Iteration 5872, loss = 0.47134414\n",
      "Iteration 5873, loss = 0.47146272\n",
      "Iteration 5874, loss = 0.47239126\n",
      "Iteration 5875, loss = 0.47245123\n",
      "Iteration 5876, loss = 0.47186138\n",
      "Iteration 5877, loss = 0.47190763\n",
      "Iteration 5878, loss = 0.47191018\n",
      "Iteration 5879, loss = 0.47189595\n",
      "Iteration 5880, loss = 0.47172342\n",
      "Iteration 5881, loss = 0.47168521\n",
      "Iteration 5882, loss = 0.47146923\n",
      "Iteration 5883, loss = 0.47153404\n",
      "Iteration 5884, loss = 0.47157647\n",
      "Iteration 5885, loss = 0.47182908\n",
      "Iteration 5886, loss = 0.47164840\n",
      "Iteration 5887, loss = 0.47158331\n",
      "Iteration 5888, loss = 0.47166104\n",
      "Iteration 5889, loss = 0.47184080\n",
      "Iteration 5890, loss = 0.47190544\n",
      "Iteration 5891, loss = 0.47164651\n",
      "Iteration 5892, loss = 0.47191225\n",
      "Iteration 5893, loss = 0.47151242\n",
      "Iteration 5894, loss = 0.47165046\n",
      "Iteration 5895, loss = 0.47181513\n",
      "Iteration 5896, loss = 0.47184815\n",
      "Iteration 5897, loss = 0.47171455\n",
      "Iteration 5898, loss = 0.47177408\n",
      "Iteration 5899, loss = 0.47159182\n",
      "Iteration 5900, loss = 0.47169907\n",
      "Iteration 5901, loss = 0.47157429\n",
      "Iteration 5902, loss = 0.47160105\n",
      "Iteration 5903, loss = 0.47156821\n",
      "Iteration 5904, loss = 0.47156355\n",
      "Iteration 5905, loss = 0.47151415\n",
      "Iteration 5906, loss = 0.47157223\n",
      "Iteration 5907, loss = 0.47151062\n",
      "Iteration 5908, loss = 0.47149852\n",
      "Iteration 5909, loss = 0.47163960\n",
      "Iteration 5910, loss = 0.47145523\n",
      "Iteration 5911, loss = 0.47135866\n",
      "Iteration 5912, loss = 0.47138890\n",
      "Iteration 5913, loss = 0.47166437\n",
      "Iteration 5914, loss = 0.47227716\n",
      "Iteration 5915, loss = 0.47238313\n",
      "Iteration 5916, loss = 0.47169685\n",
      "Iteration 5917, loss = 0.47122689\n",
      "Iteration 5918, loss = 0.47152216\n",
      "Iteration 5919, loss = 0.47231429\n",
      "Iteration 5920, loss = 0.47244727\n",
      "Iteration 5921, loss = 0.47195235\n",
      "Iteration 5922, loss = 0.47141341\n",
      "Iteration 5923, loss = 0.47129671\n",
      "Iteration 5924, loss = 0.47160775\n",
      "Iteration 5925, loss = 0.47242733\n",
      "Iteration 5926, loss = 0.47287378\n",
      "Iteration 5927, loss = 0.47179501\n",
      "Iteration 5928, loss = 0.47147655\n",
      "Iteration 5929, loss = 0.47219266\n",
      "Iteration 5930, loss = 0.47251755\n",
      "Iteration 5931, loss = 0.47247778\n",
      "Iteration 5932, loss = 0.47181731\n",
      "Iteration 5933, loss = 0.47149780\n",
      "Iteration 5934, loss = 0.47154676\n",
      "Iteration 5935, loss = 0.47173655\n",
      "Iteration 5936, loss = 0.47194217\n",
      "Iteration 5937, loss = 0.47202848\n",
      "Iteration 5938, loss = 0.47157168\n",
      "Iteration 5939, loss = 0.47141169\n",
      "Iteration 5940, loss = 0.47128358\n",
      "Iteration 5941, loss = 0.47171799\n",
      "Iteration 5942, loss = 0.47302583\n",
      "Iteration 5943, loss = 0.47321759\n",
      "Iteration 5944, loss = 0.47331777\n",
      "Iteration 5945, loss = 0.47192138\n",
      "Iteration 5946, loss = 0.47163824\n",
      "Iteration 5947, loss = 0.47146361\n",
      "Iteration 5948, loss = 0.47266937\n",
      "Iteration 5949, loss = 0.47162255\n",
      "Iteration 5950, loss = 0.47177262\n",
      "Iteration 5951, loss = 0.47193291\n",
      "Iteration 5952, loss = 0.47239794\n",
      "Iteration 5953, loss = 0.47268993\n",
      "Iteration 5954, loss = 0.47206220\n",
      "Iteration 5955, loss = 0.47186362\n",
      "Iteration 5956, loss = 0.47125579\n",
      "Iteration 5957, loss = 0.47145114\n",
      "Iteration 5958, loss = 0.47203372\n",
      "Iteration 5959, loss = 0.47223851\n",
      "Iteration 5960, loss = 0.47165046\n",
      "Iteration 5961, loss = 0.47196620\n",
      "Iteration 5962, loss = 0.47150441\n",
      "Iteration 5963, loss = 0.47165714\n",
      "Iteration 5964, loss = 0.47180919\n",
      "Iteration 5965, loss = 0.47188772\n",
      "Iteration 5966, loss = 0.47168222\n",
      "Iteration 5967, loss = 0.47151489\n",
      "Iteration 5968, loss = 0.47159366\n",
      "Iteration 5969, loss = 0.47192207\n",
      "Iteration 5970, loss = 0.47185691\n",
      "Iteration 5971, loss = 0.47213780\n",
      "Iteration 5972, loss = 0.47187704\n",
      "Iteration 5973, loss = 0.47155192\n",
      "Iteration 5974, loss = 0.47155030\n",
      "Iteration 5975, loss = 0.47182469\n",
      "Iteration 5976, loss = 0.47187745\n",
      "Iteration 5977, loss = 0.47158884\n",
      "Iteration 5978, loss = 0.47169722\n",
      "Iteration 5979, loss = 0.47157324\n",
      "Iteration 5980, loss = 0.47169137\n",
      "Iteration 5981, loss = 0.47186990\n",
      "Iteration 5982, loss = 0.47160272\n",
      "Iteration 5983, loss = 0.47163731\n",
      "Iteration 5984, loss = 0.47144177\n",
      "Iteration 5985, loss = 0.47139884\n",
      "Iteration 5986, loss = 0.47150141\n",
      "Iteration 5987, loss = 0.47149917\n",
      "Iteration 5988, loss = 0.47142505\n",
      "Iteration 5989, loss = 0.47137090\n",
      "Iteration 5990, loss = 0.47156916\n",
      "Iteration 5991, loss = 0.47143619\n",
      "Iteration 5992, loss = 0.47136716\n",
      "Iteration 5993, loss = 0.47128841\n",
      "Iteration 5994, loss = 0.47234956\n",
      "Iteration 5995, loss = 0.47166301\n",
      "Iteration 5996, loss = 0.47137234\n",
      "Iteration 5997, loss = 0.47155997\n",
      "Iteration 5998, loss = 0.47155423\n",
      "Iteration 5999, loss = 0.47170271\n",
      "Iteration 6000, loss = 0.47168579\n",
      "Iteration 6001, loss = 0.47206124\n",
      "Iteration 6002, loss = 0.47147668\n",
      "Iteration 6003, loss = 0.47177583\n",
      "Iteration 6004, loss = 0.47176141\n",
      "Iteration 6005, loss = 0.47219462\n",
      "Iteration 6006, loss = 0.47208375\n",
      "Iteration 6007, loss = 0.47163986\n",
      "Iteration 6008, loss = 0.47161448\n",
      "Iteration 6009, loss = 0.47154259\n",
      "Iteration 6010, loss = 0.47202024\n",
      "Iteration 6011, loss = 0.47222847\n",
      "Iteration 6012, loss = 0.47201089\n",
      "Iteration 6013, loss = 0.47228213\n",
      "Iteration 6014, loss = 0.47147751\n",
      "Iteration 6015, loss = 0.47150601\n",
      "Iteration 6016, loss = 0.47149031\n",
      "Iteration 6017, loss = 0.47143258\n",
      "Iteration 6018, loss = 0.47142449\n",
      "Iteration 6019, loss = 0.47139521\n",
      "Iteration 6020, loss = 0.47162523\n",
      "Iteration 6021, loss = 0.47132765\n",
      "Iteration 6022, loss = 0.47149070\n",
      "Iteration 6023, loss = 0.47170200\n",
      "Iteration 6024, loss = 0.47189243\n",
      "Iteration 6025, loss = 0.47155226\n",
      "Iteration 6026, loss = 0.47151400\n",
      "Iteration 6027, loss = 0.47141756\n",
      "Iteration 6028, loss = 0.47207399\n",
      "Iteration 6029, loss = 0.47194499\n",
      "Iteration 6030, loss = 0.47173556\n",
      "Iteration 6031, loss = 0.47143177\n",
      "Iteration 6032, loss = 0.47173032\n",
      "Iteration 6033, loss = 0.47144092\n",
      "Iteration 6034, loss = 0.47162872\n",
      "Iteration 6035, loss = 0.47156789\n",
      "Iteration 6036, loss = 0.47138208\n",
      "Iteration 6037, loss = 0.47178180\n",
      "Iteration 6038, loss = 0.47194545\n",
      "Iteration 6039, loss = 0.47183722\n",
      "Iteration 6040, loss = 0.47147620\n",
      "Iteration 6041, loss = 0.47154445\n",
      "Iteration 6042, loss = 0.47197071\n",
      "Iteration 6043, loss = 0.47138995\n",
      "Iteration 6044, loss = 0.47129087\n",
      "Iteration 6045, loss = 0.47152924\n",
      "Iteration 6046, loss = 0.47205902\n",
      "Iteration 6047, loss = 0.47159155\n",
      "Iteration 6048, loss = 0.47208125\n",
      "Iteration 6049, loss = 0.47171256\n",
      "Iteration 6050, loss = 0.47175019\n",
      "Iteration 6051, loss = 0.47195623\n",
      "Iteration 6052, loss = 0.47158346\n",
      "Iteration 6053, loss = 0.47220181\n",
      "Iteration 6054, loss = 0.47177882\n",
      "Iteration 6055, loss = 0.47166948\n",
      "Iteration 6056, loss = 0.47133215\n",
      "Iteration 6057, loss = 0.47180417\n",
      "Iteration 6058, loss = 0.47313504\n",
      "Iteration 6059, loss = 0.47250116\n",
      "Iteration 6060, loss = 0.47133313\n",
      "Iteration 6061, loss = 0.47062005\n",
      "Iteration 6062, loss = 0.47298216\n",
      "Iteration 6063, loss = 0.47490910\n",
      "Iteration 6064, loss = 0.47335650\n",
      "Iteration 6065, loss = 0.47175202\n",
      "Iteration 6066, loss = 0.47246378\n",
      "Iteration 6067, loss = 0.47227311\n",
      "Iteration 6068, loss = 0.47270733\n",
      "Iteration 6069, loss = 0.47268518\n",
      "Iteration 6070, loss = 0.47247559\n",
      "Iteration 6071, loss = 0.47205564\n",
      "Iteration 6072, loss = 0.47148241\n",
      "Iteration 6073, loss = 0.47160654\n",
      "Iteration 6074, loss = 0.47154564\n",
      "Iteration 6075, loss = 0.47194386\n",
      "Iteration 6076, loss = 0.47156421\n",
      "Iteration 6077, loss = 0.47157644\n",
      "Iteration 6078, loss = 0.47170271\n",
      "Iteration 6079, loss = 0.47170743\n",
      "Iteration 6080, loss = 0.47179263\n",
      "Iteration 6081, loss = 0.47182049\n",
      "Iteration 6082, loss = 0.47153414\n",
      "Iteration 6083, loss = 0.47148680\n",
      "Iteration 6084, loss = 0.47150419\n",
      "Iteration 6085, loss = 0.47224945\n",
      "Iteration 6086, loss = 0.47151718\n",
      "Iteration 6087, loss = 0.47138634\n",
      "Iteration 6088, loss = 0.47219523\n",
      "Iteration 6089, loss = 0.47189778\n",
      "Iteration 6090, loss = 0.47175042\n",
      "Iteration 6091, loss = 0.47138242\n",
      "Iteration 6092, loss = 0.47160360\n",
      "Iteration 6093, loss = 0.47215386\n",
      "Iteration 6094, loss = 0.47236573\n",
      "Iteration 6095, loss = 0.47227134\n",
      "Iteration 6096, loss = 0.47220869\n",
      "Iteration 6097, loss = 0.47161022\n",
      "Iteration 6098, loss = 0.47148692\n",
      "Iteration 6099, loss = 0.47159372\n",
      "Iteration 6100, loss = 0.47147635\n",
      "Iteration 6101, loss = 0.47142494\n",
      "Iteration 6102, loss = 0.47155483\n",
      "Iteration 6103, loss = 0.47199279\n",
      "Iteration 6104, loss = 0.47226249\n",
      "Iteration 6105, loss = 0.47175673\n",
      "Iteration 6106, loss = 0.47124406\n",
      "Iteration 6107, loss = 0.47143340\n",
      "Iteration 6108, loss = 0.47189591\n",
      "Iteration 6109, loss = 0.47275354\n",
      "Iteration 6110, loss = 0.47202393\n",
      "Iteration 6111, loss = 0.47216270\n",
      "Iteration 6112, loss = 0.47166634\n",
      "Iteration 6113, loss = 0.47173129\n",
      "Iteration 6114, loss = 0.47142903\n",
      "Iteration 6115, loss = 0.47148566\n",
      "Iteration 6116, loss = 0.47158330\n",
      "Iteration 6117, loss = 0.47156411\n",
      "Iteration 6118, loss = 0.47168190\n",
      "Iteration 6119, loss = 0.47170167\n",
      "Iteration 6120, loss = 0.47158501\n",
      "Iteration 6121, loss = 0.47153433\n",
      "Iteration 6122, loss = 0.47150723\n",
      "Iteration 6123, loss = 0.47143762\n",
      "Iteration 6124, loss = 0.47173425\n",
      "Iteration 6125, loss = 0.47144454\n",
      "Iteration 6126, loss = 0.47152529\n",
      "Iteration 6127, loss = 0.47152187\n",
      "Iteration 6128, loss = 0.47183777\n",
      "Iteration 6129, loss = 0.47155213\n",
      "Iteration 6130, loss = 0.47151674\n",
      "Iteration 6131, loss = 0.47193630\n",
      "Iteration 6132, loss = 0.47132138\n",
      "Iteration 6133, loss = 0.47162296\n",
      "Iteration 6134, loss = 0.47169180\n",
      "Iteration 6135, loss = 0.47162782\n",
      "Iteration 6136, loss = 0.47192283\n",
      "Iteration 6137, loss = 0.47160072\n",
      "Iteration 6138, loss = 0.47161489\n",
      "Iteration 6139, loss = 0.47208937\n",
      "Iteration 6140, loss = 0.47220116\n",
      "Iteration 6141, loss = 0.47140983\n",
      "Iteration 6142, loss = 0.47159372\n",
      "Iteration 6143, loss = 0.47153411\n",
      "Iteration 6144, loss = 0.47138506\n",
      "Iteration 6145, loss = 0.47140717\n",
      "Iteration 6146, loss = 0.47142997\n",
      "Iteration 6147, loss = 0.47141960\n",
      "Iteration 6148, loss = 0.47142474\n",
      "Iteration 6149, loss = 0.47145800\n",
      "Iteration 6150, loss = 0.47155071\n",
      "Iteration 6151, loss = 0.47151903\n",
      "Iteration 6152, loss = 0.47151009\n",
      "Iteration 6153, loss = 0.47137853\n",
      "Iteration 6154, loss = 0.47150737\n",
      "Iteration 6155, loss = 0.47153153\n",
      "Iteration 6156, loss = 0.47142210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6157, loss = 0.47130979\n",
      "Iteration 6158, loss = 0.47134745\n",
      "Iteration 6159, loss = 0.47147255\n",
      "Iteration 6160, loss = 0.47148878\n",
      "Iteration 6161, loss = 0.47150111\n",
      "Iteration 6162, loss = 0.47126136\n",
      "Iteration 6163, loss = 0.47122464\n",
      "Iteration 6164, loss = 0.47172612\n",
      "Iteration 6165, loss = 0.47195437\n",
      "Iteration 6166, loss = 0.47187281\n",
      "Iteration 6167, loss = 0.47153357\n",
      "Iteration 6168, loss = 0.47113763\n",
      "Iteration 6169, loss = 0.47133473\n",
      "Iteration 6170, loss = 0.47282772\n",
      "Iteration 6171, loss = 0.47254252\n",
      "Iteration 6172, loss = 0.47183988\n",
      "Iteration 6173, loss = 0.47137555\n",
      "Iteration 6174, loss = 0.47116312\n",
      "Iteration 6175, loss = 0.47159867\n",
      "Iteration 6176, loss = 0.47216198\n",
      "Iteration 6177, loss = 0.47242208\n",
      "Iteration 6178, loss = 0.47222470\n",
      "Iteration 6179, loss = 0.47246412\n",
      "Iteration 6180, loss = 0.47189694\n",
      "Iteration 6181, loss = 0.47175794\n",
      "Iteration 6182, loss = 0.47161489\n",
      "Iteration 6183, loss = 0.47155050\n",
      "Iteration 6184, loss = 0.47151069\n",
      "Iteration 6185, loss = 0.47155566\n",
      "Iteration 6186, loss = 0.47153671\n",
      "Iteration 6187, loss = 0.47140511\n",
      "Iteration 6188, loss = 0.47156921\n",
      "Iteration 6189, loss = 0.47137511\n",
      "Iteration 6190, loss = 0.47146279\n",
      "Iteration 6191, loss = 0.47158391\n",
      "Iteration 6192, loss = 0.47164758\n",
      "Iteration 6193, loss = 0.47162517\n",
      "Iteration 6194, loss = 0.47167533\n",
      "Iteration 6195, loss = 0.47165105\n",
      "Iteration 6196, loss = 0.47151328\n",
      "Iteration 6197, loss = 0.47144630\n",
      "Iteration 6198, loss = 0.47153031\n",
      "Iteration 6199, loss = 0.47189836\n",
      "Iteration 6200, loss = 0.47135326\n",
      "Iteration 6201, loss = 0.47146080\n",
      "Iteration 6202, loss = 0.47158652\n",
      "Iteration 6203, loss = 0.47139073\n",
      "Iteration 6204, loss = 0.47131145\n",
      "Iteration 6205, loss = 0.47167592\n",
      "Iteration 6206, loss = 0.47161520\n",
      "Iteration 6207, loss = 0.47149867\n",
      "Iteration 6208, loss = 0.47159703\n",
      "Iteration 6209, loss = 0.47139713\n",
      "Iteration 6210, loss = 0.47150034\n",
      "Iteration 6211, loss = 0.47136846\n",
      "Iteration 6212, loss = 0.47194269\n",
      "Iteration 6213, loss = 0.47132074\n",
      "Iteration 6214, loss = 0.47132049\n",
      "Iteration 6215, loss = 0.47142437\n",
      "Iteration 6216, loss = 0.47154232\n",
      "Iteration 6217, loss = 0.47135427\n",
      "Iteration 6218, loss = 0.47139775\n",
      "Iteration 6219, loss = 0.47139007\n",
      "Iteration 6220, loss = 0.47150088\n",
      "Iteration 6221, loss = 0.47148065\n",
      "Iteration 6222, loss = 0.47131847\n",
      "Iteration 6223, loss = 0.47135635\n",
      "Iteration 6224, loss = 0.47138231\n",
      "Iteration 6225, loss = 0.47136743\n",
      "Iteration 6226, loss = 0.47141478\n",
      "Iteration 6227, loss = 0.47160983\n",
      "Iteration 6228, loss = 0.47234394\n",
      "Iteration 6229, loss = 0.47210592\n",
      "Iteration 6230, loss = 0.47146116\n",
      "Iteration 6231, loss = 0.47154945\n",
      "Iteration 6232, loss = 0.47133587\n",
      "Iteration 6233, loss = 0.47134988\n",
      "Iteration 6234, loss = 0.47154781\n",
      "Iteration 6235, loss = 0.47162910\n",
      "Iteration 6236, loss = 0.47181292\n",
      "Iteration 6237, loss = 0.47149353\n",
      "Iteration 6238, loss = 0.47145062\n",
      "Iteration 6239, loss = 0.47143029\n",
      "Iteration 6240, loss = 0.47153007\n",
      "Iteration 6241, loss = 0.47171223\n",
      "Iteration 6242, loss = 0.47142257\n",
      "Iteration 6243, loss = 0.47136574\n",
      "Iteration 6244, loss = 0.47158489\n",
      "Iteration 6245, loss = 0.47169276\n",
      "Iteration 6246, loss = 0.47163211\n",
      "Iteration 6247, loss = 0.47151921\n",
      "Iteration 6248, loss = 0.47128818\n",
      "Iteration 6249, loss = 0.47127143\n",
      "Iteration 6250, loss = 0.47189848\n",
      "Iteration 6251, loss = 0.47219715\n",
      "Iteration 6252, loss = 0.47182397\n",
      "Iteration 6253, loss = 0.47137043\n",
      "Iteration 6254, loss = 0.47207179\n",
      "Iteration 6255, loss = 0.47238812\n",
      "Iteration 6256, loss = 0.47217210\n",
      "Iteration 6257, loss = 0.47164216\n",
      "Iteration 6258, loss = 0.47223228\n",
      "Iteration 6259, loss = 0.47142293\n",
      "Iteration 6260, loss = 0.47133121\n",
      "Iteration 6261, loss = 0.47156925\n",
      "Iteration 6262, loss = 0.47136621\n",
      "Iteration 6263, loss = 0.47139071\n",
      "Iteration 6264, loss = 0.47144240\n",
      "Iteration 6265, loss = 0.47143887\n",
      "Iteration 6266, loss = 0.47135619\n",
      "Iteration 6267, loss = 0.47130154\n",
      "Iteration 6268, loss = 0.47129616\n",
      "Iteration 6269, loss = 0.47141387\n",
      "Iteration 6270, loss = 0.47159479\n",
      "Iteration 6271, loss = 0.47188851\n",
      "Iteration 6272, loss = 0.47134065\n",
      "Iteration 6273, loss = 0.47132998\n",
      "Iteration 6274, loss = 0.47138598\n",
      "Iteration 6275, loss = 0.47139476\n",
      "Iteration 6276, loss = 0.47162362\n",
      "Iteration 6277, loss = 0.47154630\n",
      "Iteration 6278, loss = 0.47156062\n",
      "Iteration 6279, loss = 0.47117872\n",
      "Iteration 6280, loss = 0.47144152\n",
      "Iteration 6281, loss = 0.47183204\n",
      "Iteration 6282, loss = 0.47196382\n",
      "Iteration 6283, loss = 0.47175243\n",
      "Iteration 6284, loss = 0.47181912\n",
      "Iteration 6285, loss = 0.47135690\n",
      "Iteration 6286, loss = 0.47184497\n",
      "Iteration 6287, loss = 0.47166291\n",
      "Iteration 6288, loss = 0.47152096\n",
      "Iteration 6289, loss = 0.47157668\n",
      "Iteration 6290, loss = 0.47144825\n",
      "Iteration 6291, loss = 0.47153493\n",
      "Iteration 6292, loss = 0.47153373\n",
      "Iteration 6293, loss = 0.47154384\n",
      "Iteration 6294, loss = 0.47173002\n",
      "Iteration 6295, loss = 0.47151137\n",
      "Iteration 6296, loss = 0.47132577\n",
      "Iteration 6297, loss = 0.47147430\n",
      "Iteration 6298, loss = 0.47139481\n",
      "Iteration 6299, loss = 0.47132359\n",
      "Iteration 6300, loss = 0.47147622\n",
      "Iteration 6301, loss = 0.47169707\n",
      "Iteration 6302, loss = 0.47138893\n",
      "Iteration 6303, loss = 0.47192825\n",
      "Iteration 6304, loss = 0.47166325\n",
      "Iteration 6305, loss = 0.47177102\n",
      "Iteration 6306, loss = 0.47131242\n",
      "Iteration 6307, loss = 0.47131249\n",
      "Iteration 6308, loss = 0.47130036\n",
      "Iteration 6309, loss = 0.47146129\n",
      "Iteration 6310, loss = 0.47172794\n",
      "Iteration 6311, loss = 0.47137932\n",
      "Iteration 6312, loss = 0.47130160\n",
      "Iteration 6313, loss = 0.47134265\n",
      "Iteration 6314, loss = 0.47143421\n",
      "Iteration 6315, loss = 0.47146080\n",
      "Iteration 6316, loss = 0.47149259\n",
      "Iteration 6317, loss = 0.47137039\n",
      "Iteration 6318, loss = 0.47128403\n",
      "Iteration 6319, loss = 0.47135426\n",
      "Iteration 6320, loss = 0.47139397\n",
      "Iteration 6321, loss = 0.47138359\n",
      "Iteration 6322, loss = 0.47142011\n",
      "Iteration 6323, loss = 0.47137074\n",
      "Iteration 6324, loss = 0.47130745\n",
      "Iteration 6325, loss = 0.47178420\n",
      "Iteration 6326, loss = 0.47217558\n",
      "Iteration 6327, loss = 0.47136917\n",
      "Iteration 6328, loss = 0.47164685\n",
      "Iteration 6329, loss = 0.47158265\n",
      "Iteration 6330, loss = 0.47161868\n",
      "Iteration 6331, loss = 0.47138413\n",
      "Iteration 6332, loss = 0.47141081\n",
      "Iteration 6333, loss = 0.47157286\n",
      "Iteration 6334, loss = 0.47178443\n",
      "Iteration 6335, loss = 0.47186260\n",
      "Iteration 6336, loss = 0.47177389\n",
      "Iteration 6337, loss = 0.47212623\n",
      "Iteration 6338, loss = 0.47170884\n",
      "Iteration 6339, loss = 0.47136955\n",
      "Iteration 6340, loss = 0.47201834\n",
      "Iteration 6341, loss = 0.47212739\n",
      "Iteration 6342, loss = 0.47159364\n",
      "Iteration 6343, loss = 0.47156267\n",
      "Iteration 6344, loss = 0.47135933\n",
      "Iteration 6345, loss = 0.47137210\n",
      "Iteration 6346, loss = 0.47154868\n",
      "Iteration 6347, loss = 0.47158970\n",
      "Iteration 6348, loss = 0.47195956\n",
      "Iteration 6349, loss = 0.47220038\n",
      "Iteration 6350, loss = 0.47166932\n",
      "Iteration 6351, loss = 0.47189466\n",
      "Iteration 6352, loss = 0.47201910\n",
      "Iteration 6353, loss = 0.47149856\n",
      "Iteration 6354, loss = 0.47114835\n",
      "Iteration 6355, loss = 0.47153444\n",
      "Iteration 6356, loss = 0.47195966\n",
      "Iteration 6357, loss = 0.47228662\n",
      "Iteration 6358, loss = 0.47267592\n",
      "Iteration 6359, loss = 0.47235224\n",
      "Iteration 6360, loss = 0.47192721\n",
      "Iteration 6361, loss = 0.47174069\n",
      "Iteration 6362, loss = 0.47141819\n",
      "Iteration 6363, loss = 0.47215364\n",
      "Iteration 6364, loss = 0.47276230\n",
      "Iteration 6365, loss = 0.47227434\n",
      "Iteration 6366, loss = 0.47135341\n",
      "Iteration 6367, loss = 0.47139050\n",
      "Iteration 6368, loss = 0.47170854\n",
      "Iteration 6369, loss = 0.47191964\n",
      "Iteration 6370, loss = 0.47164364\n",
      "Iteration 6371, loss = 0.47134638\n",
      "Iteration 6372, loss = 0.47156123\n",
      "Iteration 6373, loss = 0.47140342\n",
      "Iteration 6374, loss = 0.47154187\n",
      "Iteration 6375, loss = 0.47135502\n",
      "Iteration 6376, loss = 0.47135511\n",
      "Iteration 6377, loss = 0.47155564\n",
      "Iteration 6378, loss = 0.47162460\n",
      "Iteration 6379, loss = 0.47195478\n",
      "Iteration 6380, loss = 0.47200285\n",
      "Iteration 6381, loss = 0.47184482\n",
      "Iteration 6382, loss = 0.47234107\n",
      "Iteration 6383, loss = 0.47155392\n",
      "Iteration 6384, loss = 0.47143298\n",
      "Iteration 6385, loss = 0.47172448\n",
      "Iteration 6386, loss = 0.47200152\n",
      "Iteration 6387, loss = 0.47165906\n",
      "Iteration 6388, loss = 0.47114309\n",
      "Iteration 6389, loss = 0.47182878\n",
      "Iteration 6390, loss = 0.47181322\n",
      "Iteration 6391, loss = 0.47193404\n",
      "Iteration 6392, loss = 0.47139201\n",
      "Iteration 6393, loss = 0.47146929\n",
      "Iteration 6394, loss = 0.47209632\n",
      "Iteration 6395, loss = 0.47154916\n",
      "Iteration 6396, loss = 0.47124938\n",
      "Iteration 6397, loss = 0.47129376\n",
      "Iteration 6398, loss = 0.47163515\n",
      "Iteration 6399, loss = 0.47209842\n",
      "Iteration 6400, loss = 0.47199019\n",
      "Iteration 6401, loss = 0.47196799\n",
      "Iteration 6402, loss = 0.47161150\n",
      "Iteration 6403, loss = 0.47121575\n",
      "Iteration 6404, loss = 0.47132446\n",
      "Iteration 6405, loss = 0.47206514\n",
      "Iteration 6406, loss = 0.47239196\n",
      "Iteration 6407, loss = 0.47218046\n",
      "Iteration 6408, loss = 0.47221958\n",
      "Iteration 6409, loss = 0.47134627\n",
      "Iteration 6410, loss = 0.47212555\n",
      "Iteration 6411, loss = 0.47147969\n",
      "Iteration 6412, loss = 0.47128374\n",
      "Iteration 6413, loss = 0.47133891\n",
      "Iteration 6414, loss = 0.47197493\n",
      "Iteration 6415, loss = 0.47219753\n",
      "Iteration 6416, loss = 0.47187083\n",
      "Iteration 6417, loss = 0.47202721\n",
      "Iteration 6418, loss = 0.47193727\n",
      "Iteration 6419, loss = 0.47141785\n",
      "Iteration 6420, loss = 0.47164142\n",
      "Iteration 6421, loss = 0.47143880\n",
      "Iteration 6422, loss = 0.47143688\n",
      "Iteration 6423, loss = 0.47161503\n",
      "Iteration 6424, loss = 0.47147999\n",
      "Iteration 6425, loss = 0.47147596\n",
      "Iteration 6426, loss = 0.47133729\n",
      "Iteration 6427, loss = 0.47160517\n",
      "Iteration 6428, loss = 0.47187066\n",
      "Iteration 6429, loss = 0.47172217\n",
      "Iteration 6430, loss = 0.47142308\n",
      "Iteration 6431, loss = 0.47145559\n",
      "Iteration 6432, loss = 0.47142857\n",
      "Iteration 6433, loss = 0.47157753\n",
      "Iteration 6434, loss = 0.47233087\n",
      "Iteration 6435, loss = 0.47156884\n",
      "Iteration 6436, loss = 0.47125535\n",
      "Iteration 6437, loss = 0.47185030\n",
      "Iteration 6438, loss = 0.47170660\n",
      "Iteration 6439, loss = 0.47169694\n",
      "Iteration 6440, loss = 0.47196381\n",
      "Iteration 6441, loss = 0.47132783\n",
      "Iteration 6442, loss = 0.47137204\n",
      "Iteration 6443, loss = 0.47154207\n",
      "Iteration 6444, loss = 0.47160404\n",
      "Iteration 6445, loss = 0.47205142\n",
      "Iteration 6446, loss = 0.47192633\n",
      "Iteration 6447, loss = 0.47122511\n",
      "Iteration 6448, loss = 0.47140686\n",
      "Iteration 6449, loss = 0.47153216\n",
      "Iteration 6450, loss = 0.47149256\n",
      "Iteration 6451, loss = 0.47154677\n",
      "Iteration 6452, loss = 0.47147459\n",
      "Iteration 6453, loss = 0.47147808\n",
      "Iteration 6454, loss = 0.47138719\n",
      "Iteration 6455, loss = 0.47134183\n",
      "Iteration 6456, loss = 0.47135758\n",
      "Iteration 6457, loss = 0.47130059\n",
      "Iteration 6458, loss = 0.47198930\n",
      "Iteration 6459, loss = 0.47239412\n",
      "Iteration 6460, loss = 0.47149876\n",
      "Iteration 6461, loss = 0.47170668\n",
      "Iteration 6462, loss = 0.47159881\n",
      "Iteration 6463, loss = 0.47153895\n",
      "Iteration 6464, loss = 0.47129710\n",
      "Iteration 6465, loss = 0.47130993\n",
      "Iteration 6466, loss = 0.47142835\n",
      "Iteration 6467, loss = 0.47136384\n",
      "Iteration 6468, loss = 0.47140267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6469, loss = 0.47147822\n",
      "Iteration 6470, loss = 0.47138977\n",
      "Iteration 6471, loss = 0.47137040\n",
      "Iteration 6472, loss = 0.47134592\n",
      "Iteration 6473, loss = 0.47145472\n",
      "Iteration 6474, loss = 0.47146791\n",
      "Iteration 6475, loss = 0.47159063\n",
      "Iteration 6476, loss = 0.47179340\n",
      "Iteration 6477, loss = 0.47187435\n",
      "Iteration 6478, loss = 0.47146699\n",
      "Iteration 6479, loss = 0.47173559\n",
      "Iteration 6480, loss = 0.47136923\n",
      "Iteration 6481, loss = 0.47163497\n",
      "Iteration 6482, loss = 0.47183909\n",
      "Iteration 6483, loss = 0.47152621\n",
      "Iteration 6484, loss = 0.47160752\n",
      "Iteration 6485, loss = 0.47159442\n",
      "Iteration 6486, loss = 0.47148203\n",
      "Iteration 6487, loss = 0.47156457\n",
      "Iteration 6488, loss = 0.47143278\n",
      "Iteration 6489, loss = 0.47139944\n",
      "Iteration 6490, loss = 0.47140667\n",
      "Iteration 6491, loss = 0.47145064\n",
      "Iteration 6492, loss = 0.47144847\n",
      "Iteration 6493, loss = 0.47160092\n",
      "Iteration 6494, loss = 0.47140421\n",
      "Iteration 6495, loss = 0.47138896\n",
      "Iteration 6496, loss = 0.47126814\n",
      "Iteration 6497, loss = 0.47138609\n",
      "Iteration 6498, loss = 0.47164643\n",
      "Iteration 6499, loss = 0.47175626\n",
      "Iteration 6500, loss = 0.47245846\n",
      "Iteration 6501, loss = 0.47130816\n",
      "Iteration 6502, loss = 0.47164590\n",
      "Iteration 6503, loss = 0.47132161\n",
      "Iteration 6504, loss = 0.47136689\n",
      "Iteration 6505, loss = 0.47144205\n",
      "Iteration 6506, loss = 0.47131145\n",
      "Iteration 6507, loss = 0.47132385\n",
      "Iteration 6508, loss = 0.47135084\n",
      "Iteration 6509, loss = 0.47147201\n",
      "Iteration 6510, loss = 0.47202042\n",
      "Iteration 6511, loss = 0.47179484\n",
      "Iteration 6512, loss = 0.47148823\n",
      "Iteration 6513, loss = 0.47122378\n",
      "Iteration 6514, loss = 0.47118565\n",
      "Iteration 6515, loss = 0.47150630\n",
      "Iteration 6516, loss = 0.47245782\n",
      "Iteration 6517, loss = 0.47344594\n",
      "Iteration 6518, loss = 0.47301917\n",
      "Iteration 6519, loss = 0.47179276\n",
      "Iteration 6520, loss = 0.47115288\n",
      "Iteration 6521, loss = 0.47291389\n",
      "Iteration 6522, loss = 0.47240196\n",
      "Iteration 6523, loss = 0.47234500\n",
      "Iteration 6524, loss = 0.47157040\n",
      "Iteration 6525, loss = 0.47139550\n",
      "Iteration 6526, loss = 0.47193518\n",
      "Iteration 6527, loss = 0.47191875\n",
      "Iteration 6528, loss = 0.47165835\n",
      "Iteration 6529, loss = 0.47168491\n",
      "Iteration 6530, loss = 0.47161573\n",
      "Iteration 6531, loss = 0.47126427\n",
      "Iteration 6532, loss = 0.47119580\n",
      "Iteration 6533, loss = 0.47156474\n",
      "Iteration 6534, loss = 0.47210918\n",
      "Iteration 6535, loss = 0.47172976\n",
      "Iteration 6536, loss = 0.47111134\n",
      "Iteration 6537, loss = 0.47176966\n",
      "Iteration 6538, loss = 0.47197843\n",
      "Iteration 6539, loss = 0.47236986\n",
      "Iteration 6540, loss = 0.47174609\n",
      "Iteration 6541, loss = 0.47179558\n",
      "Iteration 6542, loss = 0.47164894\n",
      "Iteration 6543, loss = 0.47183295\n",
      "Iteration 6544, loss = 0.47134428\n",
      "Iteration 6545, loss = 0.47129176\n",
      "Iteration 6546, loss = 0.47139320\n",
      "Iteration 6547, loss = 0.47138522\n",
      "Iteration 6548, loss = 0.47184377\n",
      "Iteration 6549, loss = 0.47149173\n",
      "Iteration 6550, loss = 0.47153886\n",
      "Iteration 6551, loss = 0.47130557\n",
      "Iteration 6552, loss = 0.47137220\n",
      "Iteration 6553, loss = 0.47142477\n",
      "Iteration 6554, loss = 0.47136632\n",
      "Iteration 6555, loss = 0.47191011\n",
      "Iteration 6556, loss = 0.47138392\n",
      "Iteration 6557, loss = 0.47128747\n",
      "Iteration 6558, loss = 0.47133893\n",
      "Iteration 6559, loss = 0.47127769\n",
      "Iteration 6560, loss = 0.47150164\n",
      "Iteration 6561, loss = 0.47133667\n",
      "Iteration 6562, loss = 0.47132060\n",
      "Iteration 6563, loss = 0.47166120\n",
      "Iteration 6564, loss = 0.47179568\n",
      "Iteration 6565, loss = 0.47142115\n",
      "Iteration 6566, loss = 0.47140635\n",
      "Iteration 6567, loss = 0.47129306\n",
      "Iteration 6568, loss = 0.47163231\n",
      "Iteration 6569, loss = 0.47151457\n",
      "Iteration 6570, loss = 0.47155707\n",
      "Iteration 6571, loss = 0.47179315\n",
      "Iteration 6572, loss = 0.47203969\n",
      "Iteration 6573, loss = 0.47197201\n",
      "Iteration 6574, loss = 0.47170003\n",
      "Iteration 6575, loss = 0.47152181\n",
      "Iteration 6576, loss = 0.47135920\n",
      "Iteration 6577, loss = 0.47140787\n",
      "Iteration 6578, loss = 0.47133849\n",
      "Iteration 6579, loss = 0.47150650\n",
      "Iteration 6580, loss = 0.47173988\n",
      "Iteration 6581, loss = 0.47194367\n",
      "Iteration 6582, loss = 0.47153043\n",
      "Iteration 6583, loss = 0.47164867\n",
      "Iteration 6584, loss = 0.47117734\n",
      "Iteration 6585, loss = 0.47127052\n",
      "Iteration 6586, loss = 0.47172545\n",
      "Iteration 6587, loss = 0.47205665\n",
      "Iteration 6588, loss = 0.47194673\n",
      "Iteration 6589, loss = 0.47163279\n",
      "Iteration 6590, loss = 0.47168106\n",
      "Iteration 6591, loss = 0.47131538\n",
      "Iteration 6592, loss = 0.47132931\n",
      "Iteration 6593, loss = 0.47154983\n",
      "Iteration 6594, loss = 0.47153596\n",
      "Iteration 6595, loss = 0.47136501\n",
      "Iteration 6596, loss = 0.47215470\n",
      "Iteration 6597, loss = 0.47153330\n",
      "Iteration 6598, loss = 0.47156526\n",
      "Iteration 6599, loss = 0.47127305\n",
      "Iteration 6600, loss = 0.47137150\n",
      "Iteration 6601, loss = 0.47138563\n",
      "Iteration 6602, loss = 0.47145373\n",
      "Iteration 6603, loss = 0.47144675\n",
      "Iteration 6604, loss = 0.47168893\n",
      "Iteration 6605, loss = 0.47141901\n",
      "Iteration 6606, loss = 0.47139222\n",
      "Iteration 6607, loss = 0.47103490\n",
      "Iteration 6608, loss = 0.47134892\n",
      "Iteration 6609, loss = 0.47226402\n",
      "Iteration 6610, loss = 0.47262615\n",
      "Iteration 6611, loss = 0.47240705\n",
      "Iteration 6612, loss = 0.47198185\n",
      "Iteration 6613, loss = 0.47136648\n",
      "Iteration 6614, loss = 0.47166136\n",
      "Iteration 6615, loss = 0.47140555\n",
      "Iteration 6616, loss = 0.47158432\n",
      "Iteration 6617, loss = 0.47210304\n",
      "Iteration 6618, loss = 0.47138244\n",
      "Iteration 6619, loss = 0.47121961\n",
      "Iteration 6620, loss = 0.47121399\n",
      "Iteration 6621, loss = 0.47218107\n",
      "Iteration 6622, loss = 0.47140638\n",
      "Iteration 6623, loss = 0.47158186\n",
      "Iteration 6624, loss = 0.47136105\n",
      "Iteration 6625, loss = 0.47120704\n",
      "Iteration 6626, loss = 0.47122301\n",
      "Iteration 6627, loss = 0.47155098\n",
      "Iteration 6628, loss = 0.47157402\n",
      "Iteration 6629, loss = 0.47150161\n",
      "Iteration 6630, loss = 0.47117436\n",
      "Iteration 6631, loss = 0.47178676\n",
      "Iteration 6632, loss = 0.47178419\n",
      "Iteration 6633, loss = 0.47149556\n",
      "Iteration 6634, loss = 0.47127299\n",
      "Iteration 6635, loss = 0.47126850\n",
      "Iteration 6636, loss = 0.47132591\n",
      "Iteration 6637, loss = 0.47141088\n",
      "Iteration 6638, loss = 0.47179538\n",
      "Iteration 6639, loss = 0.47171284\n",
      "Iteration 6640, loss = 0.47127566\n",
      "Iteration 6641, loss = 0.47176316\n",
      "Iteration 6642, loss = 0.47171868\n",
      "Iteration 6643, loss = 0.47176707\n",
      "Iteration 6644, loss = 0.47150741\n",
      "Iteration 6645, loss = 0.47119966\n",
      "Iteration 6646, loss = 0.47125194\n",
      "Iteration 6647, loss = 0.47142578\n",
      "Iteration 6648, loss = 0.47193494\n",
      "Iteration 6649, loss = 0.47188736\n",
      "Iteration 6650, loss = 0.47180295\n",
      "Iteration 6651, loss = 0.47137090\n",
      "Iteration 6652, loss = 0.47131019\n",
      "Iteration 6653, loss = 0.47143836\n",
      "Iteration 6654, loss = 0.47150247\n",
      "Iteration 6655, loss = 0.47123620\n",
      "Iteration 6656, loss = 0.47131927\n",
      "Iteration 6657, loss = 0.47132549\n",
      "Iteration 6658, loss = 0.47124695\n",
      "Iteration 6659, loss = 0.47127138\n",
      "Iteration 6660, loss = 0.47125685\n",
      "Iteration 6661, loss = 0.47125012\n",
      "Iteration 6662, loss = 0.47139449\n",
      "Iteration 6663, loss = 0.47126986\n",
      "Iteration 6664, loss = 0.47133372\n",
      "Iteration 6665, loss = 0.47129168\n",
      "Iteration 6666, loss = 0.47133583\n",
      "Iteration 6667, loss = 0.47138046\n",
      "Iteration 6668, loss = 0.47138358\n",
      "Iteration 6669, loss = 0.47142468\n",
      "Iteration 6670, loss = 0.47132601\n",
      "Iteration 6671, loss = 0.47127219\n",
      "Iteration 6672, loss = 0.47118894\n",
      "Iteration 6673, loss = 0.47120480\n",
      "Iteration 6674, loss = 0.47134510\n",
      "Iteration 6675, loss = 0.47133127\n",
      "Iteration 6676, loss = 0.47146043\n",
      "Iteration 6677, loss = 0.47131086\n",
      "Iteration 6678, loss = 0.47142480\n",
      "Iteration 6679, loss = 0.47137346\n",
      "Iteration 6680, loss = 0.47173859\n",
      "Iteration 6681, loss = 0.47141859\n",
      "Iteration 6682, loss = 0.47145775\n",
      "Iteration 6683, loss = 0.47176326\n",
      "Iteration 6684, loss = 0.47167873\n",
      "Iteration 6685, loss = 0.47167883\n",
      "Iteration 6686, loss = 0.47181878\n",
      "Iteration 6687, loss = 0.47157763\n",
      "Iteration 6688, loss = 0.47155401\n",
      "Iteration 6689, loss = 0.47151758\n",
      "Iteration 6690, loss = 0.47120565\n",
      "Iteration 6691, loss = 0.47125333\n",
      "Iteration 6692, loss = 0.47170354\n",
      "Iteration 6693, loss = 0.47162339\n",
      "Iteration 6694, loss = 0.47131636\n",
      "Iteration 6695, loss = 0.47139856\n",
      "Iteration 6696, loss = 0.47129001\n",
      "Iteration 6697, loss = 0.47138539\n",
      "Iteration 6698, loss = 0.47136537\n",
      "Iteration 6699, loss = 0.47139185\n",
      "Iteration 6700, loss = 0.47128569\n",
      "Iteration 6701, loss = 0.47131019\n",
      "Iteration 6702, loss = 0.47128018\n",
      "Iteration 6703, loss = 0.47125491\n",
      "Iteration 6704, loss = 0.47148033\n",
      "Iteration 6705, loss = 0.47141913\n",
      "Iteration 6706, loss = 0.47132122\n",
      "Iteration 6707, loss = 0.47130612\n",
      "Iteration 6708, loss = 0.47142687\n",
      "Iteration 6709, loss = 0.47139908\n",
      "Iteration 6710, loss = 0.47131833\n",
      "Iteration 6711, loss = 0.47133409\n",
      "Iteration 6712, loss = 0.47136208\n",
      "Iteration 6713, loss = 0.47164175\n",
      "Iteration 6714, loss = 0.47118421\n",
      "Iteration 6715, loss = 0.47115732\n",
      "Iteration 6716, loss = 0.47179991\n",
      "Iteration 6717, loss = 0.47238341\n",
      "Iteration 6718, loss = 0.47183871\n",
      "Iteration 6719, loss = 0.47163352\n",
      "Iteration 6720, loss = 0.47142922\n",
      "Iteration 6721, loss = 0.47153016\n",
      "Iteration 6722, loss = 0.47180648\n",
      "Iteration 6723, loss = 0.47154183\n",
      "Iteration 6724, loss = 0.47153863\n",
      "Iteration 6725, loss = 0.47137029\n",
      "Iteration 6726, loss = 0.47132724\n",
      "Iteration 6727, loss = 0.47169045\n",
      "Iteration 6728, loss = 0.47132823\n",
      "Iteration 6729, loss = 0.47125184\n",
      "Iteration 6730, loss = 0.47115040\n",
      "Iteration 6731, loss = 0.47118574\n",
      "Iteration 6732, loss = 0.47150171\n",
      "Iteration 6733, loss = 0.47159535\n",
      "Iteration 6734, loss = 0.47156293\n",
      "Iteration 6735, loss = 0.47136925\n",
      "Iteration 6736, loss = 0.47121703\n",
      "Iteration 6737, loss = 0.47122158\n",
      "Iteration 6738, loss = 0.47130573\n",
      "Iteration 6739, loss = 0.47128673\n",
      "Iteration 6740, loss = 0.47139728\n",
      "Iteration 6741, loss = 0.47144690\n",
      "Iteration 6742, loss = 0.47142820\n",
      "Iteration 6743, loss = 0.47135452\n",
      "Iteration 6744, loss = 0.47142489\n",
      "Iteration 6745, loss = 0.47149891\n",
      "Iteration 6746, loss = 0.47158016\n",
      "Iteration 6747, loss = 0.47149222\n",
      "Iteration 6748, loss = 0.47198816\n",
      "Iteration 6749, loss = 0.47212254\n",
      "Iteration 6750, loss = 0.47187871\n",
      "Iteration 6751, loss = 0.47211418\n",
      "Iteration 6752, loss = 0.47153482\n",
      "Iteration 6753, loss = 0.47160596\n",
      "Iteration 6754, loss = 0.47121200\n",
      "Iteration 6755, loss = 0.47127358\n",
      "Iteration 6756, loss = 0.47155301\n",
      "Iteration 6757, loss = 0.47125164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6758, loss = 0.47119820\n",
      "Iteration 6759, loss = 0.47130745\n",
      "Iteration 6760, loss = 0.47144439\n",
      "Iteration 6761, loss = 0.47130893\n",
      "Iteration 6762, loss = 0.47104431\n",
      "Iteration 6763, loss = 0.47165227\n",
      "Iteration 6764, loss = 0.47168285\n",
      "Iteration 6765, loss = 0.47156310\n",
      "Iteration 6766, loss = 0.47142063\n",
      "Iteration 6767, loss = 0.47108081\n",
      "Iteration 6768, loss = 0.47216925\n",
      "Iteration 6769, loss = 0.47186599\n",
      "Iteration 6770, loss = 0.47128867\n",
      "Iteration 6771, loss = 0.47127801\n",
      "Iteration 6772, loss = 0.47147731\n",
      "Iteration 6773, loss = 0.47162059\n",
      "Iteration 6774, loss = 0.47159485\n",
      "Iteration 6775, loss = 0.47227278\n",
      "Iteration 6776, loss = 0.47132750\n",
      "Iteration 6777, loss = 0.47141209\n",
      "Iteration 6778, loss = 0.47203638\n",
      "Iteration 6779, loss = 0.47118918\n",
      "Iteration 6780, loss = 0.47125436\n",
      "Iteration 6781, loss = 0.47150989\n",
      "Iteration 6782, loss = 0.47129357\n",
      "Iteration 6783, loss = 0.47163831\n",
      "Iteration 6784, loss = 0.47146104\n",
      "Iteration 6785, loss = 0.47197801\n",
      "Iteration 6786, loss = 0.47133275\n",
      "Iteration 6787, loss = 0.47115618\n",
      "Iteration 6788, loss = 0.47105485\n",
      "Iteration 6789, loss = 0.47126841\n",
      "Iteration 6790, loss = 0.47176375\n",
      "Iteration 6791, loss = 0.47207275\n",
      "Iteration 6792, loss = 0.47183703\n",
      "Iteration 6793, loss = 0.47204756\n",
      "Iteration 6794, loss = 0.47143756\n",
      "Iteration 6795, loss = 0.47191858\n",
      "Iteration 6796, loss = 0.47149460\n",
      "Iteration 6797, loss = 0.47146928\n",
      "Iteration 6798, loss = 0.47150938\n",
      "Iteration 6799, loss = 0.47175160\n",
      "Iteration 6800, loss = 0.47231505\n",
      "Iteration 6801, loss = 0.47186312\n",
      "Iteration 6802, loss = 0.47169552\n",
      "Iteration 6803, loss = 0.47170730\n",
      "Iteration 6804, loss = 0.47134645\n",
      "Iteration 6805, loss = 0.47130755\n",
      "Iteration 6806, loss = 0.47134044\n",
      "Iteration 6807, loss = 0.47154648\n",
      "Iteration 6808, loss = 0.47156858\n",
      "Iteration 6809, loss = 0.47155996\n",
      "Iteration 6810, loss = 0.47162064\n",
      "Iteration 6811, loss = 0.47148611\n",
      "Iteration 6812, loss = 0.47128858\n",
      "Iteration 6813, loss = 0.47159522\n",
      "Iteration 6814, loss = 0.47125291\n",
      "Iteration 6815, loss = 0.47117331\n",
      "Iteration 6816, loss = 0.47183321\n",
      "Iteration 6817, loss = 0.47135154\n",
      "Iteration 6818, loss = 0.47127519\n",
      "Iteration 6819, loss = 0.47112003\n",
      "Iteration 6820, loss = 0.47109220\n",
      "Iteration 6821, loss = 0.47169156\n",
      "Iteration 6822, loss = 0.47229147\n",
      "Iteration 6823, loss = 0.47200863\n",
      "Iteration 6824, loss = 0.47173034\n",
      "Iteration 6825, loss = 0.47139277\n",
      "Iteration 6826, loss = 0.47137212\n",
      "Iteration 6827, loss = 0.47162792\n",
      "Iteration 6828, loss = 0.47181330\n",
      "Iteration 6829, loss = 0.47190283\n",
      "Iteration 6830, loss = 0.47179295\n",
      "Iteration 6831, loss = 0.47153969\n",
      "Iteration 6832, loss = 0.47163798\n",
      "Iteration 6833, loss = 0.47161293\n",
      "Iteration 6834, loss = 0.47132432\n",
      "Iteration 6835, loss = 0.47150075\n",
      "Iteration 6836, loss = 0.47155712\n",
      "Iteration 6837, loss = 0.47137513\n",
      "Iteration 6838, loss = 0.47153722\n",
      "Iteration 6839, loss = 0.47125360\n",
      "Iteration 6840, loss = 0.47123162\n",
      "Iteration 6841, loss = 0.47133318\n",
      "Iteration 6842, loss = 0.47116521\n",
      "Iteration 6843, loss = 0.47121561\n",
      "Iteration 6844, loss = 0.47141062\n",
      "Iteration 6845, loss = 0.47163491\n",
      "Iteration 6846, loss = 0.47136417\n",
      "Iteration 6847, loss = 0.47178330\n",
      "Iteration 6848, loss = 0.47154004\n",
      "Iteration 6849, loss = 0.47194830\n",
      "Iteration 6850, loss = 0.47149784\n",
      "Iteration 6851, loss = 0.47115080\n",
      "Iteration 6852, loss = 0.47131249\n",
      "Iteration 6853, loss = 0.47142459\n",
      "Iteration 6854, loss = 0.47149363\n",
      "Iteration 6855, loss = 0.47126796\n",
      "Iteration 6856, loss = 0.47157294\n",
      "Iteration 6857, loss = 0.47160149\n",
      "Iteration 6858, loss = 0.47158864\n",
      "Iteration 6859, loss = 0.47145356\n",
      "Iteration 6860, loss = 0.47149010\n",
      "Iteration 6861, loss = 0.47153704\n",
      "Iteration 6862, loss = 0.47144553\n",
      "Iteration 6863, loss = 0.47138209\n",
      "Iteration 6864, loss = 0.47147642\n",
      "Iteration 6865, loss = 0.47153527\n",
      "Iteration 6866, loss = 0.47142657\n",
      "Iteration 6867, loss = 0.47136173\n",
      "Iteration 6868, loss = 0.47138674\n",
      "Iteration 6869, loss = 0.47135444\n",
      "Iteration 6870, loss = 0.47137720\n",
      "Iteration 6871, loss = 0.47132341\n",
      "Iteration 6872, loss = 0.47138071\n",
      "Iteration 6873, loss = 0.47131871\n",
      "Iteration 6874, loss = 0.47134100\n",
      "Iteration 6875, loss = 0.47155804\n",
      "Iteration 6876, loss = 0.47129844\n",
      "Iteration 6877, loss = 0.47126474\n",
      "Iteration 6878, loss = 0.47129304\n",
      "Iteration 6879, loss = 0.47192634\n",
      "Iteration 6880, loss = 0.47208410\n",
      "Iteration 6881, loss = 0.47180999\n",
      "Iteration 6882, loss = 0.47137967\n",
      "Iteration 6883, loss = 0.47111770\n",
      "Iteration 6884, loss = 0.47124854\n",
      "Iteration 6885, loss = 0.47197913\n",
      "Iteration 6886, loss = 0.47237617\n",
      "Iteration 6887, loss = 0.47235320\n",
      "Iteration 6888, loss = 0.47186005\n",
      "Iteration 6889, loss = 0.47202357\n",
      "Iteration 6890, loss = 0.47133530\n",
      "Iteration 6891, loss = 0.47142058\n",
      "Iteration 6892, loss = 0.47123695\n",
      "Iteration 6893, loss = 0.47130362\n",
      "Iteration 6894, loss = 0.47129641\n",
      "Iteration 6895, loss = 0.47120774\n",
      "Iteration 6896, loss = 0.47118996\n",
      "Iteration 6897, loss = 0.47122437\n",
      "Iteration 6898, loss = 0.47151182\n",
      "Iteration 6899, loss = 0.47162398\n",
      "Iteration 6900, loss = 0.47127841\n",
      "Iteration 6901, loss = 0.47123705\n",
      "Iteration 6902, loss = 0.47125480\n",
      "Iteration 6903, loss = 0.47140148\n",
      "Iteration 6904, loss = 0.47119581\n",
      "Iteration 6905, loss = 0.47170820\n",
      "Iteration 6906, loss = 0.47150356\n",
      "Iteration 6907, loss = 0.47137559\n",
      "Iteration 6908, loss = 0.47136658\n",
      "Iteration 6909, loss = 0.47109920\n",
      "Iteration 6910, loss = 0.47153887\n",
      "Iteration 6911, loss = 0.47194232\n",
      "Iteration 6912, loss = 0.47173827\n",
      "Iteration 6913, loss = 0.47150062\n",
      "Iteration 6914, loss = 0.47124247\n",
      "Iteration 6915, loss = 0.47135067\n",
      "Iteration 6916, loss = 0.47167801\n",
      "Iteration 6917, loss = 0.47211610\n",
      "Iteration 6918, loss = 0.47235850\n",
      "Iteration 6919, loss = 0.47182036\n",
      "Iteration 6920, loss = 0.47145397\n",
      "Iteration 6921, loss = 0.47170095\n",
      "Iteration 6922, loss = 0.47131865\n",
      "Iteration 6923, loss = 0.47138324\n",
      "Iteration 6924, loss = 0.47129158\n",
      "Iteration 6925, loss = 0.47135281\n",
      "Iteration 6926, loss = 0.47126902\n",
      "Iteration 6927, loss = 0.47133526\n",
      "Iteration 6928, loss = 0.47128345\n",
      "Iteration 6929, loss = 0.47152561\n",
      "Iteration 6930, loss = 0.47109598\n",
      "Iteration 6931, loss = 0.47177591\n",
      "Iteration 6932, loss = 0.47145292\n",
      "Iteration 6933, loss = 0.47122453\n",
      "Iteration 6934, loss = 0.47155453\n",
      "Iteration 6935, loss = 0.47140591\n",
      "Iteration 6936, loss = 0.47161110\n",
      "Iteration 6937, loss = 0.47189661\n",
      "Iteration 6938, loss = 0.47171547\n",
      "Iteration 6939, loss = 0.47141522\n",
      "Iteration 6940, loss = 0.47117459\n",
      "Iteration 6941, loss = 0.47143670\n",
      "Iteration 6942, loss = 0.47162813\n",
      "Iteration 6943, loss = 0.47145121\n",
      "Iteration 6944, loss = 0.47187885\n",
      "Iteration 6945, loss = 0.47123436\n",
      "Iteration 6946, loss = 0.47135805\n",
      "Iteration 6947, loss = 0.47122842\n",
      "Iteration 6948, loss = 0.47126052\n",
      "Iteration 6949, loss = 0.47134103\n",
      "Iteration 6950, loss = 0.47141948\n",
      "Iteration 6951, loss = 0.47156704\n",
      "Iteration 6952, loss = 0.47172106\n",
      "Iteration 6953, loss = 0.47154874\n",
      "Iteration 6954, loss = 0.47153416\n",
      "Iteration 6955, loss = 0.47132887\n",
      "Iteration 6956, loss = 0.47155639\n",
      "Iteration 6957, loss = 0.47124100\n",
      "Iteration 6958, loss = 0.47139370\n",
      "Iteration 6959, loss = 0.47151576\n",
      "Iteration 6960, loss = 0.47175479\n",
      "Iteration 6961, loss = 0.47170180\n",
      "Iteration 6962, loss = 0.47216439\n",
      "Iteration 6963, loss = 0.47188645\n",
      "Iteration 6964, loss = 0.47151856\n",
      "Iteration 6965, loss = 0.47270877\n",
      "Iteration 6966, loss = 0.47140293\n",
      "Iteration 6967, loss = 0.47130731\n",
      "Iteration 6968, loss = 0.47149003\n",
      "Iteration 6969, loss = 0.47135337\n",
      "Iteration 6970, loss = 0.47116597\n",
      "Iteration 6971, loss = 0.47117861\n",
      "Iteration 6972, loss = 0.47123794\n",
      "Iteration 6973, loss = 0.47113990\n",
      "Iteration 6974, loss = 0.47117509\n",
      "Iteration 6975, loss = 0.47119898\n",
      "Iteration 6976, loss = 0.47122383\n",
      "Iteration 6977, loss = 0.47113579\n",
      "Iteration 6978, loss = 0.47108658\n",
      "Iteration 6979, loss = 0.47126021\n",
      "Iteration 6980, loss = 0.47163709\n",
      "Iteration 6981, loss = 0.47149865\n",
      "Iteration 6982, loss = 0.47125415\n",
      "Iteration 6983, loss = 0.47115755\n",
      "Iteration 6984, loss = 0.47119541\n",
      "Iteration 6985, loss = 0.47123900\n",
      "Iteration 6986, loss = 0.47112693\n",
      "Iteration 6987, loss = 0.47155199\n",
      "Iteration 6988, loss = 0.47121955\n",
      "Iteration 6989, loss = 0.47203104\n",
      "Iteration 6990, loss = 0.47128254\n",
      "Iteration 6991, loss = 0.47115472\n",
      "Iteration 6992, loss = 0.47109151\n",
      "Iteration 6993, loss = 0.47135342\n",
      "Iteration 6994, loss = 0.47132652\n",
      "Iteration 6995, loss = 0.47195432\n",
      "Iteration 6996, loss = 0.47278630\n",
      "Iteration 6997, loss = 0.47181111\n",
      "Iteration 6998, loss = 0.47182283\n",
      "Iteration 6999, loss = 0.47139144\n",
      "Iteration 7000, loss = 0.47164640\n",
      "Iteration 7001, loss = 0.47157047\n",
      "Iteration 7002, loss = 0.47171762\n",
      "Iteration 7003, loss = 0.47131739\n",
      "Iteration 7004, loss = 0.47129585\n",
      "Iteration 7005, loss = 0.47143590\n",
      "Iteration 7006, loss = 0.47154481\n",
      "Iteration 7007, loss = 0.47125821\n",
      "Iteration 7008, loss = 0.47120909\n",
      "Iteration 7009, loss = 0.47174178\n",
      "Iteration 7010, loss = 0.47138762\n",
      "Iteration 7011, loss = 0.47142845\n",
      "Iteration 7012, loss = 0.47115006\n",
      "Iteration 7013, loss = 0.47136828\n",
      "Iteration 7014, loss = 0.47140904\n",
      "Iteration 7015, loss = 0.47175169\n",
      "Iteration 7016, loss = 0.47139735\n",
      "Iteration 7017, loss = 0.47138363\n",
      "Iteration 7018, loss = 0.47132783\n",
      "Iteration 7019, loss = 0.47124854\n",
      "Iteration 7020, loss = 0.47126256\n",
      "Iteration 7021, loss = 0.47120329\n",
      "Iteration 7022, loss = 0.47116480\n",
      "Iteration 7023, loss = 0.47123233\n",
      "Iteration 7024, loss = 0.47120109\n",
      "Iteration 7025, loss = 0.47140959\n",
      "Iteration 7026, loss = 0.47123901\n",
      "Iteration 7027, loss = 0.47133191\n",
      "Iteration 7028, loss = 0.47128996\n",
      "Iteration 7029, loss = 0.47146378\n",
      "Iteration 7030, loss = 0.47102012\n",
      "Iteration 7031, loss = 0.47195861\n",
      "Iteration 7032, loss = 0.47161524\n",
      "Iteration 7033, loss = 0.47137110\n",
      "Iteration 7034, loss = 0.47119818\n",
      "Iteration 7035, loss = 0.47110725\n",
      "Iteration 7036, loss = 0.47134823\n",
      "Iteration 7037, loss = 0.47179382\n",
      "Iteration 7038, loss = 0.47125108\n",
      "Iteration 7039, loss = 0.47164274\n",
      "Iteration 7040, loss = 0.47174043\n",
      "Iteration 7041, loss = 0.47165597\n",
      "Iteration 7042, loss = 0.47153619\n",
      "Iteration 7043, loss = 0.47171172\n",
      "Iteration 7044, loss = 0.47165960\n",
      "Iteration 7045, loss = 0.47166074\n",
      "Iteration 7046, loss = 0.47156477\n",
      "Iteration 7047, loss = 0.47156548\n",
      "Iteration 7048, loss = 0.47193575\n",
      "Iteration 7049, loss = 0.47139482\n",
      "Iteration 7050, loss = 0.47125452\n",
      "Iteration 7051, loss = 0.47145253\n",
      "Iteration 7052, loss = 0.47188597\n",
      "Iteration 7053, loss = 0.47190644\n",
      "Iteration 7054, loss = 0.47180089\n",
      "Iteration 7055, loss = 0.47223937\n",
      "Iteration 7056, loss = 0.47120584\n",
      "Iteration 7057, loss = 0.47135943\n",
      "Iteration 7058, loss = 0.47150206\n",
      "Iteration 7059, loss = 0.47171396\n",
      "Iteration 7060, loss = 0.47163243\n",
      "Iteration 7061, loss = 0.47151677\n",
      "Iteration 7062, loss = 0.47145897\n",
      "Iteration 7063, loss = 0.47120473\n",
      "Iteration 7064, loss = 0.47130338\n",
      "Iteration 7065, loss = 0.47122938\n",
      "Iteration 7066, loss = 0.47113523\n",
      "Iteration 7067, loss = 0.47118363\n",
      "Iteration 7068, loss = 0.47123105\n",
      "Iteration 7069, loss = 0.47137200\n",
      "Iteration 7070, loss = 0.47143951\n",
      "Iteration 7071, loss = 0.47132938\n",
      "Iteration 7072, loss = 0.47120997\n",
      "Iteration 7073, loss = 0.47097380\n",
      "Iteration 7074, loss = 0.47147076\n",
      "Iteration 7075, loss = 0.47161534\n",
      "Iteration 7076, loss = 0.47166022\n",
      "Iteration 7077, loss = 0.47151691\n",
      "Iteration 7078, loss = 0.47152105\n",
      "Iteration 7079, loss = 0.47126304\n",
      "Iteration 7080, loss = 0.47124663\n",
      "Iteration 7081, loss = 0.47128755\n",
      "Iteration 7082, loss = 0.47140070\n",
      "Iteration 7083, loss = 0.47154725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7084, loss = 0.47151888\n",
      "Iteration 7085, loss = 0.47181948\n",
      "Iteration 7086, loss = 0.47134543\n",
      "Iteration 7087, loss = 0.47138634\n",
      "Iteration 7088, loss = 0.47124646\n",
      "Iteration 7089, loss = 0.47149588\n",
      "Iteration 7090, loss = 0.47125980\n",
      "Iteration 7091, loss = 0.47119781\n",
      "Iteration 7092, loss = 0.47115540\n",
      "Iteration 7093, loss = 0.47118810\n",
      "Iteration 7094, loss = 0.47123383\n",
      "Iteration 7095, loss = 0.47187077\n",
      "Iteration 7096, loss = 0.47156501\n",
      "Iteration 7097, loss = 0.47110304\n",
      "Iteration 7098, loss = 0.47092229\n",
      "Iteration 7099, loss = 0.47133125\n",
      "Iteration 7100, loss = 0.47198383\n",
      "Iteration 7101, loss = 0.47233014\n",
      "Iteration 7102, loss = 0.47231651\n",
      "Iteration 7103, loss = 0.47151435\n",
      "Iteration 7104, loss = 0.47133193\n",
      "Iteration 7105, loss = 0.47143068\n",
      "Iteration 7106, loss = 0.47115328\n",
      "Iteration 7107, loss = 0.47122775\n",
      "Iteration 7108, loss = 0.47149145\n",
      "Iteration 7109, loss = 0.47144151\n",
      "Iteration 7110, loss = 0.47156724\n",
      "Iteration 7111, loss = 0.47159630\n",
      "Iteration 7112, loss = 0.47138724\n",
      "Iteration 7113, loss = 0.47149928\n",
      "Iteration 7114, loss = 0.47124361\n",
      "Iteration 7115, loss = 0.47129989\n",
      "Iteration 7116, loss = 0.47136446\n",
      "Iteration 7117, loss = 0.47127062\n",
      "Iteration 7118, loss = 0.47121994\n",
      "Iteration 7119, loss = 0.47117597\n",
      "Iteration 7120, loss = 0.47125982\n",
      "Iteration 7121, loss = 0.47107298\n",
      "Iteration 7122, loss = 0.47133671\n",
      "Iteration 7123, loss = 0.47137501\n",
      "Iteration 7124, loss = 0.47144493\n",
      "Iteration 7125, loss = 0.47157506\n",
      "Iteration 7126, loss = 0.47128314\n",
      "Iteration 7127, loss = 0.47151456\n",
      "Iteration 7128, loss = 0.47128449\n",
      "Iteration 7129, loss = 0.47116568\n",
      "Iteration 7130, loss = 0.47155012\n",
      "Iteration 7131, loss = 0.47121244\n",
      "Iteration 7132, loss = 0.47118936\n",
      "Iteration 7133, loss = 0.47121800\n",
      "Iteration 7134, loss = 0.47119565\n",
      "Iteration 7135, loss = 0.47123139\n",
      "Iteration 7136, loss = 0.47118453\n",
      "Iteration 7137, loss = 0.47120319\n",
      "Iteration 7138, loss = 0.47130456\n",
      "Iteration 7139, loss = 0.47119886\n",
      "Iteration 7140, loss = 0.47115824\n",
      "Iteration 7141, loss = 0.47131750\n",
      "Iteration 7142, loss = 0.47116424\n",
      "Iteration 7143, loss = 0.47110670\n",
      "Iteration 7144, loss = 0.47121492\n",
      "Iteration 7145, loss = 0.47127310\n",
      "Iteration 7146, loss = 0.47121852\n",
      "Iteration 7147, loss = 0.47189736\n",
      "Iteration 7148, loss = 0.47128883\n",
      "Iteration 7149, loss = 0.47132604\n",
      "Iteration 7150, loss = 0.47133326\n",
      "Iteration 7151, loss = 0.47142475\n",
      "Iteration 7152, loss = 0.47133059\n",
      "Iteration 7153, loss = 0.47134408\n",
      "Iteration 7154, loss = 0.47127625\n",
      "Iteration 7155, loss = 0.47131521\n",
      "Iteration 7156, loss = 0.47137244\n",
      "Iteration 7157, loss = 0.47145600\n",
      "Iteration 7158, loss = 0.47183051\n",
      "Iteration 7159, loss = 0.47139118\n",
      "Iteration 7160, loss = 0.47154440\n",
      "Iteration 7161, loss = 0.47141803\n",
      "Iteration 7162, loss = 0.47138152\n",
      "Iteration 7163, loss = 0.47141593\n",
      "Iteration 7164, loss = 0.47131250\n",
      "Iteration 7165, loss = 0.47114982\n",
      "Iteration 7166, loss = 0.47119296\n",
      "Iteration 7167, loss = 0.47151707\n",
      "Iteration 7168, loss = 0.47186821\n",
      "Iteration 7169, loss = 0.47187632\n",
      "Iteration 7170, loss = 0.47177678\n",
      "Iteration 7171, loss = 0.47155180\n",
      "Iteration 7172, loss = 0.47135054\n",
      "Iteration 7173, loss = 0.47124194\n",
      "Iteration 7174, loss = 0.47130174\n",
      "Iteration 7175, loss = 0.47141317\n",
      "Iteration 7176, loss = 0.47147404\n",
      "Iteration 7177, loss = 0.47165252\n",
      "Iteration 7178, loss = 0.47156827\n",
      "Iteration 7179, loss = 0.47110513\n",
      "Iteration 7180, loss = 0.47122472\n",
      "Iteration 7181, loss = 0.47158617\n",
      "Iteration 7182, loss = 0.47194871\n",
      "Iteration 7183, loss = 0.47180343\n",
      "Iteration 7184, loss = 0.47210534\n",
      "Iteration 7185, loss = 0.47135709\n",
      "Iteration 7186, loss = 0.47145380\n",
      "Iteration 7187, loss = 0.47119906\n",
      "Iteration 7188, loss = 0.47175900\n",
      "Iteration 7189, loss = 0.47121606\n",
      "Iteration 7190, loss = 0.47121107\n",
      "Iteration 7191, loss = 0.47106443\n",
      "Iteration 7192, loss = 0.47128003\n",
      "Iteration 7193, loss = 0.47125030\n",
      "Iteration 7194, loss = 0.47117060\n",
      "Iteration 7195, loss = 0.47158573\n",
      "Iteration 7196, loss = 0.47109416\n",
      "Iteration 7197, loss = 0.47122494\n",
      "Iteration 7198, loss = 0.47118882\n",
      "Iteration 7199, loss = 0.47136453\n",
      "Iteration 7200, loss = 0.47141999\n",
      "Iteration 7201, loss = 0.47138675\n",
      "Iteration 7202, loss = 0.47120855\n",
      "Iteration 7203, loss = 0.47132348\n",
      "Iteration 7204, loss = 0.47120249\n",
      "Iteration 7205, loss = 0.47116259\n",
      "Iteration 7206, loss = 0.47103723\n",
      "Iteration 7207, loss = 0.47107011\n",
      "Iteration 7208, loss = 0.47150191\n",
      "Iteration 7209, loss = 0.47159773\n",
      "Iteration 7210, loss = 0.47109798\n",
      "Iteration 7211, loss = 0.47090669\n",
      "Iteration 7212, loss = 0.47136023\n",
      "Iteration 7213, loss = 0.47213139\n",
      "Iteration 7214, loss = 0.47235072\n",
      "Iteration 7215, loss = 0.47157738\n",
      "Iteration 7216, loss = 0.47116777\n",
      "Iteration 7217, loss = 0.47116597\n",
      "Iteration 7218, loss = 0.47229516\n",
      "Iteration 7219, loss = 0.47285419\n",
      "Iteration 7220, loss = 0.47282145\n",
      "Iteration 7221, loss = 0.47225022\n",
      "Iteration 7222, loss = 0.47166725\n",
      "Iteration 7223, loss = 0.47126989\n",
      "Iteration 7224, loss = 0.47131437\n",
      "Iteration 7225, loss = 0.47116564\n",
      "Iteration 7226, loss = 0.47154578\n",
      "Iteration 7227, loss = 0.47191198\n",
      "Iteration 7228, loss = 0.47122742\n",
      "Iteration 7229, loss = 0.47123921\n",
      "Iteration 7230, loss = 0.47139263\n",
      "Iteration 7231, loss = 0.47118928\n",
      "Iteration 7232, loss = 0.47116300\n",
      "Iteration 7233, loss = 0.47128380\n",
      "Iteration 7234, loss = 0.47137291\n",
      "Iteration 7235, loss = 0.47141346\n",
      "Iteration 7236, loss = 0.47136144\n",
      "Iteration 7237, loss = 0.47111802\n",
      "Iteration 7238, loss = 0.47146418\n",
      "Iteration 7239, loss = 0.47144102\n",
      "Iteration 7240, loss = 0.47211873\n",
      "Iteration 7241, loss = 0.47134944\n",
      "Iteration 7242, loss = 0.47116847\n",
      "Iteration 7243, loss = 0.47175967\n",
      "Iteration 7244, loss = 0.47187468\n",
      "Iteration 7245, loss = 0.47144488\n",
      "Iteration 7246, loss = 0.47182741\n",
      "Iteration 7247, loss = 0.47117582\n",
      "Iteration 7248, loss = 0.47120863\n",
      "Iteration 7249, loss = 0.47112873\n",
      "Iteration 7250, loss = 0.47148894\n",
      "Iteration 7251, loss = 0.47130302\n",
      "Iteration 7252, loss = 0.47117749\n",
      "Iteration 7253, loss = 0.47124416\n",
      "Iteration 7254, loss = 0.47143387\n",
      "Iteration 7255, loss = 0.47115812\n",
      "Iteration 7256, loss = 0.47123051\n",
      "Iteration 7257, loss = 0.47118453\n",
      "Iteration 7258, loss = 0.47115635\n",
      "Iteration 7259, loss = 0.47114165\n",
      "Iteration 7260, loss = 0.47150311\n",
      "Iteration 7261, loss = 0.47112023\n",
      "Iteration 7262, loss = 0.47134747\n",
      "Iteration 7263, loss = 0.47142448\n",
      "Iteration 7264, loss = 0.47133585\n",
      "Iteration 7265, loss = 0.47118754\n",
      "Iteration 7266, loss = 0.47118535\n",
      "Iteration 7267, loss = 0.47160667\n",
      "Iteration 7268, loss = 0.47145740\n",
      "Iteration 7269, loss = 0.47128348\n",
      "Iteration 7270, loss = 0.47096104\n",
      "Iteration 7271, loss = 0.47136186\n",
      "Iteration 7272, loss = 0.47208860\n",
      "Iteration 7273, loss = 0.47175420\n",
      "Iteration 7274, loss = 0.47100747\n",
      "Iteration 7275, loss = 0.47122453\n",
      "Iteration 7276, loss = 0.47152821\n",
      "Iteration 7277, loss = 0.47163682\n",
      "Iteration 7278, loss = 0.47166952\n",
      "Iteration 7279, loss = 0.47152455\n",
      "Iteration 7280, loss = 0.47158213\n",
      "Iteration 7281, loss = 0.47150283\n",
      "Iteration 7282, loss = 0.47136963\n",
      "Iteration 7283, loss = 0.47134461\n",
      "Iteration 7284, loss = 0.47110581\n",
      "Iteration 7285, loss = 0.47115497\n",
      "Iteration 7286, loss = 0.47105028\n",
      "Iteration 7287, loss = 0.47132207\n",
      "Iteration 7288, loss = 0.47139793\n",
      "Iteration 7289, loss = 0.47112123\n",
      "Iteration 7290, loss = 0.47202276\n",
      "Iteration 7291, loss = 0.47121349\n",
      "Iteration 7292, loss = 0.47110927\n",
      "Iteration 7293, loss = 0.47112661\n",
      "Iteration 7294, loss = 0.47127162\n",
      "Iteration 7295, loss = 0.47114966\n",
      "Iteration 7296, loss = 0.47156276\n",
      "Iteration 7297, loss = 0.47144863\n",
      "Iteration 7298, loss = 0.47105725\n",
      "Iteration 7299, loss = 0.47122720\n",
      "Iteration 7300, loss = 0.47134800\n",
      "Iteration 7301, loss = 0.47157284\n",
      "Iteration 7302, loss = 0.47127524\n",
      "Iteration 7303, loss = 0.47103716\n",
      "Iteration 7304, loss = 0.47115584\n",
      "Iteration 7305, loss = 0.47148399\n",
      "Iteration 7306, loss = 0.47197784\n",
      "Iteration 7307, loss = 0.47136657\n",
      "Iteration 7308, loss = 0.47140870\n",
      "Iteration 7309, loss = 0.47151640\n",
      "Iteration 7310, loss = 0.47179525\n",
      "Iteration 7311, loss = 0.47174621\n",
      "Iteration 7312, loss = 0.47139040\n",
      "Iteration 7313, loss = 0.47151868\n",
      "Iteration 7314, loss = 0.47118445\n",
      "Iteration 7315, loss = 0.47175445\n",
      "Iteration 7316, loss = 0.47159781\n",
      "Iteration 7317, loss = 0.47131163\n",
      "Iteration 7318, loss = 0.47200186\n",
      "Iteration 7319, loss = 0.47121363\n",
      "Iteration 7320, loss = 0.47098313\n",
      "Iteration 7321, loss = 0.47145668\n",
      "Iteration 7322, loss = 0.47168414\n",
      "Iteration 7323, loss = 0.47118061\n",
      "Iteration 7324, loss = 0.47144323\n",
      "Iteration 7325, loss = 0.47151033\n",
      "Iteration 7326, loss = 0.47142593\n",
      "Iteration 7327, loss = 0.47116691\n",
      "Iteration 7328, loss = 0.47141201\n",
      "Iteration 7329, loss = 0.47128989\n",
      "Iteration 7330, loss = 0.47109707\n",
      "Iteration 7331, loss = 0.47122437\n",
      "Iteration 7332, loss = 0.47115822\n",
      "Iteration 7333, loss = 0.47116020\n",
      "Iteration 7334, loss = 0.47128631\n",
      "Iteration 7335, loss = 0.47125305\n",
      "Iteration 7336, loss = 0.47172773\n",
      "Iteration 7337, loss = 0.47196949\n",
      "Iteration 7338, loss = 0.47112168\n",
      "Iteration 7339, loss = 0.47106152\n",
      "Iteration 7340, loss = 0.47117128\n",
      "Iteration 7341, loss = 0.47110222\n",
      "Iteration 7342, loss = 0.47107124\n",
      "Iteration 7343, loss = 0.47125961\n",
      "Iteration 7344, loss = 0.47168615\n",
      "Iteration 7345, loss = 0.47168956\n",
      "Iteration 7346, loss = 0.47107193\n",
      "Iteration 7347, loss = 0.47130126\n",
      "Iteration 7348, loss = 0.47180207\n",
      "Iteration 7349, loss = 0.47182155\n",
      "Iteration 7350, loss = 0.47145905\n",
      "Iteration 7351, loss = 0.47093637\n",
      "Iteration 7352, loss = 0.47179850\n",
      "Iteration 7353, loss = 0.47181414\n",
      "Iteration 7354, loss = 0.47154117\n",
      "Iteration 7355, loss = 0.47209127\n",
      "Iteration 7356, loss = 0.47137061\n",
      "Iteration 7357, loss = 0.47141521\n",
      "Iteration 7358, loss = 0.47133283\n",
      "Iteration 7359, loss = 0.47122088\n",
      "Iteration 7360, loss = 0.47134895\n",
      "Iteration 7361, loss = 0.47152487\n",
      "Iteration 7362, loss = 0.47147881\n",
      "Iteration 7363, loss = 0.47154988\n",
      "Iteration 7364, loss = 0.47126568\n",
      "Iteration 7365, loss = 0.47117298\n",
      "Iteration 7366, loss = 0.47127685\n",
      "Iteration 7367, loss = 0.47151619\n",
      "Iteration 7368, loss = 0.47156397\n",
      "Iteration 7369, loss = 0.47159184\n",
      "Iteration 7370, loss = 0.47111971\n",
      "Iteration 7371, loss = 0.47098289\n",
      "Iteration 7372, loss = 0.47140537\n",
      "Iteration 7373, loss = 0.47187044\n",
      "Iteration 7374, loss = 0.47165610\n",
      "Iteration 7375, loss = 0.47117126\n",
      "Iteration 7376, loss = 0.47093785\n",
      "Iteration 7377, loss = 0.47134320\n",
      "Iteration 7378, loss = 0.47264853\n",
      "Iteration 7379, loss = 0.47256971\n",
      "Iteration 7380, loss = 0.47261890\n",
      "Iteration 7381, loss = 0.47131478\n",
      "Iteration 7382, loss = 0.47099174\n",
      "Iteration 7383, loss = 0.47139927\n",
      "Iteration 7384, loss = 0.47163263\n",
      "Iteration 7385, loss = 0.47162264\n",
      "Iteration 7386, loss = 0.47139616\n",
      "Iteration 7387, loss = 0.47101059\n",
      "Iteration 7388, loss = 0.47149807\n",
      "Iteration 7389, loss = 0.47137659\n",
      "Iteration 7390, loss = 0.47127684\n",
      "Iteration 7391, loss = 0.47157705\n",
      "Iteration 7392, loss = 0.47124283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7393, loss = 0.47113825\n",
      "Iteration 7394, loss = 0.47133103\n",
      "Iteration 7395, loss = 0.47114168\n",
      "Iteration 7396, loss = 0.47118102\n",
      "Iteration 7397, loss = 0.47105080\n",
      "Iteration 7398, loss = 0.47135435\n",
      "Iteration 7399, loss = 0.47130638\n",
      "Iteration 7400, loss = 0.47126568\n",
      "Iteration 7401, loss = 0.47106259\n",
      "Iteration 7402, loss = 0.47112099\n",
      "Iteration 7403, loss = 0.47124799\n",
      "Iteration 7404, loss = 0.47137889\n",
      "Iteration 7405, loss = 0.47153985\n",
      "Iteration 7406, loss = 0.47129542\n",
      "Iteration 7407, loss = 0.47151470\n",
      "Iteration 7408, loss = 0.47127095\n",
      "Iteration 7409, loss = 0.47118900\n",
      "Iteration 7410, loss = 0.47105652\n",
      "Iteration 7411, loss = 0.47115032\n",
      "Iteration 7412, loss = 0.47120675\n",
      "Iteration 7413, loss = 0.47150062\n",
      "Iteration 7414, loss = 0.47115013\n",
      "Iteration 7415, loss = 0.47121840\n",
      "Iteration 7416, loss = 0.47142043\n",
      "Iteration 7417, loss = 0.47127175\n",
      "Iteration 7418, loss = 0.47109747\n",
      "Iteration 7419, loss = 0.47111691\n",
      "Iteration 7420, loss = 0.47108970\n",
      "Iteration 7421, loss = 0.47132730\n",
      "Iteration 7422, loss = 0.47148475\n",
      "Iteration 7423, loss = 0.47150782\n",
      "Iteration 7424, loss = 0.47146192\n",
      "Iteration 7425, loss = 0.47139682\n",
      "Iteration 7426, loss = 0.47164099\n",
      "Iteration 7427, loss = 0.47163801\n",
      "Iteration 7428, loss = 0.47118351\n",
      "Iteration 7429, loss = 0.47125979\n",
      "Iteration 7430, loss = 0.47113841\n",
      "Iteration 7431, loss = 0.47140640\n",
      "Iteration 7432, loss = 0.47140466\n",
      "Iteration 7433, loss = 0.47123997\n",
      "Iteration 7434, loss = 0.47127148\n",
      "Iteration 7435, loss = 0.47181812\n",
      "Iteration 7436, loss = 0.47120962\n",
      "Iteration 7437, loss = 0.47118183\n",
      "Iteration 7438, loss = 0.47115335\n",
      "Iteration 7439, loss = 0.47107956\n",
      "Iteration 7440, loss = 0.47118576\n",
      "Iteration 7441, loss = 0.47117959\n",
      "Iteration 7442, loss = 0.47140155\n",
      "Iteration 7443, loss = 0.47117163\n",
      "Iteration 7444, loss = 0.47097088\n",
      "Iteration 7445, loss = 0.47111924\n",
      "Iteration 7446, loss = 0.47146198\n",
      "Iteration 7447, loss = 0.47150382\n",
      "Iteration 7448, loss = 0.47093491\n",
      "Iteration 7449, loss = 0.47146653\n",
      "Iteration 7450, loss = 0.47161645\n",
      "Iteration 7451, loss = 0.47190425\n",
      "Iteration 7452, loss = 0.47216676\n",
      "Iteration 7453, loss = 0.47149726\n",
      "Iteration 7454, loss = 0.47125576\n",
      "Iteration 7455, loss = 0.47199436\n",
      "Iteration 7456, loss = 0.47130063\n",
      "Iteration 7457, loss = 0.47169861\n",
      "Iteration 7458, loss = 0.47104004\n",
      "Iteration 7459, loss = 0.47121091\n",
      "Iteration 7460, loss = 0.47106995\n",
      "Iteration 7461, loss = 0.47104291\n",
      "Iteration 7462, loss = 0.47173609\n",
      "Iteration 7463, loss = 0.47126627\n",
      "Iteration 7464, loss = 0.47135475\n",
      "Iteration 7465, loss = 0.47110713\n",
      "Iteration 7466, loss = 0.47145775\n",
      "Iteration 7467, loss = 0.47132159\n",
      "Iteration 7468, loss = 0.47110675\n",
      "Iteration 7469, loss = 0.47113651\n",
      "Iteration 7470, loss = 0.47119968\n",
      "Iteration 7471, loss = 0.47136383\n",
      "Iteration 7472, loss = 0.47157463\n",
      "Iteration 7473, loss = 0.47152664\n",
      "Iteration 7474, loss = 0.47128346\n",
      "Iteration 7475, loss = 0.47124796\n",
      "Iteration 7476, loss = 0.47124242\n",
      "Iteration 7477, loss = 0.47142971\n",
      "Iteration 7478, loss = 0.47183302\n",
      "Iteration 7479, loss = 0.47194646\n",
      "Iteration 7480, loss = 0.47161394\n",
      "Iteration 7481, loss = 0.47161619\n",
      "Iteration 7482, loss = 0.47159609\n",
      "Iteration 7483, loss = 0.47120916\n",
      "Iteration 7484, loss = 0.47115562\n",
      "Iteration 7485, loss = 0.47122943\n",
      "Iteration 7486, loss = 0.47144970\n",
      "Iteration 7487, loss = 0.47114372\n",
      "Iteration 7488, loss = 0.47141770\n",
      "Iteration 7489, loss = 0.47189911\n",
      "Iteration 7490, loss = 0.47144973\n",
      "Iteration 7491, loss = 0.47131536\n",
      "Iteration 7492, loss = 0.47126910\n",
      "Iteration 7493, loss = 0.47140170\n",
      "Iteration 7494, loss = 0.47136791\n",
      "Iteration 7495, loss = 0.47103810\n",
      "Iteration 7496, loss = 0.47127628\n",
      "Iteration 7497, loss = 0.47123939\n",
      "Iteration 7498, loss = 0.47131788\n",
      "Iteration 7499, loss = 0.47120294\n",
      "Iteration 7500, loss = 0.47107746\n",
      "Iteration 7501, loss = 0.47115601\n",
      "Iteration 7502, loss = 0.47148735\n",
      "Iteration 7503, loss = 0.47192354\n",
      "Iteration 7504, loss = 0.47152051\n",
      "Iteration 7505, loss = 0.47165813\n",
      "Iteration 7506, loss = 0.47176245\n",
      "Iteration 7507, loss = 0.47162536\n",
      "Iteration 7508, loss = 0.47104096\n",
      "Iteration 7509, loss = 0.47135306\n",
      "Iteration 7510, loss = 0.47134575\n",
      "Iteration 7511, loss = 0.47131566\n",
      "Iteration 7512, loss = 0.47166779\n",
      "Iteration 7513, loss = 0.47123959\n",
      "Iteration 7514, loss = 0.47130159\n",
      "Iteration 7515, loss = 0.47105511\n",
      "Iteration 7516, loss = 0.47111707\n",
      "Iteration 7517, loss = 0.47104106\n",
      "Iteration 7518, loss = 0.47112686\n",
      "Iteration 7519, loss = 0.47116020\n",
      "Iteration 7520, loss = 0.47113008\n",
      "Iteration 7521, loss = 0.47112876\n",
      "Iteration 7522, loss = 0.47124653\n",
      "Iteration 7523, loss = 0.47120813\n",
      "Iteration 7524, loss = 0.47120467\n",
      "Iteration 7525, loss = 0.47107994\n",
      "Iteration 7526, loss = 0.47120052\n",
      "Iteration 7527, loss = 0.47153993\n",
      "Iteration 7528, loss = 0.47090058\n",
      "Iteration 7529, loss = 0.47100437\n",
      "Iteration 7530, loss = 0.47219240\n",
      "Iteration 7531, loss = 0.47182071\n",
      "Iteration 7532, loss = 0.47145177\n",
      "Iteration 7533, loss = 0.47104456\n",
      "Iteration 7534, loss = 0.47165202\n",
      "Iteration 7535, loss = 0.47134654\n",
      "Iteration 7536, loss = 0.47113482\n",
      "Iteration 7537, loss = 0.47110587\n",
      "Iteration 7538, loss = 0.47100899\n",
      "Iteration 7539, loss = 0.47121348\n",
      "Iteration 7540, loss = 0.47140246\n",
      "Iteration 7541, loss = 0.47135001\n",
      "Iteration 7542, loss = 0.47144077\n",
      "Iteration 7543, loss = 0.47126810\n",
      "Iteration 7544, loss = 0.47168199\n",
      "Iteration 7545, loss = 0.47116370\n",
      "Iteration 7546, loss = 0.47104208\n",
      "Iteration 7547, loss = 0.47145156\n",
      "Iteration 7548, loss = 0.47178555\n",
      "Iteration 7549, loss = 0.47159055\n",
      "Iteration 7550, loss = 0.47178017\n",
      "Iteration 7551, loss = 0.47153495\n",
      "Iteration 7552, loss = 0.47132070\n",
      "Iteration 7553, loss = 0.47122032\n",
      "Iteration 7554, loss = 0.47123714\n",
      "Iteration 7555, loss = 0.47112574\n",
      "Iteration 7556, loss = 0.47108196\n",
      "Iteration 7557, loss = 0.47110222\n",
      "Iteration 7558, loss = 0.47116508\n",
      "Iteration 7559, loss = 0.47103928\n",
      "Iteration 7560, loss = 0.47157642\n",
      "Iteration 7561, loss = 0.47097772\n",
      "Iteration 7562, loss = 0.47138112\n",
      "Iteration 7563, loss = 0.47158744\n",
      "Iteration 7564, loss = 0.47131846\n",
      "Iteration 7565, loss = 0.47108042\n",
      "Iteration 7566, loss = 0.47127558\n",
      "Iteration 7567, loss = 0.47121946\n",
      "Iteration 7568, loss = 0.47128144\n",
      "Iteration 7569, loss = 0.47110883\n",
      "Iteration 7570, loss = 0.47095466\n",
      "Iteration 7571, loss = 0.47112658\n",
      "Iteration 7572, loss = 0.47129595\n",
      "Iteration 7573, loss = 0.47134080\n",
      "Iteration 7574, loss = 0.47131864\n",
      "Iteration 7575, loss = 0.47103746\n",
      "Iteration 7576, loss = 0.47125333\n",
      "Iteration 7577, loss = 0.47098591\n",
      "Iteration 7578, loss = 0.47105009\n",
      "Iteration 7579, loss = 0.47131973\n",
      "Iteration 7580, loss = 0.47113183\n",
      "Iteration 7581, loss = 0.47174714\n",
      "Iteration 7582, loss = 0.47218753\n",
      "Iteration 7583, loss = 0.47225286\n",
      "Iteration 7584, loss = 0.47187562\n",
      "Iteration 7585, loss = 0.47138853\n",
      "Iteration 7586, loss = 0.47140794\n",
      "Iteration 7587, loss = 0.47130093\n",
      "Iteration 7588, loss = 0.47128320\n",
      "Iteration 7589, loss = 0.47138338\n",
      "Iteration 7590, loss = 0.47132047\n",
      "Iteration 7591, loss = 0.47121732\n",
      "Iteration 7592, loss = 0.47140733\n",
      "Iteration 7593, loss = 0.47169273\n",
      "Iteration 7594, loss = 0.47185172\n",
      "Iteration 7595, loss = 0.47186906\n",
      "Iteration 7596, loss = 0.47131373\n",
      "Iteration 7597, loss = 0.47125824\n",
      "Iteration 7598, loss = 0.47120722\n",
      "Iteration 7599, loss = 0.47104030\n",
      "Iteration 7600, loss = 0.47100196\n",
      "Iteration 7601, loss = 0.47101541\n",
      "Iteration 7602, loss = 0.47111638\n",
      "Iteration 7603, loss = 0.47144314\n",
      "Iteration 7604, loss = 0.47159030\n",
      "Iteration 7605, loss = 0.47103818\n",
      "Iteration 7606, loss = 0.47112210\n",
      "Iteration 7607, loss = 0.47106676\n",
      "Iteration 7608, loss = 0.47104191\n",
      "Iteration 7609, loss = 0.47098812\n",
      "Iteration 7610, loss = 0.47134022\n",
      "Iteration 7611, loss = 0.47119565\n",
      "Iteration 7612, loss = 0.47108945\n",
      "Iteration 7613, loss = 0.47202584\n",
      "Iteration 7614, loss = 0.47146683\n",
      "Iteration 7615, loss = 0.47166204\n",
      "Iteration 7616, loss = 0.47118004\n",
      "Iteration 7617, loss = 0.47153787\n",
      "Iteration 7618, loss = 0.47120719\n",
      "Iteration 7619, loss = 0.47116712\n",
      "Iteration 7620, loss = 0.47103526\n",
      "Iteration 7621, loss = 0.47150103\n",
      "Iteration 7622, loss = 0.47136891\n",
      "Iteration 7623, loss = 0.47109459\n",
      "Iteration 7624, loss = 0.47115773\n",
      "Iteration 7625, loss = 0.47114216\n",
      "Iteration 7626, loss = 0.47112577\n",
      "Iteration 7627, loss = 0.47107101\n",
      "Iteration 7628, loss = 0.47138447\n",
      "Iteration 7629, loss = 0.47116157\n",
      "Iteration 7630, loss = 0.47120243\n",
      "Iteration 7631, loss = 0.47113908\n",
      "Iteration 7632, loss = 0.47130213\n",
      "Iteration 7633, loss = 0.47120561\n",
      "Iteration 7634, loss = 0.47117037\n",
      "Iteration 7635, loss = 0.47105110\n",
      "Iteration 7636, loss = 0.47103200\n",
      "Iteration 7637, loss = 0.47119135\n",
      "Iteration 7638, loss = 0.47119519\n",
      "Iteration 7639, loss = 0.47127978\n",
      "Iteration 7640, loss = 0.47111044\n",
      "Iteration 7641, loss = 0.47106246\n",
      "Iteration 7642, loss = 0.47139483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7643, loss = 0.47108392\n",
      "Iteration 7644, loss = 0.47103901\n",
      "Iteration 7645, loss = 0.47104610\n",
      "Iteration 7646, loss = 0.47105668\n",
      "Iteration 7647, loss = 0.47119736\n",
      "Iteration 7648, loss = 0.47113961\n",
      "Iteration 7649, loss = 0.47108252\n",
      "Iteration 7650, loss = 0.47123401\n",
      "Iteration 7651, loss = 0.47119228\n",
      "Iteration 7652, loss = 0.47109383\n",
      "Iteration 7653, loss = 0.47107976\n",
      "Iteration 7654, loss = 0.47136248\n",
      "Iteration 7655, loss = 0.47177356\n",
      "Iteration 7656, loss = 0.47164649\n",
      "Iteration 7657, loss = 0.47178076\n",
      "Iteration 7658, loss = 0.47131052\n",
      "Iteration 7659, loss = 0.47108669\n",
      "Iteration 7660, loss = 0.47102117\n",
      "Iteration 7661, loss = 0.47111277\n",
      "Iteration 7662, loss = 0.47113929\n",
      "Iteration 7663, loss = 0.47121587\n",
      "Iteration 7664, loss = 0.47101491\n",
      "Iteration 7665, loss = 0.47099670\n",
      "Iteration 7666, loss = 0.47105852\n",
      "Iteration 7667, loss = 0.47126104\n",
      "Iteration 7668, loss = 0.47136231\n",
      "Iteration 7669, loss = 0.47145136\n",
      "Iteration 7670, loss = 0.47159910\n",
      "Iteration 7671, loss = 0.47120661\n",
      "Iteration 7672, loss = 0.47136263\n",
      "Iteration 7673, loss = 0.47125118\n",
      "Iteration 7674, loss = 0.47106075\n",
      "Iteration 7675, loss = 0.47121594\n",
      "Iteration 7676, loss = 0.47120305\n",
      "Iteration 7677, loss = 0.47136408\n",
      "Iteration 7678, loss = 0.47135502\n",
      "Iteration 7679, loss = 0.47104893\n",
      "Iteration 7680, loss = 0.47095003\n",
      "Iteration 7681, loss = 0.47120004\n",
      "Iteration 7682, loss = 0.47117670\n",
      "Iteration 7683, loss = 0.47115488\n",
      "Iteration 7684, loss = 0.47111063\n",
      "Iteration 7685, loss = 0.47104756\n",
      "Iteration 7686, loss = 0.47113998\n",
      "Iteration 7687, loss = 0.47112115\n",
      "Iteration 7688, loss = 0.47133664\n",
      "Iteration 7689, loss = 0.47110586\n",
      "Iteration 7690, loss = 0.47088754\n",
      "Iteration 7691, loss = 0.47124945\n",
      "Iteration 7692, loss = 0.47139334\n",
      "Iteration 7693, loss = 0.47140579\n",
      "Iteration 7694, loss = 0.47105664\n",
      "Iteration 7695, loss = 0.47104369\n",
      "Iteration 7696, loss = 0.47144278\n",
      "Iteration 7697, loss = 0.47256221\n",
      "Iteration 7698, loss = 0.47284002\n",
      "Iteration 7699, loss = 0.47217891\n",
      "Iteration 7700, loss = 0.47160533\n",
      "Iteration 7701, loss = 0.47159041\n",
      "Iteration 7702, loss = 0.47155474\n",
      "Iteration 7703, loss = 0.47154055\n",
      "Iteration 7704, loss = 0.47120513\n",
      "Iteration 7705, loss = 0.47099037\n",
      "Iteration 7706, loss = 0.47140496\n",
      "Iteration 7707, loss = 0.47144703\n",
      "Iteration 7708, loss = 0.47114858\n",
      "Iteration 7709, loss = 0.47126695\n",
      "Iteration 7710, loss = 0.47116454\n",
      "Iteration 7711, loss = 0.47125960\n",
      "Iteration 7712, loss = 0.47116548\n",
      "Iteration 7713, loss = 0.47137287\n",
      "Iteration 7714, loss = 0.47146104\n",
      "Iteration 7715, loss = 0.47136443\n",
      "Iteration 7716, loss = 0.47125094\n",
      "Iteration 7717, loss = 0.47096253\n",
      "Iteration 7718, loss = 0.47102055\n",
      "Iteration 7719, loss = 0.47115834\n",
      "Iteration 7720, loss = 0.47113181\n",
      "Iteration 7721, loss = 0.47125600\n",
      "Iteration 7722, loss = 0.47130902\n",
      "Iteration 7723, loss = 0.47121487\n",
      "Iteration 7724, loss = 0.47130458\n",
      "Iteration 7725, loss = 0.47132285\n",
      "Iteration 7726, loss = 0.47142521\n",
      "Iteration 7727, loss = 0.47155554\n",
      "Iteration 7728, loss = 0.47140765\n",
      "Iteration 7729, loss = 0.47161950\n",
      "Iteration 7730, loss = 0.47175280\n",
      "Iteration 7731, loss = 0.47137992\n",
      "Iteration 7732, loss = 0.47103736\n",
      "Iteration 7733, loss = 0.47126998\n",
      "Iteration 7734, loss = 0.47133352\n",
      "Iteration 7735, loss = 0.47141743\n",
      "Iteration 7736, loss = 0.47166159\n",
      "Iteration 7737, loss = 0.47142665\n",
      "Iteration 7738, loss = 0.47101760\n",
      "Iteration 7739, loss = 0.47131367\n",
      "Iteration 7740, loss = 0.47146675\n",
      "Iteration 7741, loss = 0.47140384\n",
      "Iteration 7742, loss = 0.47119872\n",
      "Iteration 7743, loss = 0.47128706\n",
      "Iteration 7744, loss = 0.47126156\n",
      "Iteration 7745, loss = 0.47134438\n",
      "Iteration 7746, loss = 0.47134171\n",
      "Iteration 7747, loss = 0.47135445\n",
      "Iteration 7748, loss = 0.47137273\n",
      "Iteration 7749, loss = 0.47133679\n",
      "Iteration 7750, loss = 0.47147859\n",
      "Iteration 7751, loss = 0.47115846\n",
      "Iteration 7752, loss = 0.47119314\n",
      "Iteration 7753, loss = 0.47145645\n",
      "Iteration 7754, loss = 0.47127838\n",
      "Iteration 7755, loss = 0.47127715\n",
      "Iteration 7756, loss = 0.47116756\n",
      "Iteration 7757, loss = 0.47117268\n",
      "Iteration 7758, loss = 0.47132670\n",
      "Iteration 7759, loss = 0.47132362\n",
      "Iteration 7760, loss = 0.47142497\n",
      "Iteration 7761, loss = 0.47127119\n",
      "Iteration 7762, loss = 0.47126194\n",
      "Iteration 7763, loss = 0.47112954\n",
      "Iteration 7764, loss = 0.47114674\n",
      "Iteration 7765, loss = 0.47125828\n",
      "Iteration 7766, loss = 0.47136647\n",
      "Iteration 7767, loss = 0.47124051\n",
      "Iteration 7768, loss = 0.47134640\n",
      "Iteration 7769, loss = 0.47191675\n",
      "Iteration 7770, loss = 0.47117346\n",
      "Iteration 7771, loss = 0.47122309\n",
      "Iteration 7772, loss = 0.47114182\n",
      "Iteration 7773, loss = 0.47107421\n",
      "Iteration 7774, loss = 0.47113353\n",
      "Iteration 7775, loss = 0.47141001\n",
      "Iteration 7776, loss = 0.47124862\n",
      "Iteration 7777, loss = 0.47128416\n",
      "Iteration 7778, loss = 0.47113739\n",
      "Iteration 7779, loss = 0.47120139\n",
      "Iteration 7780, loss = 0.47115795\n",
      "Iteration 7781, loss = 0.47124599\n",
      "Iteration 7782, loss = 0.47110516\n",
      "Iteration 7783, loss = 0.47102929\n",
      "Iteration 7784, loss = 0.47110080\n",
      "Iteration 7785, loss = 0.47134395\n",
      "Iteration 7786, loss = 0.47155645\n",
      "Iteration 7787, loss = 0.47177028\n",
      "Iteration 7788, loss = 0.47191345\n",
      "Iteration 7789, loss = 0.47114875\n",
      "Iteration 7790, loss = 0.47122729\n",
      "Iteration 7791, loss = 0.47112512\n",
      "Iteration 7792, loss = 0.47130128\n",
      "Iteration 7793, loss = 0.47152252\n",
      "Iteration 7794, loss = 0.47112663\n",
      "Iteration 7795, loss = 0.47122248\n",
      "Iteration 7796, loss = 0.47131175\n",
      "Iteration 7797, loss = 0.47209857\n",
      "Iteration 7798, loss = 0.47193693\n",
      "Iteration 7799, loss = 0.47141416\n",
      "Iteration 7800, loss = 0.47113754\n",
      "Iteration 7801, loss = 0.47106084\n",
      "Iteration 7802, loss = 0.47112236\n",
      "Iteration 7803, loss = 0.47142454\n",
      "Iteration 7804, loss = 0.47163151\n",
      "Iteration 7805, loss = 0.47158758\n",
      "Iteration 7806, loss = 0.47128376\n",
      "Iteration 7807, loss = 0.47139679\n",
      "Iteration 7808, loss = 0.47117816\n",
      "Iteration 7809, loss = 0.47132661\n",
      "Iteration 7810, loss = 0.47107032\n",
      "Iteration 7811, loss = 0.47137174\n",
      "Iteration 7812, loss = 0.47143175\n",
      "Iteration 7813, loss = 0.47139659\n",
      "Iteration 7814, loss = 0.47127903\n",
      "Iteration 7815, loss = 0.47113493\n",
      "Iteration 7816, loss = 0.47112163\n",
      "Iteration 7817, loss = 0.47110013\n",
      "Iteration 7818, loss = 0.47119012\n",
      "Iteration 7819, loss = 0.47141594\n",
      "Iteration 7820, loss = 0.47186880\n",
      "Iteration 7821, loss = 0.47134902\n",
      "Iteration 7822, loss = 0.47096999\n",
      "Iteration 7823, loss = 0.47135092\n",
      "Iteration 7824, loss = 0.47159586\n",
      "Iteration 7825, loss = 0.47169933\n",
      "Iteration 7826, loss = 0.47137282\n",
      "Iteration 7827, loss = 0.47122214\n",
      "Iteration 7828, loss = 0.47107855\n",
      "Iteration 7829, loss = 0.47125502\n",
      "Iteration 7830, loss = 0.47144079\n",
      "Iteration 7831, loss = 0.47158313\n",
      "Iteration 7832, loss = 0.47132755\n",
      "Iteration 7833, loss = 0.47123340\n",
      "Iteration 7834, loss = 0.47146888\n",
      "Iteration 7835, loss = 0.47112259\n",
      "Iteration 7836, loss = 0.47120123\n",
      "Iteration 7837, loss = 0.47133912\n",
      "Iteration 7838, loss = 0.47140971\n",
      "Iteration 7839, loss = 0.47140919\n",
      "Iteration 7840, loss = 0.47119678\n",
      "Iteration 7841, loss = 0.47099822\n",
      "Iteration 7842, loss = 0.47177709\n",
      "Iteration 7843, loss = 0.47127947\n",
      "Iteration 7844, loss = 0.47104788\n",
      "Iteration 7845, loss = 0.47104389\n",
      "Iteration 7846, loss = 0.47130376\n",
      "Iteration 7847, loss = 0.47126675\n",
      "Iteration 7848, loss = 0.47119324\n",
      "Iteration 7849, loss = 0.47116532\n",
      "Iteration 7850, loss = 0.47113854\n",
      "Iteration 7851, loss = 0.47121441\n",
      "Iteration 7852, loss = 0.47124084\n",
      "Iteration 7853, loss = 0.47104567\n",
      "Iteration 7854, loss = 0.47116880\n",
      "Iteration 7855, loss = 0.47116154\n",
      "Iteration 7856, loss = 0.47102156\n",
      "Iteration 7857, loss = 0.47120873\n",
      "Iteration 7858, loss = 0.47139629\n",
      "Iteration 7859, loss = 0.47115242\n",
      "Iteration 7860, loss = 0.47122539\n",
      "Iteration 7861, loss = 0.47138835\n",
      "Iteration 7862, loss = 0.47140550\n",
      "Iteration 7863, loss = 0.47150472\n",
      "Iteration 7864, loss = 0.47135862\n",
      "Iteration 7865, loss = 0.47110289\n",
      "Iteration 7866, loss = 0.47088351\n",
      "Iteration 7867, loss = 0.47170745\n",
      "Iteration 7868, loss = 0.47143558\n",
      "Iteration 7869, loss = 0.47137288\n",
      "Iteration 7870, loss = 0.47100954\n",
      "Iteration 7871, loss = 0.47102296\n",
      "Iteration 7872, loss = 0.47107980\n",
      "Iteration 7873, loss = 0.47119262\n",
      "Iteration 7874, loss = 0.47118455\n",
      "Iteration 7875, loss = 0.47100298\n",
      "Iteration 7876, loss = 0.47097829\n",
      "Iteration 7877, loss = 0.47108670\n",
      "Iteration 7878, loss = 0.47141455\n",
      "Iteration 7879, loss = 0.47168121\n",
      "Iteration 7880, loss = 0.47124210\n",
      "Iteration 7881, loss = 0.47119935\n",
      "Iteration 7882, loss = 0.47134272\n",
      "Iteration 7883, loss = 0.47109358\n",
      "Iteration 7884, loss = 0.47120343\n",
      "Iteration 7885, loss = 0.47108783\n",
      "Iteration 7886, loss = 0.47127132\n",
      "Iteration 7887, loss = 0.47137709\n",
      "Iteration 7888, loss = 0.47187722\n",
      "Iteration 7889, loss = 0.47159164\n",
      "Iteration 7890, loss = 0.47149663\n",
      "Iteration 7891, loss = 0.47121097\n",
      "Iteration 7892, loss = 0.47114555\n",
      "Iteration 7893, loss = 0.47108497\n",
      "Iteration 7894, loss = 0.47100123\n",
      "Iteration 7895, loss = 0.47143562\n",
      "Iteration 7896, loss = 0.47152276\n",
      "Iteration 7897, loss = 0.47130524\n",
      "Iteration 7898, loss = 0.47206136\n",
      "Iteration 7899, loss = 0.47097922\n",
      "Iteration 7900, loss = 0.47117636\n",
      "Iteration 7901, loss = 0.47107085\n",
      "Iteration 7902, loss = 0.47098139\n",
      "Iteration 7903, loss = 0.47139725\n",
      "Iteration 7904, loss = 0.47108903\n",
      "Iteration 7905, loss = 0.47104758\n",
      "Iteration 7906, loss = 0.47112994\n",
      "Iteration 7907, loss = 0.47132036\n",
      "Iteration 7908, loss = 0.47118913\n",
      "Iteration 7909, loss = 0.47110066\n",
      "Iteration 7910, loss = 0.47106353\n",
      "Iteration 7911, loss = 0.47107435\n",
      "Iteration 7912, loss = 0.47113038\n",
      "Iteration 7913, loss = 0.47103669\n",
      "Iteration 7914, loss = 0.47126988\n",
      "Iteration 7915, loss = 0.47096889\n",
      "Iteration 7916, loss = 0.47157900\n",
      "Iteration 7917, loss = 0.47126403\n",
      "Iteration 7918, loss = 0.47105810\n",
      "Iteration 7919, loss = 0.47092504\n",
      "Iteration 7920, loss = 0.47099483\n",
      "Iteration 7921, loss = 0.47139266\n",
      "Iteration 7922, loss = 0.47118984\n",
      "Iteration 7923, loss = 0.47088036\n",
      "Iteration 7924, loss = 0.47160052\n",
      "Iteration 7925, loss = 0.47114302\n",
      "Iteration 7926, loss = 0.47114779\n",
      "Iteration 7927, loss = 0.47111840\n",
      "Iteration 7928, loss = 0.47138598\n",
      "Iteration 7929, loss = 0.47202125\n",
      "Iteration 7930, loss = 0.47154015\n",
      "Iteration 7931, loss = 0.47118640\n",
      "Iteration 7932, loss = 0.47094039\n",
      "Iteration 7933, loss = 0.47139530\n",
      "Iteration 7934, loss = 0.47171399\n",
      "Iteration 7935, loss = 0.47165206\n",
      "Iteration 7936, loss = 0.47140469\n",
      "Iteration 7937, loss = 0.47107744\n",
      "Iteration 7938, loss = 0.47224989\n",
      "Iteration 7939, loss = 0.47137393\n",
      "Iteration 7940, loss = 0.47084996\n",
      "Iteration 7941, loss = 0.47108755\n",
      "Iteration 7942, loss = 0.47146341\n",
      "Iteration 7943, loss = 0.47167713\n",
      "Iteration 7944, loss = 0.47160162\n",
      "Iteration 7945, loss = 0.47166219\n",
      "Iteration 7946, loss = 0.47128120\n",
      "Iteration 7947, loss = 0.47120114\n",
      "Iteration 7948, loss = 0.47103552\n",
      "Iteration 7949, loss = 0.47115346\n",
      "Iteration 7950, loss = 0.47113391\n",
      "Iteration 7951, loss = 0.47106693\n",
      "Iteration 7952, loss = 0.47094546\n",
      "Iteration 7953, loss = 0.47082509\n",
      "Iteration 7954, loss = 0.47159874\n",
      "Iteration 7955, loss = 0.47213488\n",
      "Iteration 7956, loss = 0.47182154\n",
      "Iteration 7957, loss = 0.47137406\n",
      "Iteration 7958, loss = 0.47104145\n",
      "Iteration 7959, loss = 0.47111048\n",
      "Iteration 7960, loss = 0.47144528\n",
      "Iteration 7961, loss = 0.47183115\n",
      "Iteration 7962, loss = 0.47172603\n",
      "Iteration 7963, loss = 0.47134114\n",
      "Iteration 7964, loss = 0.47114520\n",
      "Iteration 7965, loss = 0.47099054\n",
      "Iteration 7966, loss = 0.47118717\n",
      "Iteration 7967, loss = 0.47112039\n",
      "Iteration 7968, loss = 0.47120264\n",
      "Iteration 7969, loss = 0.47110004\n",
      "Iteration 7970, loss = 0.47093365\n",
      "Iteration 7971, loss = 0.47128420\n",
      "Iteration 7972, loss = 0.47126024\n",
      "Iteration 7973, loss = 0.47126772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7974, loss = 0.47125486\n",
      "Iteration 7975, loss = 0.47103977\n",
      "Iteration 7976, loss = 0.47115221\n",
      "Iteration 7977, loss = 0.47110535\n",
      "Iteration 7978, loss = 0.47095498\n",
      "Iteration 7979, loss = 0.47095023\n",
      "Iteration 7980, loss = 0.47200883\n",
      "Iteration 7981, loss = 0.47207848\n",
      "Iteration 7982, loss = 0.47242405\n",
      "Iteration 7983, loss = 0.47135453\n",
      "Iteration 7984, loss = 0.47126057\n",
      "Iteration 7985, loss = 0.47138646\n",
      "Iteration 7986, loss = 0.47135880\n",
      "Iteration 7987, loss = 0.47153136\n",
      "Iteration 7988, loss = 0.47145928\n",
      "Iteration 7989, loss = 0.47142697\n",
      "Iteration 7990, loss = 0.47118719\n",
      "Iteration 7991, loss = 0.47124429\n",
      "Iteration 7992, loss = 0.47105002\n",
      "Iteration 7993, loss = 0.47113406\n",
      "Iteration 7994, loss = 0.47135743\n",
      "Iteration 7995, loss = 0.47168855\n",
      "Iteration 7996, loss = 0.47105059\n",
      "Iteration 7997, loss = 0.47094610\n",
      "Iteration 7998, loss = 0.47095100\n",
      "Iteration 7999, loss = 0.47137474\n",
      "Iteration 8000, loss = 0.47139975\n",
      "Iteration 8001, loss = 0.47117798\n",
      "Iteration 8002, loss = 0.47077885\n",
      "Iteration 8003, loss = 0.47099540\n",
      "Iteration 8004, loss = 0.47156068\n",
      "Iteration 8005, loss = 0.47176031\n",
      "Iteration 8006, loss = 0.47156742\n",
      "Iteration 8007, loss = 0.47110056\n",
      "Iteration 8008, loss = 0.47090815\n",
      "Iteration 8009, loss = 0.47126810\n",
      "Iteration 8010, loss = 0.47119489\n",
      "Iteration 8011, loss = 0.47113191\n",
      "Iteration 8012, loss = 0.47103560\n",
      "Iteration 8013, loss = 0.47100787\n",
      "Iteration 8014, loss = 0.47096687\n",
      "Iteration 8015, loss = 0.47098982\n",
      "Iteration 8016, loss = 0.47106538\n",
      "Iteration 8017, loss = 0.47112160\n",
      "Iteration 8018, loss = 0.47098477\n",
      "Iteration 8019, loss = 0.47107304\n",
      "Iteration 8020, loss = 0.47151313\n",
      "Iteration 8021, loss = 0.47124270\n",
      "Iteration 8022, loss = 0.47110789\n",
      "Iteration 8023, loss = 0.47100985\n",
      "Iteration 8024, loss = 0.47111835\n",
      "Iteration 8025, loss = 0.47124524\n",
      "Iteration 8026, loss = 0.47125616\n",
      "Iteration 8027, loss = 0.47114836\n",
      "Iteration 8028, loss = 0.47115802\n",
      "Iteration 8029, loss = 0.47180732\n",
      "Iteration 8030, loss = 0.47120552\n",
      "Iteration 8031, loss = 0.47124145\n",
      "Iteration 8032, loss = 0.47116479\n",
      "Iteration 8033, loss = 0.47130630\n",
      "Iteration 8034, loss = 0.47127495\n",
      "Iteration 8035, loss = 0.47109308\n",
      "Iteration 8036, loss = 0.47106347\n",
      "Iteration 8037, loss = 0.47108702\n",
      "Iteration 8038, loss = 0.47113362\n",
      "Iteration 8039, loss = 0.47105450\n",
      "Iteration 8040, loss = 0.47139191\n",
      "Iteration 8041, loss = 0.47101520\n",
      "Iteration 8042, loss = 0.47094412\n",
      "Iteration 8043, loss = 0.47181458\n",
      "Iteration 8044, loss = 0.47116797\n",
      "Iteration 8045, loss = 0.47124478\n",
      "Iteration 8046, loss = 0.47105242\n",
      "Iteration 8047, loss = 0.47099183\n",
      "Iteration 8048, loss = 0.47119189\n",
      "Iteration 8049, loss = 0.47151561\n",
      "Iteration 8050, loss = 0.47102439\n",
      "Iteration 8051, loss = 0.47102974\n",
      "Iteration 8052, loss = 0.47104481\n",
      "Iteration 8053, loss = 0.47093581\n",
      "Iteration 8054, loss = 0.47097278\n",
      "Iteration 8055, loss = 0.47105253\n",
      "Iteration 8056, loss = 0.47122550\n",
      "Iteration 8057, loss = 0.47130103\n",
      "Iteration 8058, loss = 0.47129270\n",
      "Iteration 8059, loss = 0.47098026\n",
      "Iteration 8060, loss = 0.47093989\n",
      "Iteration 8061, loss = 0.47120388\n",
      "Iteration 8062, loss = 0.47147566\n",
      "Training loss did not improve more than tol=0.000100 for 2000 consecutive epochs. Stopping.\n",
      "MLPClassifier(alpha=0.1, hidden_layer_sizes=(16, 32, 8, 4), max_iter=100000,\n",
      "              n_iter_no_change=2000, random_state=0, verbose=1)\n",
      "Iteration 1, loss = 0.63046817\n",
      "Iteration 2, loss = 0.63016229\n",
      "Iteration 3, loss = 0.62983338\n",
      "Iteration 4, loss = 0.62957536\n",
      "Iteration 5, loss = 0.62941379\n",
      "Iteration 6, loss = 0.62917812\n",
      "Iteration 7, loss = 0.62894204\n",
      "Iteration 8, loss = 0.62872041\n",
      "Iteration 9, loss = 0.62853029\n",
      "Iteration 10, loss = 0.62837267\n",
      "Iteration 11, loss = 0.62821490\n",
      "Iteration 12, loss = 0.62804703\n",
      "Iteration 13, loss = 0.62785229\n",
      "Iteration 14, loss = 0.62768400\n",
      "Iteration 15, loss = 0.62751244\n",
      "Iteration 16, loss = 0.62730662\n",
      "Iteration 17, loss = 0.62713777\n",
      "Iteration 18, loss = 0.62696627\n",
      "Iteration 19, loss = 0.62680945\n",
      "Iteration 20, loss = 0.62663014\n",
      "Iteration 21, loss = 0.62648167\n",
      "Iteration 22, loss = 0.62632498\n",
      "Iteration 23, loss = 0.62616787\n",
      "Iteration 24, loss = 0.62602920\n",
      "Iteration 25, loss = 0.62586937\n",
      "Iteration 26, loss = 0.62571953\n",
      "Iteration 27, loss = 0.62557499\n",
      "Iteration 28, loss = 0.62543726\n",
      "Iteration 29, loss = 0.62530200\n",
      "Iteration 30, loss = 0.62515619\n",
      "Iteration 31, loss = 0.62502498\n",
      "Iteration 32, loss = 0.62487945\n",
      "Iteration 33, loss = 0.62469380\n",
      "Iteration 34, loss = 0.62456136\n",
      "Iteration 35, loss = 0.62444285\n",
      "Iteration 36, loss = 0.62432293\n",
      "Iteration 37, loss = 0.62417903\n",
      "Iteration 38, loss = 0.62404840\n",
      "Iteration 39, loss = 0.62391574\n",
      "Iteration 40, loss = 0.62377722\n",
      "Iteration 41, loss = 0.62367847\n",
      "Iteration 42, loss = 0.62354067\n",
      "Iteration 43, loss = 0.62342326\n",
      "Iteration 44, loss = 0.62333657\n",
      "Iteration 45, loss = 0.62321008\n",
      "Iteration 46, loss = 0.62308369\n",
      "Iteration 47, loss = 0.62297716\n",
      "Iteration 48, loss = 0.62284780\n",
      "Iteration 49, loss = 0.62274080\n",
      "Iteration 50, loss = 0.62262648\n",
      "Iteration 51, loss = 0.62250899\n",
      "Iteration 52, loss = 0.62240228\n",
      "Iteration 53, loss = 0.62229652\n",
      "Iteration 54, loss = 0.62218221\n",
      "Iteration 55, loss = 0.62206480\n",
      "Iteration 56, loss = 0.62194389\n",
      "Iteration 57, loss = 0.62184108\n",
      "Iteration 58, loss = 0.62174201\n",
      "Iteration 59, loss = 0.62163412\n",
      "Iteration 60, loss = 0.62152647\n",
      "Iteration 61, loss = 0.62143429\n",
      "Iteration 62, loss = 0.62133421\n",
      "Iteration 63, loss = 0.62121976\n",
      "Iteration 64, loss = 0.62110039\n",
      "Iteration 65, loss = 0.62100188\n",
      "Iteration 66, loss = 0.62088962\n",
      "Iteration 67, loss = 0.62080206\n",
      "Iteration 68, loss = 0.62069989\n",
      "Iteration 69, loss = 0.62057904\n",
      "Iteration 70, loss = 0.62046863\n",
      "Iteration 71, loss = 0.62037781\n",
      "Iteration 72, loss = 0.62026623\n",
      "Iteration 73, loss = 0.62015694\n",
      "Iteration 74, loss = 0.62014252\n",
      "Iteration 75, loss = 0.61994656\n",
      "Iteration 76, loss = 0.61984422\n",
      "Iteration 77, loss = 0.61978538\n",
      "Iteration 78, loss = 0.61968063\n",
      "Iteration 79, loss = 0.61956878\n",
      "Iteration 80, loss = 0.61948121\n",
      "Iteration 81, loss = 0.61940115\n",
      "Iteration 82, loss = 0.61927839\n",
      "Iteration 83, loss = 0.61913699\n",
      "Iteration 84, loss = 0.61902760\n",
      "Iteration 85, loss = 0.61895004\n",
      "Iteration 86, loss = 0.61882422\n",
      "Iteration 87, loss = 0.61870644\n",
      "Iteration 88, loss = 0.61862250\n",
      "Iteration 89, loss = 0.61846225\n",
      "Iteration 90, loss = 0.61838474\n",
      "Iteration 91, loss = 0.61823583\n",
      "Iteration 92, loss = 0.61808723\n",
      "Iteration 93, loss = 0.61799614\n",
      "Iteration 94, loss = 0.61784475\n",
      "Iteration 95, loss = 0.61769580\n",
      "Iteration 96, loss = 0.61756021\n",
      "Iteration 97, loss = 0.61743533\n",
      "Iteration 98, loss = 0.61728020\n",
      "Iteration 99, loss = 0.61711949\n",
      "Iteration 100, loss = 0.61696796\n",
      "Iteration 101, loss = 0.61687055\n",
      "Iteration 102, loss = 0.61665422\n",
      "Iteration 103, loss = 0.61650717\n",
      "Iteration 104, loss = 0.61644954\n",
      "Iteration 105, loss = 0.61629430\n",
      "Iteration 106, loss = 0.61590984\n",
      "Iteration 107, loss = 0.61575587\n",
      "Iteration 108, loss = 0.61564350\n",
      "Iteration 109, loss = 0.61546426\n",
      "Iteration 110, loss = 0.61531235\n",
      "Iteration 111, loss = 0.61488710\n",
      "Iteration 112, loss = 0.61484321\n",
      "Iteration 113, loss = 0.61475450\n",
      "Iteration 114, loss = 0.61425545\n",
      "Iteration 115, loss = 0.61407051\n",
      "Iteration 116, loss = 0.61391873\n",
      "Iteration 117, loss = 0.61379769\n",
      "Iteration 118, loss = 0.61361931\n",
      "Iteration 119, loss = 0.61338470\n",
      "Iteration 120, loss = 0.61298016\n",
      "Iteration 121, loss = 0.61253070\n",
      "Iteration 122, loss = 0.61240051\n",
      "Iteration 123, loss = 0.61206895\n",
      "Iteration 124, loss = 0.61153779\n",
      "Iteration 125, loss = 0.61130270\n",
      "Iteration 126, loss = 0.61098277\n",
      "Iteration 127, loss = 0.61061579\n",
      "Iteration 128, loss = 0.61014143\n",
      "Iteration 129, loss = 0.60972957\n",
      "Iteration 130, loss = 0.60929815\n",
      "Iteration 131, loss = 0.60885964\n",
      "Iteration 132, loss = 0.60846450\n",
      "Iteration 133, loss = 0.60808198\n",
      "Iteration 134, loss = 0.60757424\n",
      "Iteration 135, loss = 0.60695004\n",
      "Iteration 136, loss = 0.60668350\n",
      "Iteration 137, loss = 0.60610432\n",
      "Iteration 138, loss = 0.60551819\n",
      "Iteration 139, loss = 0.60503248\n",
      "Iteration 140, loss = 0.60454458\n",
      "Iteration 141, loss = 0.60380198\n",
      "Iteration 142, loss = 0.60339977\n",
      "Iteration 143, loss = 0.60290107\n",
      "Iteration 144, loss = 0.60214231\n",
      "Iteration 145, loss = 0.60154214\n",
      "Iteration 146, loss = 0.60096097\n",
      "Iteration 147, loss = 0.60036166\n",
      "Iteration 148, loss = 0.59975661\n",
      "Iteration 149, loss = 0.59893804\n",
      "Iteration 150, loss = 0.59839614\n",
      "Iteration 151, loss = 0.59762376\n",
      "Iteration 152, loss = 0.59689995\n",
      "Iteration 153, loss = 0.59615849\n",
      "Iteration 154, loss = 0.59546862\n",
      "Iteration 155, loss = 0.59475956\n",
      "Iteration 156, loss = 0.59407767\n",
      "Iteration 157, loss = 0.59329570\n",
      "Iteration 158, loss = 0.59302500\n",
      "Iteration 159, loss = 0.59185714\n",
      "Iteration 160, loss = 0.59127991\n",
      "Iteration 161, loss = 0.59058916\n",
      "Iteration 162, loss = 0.58983244\n",
      "Iteration 163, loss = 0.58907891\n",
      "Iteration 164, loss = 0.58891639\n",
      "Iteration 165, loss = 0.58784042\n",
      "Iteration 166, loss = 0.58710666\n",
      "Iteration 167, loss = 0.58654828\n",
      "Iteration 168, loss = 0.58590643\n",
      "Iteration 169, loss = 0.58543986\n",
      "Iteration 170, loss = 0.58440251\n",
      "Iteration 171, loss = 0.58456030\n",
      "Iteration 172, loss = 0.58396302\n",
      "Iteration 173, loss = 0.58338473\n",
      "Iteration 174, loss = 0.58254576\n",
      "Iteration 175, loss = 0.58159938\n",
      "Iteration 176, loss = 0.58138497\n",
      "Iteration 177, loss = 0.58056809\n",
      "Iteration 178, loss = 0.58024954\n",
      "Iteration 179, loss = 0.57956128\n",
      "Iteration 180, loss = 0.57881712\n",
      "Iteration 181, loss = 0.57871382\n",
      "Iteration 182, loss = 0.57833954\n",
      "Iteration 183, loss = 0.57743237\n",
      "Iteration 184, loss = 0.57693743\n",
      "Iteration 185, loss = 0.57668864\n",
      "Iteration 186, loss = 0.57590709\n",
      "Iteration 187, loss = 0.57585689\n",
      "Iteration 188, loss = 0.57528829\n",
      "Iteration 189, loss = 0.57520001\n",
      "Iteration 190, loss = 0.57463455\n",
      "Iteration 191, loss = 0.57415122\n",
      "Iteration 192, loss = 0.57318114\n",
      "Iteration 193, loss = 0.57254482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 194, loss = 0.57236856\n",
      "Iteration 195, loss = 0.57175265\n",
      "Iteration 196, loss = 0.57126496\n",
      "Iteration 197, loss = 0.57113517\n",
      "Iteration 198, loss = 0.57162937\n",
      "Iteration 199, loss = 0.56987757\n",
      "Iteration 200, loss = 0.56977211\n",
      "Iteration 201, loss = 0.56916170\n",
      "Iteration 202, loss = 0.56853277\n",
      "Iteration 203, loss = 0.56829175\n",
      "Iteration 204, loss = 0.56795286\n",
      "Iteration 205, loss = 0.56719821\n",
      "Iteration 206, loss = 0.56715831\n",
      "Iteration 207, loss = 0.56664314\n",
      "Iteration 208, loss = 0.56592288\n",
      "Iteration 209, loss = 0.56566485\n",
      "Iteration 210, loss = 0.56540250\n",
      "Iteration 211, loss = 0.56463624\n",
      "Iteration 212, loss = 0.56406626\n",
      "Iteration 213, loss = 0.56370473\n",
      "Iteration 214, loss = 0.56308664\n",
      "Iteration 215, loss = 0.56265184\n",
      "Iteration 216, loss = 0.56238232\n",
      "Iteration 217, loss = 0.56211334\n",
      "Iteration 218, loss = 0.56148827\n",
      "Iteration 219, loss = 0.56087926\n",
      "Iteration 220, loss = 0.56022382\n",
      "Iteration 221, loss = 0.55978161\n",
      "Iteration 222, loss = 0.55928222\n",
      "Iteration 223, loss = 0.55850863\n",
      "Iteration 224, loss = 0.55817367\n",
      "Iteration 225, loss = 0.55750327\n",
      "Iteration 226, loss = 0.55692529\n",
      "Iteration 227, loss = 0.55652568\n",
      "Iteration 228, loss = 0.55594528\n",
      "Iteration 229, loss = 0.55526571\n",
      "Iteration 230, loss = 0.55523650\n",
      "Iteration 231, loss = 0.55434324\n",
      "Iteration 232, loss = 0.55361992\n",
      "Iteration 233, loss = 0.55306428\n",
      "Iteration 234, loss = 0.55254778\n",
      "Iteration 235, loss = 0.55197246\n",
      "Iteration 236, loss = 0.55136743\n",
      "Iteration 237, loss = 0.55071671\n",
      "Iteration 238, loss = 0.55003009\n",
      "Iteration 239, loss = 0.54987870\n",
      "Iteration 240, loss = 0.54924713\n",
      "Iteration 241, loss = 0.54838981\n",
      "Iteration 242, loss = 0.54765743\n",
      "Iteration 243, loss = 0.54701424\n",
      "Iteration 244, loss = 0.54665569\n",
      "Iteration 245, loss = 0.54588346\n",
      "Iteration 246, loss = 0.54523868\n",
      "Iteration 247, loss = 0.54433661\n",
      "Iteration 248, loss = 0.54396289\n",
      "Iteration 249, loss = 0.54326306\n",
      "Iteration 250, loss = 0.54227784\n",
      "Iteration 251, loss = 0.54194955\n",
      "Iteration 252, loss = 0.54178595\n",
      "Iteration 253, loss = 0.54066534\n",
      "Iteration 254, loss = 0.53946648\n",
      "Iteration 255, loss = 0.53919081\n",
      "Iteration 256, loss = 0.53869618\n",
      "Iteration 257, loss = 0.53747419\n",
      "Iteration 258, loss = 0.53643030\n",
      "Iteration 259, loss = 0.53647878\n",
      "Iteration 260, loss = 0.53618404\n",
      "Iteration 261, loss = 0.53452137\n",
      "Iteration 262, loss = 0.53346495\n",
      "Iteration 263, loss = 0.53271209\n",
      "Iteration 264, loss = 0.53212955\n",
      "Iteration 265, loss = 0.53122514\n",
      "Iteration 266, loss = 0.53086272\n",
      "Iteration 267, loss = 0.52968736\n",
      "Iteration 268, loss = 0.52883390\n",
      "Iteration 269, loss = 0.52804699\n",
      "Iteration 270, loss = 0.52785036\n",
      "Iteration 271, loss = 0.52628164\n",
      "Iteration 272, loss = 0.52622063\n",
      "Iteration 273, loss = 0.52568233\n",
      "Iteration 274, loss = 0.52407404\n",
      "Iteration 275, loss = 0.52349104\n",
      "Iteration 276, loss = 0.52318525\n",
      "Iteration 277, loss = 0.52200097\n",
      "Iteration 278, loss = 0.52082125\n",
      "Iteration 279, loss = 0.52098710\n",
      "Iteration 280, loss = 0.52051716\n",
      "Iteration 281, loss = 0.52030587\n",
      "Iteration 282, loss = 0.51832318\n",
      "Iteration 283, loss = 0.51721382\n",
      "Iteration 284, loss = 0.51655331\n",
      "Iteration 285, loss = 0.51712490\n",
      "Iteration 286, loss = 0.51639587\n",
      "Iteration 287, loss = 0.51496383\n",
      "Iteration 288, loss = 0.51357004\n",
      "Iteration 289, loss = 0.51387200\n",
      "Iteration 290, loss = 0.51300875\n",
      "Iteration 291, loss = 0.51141076\n",
      "Iteration 292, loss = 0.51101233\n",
      "Iteration 293, loss = 0.51005210\n",
      "Iteration 294, loss = 0.50946333\n",
      "Iteration 295, loss = 0.50920823\n",
      "Iteration 296, loss = 0.50800951\n",
      "Iteration 297, loss = 0.50730701\n",
      "Iteration 298, loss = 0.50670289\n",
      "Iteration 299, loss = 0.50590048\n",
      "Iteration 300, loss = 0.50539883\n",
      "Iteration 301, loss = 0.50462514\n",
      "Iteration 302, loss = 0.50415888\n",
      "Iteration 303, loss = 0.50367181\n",
      "Iteration 304, loss = 0.50283140\n",
      "Iteration 305, loss = 0.50217163\n",
      "Iteration 306, loss = 0.50200833\n",
      "Iteration 307, loss = 0.50103771\n",
      "Iteration 308, loss = 0.50036750\n",
      "Iteration 309, loss = 0.50030620\n",
      "Iteration 310, loss = 0.49996386\n",
      "Iteration 311, loss = 0.49883786\n",
      "Iteration 312, loss = 0.49833877\n",
      "Iteration 313, loss = 0.49793178\n",
      "Iteration 314, loss = 0.49756586\n",
      "Iteration 315, loss = 0.49704594\n",
      "Iteration 316, loss = 0.49711124\n",
      "Iteration 317, loss = 0.49637303\n",
      "Iteration 318, loss = 0.49551970\n",
      "Iteration 319, loss = 0.49571409\n",
      "Iteration 320, loss = 0.49565868\n",
      "Iteration 321, loss = 0.49474150\n",
      "Iteration 322, loss = 0.49409590\n",
      "Iteration 323, loss = 0.49485669\n",
      "Iteration 324, loss = 0.49394885\n",
      "Iteration 325, loss = 0.49306370\n",
      "Iteration 326, loss = 0.49369809\n",
      "Iteration 327, loss = 0.49413780\n",
      "Iteration 328, loss = 0.49171122\n",
      "Iteration 329, loss = 0.49239047\n",
      "Iteration 330, loss = 0.49410413\n",
      "Iteration 331, loss = 0.49268583\n",
      "Iteration 332, loss = 0.49054392\n",
      "Iteration 333, loss = 0.49135136\n",
      "Iteration 334, loss = 0.49148381\n",
      "Iteration 335, loss = 0.49073291\n",
      "Iteration 336, loss = 0.48975310\n",
      "Iteration 337, loss = 0.48972523\n",
      "Iteration 338, loss = 0.49101175\n",
      "Iteration 339, loss = 0.49056297\n",
      "Iteration 340, loss = 0.48907444\n",
      "Iteration 341, loss = 0.48924898\n",
      "Iteration 342, loss = 0.48873089\n",
      "Iteration 343, loss = 0.48876714\n",
      "Iteration 344, loss = 0.48812216\n",
      "Iteration 345, loss = 0.48812492\n",
      "Iteration 346, loss = 0.48827735\n",
      "Iteration 347, loss = 0.48770428\n",
      "Iteration 348, loss = 0.48775877\n",
      "Iteration 349, loss = 0.48768421\n",
      "Iteration 350, loss = 0.48737647\n",
      "Iteration 351, loss = 0.48725945\n",
      "Iteration 352, loss = 0.48685236\n",
      "Iteration 353, loss = 0.48741909\n",
      "Iteration 354, loss = 0.48780884\n",
      "Iteration 355, loss = 0.48859259\n",
      "Iteration 356, loss = 0.48752485\n",
      "Iteration 357, loss = 0.48771857\n",
      "Iteration 358, loss = 0.48651935\n",
      "Iteration 359, loss = 0.48669880\n",
      "Iteration 360, loss = 0.48597108\n",
      "Iteration 361, loss = 0.48703842\n",
      "Iteration 362, loss = 0.48682548\n",
      "Iteration 363, loss = 0.48613332\n",
      "Iteration 364, loss = 0.48632138\n",
      "Iteration 365, loss = 0.48587075\n",
      "Iteration 366, loss = 0.48561586\n",
      "Iteration 367, loss = 0.48637715\n",
      "Iteration 368, loss = 0.48528604\n",
      "Iteration 369, loss = 0.48522235\n",
      "Iteration 370, loss = 0.48600362\n",
      "Iteration 371, loss = 0.48643258\n",
      "Iteration 372, loss = 0.48545240\n",
      "Iteration 373, loss = 0.48498566\n",
      "Iteration 374, loss = 0.48564494\n",
      "Iteration 375, loss = 0.48540334\n",
      "Iteration 376, loss = 0.48465419\n",
      "Iteration 377, loss = 0.48516995\n",
      "Iteration 378, loss = 0.48636382\n",
      "Iteration 379, loss = 0.48574211\n",
      "Iteration 380, loss = 0.48602157\n",
      "Iteration 381, loss = 0.48485918\n",
      "Iteration 382, loss = 0.48621347\n",
      "Iteration 383, loss = 0.48581596\n",
      "Iteration 384, loss = 0.48496459\n",
      "Iteration 385, loss = 0.48513511\n",
      "Iteration 386, loss = 0.48490902\n",
      "Iteration 387, loss = 0.48470036\n",
      "Iteration 388, loss = 0.48441267\n",
      "Iteration 389, loss = 0.48452909\n",
      "Iteration 390, loss = 0.48471311\n",
      "Iteration 391, loss = 0.48440476\n",
      "Iteration 392, loss = 0.48430244\n",
      "Iteration 393, loss = 0.48420558\n",
      "Iteration 394, loss = 0.48415836\n",
      "Iteration 395, loss = 0.48414636\n",
      "Iteration 396, loss = 0.48458217\n",
      "Iteration 397, loss = 0.48453110\n",
      "Iteration 398, loss = 0.48447998\n",
      "Iteration 399, loss = 0.48443007\n",
      "Iteration 400, loss = 0.48402126\n",
      "Iteration 401, loss = 0.48444283\n",
      "Iteration 402, loss = 0.48395399\n",
      "Iteration 403, loss = 0.48477766\n",
      "Iteration 404, loss = 0.48474661\n",
      "Iteration 405, loss = 0.48417794\n",
      "Iteration 406, loss = 0.48410856\n",
      "Iteration 407, loss = 0.48447940\n",
      "Iteration 408, loss = 0.48495412\n",
      "Iteration 409, loss = 0.48394836\n",
      "Iteration 410, loss = 0.48408677\n",
      "Iteration 411, loss = 0.48376164\n",
      "Iteration 412, loss = 0.48413521\n",
      "Iteration 413, loss = 0.48462062\n",
      "Iteration 414, loss = 0.48408197\n",
      "Iteration 415, loss = 0.48370183\n",
      "Iteration 416, loss = 0.48366841\n",
      "Iteration 417, loss = 0.48383063\n",
      "Iteration 418, loss = 0.48482874\n",
      "Iteration 419, loss = 0.48469485\n",
      "Iteration 420, loss = 0.48381441\n",
      "Iteration 421, loss = 0.48396993\n",
      "Iteration 422, loss = 0.48427698\n",
      "Iteration 423, loss = 0.48398448\n",
      "Iteration 424, loss = 0.48376329\n",
      "Iteration 425, loss = 0.48393298\n",
      "Iteration 426, loss = 0.48385583\n",
      "Iteration 427, loss = 0.48382936\n",
      "Iteration 428, loss = 0.48393133\n",
      "Iteration 429, loss = 0.48358315\n",
      "Iteration 430, loss = 0.48353649\n",
      "Iteration 431, loss = 0.48391741\n",
      "Iteration 432, loss = 0.48349877\n",
      "Iteration 433, loss = 0.48369991\n",
      "Iteration 434, loss = 0.48414787\n",
      "Iteration 435, loss = 0.48438447\n",
      "Iteration 436, loss = 0.48371375\n",
      "Iteration 437, loss = 0.48365763\n",
      "Iteration 438, loss = 0.48358474\n",
      "Iteration 439, loss = 0.48360618\n",
      "Iteration 440, loss = 0.48371743\n",
      "Iteration 441, loss = 0.48362941\n",
      "Iteration 442, loss = 0.48339321\n",
      "Iteration 443, loss = 0.48312759\n",
      "Iteration 444, loss = 0.48399863\n",
      "Iteration 445, loss = 0.48474635\n",
      "Iteration 446, loss = 0.48434719\n",
      "Iteration 447, loss = 0.48376911\n",
      "Iteration 448, loss = 0.48340770\n",
      "Iteration 449, loss = 0.48379852\n",
      "Iteration 450, loss = 0.48337466\n",
      "Iteration 451, loss = 0.48401160\n",
      "Iteration 452, loss = 0.48342259\n",
      "Iteration 453, loss = 0.48488847\n",
      "Iteration 454, loss = 0.48341877\n",
      "Iteration 455, loss = 0.48329984\n",
      "Iteration 456, loss = 0.48404680\n",
      "Iteration 457, loss = 0.48439005\n",
      "Iteration 458, loss = 0.48360738\n",
      "Iteration 459, loss = 0.48332519\n",
      "Iteration 460, loss = 0.48381096\n",
      "Iteration 461, loss = 0.48484053\n",
      "Iteration 462, loss = 0.48345134\n",
      "Iteration 463, loss = 0.48347318\n",
      "Iteration 464, loss = 0.48365149\n",
      "Iteration 465, loss = 0.48378092\n",
      "Iteration 466, loss = 0.48486109\n",
      "Iteration 467, loss = 0.48324319\n",
      "Iteration 468, loss = 0.48315526\n",
      "Iteration 469, loss = 0.48333544\n",
      "Iteration 470, loss = 0.48326996\n",
      "Iteration 471, loss = 0.48326352\n",
      "Iteration 472, loss = 0.48320936\n",
      "Iteration 473, loss = 0.48316160\n",
      "Iteration 474, loss = 0.48311758\n",
      "Iteration 475, loss = 0.48333057\n",
      "Iteration 476, loss = 0.48320605\n",
      "Iteration 477, loss = 0.48317911\n",
      "Iteration 478, loss = 0.48316272\n",
      "Iteration 479, loss = 0.48310075\n",
      "Iteration 480, loss = 0.48401740\n",
      "Iteration 481, loss = 0.48331269\n",
      "Iteration 482, loss = 0.48315331\n",
      "Iteration 483, loss = 0.48321538\n",
      "Iteration 484, loss = 0.48313518\n",
      "Iteration 485, loss = 0.48303401\n",
      "Iteration 486, loss = 0.48302358\n",
      "Iteration 487, loss = 0.48325247\n",
      "Iteration 488, loss = 0.48353825\n",
      "Iteration 489, loss = 0.48382150\n",
      "Iteration 490, loss = 0.48313544\n",
      "Iteration 491, loss = 0.48347401\n",
      "Iteration 492, loss = 0.48316038\n",
      "Iteration 493, loss = 0.48305583\n",
      "Iteration 494, loss = 0.48307267\n",
      "Iteration 495, loss = 0.48323149\n",
      "Iteration 496, loss = 0.48299415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 497, loss = 0.48308755\n",
      "Iteration 498, loss = 0.48339011\n",
      "Iteration 499, loss = 0.48371765\n",
      "Iteration 500, loss = 0.48353636\n",
      "Iteration 501, loss = 0.48510667\n",
      "Iteration 502, loss = 0.48430698\n",
      "Iteration 503, loss = 0.48312174\n",
      "Iteration 504, loss = 0.48365062\n",
      "Iteration 505, loss = 0.48536362\n",
      "Iteration 506, loss = 0.48475004\n",
      "Iteration 507, loss = 0.48289586\n",
      "Iteration 508, loss = 0.48330061\n",
      "Iteration 509, loss = 0.48453497\n",
      "Iteration 510, loss = 0.48454225\n",
      "Iteration 511, loss = 0.48284254\n",
      "Iteration 512, loss = 0.48369584\n",
      "Iteration 513, loss = 0.48544409\n",
      "Iteration 514, loss = 0.48450790\n",
      "Iteration 515, loss = 0.48346393\n",
      "Iteration 516, loss = 0.48317848\n",
      "Iteration 517, loss = 0.48382686\n",
      "Iteration 518, loss = 0.48368083\n",
      "Iteration 519, loss = 0.48278837\n",
      "Iteration 520, loss = 0.48306625\n",
      "Iteration 521, loss = 0.48376305\n",
      "Iteration 522, loss = 0.48342721\n",
      "Iteration 523, loss = 0.48312641\n",
      "Iteration 524, loss = 0.48408751\n",
      "Iteration 525, loss = 0.48428275\n",
      "Iteration 526, loss = 0.48368401\n",
      "Iteration 527, loss = 0.48263449\n",
      "Iteration 528, loss = 0.48519893\n",
      "Iteration 529, loss = 0.48375803\n",
      "Iteration 530, loss = 0.48365431\n",
      "Iteration 531, loss = 0.48294811\n",
      "Iteration 532, loss = 0.48368673\n",
      "Iteration 533, loss = 0.48319995\n",
      "Iteration 534, loss = 0.48340753\n",
      "Iteration 535, loss = 0.48423309\n",
      "Iteration 536, loss = 0.48284106\n",
      "Iteration 537, loss = 0.48284545\n",
      "Iteration 538, loss = 0.48282464\n",
      "Iteration 539, loss = 0.48288816\n",
      "Iteration 540, loss = 0.48288867\n",
      "Iteration 541, loss = 0.48280994\n",
      "Iteration 542, loss = 0.48316154\n",
      "Iteration 543, loss = 0.48286764\n",
      "Iteration 544, loss = 0.48283506\n",
      "Iteration 545, loss = 0.48319712\n",
      "Iteration 546, loss = 0.48337200\n",
      "Iteration 547, loss = 0.48298007\n",
      "Iteration 548, loss = 0.48286204\n",
      "Iteration 549, loss = 0.48289514\n",
      "Iteration 550, loss = 0.48285925\n",
      "Iteration 551, loss = 0.48308412\n",
      "Iteration 552, loss = 0.48280288\n",
      "Iteration 553, loss = 0.48289889\n",
      "Iteration 554, loss = 0.48278298\n",
      "Iteration 555, loss = 0.48283331\n",
      "Iteration 556, loss = 0.48278122\n",
      "Iteration 557, loss = 0.48367168\n",
      "Iteration 558, loss = 0.48359545\n",
      "Iteration 559, loss = 0.48273169\n",
      "Iteration 560, loss = 0.48281227\n",
      "Iteration 561, loss = 0.48307185\n",
      "Iteration 562, loss = 0.48340879\n",
      "Iteration 563, loss = 0.48305491\n",
      "Iteration 564, loss = 0.48272829\n",
      "Iteration 565, loss = 0.48284755\n",
      "Iteration 566, loss = 0.48369931\n",
      "Iteration 567, loss = 0.48433773\n",
      "Iteration 568, loss = 0.48341082\n",
      "Iteration 569, loss = 0.48327154\n",
      "Iteration 570, loss = 0.48352880\n",
      "Iteration 571, loss = 0.48391273\n",
      "Iteration 572, loss = 0.48329434\n",
      "Iteration 573, loss = 0.48292258\n",
      "Iteration 574, loss = 0.48288440\n",
      "Iteration 575, loss = 0.48297501\n",
      "Iteration 576, loss = 0.48353056\n",
      "Iteration 577, loss = 0.48443712\n",
      "Iteration 578, loss = 0.48275672\n",
      "Iteration 579, loss = 0.48270667\n",
      "Iteration 580, loss = 0.48274804\n",
      "Iteration 581, loss = 0.48286629\n",
      "Iteration 582, loss = 0.48272022\n",
      "Iteration 583, loss = 0.48262874\n",
      "Iteration 584, loss = 0.48282072\n",
      "Iteration 585, loss = 0.48306150\n",
      "Iteration 586, loss = 0.48352581\n",
      "Iteration 587, loss = 0.48367759\n",
      "Iteration 588, loss = 0.48291392\n",
      "Iteration 589, loss = 0.48240685\n",
      "Iteration 590, loss = 0.48312994\n",
      "Iteration 591, loss = 0.48375376\n",
      "Iteration 592, loss = 0.48400748\n",
      "Iteration 593, loss = 0.48305216\n",
      "Iteration 594, loss = 0.48301399\n",
      "Iteration 595, loss = 0.48288153\n",
      "Iteration 596, loss = 0.48268103\n",
      "Iteration 597, loss = 0.48261116\n",
      "Iteration 598, loss = 0.48266545\n",
      "Iteration 599, loss = 0.48282383\n",
      "Iteration 600, loss = 0.48295170\n",
      "Iteration 601, loss = 0.48259395\n",
      "Iteration 602, loss = 0.48286000\n",
      "Iteration 603, loss = 0.48294684\n",
      "Iteration 604, loss = 0.48333817\n",
      "Iteration 605, loss = 0.48274591\n",
      "Iteration 606, loss = 0.48268765\n",
      "Iteration 607, loss = 0.48262679\n",
      "Iteration 608, loss = 0.48259438\n",
      "Iteration 609, loss = 0.48267062\n",
      "Iteration 610, loss = 0.48264859\n",
      "Iteration 611, loss = 0.48279695\n",
      "Iteration 612, loss = 0.48253882\n",
      "Iteration 613, loss = 0.48304577\n",
      "Iteration 614, loss = 0.48288013\n",
      "Iteration 615, loss = 0.48283047\n",
      "Iteration 616, loss = 0.48246842\n",
      "Iteration 617, loss = 0.48265690\n",
      "Iteration 618, loss = 0.48354854\n",
      "Iteration 619, loss = 0.48306787\n",
      "Iteration 620, loss = 0.48247049\n",
      "Iteration 621, loss = 0.48314642\n",
      "Iteration 622, loss = 0.48325832\n",
      "Iteration 623, loss = 0.48285004\n",
      "Iteration 624, loss = 0.48327784\n",
      "Iteration 625, loss = 0.48302758\n",
      "Iteration 626, loss = 0.48378479\n",
      "Iteration 627, loss = 0.48284222\n",
      "Iteration 628, loss = 0.48278383\n",
      "Iteration 629, loss = 0.48256845\n",
      "Iteration 630, loss = 0.48253165\n",
      "Iteration 631, loss = 0.48262963\n",
      "Iteration 632, loss = 0.48262262\n",
      "Iteration 633, loss = 0.48285676\n",
      "Iteration 634, loss = 0.48250722\n",
      "Iteration 635, loss = 0.48348004\n",
      "Iteration 636, loss = 0.48233203\n",
      "Iteration 637, loss = 0.48246442\n",
      "Iteration 638, loss = 0.48404253\n",
      "Iteration 639, loss = 0.48390269\n",
      "Iteration 640, loss = 0.48278942\n",
      "Iteration 641, loss = 0.48233044\n",
      "Iteration 642, loss = 0.48282599\n",
      "Iteration 643, loss = 0.48358008\n",
      "Iteration 644, loss = 0.48379494\n",
      "Iteration 645, loss = 0.48278331\n",
      "Iteration 646, loss = 0.48241501\n",
      "Iteration 647, loss = 0.48269484\n",
      "Iteration 648, loss = 0.48336410\n",
      "Iteration 649, loss = 0.48321424\n",
      "Iteration 650, loss = 0.48343510\n",
      "Iteration 651, loss = 0.48253315\n",
      "Iteration 652, loss = 0.48251558\n",
      "Iteration 653, loss = 0.48255849\n",
      "Iteration 654, loss = 0.48240941\n",
      "Iteration 655, loss = 0.48247803\n",
      "Iteration 656, loss = 0.48272728\n",
      "Iteration 657, loss = 0.48280142\n",
      "Iteration 658, loss = 0.48311541\n",
      "Iteration 659, loss = 0.48269990\n",
      "Iteration 660, loss = 0.48248978\n",
      "Iteration 661, loss = 0.48223577\n",
      "Iteration 662, loss = 0.48260502\n",
      "Iteration 663, loss = 0.48293932\n",
      "Iteration 664, loss = 0.48302513\n",
      "Iteration 665, loss = 0.48272498\n",
      "Iteration 666, loss = 0.48272122\n",
      "Iteration 667, loss = 0.48235146\n",
      "Iteration 668, loss = 0.48238418\n",
      "Iteration 669, loss = 0.48242153\n",
      "Iteration 670, loss = 0.48257740\n",
      "Iteration 671, loss = 0.48271350\n",
      "Iteration 672, loss = 0.48273746\n",
      "Iteration 673, loss = 0.48251978\n",
      "Iteration 674, loss = 0.48232022\n",
      "Iteration 675, loss = 0.48258996\n",
      "Iteration 676, loss = 0.48318006\n",
      "Iteration 677, loss = 0.48217079\n",
      "Iteration 678, loss = 0.48312138\n",
      "Iteration 679, loss = 0.48337687\n",
      "Iteration 680, loss = 0.48318176\n",
      "Iteration 681, loss = 0.48286939\n",
      "Iteration 682, loss = 0.48285332\n",
      "Iteration 683, loss = 0.48274650\n",
      "Iteration 684, loss = 0.48254118\n",
      "Iteration 685, loss = 0.48246587\n",
      "Iteration 686, loss = 0.48262615\n",
      "Iteration 687, loss = 0.48290946\n",
      "Iteration 688, loss = 0.48230049\n",
      "Iteration 689, loss = 0.48244403\n",
      "Iteration 690, loss = 0.48230269\n",
      "Iteration 691, loss = 0.48231821\n",
      "Iteration 692, loss = 0.48232928\n",
      "Iteration 693, loss = 0.48228894\n",
      "Iteration 694, loss = 0.48233843\n",
      "Iteration 695, loss = 0.48240003\n",
      "Iteration 696, loss = 0.48297025\n",
      "Iteration 697, loss = 0.48271784\n",
      "Iteration 698, loss = 0.48286547\n",
      "Iteration 699, loss = 0.48253072\n",
      "Iteration 700, loss = 0.48226842\n",
      "Iteration 701, loss = 0.48242986\n",
      "Iteration 702, loss = 0.48270263\n",
      "Iteration 703, loss = 0.48287566\n",
      "Iteration 704, loss = 0.48264991\n",
      "Iteration 705, loss = 0.48250272\n",
      "Iteration 706, loss = 0.48226235\n",
      "Iteration 707, loss = 0.48220894\n",
      "Iteration 708, loss = 0.48219531\n",
      "Iteration 709, loss = 0.48244812\n",
      "Iteration 710, loss = 0.48222996\n",
      "Iteration 711, loss = 0.48247494\n",
      "Iteration 712, loss = 0.48238815\n",
      "Iteration 713, loss = 0.48195484\n",
      "Iteration 714, loss = 0.48273259\n",
      "Iteration 715, loss = 0.48305384\n",
      "Iteration 716, loss = 0.48234856\n",
      "Iteration 717, loss = 0.48220810\n",
      "Iteration 718, loss = 0.48268984\n",
      "Iteration 719, loss = 0.48328184\n",
      "Iteration 720, loss = 0.48317887\n",
      "Iteration 721, loss = 0.48257032\n",
      "Iteration 722, loss = 0.48213577\n",
      "Iteration 723, loss = 0.48222578\n",
      "Iteration 724, loss = 0.48243757\n",
      "Iteration 725, loss = 0.48262446\n",
      "Iteration 726, loss = 0.48254285\n",
      "Iteration 727, loss = 0.48276037\n",
      "Iteration 728, loss = 0.48245570\n",
      "Iteration 729, loss = 0.48234597\n",
      "Iteration 730, loss = 0.48231159\n",
      "Iteration 731, loss = 0.48232049\n",
      "Iteration 732, loss = 0.48231881\n",
      "Iteration 733, loss = 0.48229607\n",
      "Iteration 734, loss = 0.48222116\n",
      "Iteration 735, loss = 0.48236253\n",
      "Iteration 736, loss = 0.48242142\n",
      "Iteration 737, loss = 0.48214294\n",
      "Iteration 738, loss = 0.48256988\n",
      "Iteration 739, loss = 0.48196361\n",
      "Iteration 740, loss = 0.48257091\n",
      "Iteration 741, loss = 0.48307224\n",
      "Iteration 742, loss = 0.48238028\n",
      "Iteration 743, loss = 0.48239009\n",
      "Iteration 744, loss = 0.48206554\n",
      "Iteration 745, loss = 0.48208409\n",
      "Iteration 746, loss = 0.48335845\n",
      "Iteration 747, loss = 0.48205499\n",
      "Iteration 748, loss = 0.48378460\n",
      "Iteration 749, loss = 0.48264884\n",
      "Iteration 750, loss = 0.48242206\n",
      "Iteration 751, loss = 0.48217454\n",
      "Iteration 752, loss = 0.48205669\n",
      "Iteration 753, loss = 0.48243301\n",
      "Iteration 754, loss = 0.48333025\n",
      "Iteration 755, loss = 0.48199173\n",
      "Iteration 756, loss = 0.48208797\n",
      "Iteration 757, loss = 0.48252252\n",
      "Iteration 758, loss = 0.48234380\n",
      "Iteration 759, loss = 0.48243709\n",
      "Iteration 760, loss = 0.48293456\n",
      "Iteration 761, loss = 0.48213156\n",
      "Iteration 762, loss = 0.48302800\n",
      "Iteration 763, loss = 0.48226398\n",
      "Iteration 764, loss = 0.48289069\n",
      "Iteration 765, loss = 0.48255326\n",
      "Iteration 766, loss = 0.48224569\n",
      "Iteration 767, loss = 0.48293829\n",
      "Iteration 768, loss = 0.48242784\n",
      "Iteration 769, loss = 0.48203152\n",
      "Iteration 770, loss = 0.48200982\n",
      "Iteration 771, loss = 0.48193911\n",
      "Iteration 772, loss = 0.48194887\n",
      "Iteration 773, loss = 0.48219309\n",
      "Iteration 774, loss = 0.48204942\n",
      "Iteration 775, loss = 0.48201148\n",
      "Iteration 776, loss = 0.48205768\n",
      "Iteration 777, loss = 0.48179355\n",
      "Iteration 778, loss = 0.48213175\n",
      "Iteration 779, loss = 0.48231547\n",
      "Iteration 780, loss = 0.48247711\n",
      "Iteration 781, loss = 0.48257079\n",
      "Iteration 782, loss = 0.48201695\n",
      "Iteration 783, loss = 0.48280795\n",
      "Iteration 784, loss = 0.48203509\n",
      "Iteration 785, loss = 0.48215774\n",
      "Iteration 786, loss = 0.48176530\n",
      "Iteration 787, loss = 0.48180708\n",
      "Iteration 788, loss = 0.48265495\n",
      "Iteration 789, loss = 0.48290150\n",
      "Iteration 790, loss = 0.48243121\n",
      "Iteration 791, loss = 0.48203610\n",
      "Iteration 792, loss = 0.48220492\n",
      "Iteration 793, loss = 0.48194945\n",
      "Iteration 794, loss = 0.48173807\n",
      "Iteration 795, loss = 0.48237954\n",
      "Iteration 796, loss = 0.48268087\n",
      "Iteration 797, loss = 0.48196998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 798, loss = 0.48188967\n",
      "Iteration 799, loss = 0.48180023\n",
      "Iteration 800, loss = 0.48185967\n",
      "Iteration 801, loss = 0.48241913\n",
      "Iteration 802, loss = 0.48206041\n",
      "Iteration 803, loss = 0.48174842\n",
      "Iteration 804, loss = 0.48192105\n",
      "Iteration 805, loss = 0.48194075\n",
      "Iteration 806, loss = 0.48219245\n",
      "Iteration 807, loss = 0.48180663\n",
      "Iteration 808, loss = 0.48232912\n",
      "Iteration 809, loss = 0.48172984\n",
      "Iteration 810, loss = 0.48291358\n",
      "Iteration 811, loss = 0.48211033\n",
      "Iteration 812, loss = 0.48176382\n",
      "Iteration 813, loss = 0.48204037\n",
      "Iteration 814, loss = 0.48209446\n",
      "Iteration 815, loss = 0.48206210\n",
      "Iteration 816, loss = 0.48173010\n",
      "Iteration 817, loss = 0.48176047\n",
      "Iteration 818, loss = 0.48174455\n",
      "Iteration 819, loss = 0.48162502\n",
      "Iteration 820, loss = 0.48189645\n",
      "Iteration 821, loss = 0.48197948\n",
      "Iteration 822, loss = 0.48169366\n",
      "Iteration 823, loss = 0.48202548\n",
      "Iteration 824, loss = 0.48262021\n",
      "Iteration 825, loss = 0.48243142\n",
      "Iteration 826, loss = 0.48171444\n",
      "Iteration 827, loss = 0.48204624\n",
      "Iteration 828, loss = 0.48169863\n",
      "Iteration 829, loss = 0.48165074\n",
      "Iteration 830, loss = 0.48224804\n",
      "Iteration 831, loss = 0.48153867\n",
      "Iteration 832, loss = 0.48205382\n",
      "Iteration 833, loss = 0.48207962\n",
      "Iteration 834, loss = 0.48197227\n",
      "Iteration 835, loss = 0.48120808\n",
      "Iteration 836, loss = 0.48166993\n",
      "Iteration 837, loss = 0.48404266\n",
      "Iteration 838, loss = 0.48540378\n",
      "Iteration 839, loss = 0.48371223\n",
      "Iteration 840, loss = 0.48194483\n",
      "Iteration 841, loss = 0.48331698\n",
      "Iteration 842, loss = 0.48262084\n",
      "Iteration 843, loss = 0.48259577\n",
      "Iteration 844, loss = 0.48213542\n",
      "Iteration 845, loss = 0.48296415\n",
      "Iteration 846, loss = 0.48174174\n",
      "Iteration 847, loss = 0.48264209\n",
      "Iteration 848, loss = 0.48156244\n",
      "Iteration 849, loss = 0.48163691\n",
      "Iteration 850, loss = 0.48176143\n",
      "Iteration 851, loss = 0.48195908\n",
      "Iteration 852, loss = 0.48186031\n",
      "Iteration 853, loss = 0.48160893\n",
      "Iteration 854, loss = 0.48170090\n",
      "Iteration 855, loss = 0.48182267\n",
      "Iteration 856, loss = 0.48178107\n",
      "Iteration 857, loss = 0.48161252\n",
      "Iteration 858, loss = 0.48138738\n",
      "Iteration 859, loss = 0.48243282\n",
      "Iteration 860, loss = 0.48257980\n",
      "Iteration 861, loss = 0.48208595\n",
      "Iteration 862, loss = 0.48151956\n",
      "Iteration 863, loss = 0.48144411\n",
      "Iteration 864, loss = 0.48208914\n",
      "Iteration 865, loss = 0.48257105\n",
      "Iteration 866, loss = 0.48229547\n",
      "Iteration 867, loss = 0.48203709\n",
      "Iteration 868, loss = 0.48168100\n",
      "Iteration 869, loss = 0.48180057\n",
      "Iteration 870, loss = 0.48199026\n",
      "Iteration 871, loss = 0.48208497\n",
      "Iteration 872, loss = 0.48153785\n",
      "Iteration 873, loss = 0.48122536\n",
      "Iteration 874, loss = 0.48296295\n",
      "Iteration 875, loss = 0.48250635\n",
      "Iteration 876, loss = 0.48168155\n",
      "Iteration 877, loss = 0.48105964\n",
      "Iteration 878, loss = 0.48200845\n",
      "Iteration 879, loss = 0.48303205\n",
      "Iteration 880, loss = 0.48361762\n",
      "Iteration 881, loss = 0.48159486\n",
      "Iteration 882, loss = 0.48148871\n",
      "Iteration 883, loss = 0.48199962\n",
      "Iteration 884, loss = 0.48177781\n",
      "Iteration 885, loss = 0.48169992\n",
      "Iteration 886, loss = 0.48160170\n",
      "Iteration 887, loss = 0.48149330\n",
      "Iteration 888, loss = 0.48139323\n",
      "Iteration 889, loss = 0.48145949\n",
      "Iteration 890, loss = 0.48165940\n",
      "Iteration 891, loss = 0.48173624\n",
      "Iteration 892, loss = 0.48138024\n",
      "Iteration 893, loss = 0.48176268\n",
      "Iteration 894, loss = 0.48120615\n",
      "Iteration 895, loss = 0.48152488\n",
      "Iteration 896, loss = 0.48204984\n",
      "Iteration 897, loss = 0.48275004\n",
      "Iteration 898, loss = 0.48264266\n",
      "Iteration 899, loss = 0.48192648\n",
      "Iteration 900, loss = 0.48129736\n",
      "Iteration 901, loss = 0.48119476\n",
      "Iteration 902, loss = 0.48155817\n",
      "Iteration 903, loss = 0.48332504\n",
      "Iteration 904, loss = 0.48189723\n",
      "Iteration 905, loss = 0.48109708\n",
      "Iteration 906, loss = 0.48140480\n",
      "Iteration 907, loss = 0.48255899\n",
      "Iteration 908, loss = 0.48325513\n",
      "Iteration 909, loss = 0.48295828\n",
      "Iteration 910, loss = 0.48209238\n",
      "Iteration 911, loss = 0.48237516\n",
      "Iteration 912, loss = 0.48132185\n",
      "Iteration 913, loss = 0.48137672\n",
      "Iteration 914, loss = 0.48151322\n",
      "Iteration 915, loss = 0.48151640\n",
      "Iteration 916, loss = 0.48115941\n",
      "Iteration 917, loss = 0.48195805\n",
      "Iteration 918, loss = 0.48152516\n",
      "Iteration 919, loss = 0.48148026\n",
      "Iteration 920, loss = 0.48100474\n",
      "Iteration 921, loss = 0.48130930\n",
      "Iteration 922, loss = 0.48226551\n",
      "Iteration 923, loss = 0.48191349\n",
      "Iteration 924, loss = 0.48066831\n",
      "Iteration 925, loss = 0.48244887\n",
      "Iteration 926, loss = 0.48360295\n",
      "Iteration 927, loss = 0.48255154\n",
      "Iteration 928, loss = 0.48153614\n",
      "Iteration 929, loss = 0.48140943\n",
      "Iteration 930, loss = 0.48185828\n",
      "Iteration 931, loss = 0.48208483\n",
      "Iteration 932, loss = 0.48189775\n",
      "Iteration 933, loss = 0.48141844\n",
      "Iteration 934, loss = 0.48108052\n",
      "Iteration 935, loss = 0.48108148\n",
      "Iteration 936, loss = 0.48134507\n",
      "Iteration 937, loss = 0.48146403\n",
      "Iteration 938, loss = 0.48217660\n",
      "Iteration 939, loss = 0.48165734\n",
      "Iteration 940, loss = 0.48102442\n",
      "Iteration 941, loss = 0.48127389\n",
      "Iteration 942, loss = 0.48165368\n",
      "Iteration 943, loss = 0.48183103\n",
      "Iteration 944, loss = 0.48118536\n",
      "Iteration 945, loss = 0.48236326\n",
      "Iteration 946, loss = 0.48146991\n",
      "Iteration 947, loss = 0.48116413\n",
      "Iteration 948, loss = 0.48221902\n",
      "Iteration 949, loss = 0.48127489\n",
      "Iteration 950, loss = 0.48113557\n",
      "Iteration 951, loss = 0.48139219\n",
      "Iteration 952, loss = 0.48124644\n",
      "Iteration 953, loss = 0.48095003\n",
      "Iteration 954, loss = 0.48113828\n",
      "Iteration 955, loss = 0.48109119\n",
      "Iteration 956, loss = 0.48125141\n",
      "Iteration 957, loss = 0.48100547\n",
      "Iteration 958, loss = 0.48106713\n",
      "Iteration 959, loss = 0.48130082\n",
      "Iteration 960, loss = 0.48129037\n",
      "Iteration 961, loss = 0.48100312\n",
      "Iteration 962, loss = 0.48164788\n",
      "Iteration 963, loss = 0.48172872\n",
      "Iteration 964, loss = 0.48105052\n",
      "Iteration 965, loss = 0.48111349\n",
      "Iteration 966, loss = 0.48128244\n",
      "Iteration 967, loss = 0.48067464\n",
      "Iteration 968, loss = 0.48117964\n",
      "Iteration 969, loss = 0.48200269\n",
      "Iteration 970, loss = 0.48183652\n",
      "Iteration 971, loss = 0.48107079\n",
      "Iteration 972, loss = 0.48062235\n",
      "Iteration 973, loss = 0.48195975\n",
      "Iteration 974, loss = 0.48180510\n",
      "Iteration 975, loss = 0.48152062\n",
      "Iteration 976, loss = 0.48098655\n",
      "Iteration 977, loss = 0.48135673\n",
      "Iteration 978, loss = 0.48127721\n",
      "Iteration 979, loss = 0.48096105\n",
      "Iteration 980, loss = 0.48092406\n",
      "Iteration 981, loss = 0.48140783\n",
      "Iteration 982, loss = 0.48151043\n",
      "Iteration 983, loss = 0.48107923\n",
      "Iteration 984, loss = 0.48077080\n",
      "Iteration 985, loss = 0.48127126\n",
      "Iteration 986, loss = 0.48074072\n",
      "Iteration 987, loss = 0.48120251\n",
      "Iteration 988, loss = 0.48076872\n",
      "Iteration 989, loss = 0.48093907\n",
      "Iteration 990, loss = 0.48081598\n",
      "Iteration 991, loss = 0.48077074\n",
      "Iteration 992, loss = 0.48067183\n",
      "Iteration 993, loss = 0.48098184\n",
      "Iteration 994, loss = 0.48081171\n",
      "Iteration 995, loss = 0.48074468\n",
      "Iteration 996, loss = 0.48065667\n",
      "Iteration 997, loss = 0.48066668\n",
      "Iteration 998, loss = 0.48083614\n",
      "Iteration 999, loss = 0.48099251\n",
      "Iteration 1000, loss = 0.48105532\n",
      "Iteration 1001, loss = 0.48082575\n",
      "Iteration 1002, loss = 0.48065728\n",
      "Iteration 1003, loss = 0.48079626\n",
      "Iteration 1004, loss = 0.48090064\n",
      "Iteration 1005, loss = 0.48091288\n",
      "Iteration 1006, loss = 0.48079355\n",
      "Iteration 1007, loss = 0.48068097\n",
      "Iteration 1008, loss = 0.48061515\n",
      "Iteration 1009, loss = 0.48065082\n",
      "Iteration 1010, loss = 0.48071388\n",
      "Iteration 1011, loss = 0.48075254\n",
      "Iteration 1012, loss = 0.48102005\n",
      "Iteration 1013, loss = 0.48075083\n",
      "Iteration 1014, loss = 0.48107194\n",
      "Iteration 1015, loss = 0.48042956\n",
      "Iteration 1016, loss = 0.48112559\n",
      "Iteration 1017, loss = 0.48129913\n",
      "Iteration 1018, loss = 0.48098474\n",
      "Iteration 1019, loss = 0.48077091\n",
      "Iteration 1020, loss = 0.48141550\n",
      "Iteration 1021, loss = 0.48067532\n",
      "Iteration 1022, loss = 0.48075554\n",
      "Iteration 1023, loss = 0.48062950\n",
      "Iteration 1024, loss = 0.48062442\n",
      "Iteration 1025, loss = 0.48060261\n",
      "Iteration 1026, loss = 0.48065743\n",
      "Iteration 1027, loss = 0.48068981\n",
      "Iteration 1028, loss = 0.48067549\n",
      "Iteration 1029, loss = 0.48049829\n",
      "Iteration 1030, loss = 0.48067090\n",
      "Iteration 1031, loss = 0.48050272\n",
      "Iteration 1032, loss = 0.48051908\n",
      "Iteration 1033, loss = 0.48067324\n",
      "Iteration 1034, loss = 0.48081287\n",
      "Iteration 1035, loss = 0.48024019\n",
      "Iteration 1036, loss = 0.48111836\n",
      "Iteration 1037, loss = 0.48231927\n",
      "Iteration 1038, loss = 0.48254198\n",
      "Iteration 1039, loss = 0.48116608\n",
      "Iteration 1040, loss = 0.48080364\n",
      "Iteration 1041, loss = 0.48038435\n",
      "Iteration 1042, loss = 0.48053995\n",
      "Iteration 1043, loss = 0.48155113\n",
      "Iteration 1044, loss = 0.48119632\n",
      "Iteration 1045, loss = 0.48057555\n",
      "Iteration 1046, loss = 0.48075036\n",
      "Iteration 1047, loss = 0.48043116\n",
      "Iteration 1048, loss = 0.48048607\n",
      "Iteration 1049, loss = 0.48138428\n",
      "Iteration 1050, loss = 0.48056117\n",
      "Iteration 1051, loss = 0.48034039\n",
      "Iteration 1052, loss = 0.48042188\n",
      "Iteration 1053, loss = 0.48032475\n",
      "Iteration 1054, loss = 0.48030613\n",
      "Iteration 1055, loss = 0.48040874\n",
      "Iteration 1056, loss = 0.48039730\n",
      "Iteration 1057, loss = 0.48045042\n",
      "Iteration 1058, loss = 0.48076978\n",
      "Iteration 1059, loss = 0.48099407\n",
      "Iteration 1060, loss = 0.48059115\n",
      "Iteration 1061, loss = 0.48070140\n",
      "Iteration 1062, loss = 0.48018635\n",
      "Iteration 1063, loss = 0.48032520\n",
      "Iteration 1064, loss = 0.48093358\n",
      "Iteration 1065, loss = 0.48166620\n",
      "Iteration 1066, loss = 0.48088894\n",
      "Iteration 1067, loss = 0.48034717\n",
      "Iteration 1068, loss = 0.48083090\n",
      "Iteration 1069, loss = 0.48090663\n",
      "Iteration 1070, loss = 0.48028215\n",
      "Iteration 1071, loss = 0.48057294\n",
      "Iteration 1072, loss = 0.48072831\n",
      "Iteration 1073, loss = 0.48068952\n",
      "Iteration 1074, loss = 0.48115904\n",
      "Iteration 1075, loss = 0.48096531\n",
      "Iteration 1076, loss = 0.48001094\n",
      "Iteration 1077, loss = 0.48051965\n",
      "Iteration 1078, loss = 0.48102889\n",
      "Iteration 1079, loss = 0.48085510\n",
      "Iteration 1080, loss = 0.48104605\n",
      "Iteration 1081, loss = 0.48032093\n",
      "Iteration 1082, loss = 0.48010706\n",
      "Iteration 1083, loss = 0.48006879\n",
      "Iteration 1084, loss = 0.48100240\n",
      "Iteration 1085, loss = 0.48037758\n",
      "Iteration 1086, loss = 0.48030044\n",
      "Iteration 1087, loss = 0.48014582\n",
      "Iteration 1088, loss = 0.48061510\n",
      "Iteration 1089, loss = 0.48033497\n",
      "Iteration 1090, loss = 0.48007886\n",
      "Iteration 1091, loss = 0.48007384\n",
      "Iteration 1092, loss = 0.48018848\n",
      "Iteration 1093, loss = 0.48031096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1094, loss = 0.48053054\n",
      "Iteration 1095, loss = 0.48023502\n",
      "Iteration 1096, loss = 0.48019062\n",
      "Iteration 1097, loss = 0.48044527\n",
      "Iteration 1098, loss = 0.48009387\n",
      "Iteration 1099, loss = 0.48009785\n",
      "Iteration 1100, loss = 0.48010212\n",
      "Iteration 1101, loss = 0.48002695\n",
      "Iteration 1102, loss = 0.48001747\n",
      "Iteration 1103, loss = 0.48001457\n",
      "Iteration 1104, loss = 0.47999832\n",
      "Iteration 1105, loss = 0.48007461\n",
      "Iteration 1106, loss = 0.48034375\n",
      "Iteration 1107, loss = 0.48034068\n",
      "Iteration 1108, loss = 0.48007222\n",
      "Iteration 1109, loss = 0.47988521\n",
      "Iteration 1110, loss = 0.48012934\n",
      "Iteration 1111, loss = 0.48115849\n",
      "Iteration 1112, loss = 0.48066186\n",
      "Iteration 1113, loss = 0.47993090\n",
      "Iteration 1114, loss = 0.48036396\n",
      "Iteration 1115, loss = 0.48071486\n",
      "Iteration 1116, loss = 0.48109126\n",
      "Iteration 1117, loss = 0.48061617\n",
      "Iteration 1118, loss = 0.48008097\n",
      "Iteration 1119, loss = 0.48016035\n",
      "Iteration 1120, loss = 0.48021705\n",
      "Iteration 1121, loss = 0.47995717\n",
      "Iteration 1122, loss = 0.47981226\n",
      "Iteration 1123, loss = 0.47999169\n",
      "Iteration 1124, loss = 0.48073101\n",
      "Iteration 1125, loss = 0.48090255\n",
      "Iteration 1126, loss = 0.48045039\n",
      "Iteration 1127, loss = 0.47967530\n",
      "Iteration 1128, loss = 0.47967633\n",
      "Iteration 1129, loss = 0.48058518\n",
      "Iteration 1130, loss = 0.48130014\n",
      "Iteration 1131, loss = 0.48054726\n",
      "Iteration 1132, loss = 0.47971192\n",
      "Iteration 1133, loss = 0.48025431\n",
      "Iteration 1134, loss = 0.48088892\n",
      "Iteration 1135, loss = 0.48119043\n",
      "Iteration 1136, loss = 0.47981855\n",
      "Iteration 1137, loss = 0.47974331\n",
      "Iteration 1138, loss = 0.47995914\n",
      "Iteration 1139, loss = 0.48035643\n",
      "Iteration 1140, loss = 0.48038187\n",
      "Iteration 1141, loss = 0.48002515\n",
      "Iteration 1142, loss = 0.47983195\n",
      "Iteration 1143, loss = 0.47977774\n",
      "Iteration 1144, loss = 0.48008084\n",
      "Iteration 1145, loss = 0.48071054\n",
      "Iteration 1146, loss = 0.48007578\n",
      "Iteration 1147, loss = 0.47994907\n",
      "Iteration 1148, loss = 0.47977705\n",
      "Iteration 1149, loss = 0.47970741\n",
      "Iteration 1150, loss = 0.47986529\n",
      "Iteration 1151, loss = 0.48073386\n",
      "Iteration 1152, loss = 0.48009112\n",
      "Iteration 1153, loss = 0.47944572\n",
      "Iteration 1154, loss = 0.48113532\n",
      "Iteration 1155, loss = 0.48104009\n",
      "Iteration 1156, loss = 0.48119418\n",
      "Iteration 1157, loss = 0.48058331\n",
      "Iteration 1158, loss = 0.47977261\n",
      "Iteration 1159, loss = 0.48018255\n",
      "Iteration 1160, loss = 0.48046623\n",
      "Iteration 1161, loss = 0.47989591\n",
      "Iteration 1162, loss = 0.47969415\n",
      "Iteration 1163, loss = 0.47983047\n",
      "Iteration 1164, loss = 0.47968729\n",
      "Iteration 1165, loss = 0.47968446\n",
      "Iteration 1166, loss = 0.47954680\n",
      "Iteration 1167, loss = 0.47960979\n",
      "Iteration 1168, loss = 0.47979221\n",
      "Iteration 1169, loss = 0.47976974\n",
      "Iteration 1170, loss = 0.48000999\n",
      "Iteration 1171, loss = 0.47967511\n",
      "Iteration 1172, loss = 0.47963184\n",
      "Iteration 1173, loss = 0.47951898\n",
      "Iteration 1174, loss = 0.47951729\n",
      "Iteration 1175, loss = 0.47963108\n",
      "Iteration 1176, loss = 0.47998605\n",
      "Iteration 1177, loss = 0.48007768\n",
      "Iteration 1178, loss = 0.47994806\n",
      "Iteration 1179, loss = 0.47944176\n",
      "Iteration 1180, loss = 0.47972324\n",
      "Iteration 1181, loss = 0.48132615\n",
      "Iteration 1182, loss = 0.48007503\n",
      "Iteration 1183, loss = 0.47927161\n",
      "Iteration 1184, loss = 0.48006506\n",
      "Iteration 1185, loss = 0.48052413\n",
      "Iteration 1186, loss = 0.48034176\n",
      "Iteration 1187, loss = 0.48007792\n",
      "Iteration 1188, loss = 0.47943835\n",
      "Iteration 1189, loss = 0.47951080\n",
      "Iteration 1190, loss = 0.47976969\n",
      "Iteration 1191, loss = 0.48010560\n",
      "Iteration 1192, loss = 0.48011778\n",
      "Iteration 1193, loss = 0.48051534\n",
      "Iteration 1194, loss = 0.47961912\n",
      "Iteration 1195, loss = 0.47951854\n",
      "Iteration 1196, loss = 0.48001425\n",
      "Iteration 1197, loss = 0.47957269\n",
      "Iteration 1198, loss = 0.47923484\n",
      "Iteration 1199, loss = 0.47985003\n",
      "Iteration 1200, loss = 0.47993877\n",
      "Iteration 1201, loss = 0.47948674\n",
      "Iteration 1202, loss = 0.47912167\n",
      "Iteration 1203, loss = 0.48018659\n",
      "Iteration 1204, loss = 0.48027516\n",
      "Iteration 1205, loss = 0.47992723\n",
      "Iteration 1206, loss = 0.47959353\n",
      "Iteration 1207, loss = 0.47937957\n",
      "Iteration 1208, loss = 0.47961197\n",
      "Iteration 1209, loss = 0.47957688\n",
      "Iteration 1210, loss = 0.47947495\n",
      "Iteration 1211, loss = 0.47966085\n",
      "Iteration 1212, loss = 0.47979761\n",
      "Iteration 1213, loss = 0.47993762\n",
      "Iteration 1214, loss = 0.48043862\n",
      "Iteration 1215, loss = 0.47989668\n",
      "Iteration 1216, loss = 0.47980072\n",
      "Iteration 1217, loss = 0.47943081\n",
      "Iteration 1218, loss = 0.47969295\n",
      "Iteration 1219, loss = 0.47927404\n",
      "Iteration 1220, loss = 0.47926518\n",
      "Iteration 1221, loss = 0.47942094\n",
      "Iteration 1222, loss = 0.47949460\n",
      "Iteration 1223, loss = 0.47939179\n",
      "Iteration 1224, loss = 0.47959584\n",
      "Iteration 1225, loss = 0.47944645\n",
      "Iteration 1226, loss = 0.47943496\n",
      "Iteration 1227, loss = 0.47939953\n",
      "Iteration 1228, loss = 0.47923469\n",
      "Iteration 1229, loss = 0.47930036\n",
      "Iteration 1230, loss = 0.47923090\n",
      "Iteration 1231, loss = 0.47939544\n",
      "Iteration 1232, loss = 0.47921125\n",
      "Iteration 1233, loss = 0.47916834\n",
      "Iteration 1234, loss = 0.47969079\n",
      "Iteration 1235, loss = 0.47908133\n",
      "Iteration 1236, loss = 0.47911619\n",
      "Iteration 1237, loss = 0.47950545\n",
      "Iteration 1238, loss = 0.47960937\n",
      "Iteration 1239, loss = 0.47926182\n",
      "Iteration 1240, loss = 0.47906107\n",
      "Iteration 1241, loss = 0.47920030\n",
      "Iteration 1242, loss = 0.47946815\n",
      "Iteration 1243, loss = 0.47955018\n",
      "Iteration 1244, loss = 0.47915408\n",
      "Iteration 1245, loss = 0.47904448\n",
      "Iteration 1246, loss = 0.47970462\n",
      "Iteration 1247, loss = 0.47946156\n",
      "Iteration 1248, loss = 0.47922178\n",
      "Iteration 1249, loss = 0.47901281\n",
      "Iteration 1250, loss = 0.47925465\n",
      "Iteration 1251, loss = 0.47952623\n",
      "Iteration 1252, loss = 0.47936934\n",
      "Iteration 1253, loss = 0.47900690\n",
      "Iteration 1254, loss = 0.47898422\n",
      "Iteration 1255, loss = 0.47924446\n",
      "Iteration 1256, loss = 0.47958381\n",
      "Iteration 1257, loss = 0.47959358\n",
      "Iteration 1258, loss = 0.47936292\n",
      "Iteration 1259, loss = 0.47921623\n",
      "Iteration 1260, loss = 0.47905693\n",
      "Iteration 1261, loss = 0.47918668\n",
      "Iteration 1262, loss = 0.47915486\n",
      "Iteration 1263, loss = 0.47897330\n",
      "Iteration 1264, loss = 0.47922641\n",
      "Iteration 1265, loss = 0.47910304\n",
      "Iteration 1266, loss = 0.47949417\n",
      "Iteration 1267, loss = 0.47921009\n",
      "Iteration 1268, loss = 0.47931106\n",
      "Iteration 1269, loss = 0.47921310\n",
      "Iteration 1270, loss = 0.47904240\n",
      "Iteration 1271, loss = 0.47899820\n",
      "Iteration 1272, loss = 0.48025892\n",
      "Iteration 1273, loss = 0.47873163\n",
      "Iteration 1274, loss = 0.47926317\n",
      "Iteration 1275, loss = 0.47992542\n",
      "Iteration 1276, loss = 0.47983557\n",
      "Iteration 1277, loss = 0.47897125\n",
      "Iteration 1278, loss = 0.47888337\n",
      "Iteration 1279, loss = 0.47926964\n",
      "Iteration 1280, loss = 0.47987107\n",
      "Iteration 1281, loss = 0.47990216\n",
      "Iteration 1282, loss = 0.47968764\n",
      "Iteration 1283, loss = 0.47899903\n",
      "Iteration 1284, loss = 0.47886543\n",
      "Iteration 1285, loss = 0.47899462\n",
      "Iteration 1286, loss = 0.47925245\n",
      "Iteration 1287, loss = 0.47908940\n",
      "Iteration 1288, loss = 0.47875363\n",
      "Iteration 1289, loss = 0.47913721\n",
      "Iteration 1290, loss = 0.47984000\n",
      "Iteration 1291, loss = 0.47924854\n",
      "Iteration 1292, loss = 0.47864997\n",
      "Iteration 1293, loss = 0.47891435\n",
      "Iteration 1294, loss = 0.47944480\n",
      "Iteration 1295, loss = 0.47971458\n",
      "Iteration 1296, loss = 0.48092597\n",
      "Iteration 1297, loss = 0.47884419\n",
      "Iteration 1298, loss = 0.47875994\n",
      "Iteration 1299, loss = 0.47908914\n",
      "Iteration 1300, loss = 0.47886710\n",
      "Iteration 1301, loss = 0.47920716\n",
      "Iteration 1302, loss = 0.47895940\n",
      "Iteration 1303, loss = 0.47927147\n",
      "Iteration 1304, loss = 0.47877724\n",
      "Iteration 1305, loss = 0.47891745\n",
      "Iteration 1306, loss = 0.47877816\n",
      "Iteration 1307, loss = 0.47860748\n",
      "Iteration 1308, loss = 0.47876014\n",
      "Iteration 1309, loss = 0.47924205\n",
      "Iteration 1310, loss = 0.47904279\n",
      "Iteration 1311, loss = 0.47861825\n",
      "Iteration 1312, loss = 0.47931778\n",
      "Iteration 1313, loss = 0.47904099\n",
      "Iteration 1314, loss = 0.47888922\n",
      "Iteration 1315, loss = 0.47884567\n",
      "Iteration 1316, loss = 0.47871017\n",
      "Iteration 1317, loss = 0.47883979\n",
      "Iteration 1318, loss = 0.47873647\n",
      "Iteration 1319, loss = 0.47864443\n",
      "Iteration 1320, loss = 0.47880815\n",
      "Iteration 1321, loss = 0.47899463\n",
      "Iteration 1322, loss = 0.47873820\n",
      "Iteration 1323, loss = 0.47852074\n",
      "Iteration 1324, loss = 0.47905708\n",
      "Iteration 1325, loss = 0.47905043\n",
      "Iteration 1326, loss = 0.47866071\n",
      "Iteration 1327, loss = 0.47837873\n",
      "Iteration 1328, loss = 0.47947184\n",
      "Iteration 1329, loss = 0.48020732\n",
      "Iteration 1330, loss = 0.47932328\n",
      "Iteration 1331, loss = 0.47987780\n",
      "Iteration 1332, loss = 0.47873552\n",
      "Iteration 1333, loss = 0.47911141\n",
      "Iteration 1334, loss = 0.47939823\n",
      "Iteration 1335, loss = 0.47963431\n",
      "Iteration 1336, loss = 0.47849224\n",
      "Iteration 1337, loss = 0.47858216\n",
      "Iteration 1338, loss = 0.47872121\n",
      "Iteration 1339, loss = 0.47905505\n",
      "Iteration 1340, loss = 0.47884739\n",
      "Iteration 1341, loss = 0.47878477\n",
      "Iteration 1342, loss = 0.47872770\n",
      "Iteration 1343, loss = 0.47843781\n",
      "Iteration 1344, loss = 0.47850970\n",
      "Iteration 1345, loss = 0.47868729\n",
      "Iteration 1346, loss = 0.47847096\n",
      "Iteration 1347, loss = 0.47848349\n",
      "Iteration 1348, loss = 0.47846932\n",
      "Iteration 1349, loss = 0.47842231\n",
      "Iteration 1350, loss = 0.47855570\n",
      "Iteration 1351, loss = 0.47846757\n",
      "Iteration 1352, loss = 0.47874931\n",
      "Iteration 1353, loss = 0.47894366\n",
      "Iteration 1354, loss = 0.47955742\n",
      "Iteration 1355, loss = 0.47854021\n",
      "Iteration 1356, loss = 0.47841577\n",
      "Iteration 1357, loss = 0.47859604\n",
      "Iteration 1358, loss = 0.47846070\n",
      "Iteration 1359, loss = 0.47833195\n",
      "Iteration 1360, loss = 0.47836989\n",
      "Iteration 1361, loss = 0.47887818\n",
      "Iteration 1362, loss = 0.47857506\n",
      "Iteration 1363, loss = 0.47832296\n",
      "Iteration 1364, loss = 0.47833820\n",
      "Iteration 1365, loss = 0.47896579\n",
      "Iteration 1366, loss = 0.47947839\n",
      "Iteration 1367, loss = 0.47841543\n",
      "Iteration 1368, loss = 0.47837943\n",
      "Iteration 1369, loss = 0.47829087\n",
      "Iteration 1370, loss = 0.47829983\n",
      "Iteration 1371, loss = 0.47846706\n",
      "Iteration 1372, loss = 0.47872602\n",
      "Iteration 1373, loss = 0.47856527\n",
      "Iteration 1374, loss = 0.47844309\n",
      "Iteration 1375, loss = 0.47852337\n",
      "Iteration 1376, loss = 0.47853129\n",
      "Iteration 1377, loss = 0.47822303\n",
      "Iteration 1378, loss = 0.47830281\n",
      "Iteration 1379, loss = 0.47882735\n",
      "Iteration 1380, loss = 0.47922728\n",
      "Iteration 1381, loss = 0.47909730\n",
      "Iteration 1382, loss = 0.47832793\n",
      "Iteration 1383, loss = 0.47798115\n",
      "Iteration 1384, loss = 0.47908705\n",
      "Iteration 1385, loss = 0.47944349\n",
      "Iteration 1386, loss = 0.47928849\n",
      "Iteration 1387, loss = 0.47830661\n",
      "Iteration 1388, loss = 0.47953664\n",
      "Iteration 1389, loss = 0.47883489\n",
      "Iteration 1390, loss = 0.47824262\n",
      "Iteration 1391, loss = 0.47841886\n",
      "Iteration 1392, loss = 0.47873349\n",
      "Iteration 1393, loss = 0.47868261\n",
      "Iteration 1394, loss = 0.47844876\n",
      "Iteration 1395, loss = 0.47815352\n",
      "Iteration 1396, loss = 0.47815334\n",
      "Iteration 1397, loss = 0.47825695\n",
      "Iteration 1398, loss = 0.47835318\n",
      "Iteration 1399, loss = 0.47832693\n",
      "Iteration 1400, loss = 0.47822137\n",
      "Iteration 1401, loss = 0.47841106\n",
      "Iteration 1402, loss = 0.47837433\n",
      "Iteration 1403, loss = 0.47854602\n",
      "Iteration 1404, loss = 0.47802042\n",
      "Iteration 1405, loss = 0.47885883\n",
      "Iteration 1406, loss = 0.47869879\n",
      "Iteration 1407, loss = 0.47849760\n",
      "Iteration 1408, loss = 0.47862785\n",
      "Iteration 1409, loss = 0.47849873\n",
      "Iteration 1410, loss = 0.47800261\n",
      "Iteration 1411, loss = 0.47857114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1412, loss = 0.47837053\n",
      "Iteration 1413, loss = 0.47824254\n",
      "Iteration 1414, loss = 0.47826117\n",
      "Iteration 1415, loss = 0.47809273\n",
      "Iteration 1416, loss = 0.47822108\n",
      "Iteration 1417, loss = 0.47817944\n",
      "Iteration 1418, loss = 0.47809189\n",
      "Iteration 1419, loss = 0.47825325\n",
      "Iteration 1420, loss = 0.47806656\n",
      "Iteration 1421, loss = 0.47811663\n",
      "Iteration 1422, loss = 0.47828759\n",
      "Iteration 1423, loss = 0.47817817\n",
      "Iteration 1424, loss = 0.47780889\n",
      "Iteration 1425, loss = 0.47845188\n",
      "Iteration 1426, loss = 0.47862905\n",
      "Iteration 1427, loss = 0.47852792\n",
      "Iteration 1428, loss = 0.47810288\n",
      "Iteration 1429, loss = 0.47827910\n",
      "Iteration 1430, loss = 0.47832702\n",
      "Iteration 1431, loss = 0.47823297\n",
      "Iteration 1432, loss = 0.47794914\n",
      "Iteration 1433, loss = 0.47822209\n",
      "Iteration 1434, loss = 0.47818391\n",
      "Iteration 1435, loss = 0.47840657\n",
      "Iteration 1436, loss = 0.47814836\n",
      "Iteration 1437, loss = 0.47779667\n",
      "Iteration 1438, loss = 0.47821953\n",
      "Iteration 1439, loss = 0.47876611\n",
      "Iteration 1440, loss = 0.47897296\n",
      "Iteration 1441, loss = 0.47844080\n",
      "Iteration 1442, loss = 0.47826729\n",
      "Iteration 1443, loss = 0.47796627\n",
      "Iteration 1444, loss = 0.47776862\n",
      "Iteration 1445, loss = 0.47799350\n",
      "Iteration 1446, loss = 0.47864541\n",
      "Iteration 1447, loss = 0.47865795\n",
      "Iteration 1448, loss = 0.47812533\n",
      "Iteration 1449, loss = 0.47791597\n",
      "Iteration 1450, loss = 0.47807439\n",
      "Iteration 1451, loss = 0.47799650\n",
      "Iteration 1452, loss = 0.47819890\n",
      "Iteration 1453, loss = 0.47785736\n",
      "Iteration 1454, loss = 0.47791257\n",
      "Iteration 1455, loss = 0.47783903\n",
      "Iteration 1456, loss = 0.47805822\n",
      "Iteration 1457, loss = 0.47801178\n",
      "Iteration 1458, loss = 0.47781664\n",
      "Iteration 1459, loss = 0.47789972\n",
      "Iteration 1460, loss = 0.47795270\n",
      "Iteration 1461, loss = 0.47818423\n",
      "Iteration 1462, loss = 0.47791181\n",
      "Iteration 1463, loss = 0.47798548\n",
      "Iteration 1464, loss = 0.47822083\n",
      "Iteration 1465, loss = 0.47855400\n",
      "Iteration 1466, loss = 0.47816095\n",
      "Iteration 1467, loss = 0.47821765\n",
      "Iteration 1468, loss = 0.47809273\n",
      "Iteration 1469, loss = 0.47824055\n",
      "Iteration 1470, loss = 0.47778366\n",
      "Iteration 1471, loss = 0.47775498\n",
      "Iteration 1472, loss = 0.47777682\n",
      "Iteration 1473, loss = 0.47777953\n",
      "Iteration 1474, loss = 0.47789960\n",
      "Iteration 1475, loss = 0.47788734\n",
      "Iteration 1476, loss = 0.47818303\n",
      "Iteration 1477, loss = 0.47829278\n",
      "Iteration 1478, loss = 0.47805735\n",
      "Iteration 1479, loss = 0.47766734\n",
      "Iteration 1480, loss = 0.47811051\n",
      "Iteration 1481, loss = 0.47824040\n",
      "Iteration 1482, loss = 0.47812051\n",
      "Iteration 1483, loss = 0.47770099\n",
      "Iteration 1484, loss = 0.47917045\n",
      "Iteration 1485, loss = 0.47834113\n",
      "Iteration 1486, loss = 0.47829922\n",
      "Iteration 1487, loss = 0.47766976\n",
      "Iteration 1488, loss = 0.47824490\n",
      "Iteration 1489, loss = 0.47843613\n",
      "Iteration 1490, loss = 0.47776937\n",
      "Iteration 1491, loss = 0.47758978\n",
      "Iteration 1492, loss = 0.47789685\n",
      "Iteration 1493, loss = 0.47814791\n",
      "Iteration 1494, loss = 0.47822369\n",
      "Iteration 1495, loss = 0.47806781\n",
      "Iteration 1496, loss = 0.47779555\n",
      "Iteration 1497, loss = 0.47796815\n",
      "Iteration 1498, loss = 0.47838404\n",
      "Iteration 1499, loss = 0.47799103\n",
      "Iteration 1500, loss = 0.47793661\n",
      "Iteration 1501, loss = 0.47836183\n",
      "Iteration 1502, loss = 0.47759976\n",
      "Iteration 1503, loss = 0.47789405\n",
      "Iteration 1504, loss = 0.47781592\n",
      "Iteration 1505, loss = 0.47770362\n",
      "Iteration 1506, loss = 0.47747457\n",
      "Iteration 1507, loss = 0.47765533\n",
      "Iteration 1508, loss = 0.47793610\n",
      "Iteration 1509, loss = 0.47791666\n",
      "Iteration 1510, loss = 0.47760341\n",
      "Iteration 1511, loss = 0.47785131\n",
      "Iteration 1512, loss = 0.47771811\n",
      "Iteration 1513, loss = 0.47756106\n",
      "Iteration 1514, loss = 0.47790397\n",
      "Iteration 1515, loss = 0.47774656\n",
      "Iteration 1516, loss = 0.47760524\n",
      "Iteration 1517, loss = 0.47788508\n",
      "Iteration 1518, loss = 0.47781966\n",
      "Iteration 1519, loss = 0.47817132\n",
      "Iteration 1520, loss = 0.47755120\n",
      "Iteration 1521, loss = 0.47752667\n",
      "Iteration 1522, loss = 0.47762940\n",
      "Iteration 1523, loss = 0.47776854\n",
      "Iteration 1524, loss = 0.47761228\n",
      "Iteration 1525, loss = 0.47771163\n",
      "Iteration 1526, loss = 0.47753703\n",
      "Iteration 1527, loss = 0.47746903\n",
      "Iteration 1528, loss = 0.47751837\n",
      "Iteration 1529, loss = 0.47745343\n",
      "Iteration 1530, loss = 0.47747562\n",
      "Iteration 1531, loss = 0.47754912\n",
      "Iteration 1532, loss = 0.47745294\n",
      "Iteration 1533, loss = 0.47768485\n",
      "Iteration 1534, loss = 0.47758099\n",
      "Iteration 1535, loss = 0.47746211\n",
      "Iteration 1536, loss = 0.47760059\n",
      "Iteration 1537, loss = 0.47757342\n",
      "Iteration 1538, loss = 0.47843723\n",
      "Iteration 1539, loss = 0.47872398\n",
      "Iteration 1540, loss = 0.47781827\n",
      "Iteration 1541, loss = 0.47746859\n",
      "Iteration 1542, loss = 0.47813013\n",
      "Iteration 1543, loss = 0.47774442\n",
      "Iteration 1544, loss = 0.47765631\n",
      "Iteration 1545, loss = 0.47751364\n",
      "Iteration 1546, loss = 0.47745018\n",
      "Iteration 1547, loss = 0.47745180\n",
      "Iteration 1548, loss = 0.47748051\n",
      "Iteration 1549, loss = 0.47751232\n",
      "Iteration 1550, loss = 0.47736242\n",
      "Iteration 1551, loss = 0.47735784\n",
      "Iteration 1552, loss = 0.47754628\n",
      "Iteration 1553, loss = 0.47773482\n",
      "Iteration 1554, loss = 0.47762942\n",
      "Iteration 1555, loss = 0.47716508\n",
      "Iteration 1556, loss = 0.47757553\n",
      "Iteration 1557, loss = 0.47783881\n",
      "Iteration 1558, loss = 0.47798667\n",
      "Iteration 1559, loss = 0.47754336\n",
      "Iteration 1560, loss = 0.47708840\n",
      "Iteration 1561, loss = 0.47835221\n",
      "Iteration 1562, loss = 0.47805001\n",
      "Iteration 1563, loss = 0.47754121\n",
      "Iteration 1564, loss = 0.47716472\n",
      "Iteration 1565, loss = 0.47744443\n",
      "Iteration 1566, loss = 0.47840229\n",
      "Iteration 1567, loss = 0.47867902\n",
      "Iteration 1568, loss = 0.47740696\n",
      "Iteration 1569, loss = 0.47701458\n",
      "Iteration 1570, loss = 0.47843020\n",
      "Iteration 1571, loss = 0.47908079\n",
      "Iteration 1572, loss = 0.47857029\n",
      "Iteration 1573, loss = 0.47742161\n",
      "Iteration 1574, loss = 0.47739058\n",
      "Iteration 1575, loss = 0.47763004\n",
      "Iteration 1576, loss = 0.47777510\n",
      "Iteration 1577, loss = 0.47718768\n",
      "Iteration 1578, loss = 0.47749865\n",
      "Iteration 1579, loss = 0.47776435\n",
      "Iteration 1580, loss = 0.47796023\n",
      "Iteration 1581, loss = 0.47768038\n",
      "Iteration 1582, loss = 0.47749333\n",
      "Iteration 1583, loss = 0.47721566\n",
      "Iteration 1584, loss = 0.47754511\n",
      "Iteration 1585, loss = 0.47724508\n",
      "Iteration 1586, loss = 0.47717019\n",
      "Iteration 1587, loss = 0.47725562\n",
      "Iteration 1588, loss = 0.47770212\n",
      "Iteration 1589, loss = 0.47745827\n",
      "Iteration 1590, loss = 0.47708909\n",
      "Iteration 1591, loss = 0.47794741\n",
      "Iteration 1592, loss = 0.47746119\n",
      "Iteration 1593, loss = 0.47723307\n",
      "Iteration 1594, loss = 0.47706295\n",
      "Iteration 1595, loss = 0.47787064\n",
      "Iteration 1596, loss = 0.47761132\n",
      "Iteration 1597, loss = 0.47743174\n",
      "Iteration 1598, loss = 0.47719969\n",
      "Iteration 1599, loss = 0.47705440\n",
      "Iteration 1600, loss = 0.47726956\n",
      "Iteration 1601, loss = 0.47744607\n",
      "Iteration 1602, loss = 0.47790225\n",
      "Iteration 1603, loss = 0.47737470\n",
      "Iteration 1604, loss = 0.47746602\n",
      "Iteration 1605, loss = 0.47707669\n",
      "Iteration 1606, loss = 0.47719701\n",
      "Iteration 1607, loss = 0.47746285\n",
      "Iteration 1608, loss = 0.47749000\n",
      "Iteration 1609, loss = 0.47707111\n",
      "Iteration 1610, loss = 0.47720886\n",
      "Iteration 1611, loss = 0.47797292\n",
      "Iteration 1612, loss = 0.47727407\n",
      "Iteration 1613, loss = 0.47692759\n",
      "Iteration 1614, loss = 0.47735205\n",
      "Iteration 1615, loss = 0.47758316\n",
      "Iteration 1616, loss = 0.47746298\n",
      "Iteration 1617, loss = 0.47727197\n",
      "Iteration 1618, loss = 0.47704787\n",
      "Iteration 1619, loss = 0.47730038\n",
      "Iteration 1620, loss = 0.47717411\n",
      "Iteration 1621, loss = 0.47705926\n",
      "Iteration 1622, loss = 0.47728038\n",
      "Iteration 1623, loss = 0.47822048\n",
      "Iteration 1624, loss = 0.47808405\n",
      "Iteration 1625, loss = 0.47720576\n",
      "Iteration 1626, loss = 0.47702235\n",
      "Iteration 1627, loss = 0.47716246\n",
      "Iteration 1628, loss = 0.47743467\n",
      "Iteration 1629, loss = 0.47765870\n",
      "Iteration 1630, loss = 0.47717048\n",
      "Iteration 1631, loss = 0.47749082\n",
      "Iteration 1632, loss = 0.47729581\n",
      "Iteration 1633, loss = 0.47701192\n",
      "Iteration 1634, loss = 0.47686860\n",
      "Iteration 1635, loss = 0.47694448\n",
      "Iteration 1636, loss = 0.47721266\n",
      "Iteration 1637, loss = 0.47717810\n",
      "Iteration 1638, loss = 0.47764952\n",
      "Iteration 1639, loss = 0.47733812\n",
      "Iteration 1640, loss = 0.47743948\n",
      "Iteration 1641, loss = 0.47767379\n",
      "Iteration 1642, loss = 0.47710364\n",
      "Iteration 1643, loss = 0.47707668\n",
      "Iteration 1644, loss = 0.47707131\n",
      "Iteration 1645, loss = 0.47742905\n",
      "Iteration 1646, loss = 0.47711260\n",
      "Iteration 1647, loss = 0.47704765\n",
      "Iteration 1648, loss = 0.47690051\n",
      "Iteration 1649, loss = 0.47703555\n",
      "Iteration 1650, loss = 0.47715552\n",
      "Iteration 1651, loss = 0.47715987\n",
      "Iteration 1652, loss = 0.47685667\n",
      "Iteration 1653, loss = 0.47704736\n",
      "Iteration 1654, loss = 0.47683289\n",
      "Iteration 1655, loss = 0.47739102\n",
      "Iteration 1656, loss = 0.47763737\n",
      "Iteration 1657, loss = 0.47757335\n",
      "Iteration 1658, loss = 0.47685796\n",
      "Iteration 1659, loss = 0.47686257\n",
      "Iteration 1660, loss = 0.47695160\n",
      "Iteration 1661, loss = 0.47713718\n",
      "Iteration 1662, loss = 0.47693186\n",
      "Iteration 1663, loss = 0.47680451\n",
      "Iteration 1664, loss = 0.47727362\n",
      "Iteration 1665, loss = 0.47720343\n",
      "Iteration 1666, loss = 0.47685937\n",
      "Iteration 1667, loss = 0.47738851\n",
      "Iteration 1668, loss = 0.47729633\n",
      "Iteration 1669, loss = 0.47698394\n",
      "Iteration 1670, loss = 0.47706834\n",
      "Iteration 1671, loss = 0.47701437\n",
      "Iteration 1672, loss = 0.47716383\n",
      "Iteration 1673, loss = 0.47687748\n",
      "Iteration 1674, loss = 0.47698361\n",
      "Iteration 1675, loss = 0.47706220\n",
      "Iteration 1676, loss = 0.47669371\n",
      "Iteration 1677, loss = 0.47688731\n",
      "Iteration 1678, loss = 0.47715913\n",
      "Iteration 1679, loss = 0.47742722\n",
      "Iteration 1680, loss = 0.47695468\n",
      "Iteration 1681, loss = 0.47698952\n",
      "Iteration 1682, loss = 0.47673081\n",
      "Iteration 1683, loss = 0.47671539\n",
      "Iteration 1684, loss = 0.47682841\n",
      "Iteration 1685, loss = 0.47708016\n",
      "Iteration 1686, loss = 0.47695327\n",
      "Iteration 1687, loss = 0.47670226\n",
      "Iteration 1688, loss = 0.47681349\n",
      "Iteration 1689, loss = 0.47705278\n",
      "Iteration 1690, loss = 0.47696760\n",
      "Iteration 1691, loss = 0.47671265\n",
      "Iteration 1692, loss = 0.47698890\n",
      "Iteration 1693, loss = 0.47680250\n",
      "Iteration 1694, loss = 0.47658226\n",
      "Iteration 1695, loss = 0.47759104\n",
      "Iteration 1696, loss = 0.47723125\n",
      "Iteration 1697, loss = 0.47694118\n",
      "Iteration 1698, loss = 0.47696013\n",
      "Iteration 1699, loss = 0.47782255\n",
      "Iteration 1700, loss = 0.47767137\n",
      "Iteration 1701, loss = 0.47673632\n",
      "Iteration 1702, loss = 0.47811725\n",
      "Iteration 1703, loss = 0.47692786\n",
      "Iteration 1704, loss = 0.47697787\n",
      "Iteration 1705, loss = 0.47690138\n",
      "Iteration 1706, loss = 0.47653527\n",
      "Iteration 1707, loss = 0.47665442\n",
      "Iteration 1708, loss = 0.47721449\n",
      "Iteration 1709, loss = 0.47800075\n",
      "Iteration 1710, loss = 0.47788501\n",
      "Iteration 1711, loss = 0.47675377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1712, loss = 0.47660099\n",
      "Iteration 1713, loss = 0.47657924\n",
      "Iteration 1714, loss = 0.47706269\n",
      "Iteration 1715, loss = 0.47720704\n",
      "Iteration 1716, loss = 0.47704198\n",
      "Iteration 1717, loss = 0.47692283\n",
      "Iteration 1718, loss = 0.47666155\n",
      "Iteration 1719, loss = 0.47690684\n",
      "Iteration 1720, loss = 0.47718196\n",
      "Iteration 1721, loss = 0.47711449\n",
      "Iteration 1722, loss = 0.47666013\n",
      "Iteration 1723, loss = 0.47775162\n",
      "Iteration 1724, loss = 0.47653805\n",
      "Iteration 1725, loss = 0.47651670\n",
      "Iteration 1726, loss = 0.47667248\n",
      "Iteration 1727, loss = 0.47668841\n",
      "Iteration 1728, loss = 0.47663420\n",
      "Iteration 1729, loss = 0.47675877\n",
      "Iteration 1730, loss = 0.47701652\n",
      "Iteration 1731, loss = 0.47684731\n",
      "Iteration 1732, loss = 0.47686227\n",
      "Iteration 1733, loss = 0.47661092\n",
      "Iteration 1734, loss = 0.47654576\n",
      "Iteration 1735, loss = 0.47723216\n",
      "Iteration 1736, loss = 0.47658280\n",
      "Iteration 1737, loss = 0.47648747\n",
      "Iteration 1738, loss = 0.47660543\n",
      "Iteration 1739, loss = 0.47657975\n",
      "Iteration 1740, loss = 0.47651427\n",
      "Iteration 1741, loss = 0.47647479\n",
      "Iteration 1742, loss = 0.47661460\n",
      "Iteration 1743, loss = 0.47657206\n",
      "Iteration 1744, loss = 0.47658249\n",
      "Iteration 1745, loss = 0.47647662\n",
      "Iteration 1746, loss = 0.47645166\n",
      "Iteration 1747, loss = 0.47651930\n",
      "Iteration 1748, loss = 0.47688440\n",
      "Iteration 1749, loss = 0.47693366\n",
      "Iteration 1750, loss = 0.47649016\n",
      "Iteration 1751, loss = 0.47646288\n",
      "Iteration 1752, loss = 0.47669568\n",
      "Iteration 1753, loss = 0.47647384\n",
      "Iteration 1754, loss = 0.47657872\n",
      "Iteration 1755, loss = 0.47685083\n",
      "Iteration 1756, loss = 0.47668589\n",
      "Iteration 1757, loss = 0.47663868\n",
      "Iteration 1758, loss = 0.47646584\n",
      "Iteration 1759, loss = 0.47652175\n",
      "Iteration 1760, loss = 0.47646649\n",
      "Iteration 1761, loss = 0.47650968\n",
      "Iteration 1762, loss = 0.47651857\n",
      "Iteration 1763, loss = 0.47650261\n",
      "Iteration 1764, loss = 0.47661513\n",
      "Iteration 1765, loss = 0.47673375\n",
      "Iteration 1766, loss = 0.47666942\n",
      "Iteration 1767, loss = 0.47648172\n",
      "Iteration 1768, loss = 0.47679770\n",
      "Iteration 1769, loss = 0.47646812\n",
      "Iteration 1770, loss = 0.47648876\n",
      "Iteration 1771, loss = 0.47648384\n",
      "Iteration 1772, loss = 0.47658824\n",
      "Iteration 1773, loss = 0.47718033\n",
      "Iteration 1774, loss = 0.47649666\n",
      "Iteration 1775, loss = 0.47666250\n",
      "Iteration 1776, loss = 0.47655436\n",
      "Iteration 1777, loss = 0.47625052\n",
      "Iteration 1778, loss = 0.47632488\n",
      "Iteration 1779, loss = 0.47661273\n",
      "Iteration 1780, loss = 0.47648673\n",
      "Iteration 1781, loss = 0.47615214\n",
      "Iteration 1782, loss = 0.47651229\n",
      "Iteration 1783, loss = 0.47677403\n",
      "Iteration 1784, loss = 0.47684941\n",
      "Iteration 1785, loss = 0.47675190\n",
      "Iteration 1786, loss = 0.47662738\n",
      "Iteration 1787, loss = 0.47643268\n",
      "Iteration 1788, loss = 0.47668141\n",
      "Iteration 1789, loss = 0.47682084\n",
      "Iteration 1790, loss = 0.47682565\n",
      "Iteration 1791, loss = 0.47632457\n",
      "Iteration 1792, loss = 0.47630086\n",
      "Iteration 1793, loss = 0.47643326\n",
      "Iteration 1794, loss = 0.47697917\n",
      "Iteration 1795, loss = 0.47611315\n",
      "Iteration 1796, loss = 0.47645111\n",
      "Iteration 1797, loss = 0.47691024\n",
      "Iteration 1798, loss = 0.47700495\n",
      "Iteration 1799, loss = 0.47667159\n",
      "Iteration 1800, loss = 0.47661610\n",
      "Iteration 1801, loss = 0.47624549\n",
      "Iteration 1802, loss = 0.47651567\n",
      "Iteration 1803, loss = 0.47663392\n",
      "Iteration 1804, loss = 0.47656328\n",
      "Iteration 1805, loss = 0.47635142\n",
      "Iteration 1806, loss = 0.47621532\n",
      "Iteration 1807, loss = 0.47656936\n",
      "Iteration 1808, loss = 0.47631439\n",
      "Iteration 1809, loss = 0.47676604\n",
      "Iteration 1810, loss = 0.47662345\n",
      "Iteration 1811, loss = 0.47658728\n",
      "Iteration 1812, loss = 0.47631724\n",
      "Iteration 1813, loss = 0.47623745\n",
      "Iteration 1814, loss = 0.47798953\n",
      "Iteration 1815, loss = 0.47657936\n",
      "Iteration 1816, loss = 0.47645030\n",
      "Iteration 1817, loss = 0.47645252\n",
      "Iteration 1818, loss = 0.47656147\n",
      "Iteration 1819, loss = 0.47643860\n",
      "Iteration 1820, loss = 0.47611656\n",
      "Iteration 1821, loss = 0.47639634\n",
      "Iteration 1822, loss = 0.47657236\n",
      "Iteration 1823, loss = 0.47660545\n",
      "Iteration 1824, loss = 0.47656809\n",
      "Iteration 1825, loss = 0.47625468\n",
      "Iteration 1826, loss = 0.47615674\n",
      "Iteration 1827, loss = 0.47619733\n",
      "Iteration 1828, loss = 0.47623135\n",
      "Iteration 1829, loss = 0.47634055\n",
      "Iteration 1830, loss = 0.47630153\n",
      "Iteration 1831, loss = 0.47628138\n",
      "Iteration 1832, loss = 0.47608818\n",
      "Iteration 1833, loss = 0.47655749\n",
      "Iteration 1834, loss = 0.47612326\n",
      "Iteration 1835, loss = 0.47664266\n",
      "Iteration 1836, loss = 0.47623703\n",
      "Iteration 1837, loss = 0.47622981\n",
      "Iteration 1838, loss = 0.47628578\n",
      "Iteration 1839, loss = 0.47622080\n",
      "Iteration 1840, loss = 0.47617662\n",
      "Iteration 1841, loss = 0.47605115\n",
      "Iteration 1842, loss = 0.47611098\n",
      "Iteration 1843, loss = 0.47641490\n",
      "Iteration 1844, loss = 0.47601456\n",
      "Iteration 1845, loss = 0.47696737\n",
      "Iteration 1846, loss = 0.47659994\n",
      "Iteration 1847, loss = 0.47611780\n",
      "Iteration 1848, loss = 0.47611184\n",
      "Iteration 1849, loss = 0.47628612\n",
      "Iteration 1850, loss = 0.47618988\n",
      "Iteration 1851, loss = 0.47602853\n",
      "Iteration 1852, loss = 0.47594850\n",
      "Iteration 1853, loss = 0.47600234\n",
      "Iteration 1854, loss = 0.47660612\n",
      "Iteration 1855, loss = 0.47665719\n",
      "Iteration 1856, loss = 0.47624654\n",
      "Iteration 1857, loss = 0.47614831\n",
      "Iteration 1858, loss = 0.47651946\n",
      "Iteration 1859, loss = 0.47626387\n",
      "Iteration 1860, loss = 0.47608458\n",
      "Iteration 1861, loss = 0.47603404\n",
      "Iteration 1862, loss = 0.47620041\n",
      "Iteration 1863, loss = 0.47646696\n",
      "Iteration 1864, loss = 0.47628646\n",
      "Iteration 1865, loss = 0.47627318\n",
      "Iteration 1866, loss = 0.47599014\n",
      "Iteration 1867, loss = 0.47582389\n",
      "Iteration 1868, loss = 0.47629655\n",
      "Iteration 1869, loss = 0.47636866\n",
      "Iteration 1870, loss = 0.47642622\n",
      "Iteration 1871, loss = 0.47572481\n",
      "Iteration 1872, loss = 0.47572782\n",
      "Iteration 1873, loss = 0.47705176\n",
      "Iteration 1874, loss = 0.47790856\n",
      "Iteration 1875, loss = 0.47722462\n",
      "Iteration 1876, loss = 0.47637002\n",
      "Iteration 1877, loss = 0.47601143\n",
      "Iteration 1878, loss = 0.47678798\n",
      "Iteration 1879, loss = 0.47700212\n",
      "Iteration 1880, loss = 0.47775013\n",
      "Iteration 1881, loss = 0.47618960\n",
      "Iteration 1882, loss = 0.47613946\n",
      "Iteration 1883, loss = 0.47619263\n",
      "Iteration 1884, loss = 0.47609653\n",
      "Iteration 1885, loss = 0.47609316\n",
      "Iteration 1886, loss = 0.47588672\n",
      "Iteration 1887, loss = 0.47589970\n",
      "Iteration 1888, loss = 0.47600299\n",
      "Iteration 1889, loss = 0.47595620\n",
      "Iteration 1890, loss = 0.47649327\n",
      "Iteration 1891, loss = 0.47618114\n",
      "Iteration 1892, loss = 0.47608106\n",
      "Iteration 1893, loss = 0.47609873\n",
      "Iteration 1894, loss = 0.47615153\n",
      "Iteration 1895, loss = 0.47601001\n",
      "Iteration 1896, loss = 0.47596972\n",
      "Iteration 1897, loss = 0.47593357\n",
      "Iteration 1898, loss = 0.47608938\n",
      "Iteration 1899, loss = 0.47633257\n",
      "Iteration 1900, loss = 0.47588463\n",
      "Iteration 1901, loss = 0.47660277\n",
      "Iteration 1902, loss = 0.47659627\n",
      "Iteration 1903, loss = 0.47613142\n",
      "Iteration 1904, loss = 0.47595147\n",
      "Iteration 1905, loss = 0.47589422\n",
      "Iteration 1906, loss = 0.47587902\n",
      "Iteration 1907, loss = 0.47588027\n",
      "Iteration 1908, loss = 0.47586555\n",
      "Iteration 1909, loss = 0.47586266\n",
      "Iteration 1910, loss = 0.47583159\n",
      "Iteration 1911, loss = 0.47607177\n",
      "Iteration 1912, loss = 0.47606682\n",
      "Iteration 1913, loss = 0.47591779\n",
      "Iteration 1914, loss = 0.47596664\n",
      "Iteration 1915, loss = 0.47580894\n",
      "Iteration 1916, loss = 0.47593176\n",
      "Iteration 1917, loss = 0.47607648\n",
      "Iteration 1918, loss = 0.47597046\n",
      "Iteration 1919, loss = 0.47591450\n",
      "Iteration 1920, loss = 0.47579277\n",
      "Iteration 1921, loss = 0.47601930\n",
      "Iteration 1922, loss = 0.47603657\n",
      "Iteration 1923, loss = 0.47583827\n",
      "Iteration 1924, loss = 0.47610678\n",
      "Iteration 1925, loss = 0.47611394\n",
      "Iteration 1926, loss = 0.47577992\n",
      "Iteration 1927, loss = 0.47587764\n",
      "Iteration 1928, loss = 0.47580623\n",
      "Iteration 1929, loss = 0.47589926\n",
      "Iteration 1930, loss = 0.47577201\n",
      "Iteration 1931, loss = 0.47582523\n",
      "Iteration 1932, loss = 0.47576684\n",
      "Iteration 1933, loss = 0.47594350\n",
      "Iteration 1934, loss = 0.47574481\n",
      "Iteration 1935, loss = 0.47585760\n",
      "Iteration 1936, loss = 0.47605297\n",
      "Iteration 1937, loss = 0.47606424\n",
      "Iteration 1938, loss = 0.47590800\n",
      "Iteration 1939, loss = 0.47572837\n",
      "Iteration 1940, loss = 0.47584524\n",
      "Iteration 1941, loss = 0.47621792\n",
      "Iteration 1942, loss = 0.47593003\n",
      "Iteration 1943, loss = 0.47620133\n",
      "Iteration 1944, loss = 0.47645550\n",
      "Iteration 1945, loss = 0.47542251\n",
      "Iteration 1946, loss = 0.47589007\n",
      "Iteration 1947, loss = 0.47718043\n",
      "Iteration 1948, loss = 0.47733979\n",
      "Iteration 1949, loss = 0.47678991\n",
      "Iteration 1950, loss = 0.47764365\n",
      "Iteration 1951, loss = 0.47595195\n",
      "Iteration 1952, loss = 0.47645417\n",
      "Iteration 1953, loss = 0.47607569\n",
      "Iteration 1954, loss = 0.47621538\n",
      "Iteration 1955, loss = 0.47609637\n",
      "Iteration 1956, loss = 0.47604809\n",
      "Iteration 1957, loss = 0.47593236\n",
      "Iteration 1958, loss = 0.47605705\n",
      "Iteration 1959, loss = 0.47607831\n",
      "Iteration 1960, loss = 0.47596743\n",
      "Iteration 1961, loss = 0.47572609\n",
      "Iteration 1962, loss = 0.47561016\n",
      "Iteration 1963, loss = 0.47570656\n",
      "Iteration 1964, loss = 0.47588648\n",
      "Iteration 1965, loss = 0.47623487\n",
      "Iteration 1966, loss = 0.47583229\n",
      "Iteration 1967, loss = 0.47610113\n",
      "Iteration 1968, loss = 0.47622454\n",
      "Iteration 1969, loss = 0.47577850\n",
      "Iteration 1970, loss = 0.47577603\n",
      "Iteration 1971, loss = 0.47585796\n",
      "Iteration 1972, loss = 0.47645291\n",
      "Iteration 1973, loss = 0.47698499\n",
      "Iteration 1974, loss = 0.47592182\n",
      "Iteration 1975, loss = 0.47590664\n",
      "Iteration 1976, loss = 0.47582723\n",
      "Iteration 1977, loss = 0.47575869\n",
      "Iteration 1978, loss = 0.47554841\n",
      "Iteration 1979, loss = 0.47666758\n",
      "Iteration 1980, loss = 0.47573190\n",
      "Iteration 1981, loss = 0.47638245\n",
      "Iteration 1982, loss = 0.47572875\n",
      "Iteration 1983, loss = 0.47568284\n",
      "Iteration 1984, loss = 0.47567160\n",
      "Iteration 1985, loss = 0.47579803\n",
      "Iteration 1986, loss = 0.47567389\n",
      "Iteration 1987, loss = 0.47557692\n",
      "Iteration 1988, loss = 0.47553954\n",
      "Iteration 1989, loss = 0.47589289\n",
      "Iteration 1990, loss = 0.47566192\n",
      "Iteration 1991, loss = 0.47587524\n",
      "Iteration 1992, loss = 0.47581596\n",
      "Iteration 1993, loss = 0.47562198\n",
      "Iteration 1994, loss = 0.47554943\n",
      "Iteration 1995, loss = 0.47572804\n",
      "Iteration 1996, loss = 0.47567914\n",
      "Iteration 1997, loss = 0.47553611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1998, loss = 0.47550619\n",
      "Iteration 1999, loss = 0.47630520\n",
      "Iteration 2000, loss = 0.47578549\n",
      "Iteration 2001, loss = 0.47601726\n",
      "Iteration 2002, loss = 0.47559152\n",
      "Iteration 2003, loss = 0.47575502\n",
      "Iteration 2004, loss = 0.47573454\n",
      "Iteration 2005, loss = 0.47575158\n",
      "Iteration 2006, loss = 0.47571114\n",
      "Iteration 2007, loss = 0.47553157\n",
      "Iteration 2008, loss = 0.47556444\n",
      "Iteration 2009, loss = 0.47557486\n",
      "Iteration 2010, loss = 0.47558423\n",
      "Iteration 2011, loss = 0.47558641\n",
      "Iteration 2012, loss = 0.47566976\n",
      "Iteration 2013, loss = 0.47556822\n",
      "Iteration 2014, loss = 0.47556129\n",
      "Iteration 2015, loss = 0.47574514\n",
      "Iteration 2016, loss = 0.47594272\n",
      "Iteration 2017, loss = 0.47587063\n",
      "Iteration 2018, loss = 0.47567106\n",
      "Iteration 2019, loss = 0.47608032\n",
      "Iteration 2020, loss = 0.47597498\n",
      "Iteration 2021, loss = 0.47557871\n",
      "Iteration 2022, loss = 0.47554050\n",
      "Iteration 2023, loss = 0.47548634\n",
      "Iteration 2024, loss = 0.47549781\n",
      "Iteration 2025, loss = 0.47550980\n",
      "Iteration 2026, loss = 0.47551865\n",
      "Iteration 2027, loss = 0.47551704\n",
      "Iteration 2028, loss = 0.47545050\n",
      "Iteration 2029, loss = 0.47570606\n",
      "Iteration 2030, loss = 0.47576255\n",
      "Iteration 2031, loss = 0.47567671\n",
      "Iteration 2032, loss = 0.47562004\n",
      "Iteration 2033, loss = 0.47548272\n",
      "Iteration 2034, loss = 0.47548847\n",
      "Iteration 2035, loss = 0.47543187\n",
      "Iteration 2036, loss = 0.47543332\n",
      "Iteration 2037, loss = 0.47546043\n",
      "Iteration 2038, loss = 0.47547390\n",
      "Iteration 2039, loss = 0.47544049\n",
      "Iteration 2040, loss = 0.47541347\n",
      "Iteration 2041, loss = 0.47566285\n",
      "Iteration 2042, loss = 0.47548434\n",
      "Iteration 2043, loss = 0.47551554\n",
      "Iteration 2044, loss = 0.47542731\n",
      "Iteration 2045, loss = 0.47542861\n",
      "Iteration 2046, loss = 0.47554408\n",
      "Iteration 2047, loss = 0.47566246\n",
      "Iteration 2048, loss = 0.47560633\n",
      "Iteration 2049, loss = 0.47584500\n",
      "Iteration 2050, loss = 0.47547483\n",
      "Iteration 2051, loss = 0.47548592\n",
      "Iteration 2052, loss = 0.47543601\n",
      "Iteration 2053, loss = 0.47543165\n",
      "Iteration 2054, loss = 0.47540137\n",
      "Iteration 2055, loss = 0.47560779\n",
      "Iteration 2056, loss = 0.47542244\n",
      "Iteration 2057, loss = 0.47546770\n",
      "Iteration 2058, loss = 0.47558768\n",
      "Iteration 2059, loss = 0.47607023\n",
      "Iteration 2060, loss = 0.47635237\n",
      "Iteration 2061, loss = 0.47570731\n",
      "Iteration 2062, loss = 0.47549146\n",
      "Iteration 2063, loss = 0.47627888\n",
      "Iteration 2064, loss = 0.47680629\n",
      "Iteration 2065, loss = 0.47644891\n",
      "Iteration 2066, loss = 0.47608568\n",
      "Iteration 2067, loss = 0.47546231\n",
      "Iteration 2068, loss = 0.47549174\n",
      "Iteration 2069, loss = 0.47599327\n",
      "Iteration 2070, loss = 0.47685397\n",
      "Iteration 2071, loss = 0.47588805\n",
      "Iteration 2072, loss = 0.47550911\n",
      "Iteration 2073, loss = 0.47533788\n",
      "Iteration 2074, loss = 0.47601376\n",
      "Iteration 2075, loss = 0.47538910\n",
      "Iteration 2076, loss = 0.47534418\n",
      "Iteration 2077, loss = 0.47539953\n",
      "Iteration 2078, loss = 0.47583137\n",
      "Iteration 2079, loss = 0.47547676\n",
      "Iteration 2080, loss = 0.47574403\n",
      "Iteration 2081, loss = 0.47568773\n",
      "Iteration 2082, loss = 0.47548108\n",
      "Iteration 2083, loss = 0.47558351\n",
      "Iteration 2084, loss = 0.47537586\n",
      "Iteration 2085, loss = 0.47546520\n",
      "Iteration 2086, loss = 0.47560157\n",
      "Iteration 2087, loss = 0.47538225\n",
      "Iteration 2088, loss = 0.47528004\n",
      "Iteration 2089, loss = 0.47530254\n",
      "Iteration 2090, loss = 0.47538120\n",
      "Iteration 2091, loss = 0.47552301\n",
      "Iteration 2092, loss = 0.47531210\n",
      "Iteration 2093, loss = 0.47534676\n",
      "Iteration 2094, loss = 0.47540737\n",
      "Iteration 2095, loss = 0.47538950\n",
      "Iteration 2096, loss = 0.47541106\n",
      "Iteration 2097, loss = 0.47554242\n",
      "Iteration 2098, loss = 0.47534608\n",
      "Iteration 2099, loss = 0.47540495\n",
      "Iteration 2100, loss = 0.47636035\n",
      "Iteration 2101, loss = 0.47562347\n",
      "Iteration 2102, loss = 0.47529428\n",
      "Iteration 2103, loss = 0.47597705\n",
      "Iteration 2104, loss = 0.47578728\n",
      "Iteration 2105, loss = 0.47558143\n",
      "Iteration 2106, loss = 0.47498280\n",
      "Iteration 2107, loss = 0.47611710\n",
      "Iteration 2108, loss = 0.47635533\n",
      "Iteration 2109, loss = 0.47640605\n",
      "Iteration 2110, loss = 0.47574118\n",
      "Iteration 2111, loss = 0.47530917\n",
      "Iteration 2112, loss = 0.47558951\n",
      "Iteration 2113, loss = 0.47538549\n",
      "Iteration 2114, loss = 0.47536785\n",
      "Iteration 2115, loss = 0.47557747\n",
      "Iteration 2116, loss = 0.47549302\n",
      "Iteration 2117, loss = 0.47542232\n",
      "Iteration 2118, loss = 0.47544502\n",
      "Iteration 2119, loss = 0.47549552\n",
      "Iteration 2120, loss = 0.47561154\n",
      "Iteration 2121, loss = 0.47520173\n",
      "Iteration 2122, loss = 0.47515704\n",
      "Iteration 2123, loss = 0.47630402\n",
      "Iteration 2124, loss = 0.47591042\n",
      "Iteration 2125, loss = 0.47521736\n",
      "Iteration 2126, loss = 0.47544269\n",
      "Iteration 2127, loss = 0.47546810\n",
      "Iteration 2128, loss = 0.47571399\n",
      "Iteration 2129, loss = 0.47554893\n",
      "Iteration 2130, loss = 0.47567758\n",
      "Iteration 2131, loss = 0.47514254\n",
      "Iteration 2132, loss = 0.47529595\n",
      "Iteration 2133, loss = 0.47565276\n",
      "Iteration 2134, loss = 0.47578713\n",
      "Iteration 2135, loss = 0.47561846\n",
      "Iteration 2136, loss = 0.47516765\n",
      "Iteration 2137, loss = 0.47530139\n",
      "Iteration 2138, loss = 0.47537261\n",
      "Iteration 2139, loss = 0.47549473\n",
      "Iteration 2140, loss = 0.47533736\n",
      "Iteration 2141, loss = 0.47520859\n",
      "Iteration 2142, loss = 0.47527494\n",
      "Iteration 2143, loss = 0.47517933\n",
      "Iteration 2144, loss = 0.47522098\n",
      "Iteration 2145, loss = 0.47553615\n",
      "Iteration 2146, loss = 0.47521593\n",
      "Iteration 2147, loss = 0.47534636\n",
      "Iteration 2148, loss = 0.47532658\n",
      "Iteration 2149, loss = 0.47538108\n",
      "Iteration 2150, loss = 0.47523606\n",
      "Iteration 2151, loss = 0.47508184\n",
      "Iteration 2152, loss = 0.47515908\n",
      "Iteration 2153, loss = 0.47535944\n",
      "Iteration 2154, loss = 0.47537887\n",
      "Iteration 2155, loss = 0.47522705\n",
      "Iteration 2156, loss = 0.47500322\n",
      "Iteration 2157, loss = 0.47537018\n",
      "Iteration 2158, loss = 0.47570661\n",
      "Iteration 2159, loss = 0.47562573\n",
      "Iteration 2160, loss = 0.47509309\n",
      "Iteration 2161, loss = 0.47547199\n",
      "Iteration 2162, loss = 0.47552511\n",
      "Iteration 2163, loss = 0.47519337\n",
      "Iteration 2164, loss = 0.47530121\n",
      "Iteration 2165, loss = 0.47527341\n",
      "Iteration 2166, loss = 0.47530127\n",
      "Iteration 2167, loss = 0.47534811\n",
      "Iteration 2168, loss = 0.47543112\n",
      "Iteration 2169, loss = 0.47533989\n",
      "Iteration 2170, loss = 0.47515419\n",
      "Iteration 2171, loss = 0.47505322\n",
      "Iteration 2172, loss = 0.47560607\n",
      "Iteration 2173, loss = 0.47545512\n",
      "Iteration 2174, loss = 0.47534168\n",
      "Iteration 2175, loss = 0.47561698\n",
      "Iteration 2176, loss = 0.47510763\n",
      "Iteration 2177, loss = 0.47510033\n",
      "Iteration 2178, loss = 0.47532961\n",
      "Iteration 2179, loss = 0.47572753\n",
      "Iteration 2180, loss = 0.47564732\n",
      "Iteration 2181, loss = 0.47527921\n",
      "Iteration 2182, loss = 0.47499788\n",
      "Iteration 2183, loss = 0.47507775\n",
      "Iteration 2184, loss = 0.47554343\n",
      "Iteration 2185, loss = 0.47579099\n",
      "Iteration 2186, loss = 0.47558209\n",
      "Iteration 2187, loss = 0.47503971\n",
      "Iteration 2188, loss = 0.47489758\n",
      "Iteration 2189, loss = 0.47524461\n",
      "Iteration 2190, loss = 0.47587409\n",
      "Iteration 2191, loss = 0.47575931\n",
      "Iteration 2192, loss = 0.47518026\n",
      "Iteration 2193, loss = 0.47495716\n",
      "Iteration 2194, loss = 0.47513752\n",
      "Iteration 2195, loss = 0.47529919\n",
      "Iteration 2196, loss = 0.47541231\n",
      "Iteration 2197, loss = 0.47557062\n",
      "Iteration 2198, loss = 0.47501720\n",
      "Iteration 2199, loss = 0.47599716\n",
      "Iteration 2200, loss = 0.47554267\n",
      "Iteration 2201, loss = 0.47567046\n",
      "Iteration 2202, loss = 0.47559535\n",
      "Iteration 2203, loss = 0.47526202\n",
      "Iteration 2204, loss = 0.47519850\n",
      "Iteration 2205, loss = 0.47498579\n",
      "Iteration 2206, loss = 0.47536506\n",
      "Iteration 2207, loss = 0.47504413\n",
      "Iteration 2208, loss = 0.47490341\n",
      "Iteration 2209, loss = 0.47512271\n",
      "Iteration 2210, loss = 0.47551431\n",
      "Iteration 2211, loss = 0.47572552\n",
      "Iteration 2212, loss = 0.47516949\n",
      "Iteration 2213, loss = 0.47488448\n",
      "Iteration 2214, loss = 0.47631177\n",
      "Iteration 2215, loss = 0.47573587\n",
      "Iteration 2216, loss = 0.47511505\n",
      "Iteration 2217, loss = 0.47473011\n",
      "Iteration 2218, loss = 0.47515157\n",
      "Iteration 2219, loss = 0.47616740\n",
      "Iteration 2220, loss = 0.47618687\n",
      "Iteration 2221, loss = 0.47542228\n",
      "Iteration 2222, loss = 0.47515833\n",
      "Iteration 2223, loss = 0.47557419\n",
      "Iteration 2224, loss = 0.47623566\n",
      "Iteration 2225, loss = 0.47595690\n",
      "Iteration 2226, loss = 0.47541883\n",
      "Iteration 2227, loss = 0.47492717\n",
      "Iteration 2228, loss = 0.47519814\n",
      "Iteration 2229, loss = 0.47572648\n",
      "Iteration 2230, loss = 0.47574169\n",
      "Iteration 2231, loss = 0.47623501\n",
      "Iteration 2232, loss = 0.47575537\n",
      "Iteration 2233, loss = 0.47489559\n",
      "Iteration 2234, loss = 0.47516396\n",
      "Iteration 2235, loss = 0.47562729\n",
      "Iteration 2236, loss = 0.47595610\n",
      "Iteration 2237, loss = 0.47554103\n",
      "Iteration 2238, loss = 0.47509624\n",
      "Iteration 2239, loss = 0.47492093\n",
      "Iteration 2240, loss = 0.47483619\n",
      "Iteration 2241, loss = 0.47622905\n",
      "Iteration 2242, loss = 0.47581972\n",
      "Iteration 2243, loss = 0.47493385\n",
      "Iteration 2244, loss = 0.47454133\n",
      "Iteration 2245, loss = 0.47520209\n",
      "Iteration 2246, loss = 0.47684063\n",
      "Iteration 2247, loss = 0.47715406\n",
      "Iteration 2248, loss = 0.47621556\n",
      "Iteration 2249, loss = 0.47626214\n",
      "Iteration 2250, loss = 0.47502980\n",
      "Iteration 2251, loss = 0.47495413\n",
      "Iteration 2252, loss = 0.47491068\n",
      "Iteration 2253, loss = 0.47502199\n",
      "Iteration 2254, loss = 0.47493507\n",
      "Iteration 2255, loss = 0.47488726\n",
      "Iteration 2256, loss = 0.47489071\n",
      "Iteration 2257, loss = 0.47516814\n",
      "Iteration 2258, loss = 0.47500124\n",
      "Iteration 2259, loss = 0.47484765\n",
      "Iteration 2260, loss = 0.47494612\n",
      "Iteration 2261, loss = 0.47493227\n",
      "Iteration 2262, loss = 0.47532054\n",
      "Iteration 2263, loss = 0.47485519\n",
      "Iteration 2264, loss = 0.47483813\n",
      "Iteration 2265, loss = 0.47479608\n",
      "Iteration 2266, loss = 0.47480173\n",
      "Iteration 2267, loss = 0.47527058\n",
      "Iteration 2268, loss = 0.47506825\n",
      "Iteration 2269, loss = 0.47549819\n",
      "Iteration 2270, loss = 0.47483840\n",
      "Iteration 2271, loss = 0.47483594\n",
      "Iteration 2272, loss = 0.47482804\n",
      "Iteration 2273, loss = 0.47481047\n",
      "Iteration 2274, loss = 0.47479562\n",
      "Iteration 2275, loss = 0.47486949\n",
      "Iteration 2276, loss = 0.47503164\n",
      "Iteration 2277, loss = 0.47488938\n",
      "Iteration 2278, loss = 0.47485874\n",
      "Iteration 2279, loss = 0.47491401\n",
      "Iteration 2280, loss = 0.47521960\n",
      "Iteration 2281, loss = 0.47561485\n",
      "Iteration 2282, loss = 0.47526319\n",
      "Iteration 2283, loss = 0.47477678\n",
      "Iteration 2284, loss = 0.47489400\n",
      "Iteration 2285, loss = 0.47576049\n",
      "Iteration 2286, loss = 0.47505908\n",
      "Iteration 2287, loss = 0.47464767\n",
      "Iteration 2288, loss = 0.47493166\n",
      "Iteration 2289, loss = 0.47550247\n",
      "Iteration 2290, loss = 0.47553408\n",
      "Iteration 2291, loss = 0.47508045\n",
      "Iteration 2292, loss = 0.47477402\n",
      "Iteration 2293, loss = 0.47473223\n",
      "Iteration 2294, loss = 0.47578142\n",
      "Iteration 2295, loss = 0.47514466\n",
      "Iteration 2296, loss = 0.47473294\n",
      "Iteration 2297, loss = 0.47477511\n",
      "Iteration 2298, loss = 0.47498571\n",
      "Iteration 2299, loss = 0.47545700\n",
      "Iteration 2300, loss = 0.47577907\n",
      "Iteration 2301, loss = 0.47475510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2302, loss = 0.47487185\n",
      "Iteration 2303, loss = 0.47503675\n",
      "Iteration 2304, loss = 0.47503234\n",
      "Iteration 2305, loss = 0.47515603\n",
      "Iteration 2306, loss = 0.47487199\n",
      "Iteration 2307, loss = 0.47475468\n",
      "Iteration 2308, loss = 0.47484031\n",
      "Iteration 2309, loss = 0.47501247\n",
      "Iteration 2310, loss = 0.47493307\n",
      "Iteration 2311, loss = 0.47487813\n",
      "Iteration 2312, loss = 0.47566885\n",
      "Iteration 2313, loss = 0.47470806\n",
      "Iteration 2314, loss = 0.47477737\n",
      "Iteration 2315, loss = 0.47507876\n",
      "Iteration 2316, loss = 0.47476956\n",
      "Iteration 2317, loss = 0.47500612\n",
      "Iteration 2318, loss = 0.47485085\n",
      "Iteration 2319, loss = 0.47497158\n",
      "Iteration 2320, loss = 0.47480227\n",
      "Iteration 2321, loss = 0.47463915\n",
      "Iteration 2322, loss = 0.47487135\n",
      "Iteration 2323, loss = 0.47564826\n",
      "Iteration 2324, loss = 0.47587627\n",
      "Iteration 2325, loss = 0.47594956\n",
      "Iteration 2326, loss = 0.47471917\n",
      "Iteration 2327, loss = 0.47505139\n",
      "Iteration 2328, loss = 0.47484620\n",
      "Iteration 2329, loss = 0.47470037\n",
      "Iteration 2330, loss = 0.47456241\n",
      "Iteration 2331, loss = 0.47487923\n",
      "Iteration 2332, loss = 0.47496084\n",
      "Iteration 2333, loss = 0.47489765\n",
      "Iteration 2334, loss = 0.47473125\n",
      "Iteration 2335, loss = 0.47456573\n",
      "Iteration 2336, loss = 0.47465201\n",
      "Iteration 2337, loss = 0.47491504\n",
      "Iteration 2338, loss = 0.47534433\n",
      "Iteration 2339, loss = 0.47507942\n",
      "Iteration 2340, loss = 0.47490529\n",
      "Iteration 2341, loss = 0.47466415\n",
      "Iteration 2342, loss = 0.47471941\n",
      "Iteration 2343, loss = 0.47491743\n",
      "Iteration 2344, loss = 0.47507024\n",
      "Iteration 2345, loss = 0.47521251\n",
      "Iteration 2346, loss = 0.47482231\n",
      "Iteration 2347, loss = 0.47509324\n",
      "Iteration 2348, loss = 0.47475978\n",
      "Iteration 2349, loss = 0.47465674\n",
      "Iteration 2350, loss = 0.47510665\n",
      "Iteration 2351, loss = 0.47469494\n",
      "Iteration 2352, loss = 0.47553733\n",
      "Iteration 2353, loss = 0.47525815\n",
      "Iteration 2354, loss = 0.47469082\n",
      "Iteration 2355, loss = 0.47490778\n",
      "Iteration 2356, loss = 0.47524770\n",
      "Iteration 2357, loss = 0.47537090\n",
      "Iteration 2358, loss = 0.47503304\n",
      "Iteration 2359, loss = 0.47542715\n",
      "Iteration 2360, loss = 0.47469673\n",
      "Iteration 2361, loss = 0.47476419\n",
      "Iteration 2362, loss = 0.47478550\n",
      "Iteration 2363, loss = 0.47475341\n",
      "Iteration 2364, loss = 0.47458908\n",
      "Iteration 2365, loss = 0.47498189\n",
      "Iteration 2366, loss = 0.47466787\n",
      "Iteration 2367, loss = 0.47486447\n",
      "Iteration 2368, loss = 0.47472094\n",
      "Iteration 2369, loss = 0.47492979\n",
      "Iteration 2370, loss = 0.47464940\n",
      "Iteration 2371, loss = 0.47455648\n",
      "Iteration 2372, loss = 0.47463048\n",
      "Iteration 2373, loss = 0.47464987\n",
      "Iteration 2374, loss = 0.47477780\n",
      "Iteration 2375, loss = 0.47469023\n",
      "Iteration 2376, loss = 0.47461949\n",
      "Iteration 2377, loss = 0.47497042\n",
      "Iteration 2378, loss = 0.47481136\n",
      "Iteration 2379, loss = 0.47462406\n",
      "Iteration 2380, loss = 0.47514469\n",
      "Iteration 2381, loss = 0.47459487\n",
      "Iteration 2382, loss = 0.47453121\n",
      "Iteration 2383, loss = 0.47463828\n",
      "Iteration 2384, loss = 0.47482741\n",
      "Iteration 2385, loss = 0.47497664\n",
      "Iteration 2386, loss = 0.47472169\n",
      "Iteration 2387, loss = 0.47467183\n",
      "Iteration 2388, loss = 0.47554345\n",
      "Iteration 2389, loss = 0.47488827\n",
      "Iteration 2390, loss = 0.47468561\n",
      "Iteration 2391, loss = 0.47474471\n",
      "Iteration 2392, loss = 0.47474297\n",
      "Iteration 2393, loss = 0.47500150\n",
      "Iteration 2394, loss = 0.47520532\n",
      "Iteration 2395, loss = 0.47531848\n",
      "Iteration 2396, loss = 0.47488332\n",
      "Iteration 2397, loss = 0.47472727\n",
      "Iteration 2398, loss = 0.47460843\n",
      "Iteration 2399, loss = 0.47522931\n",
      "Iteration 2400, loss = 0.47498536\n",
      "Iteration 2401, loss = 0.47496315\n",
      "Iteration 2402, loss = 0.47465212\n",
      "Iteration 2403, loss = 0.47465817\n",
      "Iteration 2404, loss = 0.47446942\n",
      "Iteration 2405, loss = 0.47455189\n",
      "Iteration 2406, loss = 0.47470499\n",
      "Iteration 2407, loss = 0.47485799\n",
      "Iteration 2408, loss = 0.47480997\n",
      "Iteration 2409, loss = 0.47503211\n",
      "Iteration 2410, loss = 0.47489734\n",
      "Iteration 2411, loss = 0.47453204\n",
      "Iteration 2412, loss = 0.47442535\n",
      "Iteration 2413, loss = 0.47551922\n",
      "Iteration 2414, loss = 0.47557484\n",
      "Iteration 2415, loss = 0.47513910\n",
      "Iteration 2416, loss = 0.47530391\n",
      "Iteration 2417, loss = 0.47452916\n",
      "Iteration 2418, loss = 0.47463562\n",
      "Iteration 2419, loss = 0.47454573\n",
      "Iteration 2420, loss = 0.47455297\n",
      "Iteration 2421, loss = 0.47453327\n",
      "Iteration 2422, loss = 0.47454144\n",
      "Iteration 2423, loss = 0.47452940\n",
      "Iteration 2424, loss = 0.47494003\n",
      "Iteration 2425, loss = 0.47469112\n",
      "Iteration 2426, loss = 0.47447716\n",
      "Iteration 2427, loss = 0.47457249\n",
      "Iteration 2428, loss = 0.47478028\n",
      "Iteration 2429, loss = 0.47452215\n",
      "Iteration 2430, loss = 0.47467018\n",
      "Iteration 2431, loss = 0.47468515\n",
      "Iteration 2432, loss = 0.47456599\n",
      "Iteration 2433, loss = 0.47465046\n",
      "Iteration 2434, loss = 0.47450375\n",
      "Iteration 2435, loss = 0.47454253\n",
      "Iteration 2436, loss = 0.47444315\n",
      "Iteration 2437, loss = 0.47458964\n",
      "Iteration 2438, loss = 0.47532547\n",
      "Iteration 2439, loss = 0.47510827\n",
      "Iteration 2440, loss = 0.47448025\n",
      "Iteration 2441, loss = 0.47459191\n",
      "Iteration 2442, loss = 0.47525876\n",
      "Iteration 2443, loss = 0.47504572\n",
      "Iteration 2444, loss = 0.47480732\n",
      "Iteration 2445, loss = 0.47430806\n",
      "Iteration 2446, loss = 0.47473111\n",
      "Iteration 2447, loss = 0.47582332\n",
      "Iteration 2448, loss = 0.47597702\n",
      "Iteration 2449, loss = 0.47498240\n",
      "Iteration 2450, loss = 0.47453811\n",
      "Iteration 2451, loss = 0.47453257\n",
      "Iteration 2452, loss = 0.47541255\n",
      "Iteration 2453, loss = 0.47562718\n",
      "Iteration 2454, loss = 0.47500211\n",
      "Iteration 2455, loss = 0.47444302\n",
      "Iteration 2456, loss = 0.47461493\n",
      "Iteration 2457, loss = 0.47491845\n",
      "Iteration 2458, loss = 0.47535165\n",
      "Iteration 2459, loss = 0.47543646\n",
      "Iteration 2460, loss = 0.47490704\n",
      "Iteration 2461, loss = 0.47449730\n",
      "Iteration 2462, loss = 0.47428130\n",
      "Iteration 2463, loss = 0.47442427\n",
      "Iteration 2464, loss = 0.47542675\n",
      "Iteration 2465, loss = 0.47547585\n",
      "Iteration 2466, loss = 0.47494332\n",
      "Iteration 2467, loss = 0.47468920\n",
      "Iteration 2468, loss = 0.47448200\n",
      "Iteration 2469, loss = 0.47472976\n",
      "Iteration 2470, loss = 0.47481042\n",
      "Iteration 2471, loss = 0.47461702\n",
      "Iteration 2472, loss = 0.47430715\n",
      "Iteration 2473, loss = 0.47454816\n",
      "Iteration 2474, loss = 0.47459405\n",
      "Iteration 2475, loss = 0.47461922\n",
      "Iteration 2476, loss = 0.47449539\n",
      "Iteration 2477, loss = 0.47432734\n",
      "Iteration 2478, loss = 0.47441182\n",
      "Iteration 2479, loss = 0.47461182\n",
      "Iteration 2480, loss = 0.47446238\n",
      "Iteration 2481, loss = 0.47422918\n",
      "Iteration 2482, loss = 0.47434222\n",
      "Iteration 2483, loss = 0.47485698\n",
      "Iteration 2484, loss = 0.47494049\n",
      "Iteration 2485, loss = 0.47467221\n",
      "Iteration 2486, loss = 0.47442856\n",
      "Iteration 2487, loss = 0.47427136\n",
      "Iteration 2488, loss = 0.47490567\n",
      "Iteration 2489, loss = 0.47578213\n",
      "Iteration 2490, loss = 0.47621077\n",
      "Iteration 2491, loss = 0.47545966\n",
      "Iteration 2492, loss = 0.47410058\n",
      "Iteration 2493, loss = 0.47517071\n",
      "Iteration 2494, loss = 0.47588086\n",
      "Iteration 2495, loss = 0.47576448\n",
      "Iteration 2496, loss = 0.47484951\n",
      "Iteration 2497, loss = 0.47433268\n",
      "Iteration 2498, loss = 0.47567382\n",
      "Iteration 2499, loss = 0.47475071\n",
      "Iteration 2500, loss = 0.47458306\n",
      "Iteration 2501, loss = 0.47458722\n",
      "Iteration 2502, loss = 0.47441235\n",
      "Iteration 2503, loss = 0.47450906\n",
      "Iteration 2504, loss = 0.47452008\n",
      "Iteration 2505, loss = 0.47441872\n",
      "Iteration 2506, loss = 0.47440215\n",
      "Iteration 2507, loss = 0.47442072\n",
      "Iteration 2508, loss = 0.47451031\n",
      "Iteration 2509, loss = 0.47445221\n",
      "Iteration 2510, loss = 0.47416216\n",
      "Iteration 2511, loss = 0.47422749\n",
      "Iteration 2512, loss = 0.47468024\n",
      "Iteration 2513, loss = 0.47504097\n",
      "Iteration 2514, loss = 0.47499249\n",
      "Iteration 2515, loss = 0.47510929\n",
      "Iteration 2516, loss = 0.47471094\n",
      "Iteration 2517, loss = 0.47434696\n",
      "Iteration 2518, loss = 0.47437916\n",
      "Iteration 2519, loss = 0.47440089\n",
      "Iteration 2520, loss = 0.47435376\n",
      "Iteration 2521, loss = 0.47435478\n",
      "Iteration 2522, loss = 0.47455518\n",
      "Iteration 2523, loss = 0.47446334\n",
      "Iteration 2524, loss = 0.47439003\n",
      "Iteration 2525, loss = 0.47454290\n",
      "Iteration 2526, loss = 0.47465017\n",
      "Iteration 2527, loss = 0.47419874\n",
      "Iteration 2528, loss = 0.47431211\n",
      "Iteration 2529, loss = 0.47461405\n",
      "Iteration 2530, loss = 0.47467465\n",
      "Iteration 2531, loss = 0.47466121\n",
      "Iteration 2532, loss = 0.47442645\n",
      "Iteration 2533, loss = 0.47418211\n",
      "Iteration 2534, loss = 0.47448366\n",
      "Iteration 2535, loss = 0.47450416\n",
      "Iteration 2536, loss = 0.47422041\n",
      "Iteration 2537, loss = 0.47439624\n",
      "Iteration 2538, loss = 0.47433464\n",
      "Iteration 2539, loss = 0.47450492\n",
      "Iteration 2540, loss = 0.47452631\n",
      "Iteration 2541, loss = 0.47464262\n",
      "Iteration 2542, loss = 0.47425592\n",
      "Iteration 2543, loss = 0.47426417\n",
      "Iteration 2544, loss = 0.47435267\n",
      "Iteration 2545, loss = 0.47493140\n",
      "Iteration 2546, loss = 0.47430015\n",
      "Iteration 2547, loss = 0.47440176\n",
      "Iteration 2548, loss = 0.47435648\n",
      "Iteration 2549, loss = 0.47415977\n",
      "Iteration 2550, loss = 0.47417450\n",
      "Iteration 2551, loss = 0.47422380\n",
      "Iteration 2552, loss = 0.47416701\n",
      "Iteration 2553, loss = 0.47413106\n",
      "Iteration 2554, loss = 0.47444767\n",
      "Iteration 2555, loss = 0.47436912\n",
      "Iteration 2556, loss = 0.47418471\n",
      "Iteration 2557, loss = 0.47436595\n",
      "Iteration 2558, loss = 0.47495565\n",
      "Iteration 2559, loss = 0.47429671\n",
      "Iteration 2560, loss = 0.47427468\n",
      "Iteration 2561, loss = 0.47445253\n",
      "Iteration 2562, loss = 0.47426948\n",
      "Iteration 2563, loss = 0.47420920\n",
      "Iteration 2564, loss = 0.47408252\n",
      "Iteration 2565, loss = 0.47448809\n",
      "Iteration 2566, loss = 0.47425394\n",
      "Iteration 2567, loss = 0.47427907\n",
      "Iteration 2568, loss = 0.47410479\n",
      "Iteration 2569, loss = 0.47405359\n",
      "Iteration 2570, loss = 0.47413676\n",
      "Iteration 2571, loss = 0.47441450\n",
      "Iteration 2572, loss = 0.47418547\n",
      "Iteration 2573, loss = 0.47411779\n",
      "Iteration 2574, loss = 0.47425187\n",
      "Iteration 2575, loss = 0.47420168\n",
      "Iteration 2576, loss = 0.47441246\n",
      "Iteration 2577, loss = 0.47445471\n",
      "Iteration 2578, loss = 0.47441251\n",
      "Iteration 2579, loss = 0.47406614\n",
      "Iteration 2580, loss = 0.47414663\n",
      "Iteration 2581, loss = 0.47403953\n",
      "Iteration 2582, loss = 0.47413109\n",
      "Iteration 2583, loss = 0.47413974\n",
      "Iteration 2584, loss = 0.47426570\n",
      "Iteration 2585, loss = 0.47435171\n",
      "Iteration 2586, loss = 0.47420017\n",
      "Iteration 2587, loss = 0.47441249\n",
      "Iteration 2588, loss = 0.47420916\n",
      "Iteration 2589, loss = 0.47409004\n",
      "Iteration 2590, loss = 0.47416841\n",
      "Iteration 2591, loss = 0.47402689\n",
      "Iteration 2592, loss = 0.47411501\n",
      "Iteration 2593, loss = 0.47412403\n",
      "Iteration 2594, loss = 0.47418782\n",
      "Iteration 2595, loss = 0.47414989\n",
      "Iteration 2596, loss = 0.47453983\n",
      "Iteration 2597, loss = 0.47383811\n",
      "Iteration 2598, loss = 0.47394167\n",
      "Iteration 2599, loss = 0.47450392\n",
      "Iteration 2600, loss = 0.47537327\n",
      "Iteration 2601, loss = 0.47453335\n",
      "Iteration 2602, loss = 0.47450900\n",
      "Iteration 2603, loss = 0.47442360\n",
      "Iteration 2604, loss = 0.47450833\n",
      "Iteration 2605, loss = 0.47439168\n",
      "Iteration 2606, loss = 0.47426430\n",
      "Iteration 2607, loss = 0.47399370\n",
      "Iteration 2608, loss = 0.47441525\n",
      "Iteration 2609, loss = 0.47438375\n",
      "Iteration 2610, loss = 0.47467725\n",
      "Iteration 2611, loss = 0.47432404\n",
      "Iteration 2612, loss = 0.47398484\n",
      "Iteration 2613, loss = 0.47403415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2614, loss = 0.47417033\n",
      "Iteration 2615, loss = 0.47428563\n",
      "Iteration 2616, loss = 0.47424825\n",
      "Iteration 2617, loss = 0.47451644\n",
      "Iteration 2618, loss = 0.47395494\n",
      "Iteration 2619, loss = 0.47436096\n",
      "Iteration 2620, loss = 0.47405211\n",
      "Iteration 2621, loss = 0.47395479\n",
      "Iteration 2622, loss = 0.47390999\n",
      "Iteration 2623, loss = 0.47417288\n",
      "Iteration 2624, loss = 0.47415945\n",
      "Iteration 2625, loss = 0.47392386\n",
      "Iteration 2626, loss = 0.47397789\n",
      "Iteration 2627, loss = 0.47455339\n",
      "Iteration 2628, loss = 0.47445398\n",
      "Iteration 2629, loss = 0.47393368\n",
      "Iteration 2630, loss = 0.47388861\n",
      "Iteration 2631, loss = 0.47398014\n",
      "Iteration 2632, loss = 0.47454100\n",
      "Iteration 2633, loss = 0.47424593\n",
      "Iteration 2634, loss = 0.47387888\n",
      "Iteration 2635, loss = 0.47384783\n",
      "Iteration 2636, loss = 0.47423155\n",
      "Iteration 2637, loss = 0.47445994\n",
      "Iteration 2638, loss = 0.47440296\n",
      "Iteration 2639, loss = 0.47403678\n",
      "Iteration 2640, loss = 0.47400776\n",
      "Iteration 2641, loss = 0.47390933\n",
      "Iteration 2642, loss = 0.47400213\n",
      "Iteration 2643, loss = 0.47395456\n",
      "Iteration 2644, loss = 0.47384616\n",
      "Iteration 2645, loss = 0.47380240\n",
      "Iteration 2646, loss = 0.47398737\n",
      "Iteration 2647, loss = 0.47411694\n",
      "Iteration 2648, loss = 0.47420396\n",
      "Iteration 2649, loss = 0.47403305\n",
      "Iteration 2650, loss = 0.47393978\n",
      "Iteration 2651, loss = 0.47383012\n",
      "Iteration 2652, loss = 0.47384618\n",
      "Iteration 2653, loss = 0.47503258\n",
      "Iteration 2654, loss = 0.47417721\n",
      "Iteration 2655, loss = 0.47385087\n",
      "Iteration 2656, loss = 0.47387837\n",
      "Iteration 2657, loss = 0.47412851\n",
      "Iteration 2658, loss = 0.47422127\n",
      "Iteration 2659, loss = 0.47405621\n",
      "Iteration 2660, loss = 0.47398256\n",
      "Iteration 2661, loss = 0.47397789\n",
      "Iteration 2662, loss = 0.47390325\n",
      "Iteration 2663, loss = 0.47395892\n",
      "Iteration 2664, loss = 0.47412490\n",
      "Iteration 2665, loss = 0.47389254\n",
      "Iteration 2666, loss = 0.47388584\n",
      "Iteration 2667, loss = 0.47385278\n",
      "Iteration 2668, loss = 0.47387306\n",
      "Iteration 2669, loss = 0.47408789\n",
      "Iteration 2670, loss = 0.47379469\n",
      "Iteration 2671, loss = 0.47425647\n",
      "Iteration 2672, loss = 0.47459730\n",
      "Iteration 2673, loss = 0.47439123\n",
      "Iteration 2674, loss = 0.47402237\n",
      "Iteration 2675, loss = 0.47381232\n",
      "Iteration 2676, loss = 0.47474637\n",
      "Iteration 2677, loss = 0.47429504\n",
      "Iteration 2678, loss = 0.47386953\n",
      "Iteration 2679, loss = 0.47368254\n",
      "Iteration 2680, loss = 0.47408548\n",
      "Iteration 2681, loss = 0.47418114\n",
      "Iteration 2682, loss = 0.47412199\n",
      "Iteration 2683, loss = 0.47385980\n",
      "Iteration 2684, loss = 0.47361792\n",
      "Iteration 2685, loss = 0.47421716\n",
      "Iteration 2686, loss = 0.47438549\n",
      "Iteration 2687, loss = 0.47420850\n",
      "Iteration 2688, loss = 0.47404179\n",
      "Iteration 2689, loss = 0.47418950\n",
      "Iteration 2690, loss = 0.47399385\n",
      "Iteration 2691, loss = 0.47387059\n",
      "Iteration 2692, loss = 0.47375965\n",
      "Iteration 2693, loss = 0.47380713\n",
      "Iteration 2694, loss = 0.47389141\n",
      "Iteration 2695, loss = 0.47380807\n",
      "Iteration 2696, loss = 0.47381648\n",
      "Iteration 2697, loss = 0.47378795\n",
      "Iteration 2698, loss = 0.47387367\n",
      "Iteration 2699, loss = 0.47392075\n",
      "Iteration 2700, loss = 0.47380638\n",
      "Iteration 2701, loss = 0.47373133\n",
      "Iteration 2702, loss = 0.47379938\n",
      "Iteration 2703, loss = 0.47399087\n",
      "Iteration 2704, loss = 0.47413242\n",
      "Iteration 2705, loss = 0.47410790\n",
      "Iteration 2706, loss = 0.47416618\n",
      "Iteration 2707, loss = 0.47376627\n",
      "Iteration 2708, loss = 0.47373329\n",
      "Iteration 2709, loss = 0.47378139\n",
      "Iteration 2710, loss = 0.47441322\n",
      "Iteration 2711, loss = 0.47422112\n",
      "Iteration 2712, loss = 0.47387703\n",
      "Iteration 2713, loss = 0.47444149\n",
      "Iteration 2714, loss = 0.47463417\n",
      "Iteration 2715, loss = 0.47402722\n",
      "Iteration 2716, loss = 0.47416104\n",
      "Iteration 2717, loss = 0.47388984\n",
      "Iteration 2718, loss = 0.47384831\n",
      "Iteration 2719, loss = 0.47399766\n",
      "Iteration 2720, loss = 0.47404657\n",
      "Iteration 2721, loss = 0.47381275\n",
      "Iteration 2722, loss = 0.47351281\n",
      "Iteration 2723, loss = 0.47431291\n",
      "Iteration 2724, loss = 0.47535652\n",
      "Iteration 2725, loss = 0.47514561\n",
      "Iteration 2726, loss = 0.47484133\n",
      "Iteration 2727, loss = 0.47441077\n",
      "Iteration 2728, loss = 0.47396090\n",
      "Iteration 2729, loss = 0.47371558\n",
      "Iteration 2730, loss = 0.47377380\n",
      "Iteration 2731, loss = 0.47374300\n",
      "Iteration 2732, loss = 0.47418478\n",
      "Iteration 2733, loss = 0.47372049\n",
      "Iteration 2734, loss = 0.47400984\n",
      "Iteration 2735, loss = 0.47393611\n",
      "Iteration 2736, loss = 0.47445242\n",
      "Iteration 2737, loss = 0.47383906\n",
      "Iteration 2738, loss = 0.47389082\n",
      "Iteration 2739, loss = 0.47363482\n",
      "Iteration 2740, loss = 0.47368965\n",
      "Iteration 2741, loss = 0.47461287\n",
      "Iteration 2742, loss = 0.47434403\n",
      "Iteration 2743, loss = 0.47385074\n",
      "Iteration 2744, loss = 0.47414872\n",
      "Iteration 2745, loss = 0.47376283\n",
      "Iteration 2746, loss = 0.47380130\n",
      "Iteration 2747, loss = 0.47387807\n",
      "Iteration 2748, loss = 0.47370625\n",
      "Iteration 2749, loss = 0.47381788\n",
      "Iteration 2750, loss = 0.47396000\n",
      "Iteration 2751, loss = 0.47405063\n",
      "Iteration 2752, loss = 0.47386332\n",
      "Iteration 2753, loss = 0.47383532\n",
      "Iteration 2754, loss = 0.47361745\n",
      "Iteration 2755, loss = 0.47372790\n",
      "Iteration 2756, loss = 0.47372460\n",
      "Iteration 2757, loss = 0.47370455\n",
      "Iteration 2758, loss = 0.47380810\n",
      "Iteration 2759, loss = 0.47413235\n",
      "Iteration 2760, loss = 0.47409240\n",
      "Iteration 2761, loss = 0.47411512\n",
      "Iteration 2762, loss = 0.47401384\n",
      "Iteration 2763, loss = 0.47385273\n",
      "Iteration 2764, loss = 0.47349824\n",
      "Iteration 2765, loss = 0.47393219\n",
      "Iteration 2766, loss = 0.47385681\n",
      "Iteration 2767, loss = 0.47392817\n",
      "Iteration 2768, loss = 0.47396125\n",
      "Iteration 2769, loss = 0.47370258\n",
      "Iteration 2770, loss = 0.47358885\n",
      "Iteration 2771, loss = 0.47359334\n",
      "Iteration 2772, loss = 0.47382608\n",
      "Iteration 2773, loss = 0.47411979\n",
      "Iteration 2774, loss = 0.47431657\n",
      "Iteration 2775, loss = 0.47360154\n",
      "Iteration 2776, loss = 0.47382676\n",
      "Iteration 2777, loss = 0.47366709\n",
      "Iteration 2778, loss = 0.47373396\n",
      "Iteration 2779, loss = 0.47420161\n",
      "Iteration 2780, loss = 0.47360741\n",
      "Iteration 2781, loss = 0.47374083\n",
      "Iteration 2782, loss = 0.47363155\n",
      "Iteration 2783, loss = 0.47380972\n",
      "Iteration 2784, loss = 0.47389876\n",
      "Iteration 2785, loss = 0.47366230\n",
      "Iteration 2786, loss = 0.47368478\n",
      "Iteration 2787, loss = 0.47361090\n",
      "Iteration 2788, loss = 0.47363063\n",
      "Iteration 2789, loss = 0.47384039\n",
      "Iteration 2790, loss = 0.47361959\n",
      "Iteration 2791, loss = 0.47367721\n",
      "Iteration 2792, loss = 0.47374451\n",
      "Iteration 2793, loss = 0.47376199\n",
      "Iteration 2794, loss = 0.47371481\n",
      "Iteration 2795, loss = 0.47361263\n",
      "Iteration 2796, loss = 0.47359298\n",
      "Iteration 2797, loss = 0.47388529\n",
      "Iteration 2798, loss = 0.47377535\n",
      "Iteration 2799, loss = 0.47366656\n",
      "Iteration 2800, loss = 0.47360100\n",
      "Iteration 2801, loss = 0.47381792\n",
      "Iteration 2802, loss = 0.47397818\n",
      "Iteration 2803, loss = 0.47384933\n",
      "Iteration 2804, loss = 0.47357008\n",
      "Iteration 2805, loss = 0.47423844\n",
      "Iteration 2806, loss = 0.47363578\n",
      "Iteration 2807, loss = 0.47374509\n",
      "Iteration 2808, loss = 0.47351748\n",
      "Iteration 2809, loss = 0.47352760\n",
      "Iteration 2810, loss = 0.47350490\n",
      "Iteration 2811, loss = 0.47377969\n",
      "Iteration 2812, loss = 0.47369622\n",
      "Iteration 2813, loss = 0.47361627\n",
      "Iteration 2814, loss = 0.47389099\n",
      "Iteration 2815, loss = 0.47350472\n",
      "Iteration 2816, loss = 0.47388682\n",
      "Iteration 2817, loss = 0.47442278\n",
      "Iteration 2818, loss = 0.47416178\n",
      "Iteration 2819, loss = 0.47362820\n",
      "Iteration 2820, loss = 0.47384292\n",
      "Iteration 2821, loss = 0.47373100\n",
      "Iteration 2822, loss = 0.47375526\n",
      "Iteration 2823, loss = 0.47359431\n",
      "Iteration 2824, loss = 0.47363881\n",
      "Iteration 2825, loss = 0.47375436\n",
      "Iteration 2826, loss = 0.47376077\n",
      "Iteration 2827, loss = 0.47390418\n",
      "Iteration 2828, loss = 0.47388389\n",
      "Iteration 2829, loss = 0.47429621\n",
      "Iteration 2830, loss = 0.47360055\n",
      "Iteration 2831, loss = 0.47393669\n",
      "Iteration 2832, loss = 0.47416572\n",
      "Iteration 2833, loss = 0.47359884\n",
      "Iteration 2834, loss = 0.47360018\n",
      "Iteration 2835, loss = 0.47368067\n",
      "Iteration 2836, loss = 0.47351703\n",
      "Iteration 2837, loss = 0.47335718\n",
      "Iteration 2838, loss = 0.47362391\n",
      "Iteration 2839, loss = 0.47368481\n",
      "Iteration 2840, loss = 0.47371194\n",
      "Iteration 2841, loss = 0.47348943\n",
      "Iteration 2842, loss = 0.47346958\n",
      "Iteration 2843, loss = 0.47358330\n",
      "Iteration 2844, loss = 0.47360583\n",
      "Iteration 2845, loss = 0.47405456\n",
      "Iteration 2846, loss = 0.47348952\n",
      "Iteration 2847, loss = 0.47349554\n",
      "Iteration 2848, loss = 0.47354685\n",
      "Iteration 2849, loss = 0.47349679\n",
      "Iteration 2850, loss = 0.47342903\n",
      "Iteration 2851, loss = 0.47358514\n",
      "Iteration 2852, loss = 0.47338251\n",
      "Iteration 2853, loss = 0.47338141\n",
      "Iteration 2854, loss = 0.47346631\n",
      "Iteration 2855, loss = 0.47349907\n",
      "Iteration 2856, loss = 0.47343055\n",
      "Iteration 2857, loss = 0.47330640\n",
      "Iteration 2858, loss = 0.47525673\n",
      "Iteration 2859, loss = 0.47428391\n",
      "Iteration 2860, loss = 0.47381762\n",
      "Iteration 2861, loss = 0.47341642\n",
      "Iteration 2862, loss = 0.47347749\n",
      "Iteration 2863, loss = 0.47370520\n",
      "Iteration 2864, loss = 0.47355002\n",
      "Iteration 2865, loss = 0.47348080\n",
      "Iteration 2866, loss = 0.47356344\n",
      "Iteration 2867, loss = 0.47363647\n",
      "Iteration 2868, loss = 0.47343868\n",
      "Iteration 2869, loss = 0.47345004\n",
      "Iteration 2870, loss = 0.47361018\n",
      "Iteration 2871, loss = 0.47349065\n",
      "Iteration 2872, loss = 0.47346671\n",
      "Iteration 2873, loss = 0.47377515\n",
      "Iteration 2874, loss = 0.47394920\n",
      "Iteration 2875, loss = 0.47429230\n",
      "Iteration 2876, loss = 0.47399193\n",
      "Iteration 2877, loss = 0.47361949\n",
      "Iteration 2878, loss = 0.47347145\n",
      "Iteration 2879, loss = 0.47361586\n",
      "Iteration 2880, loss = 0.47376170\n",
      "Iteration 2881, loss = 0.47369514\n",
      "Iteration 2882, loss = 0.47346692\n",
      "Iteration 2883, loss = 0.47334309\n",
      "Iteration 2884, loss = 0.47381393\n",
      "Iteration 2885, loss = 0.47364543\n",
      "Iteration 2886, loss = 0.47320086\n",
      "Iteration 2887, loss = 0.47345824\n",
      "Iteration 2888, loss = 0.47407642\n",
      "Iteration 2889, loss = 0.47428475\n",
      "Iteration 2890, loss = 0.47390467\n",
      "Iteration 2891, loss = 0.47347191\n",
      "Iteration 2892, loss = 0.47341617\n",
      "Iteration 2893, loss = 0.47397092\n",
      "Iteration 2894, loss = 0.47394740\n",
      "Iteration 2895, loss = 0.47351472\n",
      "Iteration 2896, loss = 0.47324123\n",
      "Iteration 2897, loss = 0.47349350\n",
      "Iteration 2898, loss = 0.47399253\n",
      "Iteration 2899, loss = 0.47406534\n",
      "Iteration 2900, loss = 0.47390376\n",
      "Iteration 2901, loss = 0.47330566\n",
      "Iteration 2902, loss = 0.47427650\n",
      "Iteration 2903, loss = 0.47376989\n",
      "Iteration 2904, loss = 0.47350056\n",
      "Iteration 2905, loss = 0.47336651\n",
      "Iteration 2906, loss = 0.47357373\n",
      "Iteration 2907, loss = 0.47337864\n",
      "Iteration 2908, loss = 0.47341993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2909, loss = 0.47334817\n",
      "Iteration 2910, loss = 0.47357348\n",
      "Iteration 2911, loss = 0.47318920\n",
      "Iteration 2912, loss = 0.47347415\n",
      "Iteration 2913, loss = 0.47388353\n",
      "Iteration 2914, loss = 0.47415568\n",
      "Iteration 2915, loss = 0.47361270\n",
      "Iteration 2916, loss = 0.47383982\n",
      "Iteration 2917, loss = 0.47367885\n",
      "Iteration 2918, loss = 0.47391189\n",
      "Iteration 2919, loss = 0.47313200\n",
      "Iteration 2920, loss = 0.47330218\n",
      "Iteration 2921, loss = 0.47405727\n",
      "Iteration 2922, loss = 0.47434421\n",
      "Iteration 2923, loss = 0.47386884\n",
      "Iteration 2924, loss = 0.47353899\n",
      "Iteration 2925, loss = 0.47339133\n",
      "Iteration 2926, loss = 0.47328767\n",
      "Iteration 2927, loss = 0.47365520\n",
      "Iteration 2928, loss = 0.47428948\n",
      "Iteration 2929, loss = 0.47384372\n",
      "Iteration 2930, loss = 0.47432406\n",
      "Iteration 2931, loss = 0.47337372\n",
      "Iteration 2932, loss = 0.47362748\n",
      "Iteration 2933, loss = 0.47337754\n",
      "Iteration 2934, loss = 0.47314271\n",
      "Iteration 2935, loss = 0.47339994\n",
      "Iteration 2936, loss = 0.47377700\n",
      "Iteration 2937, loss = 0.47403793\n",
      "Iteration 2938, loss = 0.47377960\n",
      "Iteration 2939, loss = 0.47346062\n",
      "Iteration 2940, loss = 0.47333001\n",
      "Iteration 2941, loss = 0.47351863\n",
      "Iteration 2942, loss = 0.47380729\n",
      "Iteration 2943, loss = 0.47417575\n",
      "Iteration 2944, loss = 0.47372261\n",
      "Iteration 2945, loss = 0.47338949\n",
      "Iteration 2946, loss = 0.47359589\n",
      "Iteration 2947, loss = 0.47383966\n",
      "Iteration 2948, loss = 0.47333737\n",
      "Iteration 2949, loss = 0.47354702\n",
      "Iteration 2950, loss = 0.47349325\n",
      "Iteration 2951, loss = 0.47415087\n",
      "Iteration 2952, loss = 0.47378789\n",
      "Iteration 2953, loss = 0.47324480\n",
      "Iteration 2954, loss = 0.47341685\n",
      "Iteration 2955, loss = 0.47376916\n",
      "Iteration 2956, loss = 0.47421980\n",
      "Iteration 2957, loss = 0.47354691\n",
      "Iteration 2958, loss = 0.47340881\n",
      "Iteration 2959, loss = 0.47340727\n",
      "Iteration 2960, loss = 0.47408247\n",
      "Iteration 2961, loss = 0.47422986\n",
      "Iteration 2962, loss = 0.47403360\n",
      "Iteration 2963, loss = 0.47332733\n",
      "Iteration 2964, loss = 0.47347242\n",
      "Iteration 2965, loss = 0.47335420\n",
      "Iteration 2966, loss = 0.47330512\n",
      "Iteration 2967, loss = 0.47325663\n",
      "Iteration 2968, loss = 0.47385207\n",
      "Iteration 2969, loss = 0.47320796\n",
      "Iteration 2970, loss = 0.47314642\n",
      "Iteration 2971, loss = 0.47429185\n",
      "Iteration 2972, loss = 0.47387656\n",
      "Iteration 2973, loss = 0.47353824\n",
      "Iteration 2974, loss = 0.47319372\n",
      "Iteration 2975, loss = 0.47401624\n",
      "Iteration 2976, loss = 0.47366657\n",
      "Iteration 2977, loss = 0.47335254\n",
      "Iteration 2978, loss = 0.47333920\n",
      "Iteration 2979, loss = 0.47336900\n",
      "Iteration 2980, loss = 0.47339966\n",
      "Iteration 2981, loss = 0.47334593\n",
      "Iteration 2982, loss = 0.47333091\n",
      "Iteration 2983, loss = 0.47372892\n",
      "Iteration 2984, loss = 0.47345395\n",
      "Iteration 2985, loss = 0.47363364\n",
      "Iteration 2986, loss = 0.47365009\n",
      "Iteration 2987, loss = 0.47363525\n",
      "Iteration 2988, loss = 0.47339626\n",
      "Iteration 2989, loss = 0.47393657\n",
      "Iteration 2990, loss = 0.47340769\n",
      "Iteration 2991, loss = 0.47335988\n",
      "Iteration 2992, loss = 0.47330250\n",
      "Iteration 2993, loss = 0.47329875\n",
      "Iteration 2994, loss = 0.47319085\n",
      "Iteration 2995, loss = 0.47318563\n",
      "Iteration 2996, loss = 0.47324989\n",
      "Iteration 2997, loss = 0.47360164\n",
      "Iteration 2998, loss = 0.47364819\n",
      "Iteration 2999, loss = 0.47391672\n",
      "Iteration 3000, loss = 0.47319487\n",
      "Iteration 3001, loss = 0.47305004\n",
      "Iteration 3002, loss = 0.47417103\n",
      "Iteration 3003, loss = 0.47478624\n",
      "Iteration 3004, loss = 0.47447600\n",
      "Iteration 3005, loss = 0.47341904\n",
      "Iteration 3006, loss = 0.47354069\n",
      "Iteration 3007, loss = 0.47353623\n",
      "Iteration 3008, loss = 0.47357427\n",
      "Iteration 3009, loss = 0.47336903\n",
      "Iteration 3010, loss = 0.47333445\n",
      "Iteration 3011, loss = 0.47325509\n",
      "Iteration 3012, loss = 0.47317444\n",
      "Iteration 3013, loss = 0.47319427\n",
      "Iteration 3014, loss = 0.47323592\n",
      "Iteration 3015, loss = 0.47317578\n",
      "Iteration 3016, loss = 0.47333925\n",
      "Iteration 3017, loss = 0.47323000\n",
      "Iteration 3018, loss = 0.47324452\n",
      "Iteration 3019, loss = 0.47317841\n",
      "Iteration 3020, loss = 0.47351967\n",
      "Iteration 3021, loss = 0.47318171\n",
      "Iteration 3022, loss = 0.47330947\n",
      "Iteration 3023, loss = 0.47321880\n",
      "Iteration 3024, loss = 0.47361195\n",
      "Iteration 3025, loss = 0.47326079\n",
      "Iteration 3026, loss = 0.47311481\n",
      "Iteration 3027, loss = 0.47313438\n",
      "Iteration 3028, loss = 0.47328996\n",
      "Iteration 3029, loss = 0.47347103\n",
      "Iteration 3030, loss = 0.47353197\n",
      "Iteration 3031, loss = 0.47324039\n",
      "Iteration 3032, loss = 0.47324924\n",
      "Iteration 3033, loss = 0.47329802\n",
      "Iteration 3034, loss = 0.47338737\n",
      "Iteration 3035, loss = 0.47344809\n",
      "Iteration 3036, loss = 0.47316925\n",
      "Iteration 3037, loss = 0.47318886\n",
      "Iteration 3038, loss = 0.47315181\n",
      "Iteration 3039, loss = 0.47305089\n",
      "Iteration 3040, loss = 0.47364742\n",
      "Iteration 3041, loss = 0.47324951\n",
      "Iteration 3042, loss = 0.47313905\n",
      "Iteration 3043, loss = 0.47318016\n",
      "Iteration 3044, loss = 0.47332653\n",
      "Iteration 3045, loss = 0.47326993\n",
      "Iteration 3046, loss = 0.47341153\n",
      "Iteration 3047, loss = 0.47329629\n",
      "Iteration 3048, loss = 0.47305589\n",
      "Iteration 3049, loss = 0.47307672\n",
      "Iteration 3050, loss = 0.47328640\n",
      "Iteration 3051, loss = 0.47347376\n",
      "Iteration 3052, loss = 0.47350483\n",
      "Iteration 3053, loss = 0.47337165\n",
      "Iteration 3054, loss = 0.47337936\n",
      "Iteration 3055, loss = 0.47299612\n",
      "Iteration 3056, loss = 0.47307046\n",
      "Iteration 3057, loss = 0.47381051\n",
      "Iteration 3058, loss = 0.47431288\n",
      "Iteration 3059, loss = 0.47441201\n",
      "Iteration 3060, loss = 0.47337305\n",
      "Iteration 3061, loss = 0.47341794\n",
      "Iteration 3062, loss = 0.47327823\n",
      "Iteration 3063, loss = 0.47367474\n",
      "Iteration 3064, loss = 0.47304375\n",
      "Iteration 3065, loss = 0.47311748\n",
      "Iteration 3066, loss = 0.47398786\n",
      "Iteration 3067, loss = 0.47359496\n",
      "Iteration 3068, loss = 0.47304713\n",
      "Iteration 3069, loss = 0.47318915\n",
      "Iteration 3070, loss = 0.47340877\n",
      "Iteration 3071, loss = 0.47393938\n",
      "Iteration 3072, loss = 0.47417822\n",
      "Iteration 3073, loss = 0.47358211\n",
      "Iteration 3074, loss = 0.47288758\n",
      "Iteration 3075, loss = 0.47353806\n",
      "Iteration 3076, loss = 0.47406459\n",
      "Iteration 3077, loss = 0.47385043\n",
      "Iteration 3078, loss = 0.47315109\n",
      "Iteration 3079, loss = 0.47413818\n",
      "Iteration 3080, loss = 0.47327204\n",
      "Iteration 3081, loss = 0.47316169\n",
      "Iteration 3082, loss = 0.47317179\n",
      "Iteration 3083, loss = 0.47314572\n",
      "Iteration 3084, loss = 0.47311514\n",
      "Iteration 3085, loss = 0.47311938\n",
      "Iteration 3086, loss = 0.47326111\n",
      "Iteration 3087, loss = 0.47301441\n",
      "Iteration 3088, loss = 0.47320495\n",
      "Iteration 3089, loss = 0.47353457\n",
      "Iteration 3090, loss = 0.47326296\n",
      "Iteration 3091, loss = 0.47300305\n",
      "Iteration 3092, loss = 0.47292963\n",
      "Iteration 3093, loss = 0.47386863\n",
      "Iteration 3094, loss = 0.47367124\n",
      "Iteration 3095, loss = 0.47317068\n",
      "Iteration 3096, loss = 0.47305166\n",
      "Iteration 3097, loss = 0.47308800\n",
      "Iteration 3098, loss = 0.47316248\n",
      "Iteration 3099, loss = 0.47324784\n",
      "Iteration 3100, loss = 0.47315601\n",
      "Iteration 3101, loss = 0.47307296\n",
      "Iteration 3102, loss = 0.47313147\n",
      "Iteration 3103, loss = 0.47300662\n",
      "Iteration 3104, loss = 0.47297003\n",
      "Iteration 3105, loss = 0.47298190\n",
      "Iteration 3106, loss = 0.47321883\n",
      "Iteration 3107, loss = 0.47345839\n",
      "Iteration 3108, loss = 0.47367447\n",
      "Iteration 3109, loss = 0.47385603\n",
      "Iteration 3110, loss = 0.47297958\n",
      "Iteration 3111, loss = 0.47314313\n",
      "Iteration 3112, loss = 0.47344030\n",
      "Iteration 3113, loss = 0.47410220\n",
      "Iteration 3114, loss = 0.47447972\n",
      "Iteration 3115, loss = 0.47426535\n",
      "Iteration 3116, loss = 0.47381891\n",
      "Iteration 3117, loss = 0.47324840\n",
      "Iteration 3118, loss = 0.47302433\n",
      "Iteration 3119, loss = 0.47326750\n",
      "Iteration 3120, loss = 0.47364503\n",
      "Iteration 3121, loss = 0.47299541\n",
      "Iteration 3122, loss = 0.47323352\n",
      "Iteration 3123, loss = 0.47328434\n",
      "Iteration 3124, loss = 0.47363994\n",
      "Iteration 3125, loss = 0.47391331\n",
      "Iteration 3126, loss = 0.47308235\n",
      "Iteration 3127, loss = 0.47304980\n",
      "Iteration 3128, loss = 0.47353346\n",
      "Iteration 3129, loss = 0.47314925\n",
      "Iteration 3130, loss = 0.47303220\n",
      "Iteration 3131, loss = 0.47306129\n",
      "Iteration 3132, loss = 0.47303088\n",
      "Iteration 3133, loss = 0.47318326\n",
      "Iteration 3134, loss = 0.47299377\n",
      "Iteration 3135, loss = 0.47303139\n",
      "Iteration 3136, loss = 0.47317526\n",
      "Iteration 3137, loss = 0.47310862\n",
      "Iteration 3138, loss = 0.47349974\n",
      "Iteration 3139, loss = 0.47310336\n",
      "Iteration 3140, loss = 0.47313353\n",
      "Iteration 3141, loss = 0.47344778\n",
      "Iteration 3142, loss = 0.47307601\n",
      "Iteration 3143, loss = 0.47314974\n",
      "Iteration 3144, loss = 0.47323392\n",
      "Iteration 3145, loss = 0.47330793\n",
      "Iteration 3146, loss = 0.47298008\n",
      "Iteration 3147, loss = 0.47298740\n",
      "Iteration 3148, loss = 0.47307029\n",
      "Iteration 3149, loss = 0.47300650\n",
      "Iteration 3150, loss = 0.47323580\n",
      "Iteration 3151, loss = 0.47305997\n",
      "Iteration 3152, loss = 0.47305223\n",
      "Iteration 3153, loss = 0.47298613\n",
      "Iteration 3154, loss = 0.47298090\n",
      "Iteration 3155, loss = 0.47327616\n",
      "Iteration 3156, loss = 0.47303893\n",
      "Iteration 3157, loss = 0.47351715\n",
      "Iteration 3158, loss = 0.47342333\n",
      "Iteration 3159, loss = 0.47314354\n",
      "Iteration 3160, loss = 0.47293581\n",
      "Iteration 3161, loss = 0.47281180\n",
      "Iteration 3162, loss = 0.47343641\n",
      "Iteration 3163, loss = 0.47332786\n",
      "Iteration 3164, loss = 0.47335928\n",
      "Iteration 3165, loss = 0.47317889\n",
      "Iteration 3166, loss = 0.47325175\n",
      "Iteration 3167, loss = 0.47310103\n",
      "Iteration 3168, loss = 0.47351248\n",
      "Iteration 3169, loss = 0.47308780\n",
      "Iteration 3170, loss = 0.47276991\n",
      "Iteration 3171, loss = 0.47335959\n",
      "Iteration 3172, loss = 0.47362691\n",
      "Iteration 3173, loss = 0.47386534\n",
      "Iteration 3174, loss = 0.47370514\n",
      "Iteration 3175, loss = 0.47317085\n",
      "Iteration 3176, loss = 0.47370052\n",
      "Iteration 3177, loss = 0.47305696\n",
      "Iteration 3178, loss = 0.47308239\n",
      "Iteration 3179, loss = 0.47318359\n",
      "Iteration 3180, loss = 0.47306457\n",
      "Iteration 3181, loss = 0.47309003\n",
      "Iteration 3182, loss = 0.47308286\n",
      "Iteration 3183, loss = 0.47315369\n",
      "Iteration 3184, loss = 0.47330421\n",
      "Iteration 3185, loss = 0.47331501\n",
      "Iteration 3186, loss = 0.47337068\n",
      "Iteration 3187, loss = 0.47299212\n",
      "Iteration 3188, loss = 0.47304422\n",
      "Iteration 3189, loss = 0.47310654\n",
      "Iteration 3190, loss = 0.47304232\n",
      "Iteration 3191, loss = 0.47294161\n",
      "Iteration 3192, loss = 0.47285251\n",
      "Iteration 3193, loss = 0.47305685\n",
      "Iteration 3194, loss = 0.47328203\n",
      "Iteration 3195, loss = 0.47296574\n",
      "Iteration 3196, loss = 0.47264964\n",
      "Iteration 3197, loss = 0.47382951\n",
      "Iteration 3198, loss = 0.47389052\n",
      "Iteration 3199, loss = 0.47373399\n",
      "Iteration 3200, loss = 0.47311611\n",
      "Iteration 3201, loss = 0.47286762\n",
      "Iteration 3202, loss = 0.47290390\n",
      "Iteration 3203, loss = 0.47344463\n",
      "Iteration 3204, loss = 0.47367504\n",
      "Iteration 3205, loss = 0.47382672\n",
      "Iteration 3206, loss = 0.47366739\n",
      "Iteration 3207, loss = 0.47337834\n",
      "Iteration 3208, loss = 0.47308303\n",
      "Iteration 3209, loss = 0.47314472\n",
      "Iteration 3210, loss = 0.47313499\n",
      "Iteration 3211, loss = 0.47312927\n",
      "Iteration 3212, loss = 0.47301280\n",
      "Iteration 3213, loss = 0.47307336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3214, loss = 0.47296603\n",
      "Iteration 3215, loss = 0.47291139\n",
      "Iteration 3216, loss = 0.47310840\n",
      "Iteration 3217, loss = 0.47280586\n",
      "Iteration 3218, loss = 0.47347115\n",
      "Iteration 3219, loss = 0.47349024\n",
      "Iteration 3220, loss = 0.47321778\n",
      "Iteration 3221, loss = 0.47279570\n",
      "Iteration 3222, loss = 0.47266426\n",
      "Iteration 3223, loss = 0.47375971\n",
      "Iteration 3224, loss = 0.47375033\n",
      "Iteration 3225, loss = 0.47327849\n",
      "Iteration 3226, loss = 0.47304182\n",
      "Iteration 3227, loss = 0.47309231\n",
      "Iteration 3228, loss = 0.47323417\n",
      "Iteration 3229, loss = 0.47315086\n",
      "Iteration 3230, loss = 0.47330808\n",
      "Iteration 3231, loss = 0.47301875\n",
      "Iteration 3232, loss = 0.47315455\n",
      "Iteration 3233, loss = 0.47317397\n",
      "Iteration 3234, loss = 0.47299061\n",
      "Iteration 3235, loss = 0.47287472\n",
      "Iteration 3236, loss = 0.47339268\n",
      "Iteration 3237, loss = 0.47310770\n",
      "Iteration 3238, loss = 0.47357542\n",
      "Iteration 3239, loss = 0.47335535\n",
      "Iteration 3240, loss = 0.47323497\n",
      "Iteration 3241, loss = 0.47294820\n",
      "Iteration 3242, loss = 0.47279313\n",
      "Iteration 3243, loss = 0.47303154\n",
      "Iteration 3244, loss = 0.47349191\n",
      "Iteration 3245, loss = 0.47402219\n",
      "Iteration 3246, loss = 0.47355671\n",
      "Iteration 3247, loss = 0.47339560\n",
      "Iteration 3248, loss = 0.47317913\n",
      "Iteration 3249, loss = 0.47266182\n",
      "Iteration 3250, loss = 0.47330569\n",
      "Iteration 3251, loss = 0.47352772\n",
      "Iteration 3252, loss = 0.47343359\n",
      "Iteration 3253, loss = 0.47336117\n",
      "Iteration 3254, loss = 0.47292396\n",
      "Iteration 3255, loss = 0.47288163\n",
      "Iteration 3256, loss = 0.47293960\n",
      "Iteration 3257, loss = 0.47346497\n",
      "Iteration 3258, loss = 0.47347772\n",
      "Iteration 3259, loss = 0.47318137\n",
      "Iteration 3260, loss = 0.47299860\n",
      "Iteration 3261, loss = 0.47302618\n",
      "Iteration 3262, loss = 0.47282474\n",
      "Iteration 3263, loss = 0.47305120\n",
      "Iteration 3264, loss = 0.47295631\n",
      "Iteration 3265, loss = 0.47285772\n",
      "Iteration 3266, loss = 0.47296536\n",
      "Iteration 3267, loss = 0.47297993\n",
      "Iteration 3268, loss = 0.47289446\n",
      "Iteration 3269, loss = 0.47287125\n",
      "Iteration 3270, loss = 0.47315302\n",
      "Iteration 3271, loss = 0.47308540\n",
      "Iteration 3272, loss = 0.47301096\n",
      "Iteration 3273, loss = 0.47360977\n",
      "Iteration 3274, loss = 0.47298928\n",
      "Iteration 3275, loss = 0.47315860\n",
      "Iteration 3276, loss = 0.47288881\n",
      "Iteration 3277, loss = 0.47276368\n",
      "Iteration 3278, loss = 0.47286860\n",
      "Iteration 3279, loss = 0.47311635\n",
      "Iteration 3280, loss = 0.47322600\n",
      "Iteration 3281, loss = 0.47305859\n",
      "Iteration 3282, loss = 0.47286973\n",
      "Iteration 3283, loss = 0.47271207\n",
      "Iteration 3284, loss = 0.47356143\n",
      "Iteration 3285, loss = 0.47312390\n",
      "Iteration 3286, loss = 0.47278712\n",
      "Iteration 3287, loss = 0.47268220\n",
      "Iteration 3288, loss = 0.47293630\n",
      "Iteration 3289, loss = 0.47316054\n",
      "Iteration 3290, loss = 0.47320118\n",
      "Iteration 3291, loss = 0.47290781\n",
      "Iteration 3292, loss = 0.47330830\n",
      "Iteration 3293, loss = 0.47291248\n",
      "Iteration 3294, loss = 0.47298030\n",
      "Iteration 3295, loss = 0.47324897\n",
      "Iteration 3296, loss = 0.47297011\n",
      "Iteration 3297, loss = 0.47281937\n",
      "Iteration 3298, loss = 0.47280489\n",
      "Iteration 3299, loss = 0.47382765\n",
      "Iteration 3300, loss = 0.47337421\n",
      "Iteration 3301, loss = 0.47313528\n",
      "Iteration 3302, loss = 0.47320207\n",
      "Iteration 3303, loss = 0.47295721\n",
      "Iteration 3304, loss = 0.47318317\n",
      "Iteration 3305, loss = 0.47313339\n",
      "Iteration 3306, loss = 0.47284822\n",
      "Iteration 3307, loss = 0.47276834\n",
      "Iteration 3308, loss = 0.47295613\n",
      "Iteration 3309, loss = 0.47328131\n",
      "Iteration 3310, loss = 0.47284196\n",
      "Iteration 3311, loss = 0.47360143\n",
      "Iteration 3312, loss = 0.47318445\n",
      "Iteration 3313, loss = 0.47281732\n",
      "Iteration 3314, loss = 0.47306656\n",
      "Iteration 3315, loss = 0.47280818\n",
      "Iteration 3316, loss = 0.47285746\n",
      "Iteration 3317, loss = 0.47282083\n",
      "Iteration 3318, loss = 0.47286744\n",
      "Iteration 3319, loss = 0.47296471\n",
      "Iteration 3320, loss = 0.47297849\n",
      "Iteration 3321, loss = 0.47296117\n",
      "Iteration 3322, loss = 0.47283232\n",
      "Iteration 3323, loss = 0.47284932\n",
      "Iteration 3324, loss = 0.47282046\n",
      "Iteration 3325, loss = 0.47292825\n",
      "Iteration 3326, loss = 0.47283006\n",
      "Iteration 3327, loss = 0.47319713\n",
      "Iteration 3328, loss = 0.47266660\n",
      "Iteration 3329, loss = 0.47289161\n",
      "Iteration 3330, loss = 0.47328995\n",
      "Iteration 3331, loss = 0.47329727\n",
      "Iteration 3332, loss = 0.47307425\n",
      "Iteration 3333, loss = 0.47278678\n",
      "Iteration 3334, loss = 0.47311897\n",
      "Iteration 3335, loss = 0.47331373\n",
      "Iteration 3336, loss = 0.47284402\n",
      "Iteration 3337, loss = 0.47281478\n",
      "Iteration 3338, loss = 0.47312030\n",
      "Iteration 3339, loss = 0.47313157\n",
      "Iteration 3340, loss = 0.47289021\n",
      "Iteration 3341, loss = 0.47277638\n",
      "Iteration 3342, loss = 0.47303526\n",
      "Iteration 3343, loss = 0.47314732\n",
      "Iteration 3344, loss = 0.47282570\n",
      "Iteration 3345, loss = 0.47304562\n",
      "Iteration 3346, loss = 0.47324289\n",
      "Iteration 3347, loss = 0.47337531\n",
      "Iteration 3348, loss = 0.47297693\n",
      "Iteration 3349, loss = 0.47268161\n",
      "Iteration 3350, loss = 0.47257175\n",
      "Iteration 3351, loss = 0.47302823\n",
      "Iteration 3352, loss = 0.47321688\n",
      "Iteration 3353, loss = 0.47308659\n",
      "Iteration 3354, loss = 0.47304253\n",
      "Iteration 3355, loss = 0.47272516\n",
      "Iteration 3356, loss = 0.47284990\n",
      "Iteration 3357, loss = 0.47300743\n",
      "Iteration 3358, loss = 0.47311235\n",
      "Iteration 3359, loss = 0.47277195\n",
      "Iteration 3360, loss = 0.47267575\n",
      "Iteration 3361, loss = 0.47265253\n",
      "Iteration 3362, loss = 0.47272405\n",
      "Iteration 3363, loss = 0.47277051\n",
      "Iteration 3364, loss = 0.47275112\n",
      "Iteration 3365, loss = 0.47272732\n",
      "Iteration 3366, loss = 0.47277148\n",
      "Iteration 3367, loss = 0.47302810\n",
      "Iteration 3368, loss = 0.47354324\n",
      "Iteration 3369, loss = 0.47333527\n",
      "Iteration 3370, loss = 0.47321182\n",
      "Iteration 3371, loss = 0.47259968\n",
      "Iteration 3372, loss = 0.47320892\n",
      "Iteration 3373, loss = 0.47342959\n",
      "Iteration 3374, loss = 0.47314935\n",
      "Iteration 3375, loss = 0.47264641\n",
      "Iteration 3376, loss = 0.47289766\n",
      "Iteration 3377, loss = 0.47299114\n",
      "Iteration 3378, loss = 0.47332833\n",
      "Iteration 3379, loss = 0.47316689\n",
      "Iteration 3380, loss = 0.47335294\n",
      "Iteration 3381, loss = 0.47295023\n",
      "Iteration 3382, loss = 0.47295968\n",
      "Iteration 3383, loss = 0.47271837\n",
      "Iteration 3384, loss = 0.47270410\n",
      "Iteration 3385, loss = 0.47281384\n",
      "Iteration 3386, loss = 0.47307366\n",
      "Iteration 3387, loss = 0.47356220\n",
      "Iteration 3388, loss = 0.47417814\n",
      "Iteration 3389, loss = 0.47319573\n",
      "Iteration 3390, loss = 0.47293631\n",
      "Iteration 3391, loss = 0.47257249\n",
      "Iteration 3392, loss = 0.47248184\n",
      "Iteration 3393, loss = 0.47409119\n",
      "Iteration 3394, loss = 0.47410365\n",
      "Iteration 3395, loss = 0.47346223\n",
      "Iteration 3396, loss = 0.47251282\n",
      "Iteration 3397, loss = 0.47275312\n",
      "Iteration 3398, loss = 0.47345228\n",
      "Iteration 3399, loss = 0.47363930\n",
      "Iteration 3400, loss = 0.47307830\n",
      "Iteration 3401, loss = 0.47296325\n",
      "Iteration 3402, loss = 0.47295832\n",
      "Iteration 3403, loss = 0.47311423\n",
      "Iteration 3404, loss = 0.47298799\n",
      "Iteration 3405, loss = 0.47265846\n",
      "Iteration 3406, loss = 0.47266003\n",
      "Iteration 3407, loss = 0.47283175\n",
      "Iteration 3408, loss = 0.47318767\n",
      "Iteration 3409, loss = 0.47348649\n",
      "Iteration 3410, loss = 0.47306376\n",
      "Iteration 3411, loss = 0.47299651\n",
      "Iteration 3412, loss = 0.47282392\n",
      "Iteration 3413, loss = 0.47293985\n",
      "Iteration 3414, loss = 0.47266986\n",
      "Iteration 3415, loss = 0.47275778\n",
      "Iteration 3416, loss = 0.47272911\n",
      "Iteration 3417, loss = 0.47265346\n",
      "Iteration 3418, loss = 0.47266972\n",
      "Iteration 3419, loss = 0.47279666\n",
      "Iteration 3420, loss = 0.47295333\n",
      "Iteration 3421, loss = 0.47323263\n",
      "Iteration 3422, loss = 0.47297443\n",
      "Iteration 3423, loss = 0.47282117\n",
      "Iteration 3424, loss = 0.47265301\n",
      "Iteration 3425, loss = 0.47270467\n",
      "Iteration 3426, loss = 0.47288187\n",
      "Iteration 3427, loss = 0.47322040\n",
      "Iteration 3428, loss = 0.47279645\n",
      "Iteration 3429, loss = 0.47259957\n",
      "Iteration 3430, loss = 0.47290324\n",
      "Iteration 3431, loss = 0.47438477\n",
      "Iteration 3432, loss = 0.47374139\n",
      "Iteration 3433, loss = 0.47337170\n",
      "Iteration 3434, loss = 0.47257407\n",
      "Iteration 3435, loss = 0.47271623\n",
      "Iteration 3436, loss = 0.47309455\n",
      "Iteration 3437, loss = 0.47344294\n",
      "Iteration 3438, loss = 0.47308875\n",
      "Iteration 3439, loss = 0.47300852\n",
      "Iteration 3440, loss = 0.47280145\n",
      "Iteration 3441, loss = 0.47269769\n",
      "Iteration 3442, loss = 0.47264876\n",
      "Iteration 3443, loss = 0.47265111\n",
      "Iteration 3444, loss = 0.47265452\n",
      "Iteration 3445, loss = 0.47275225\n",
      "Iteration 3446, loss = 0.47262618\n",
      "Iteration 3447, loss = 0.47274331\n",
      "Iteration 3448, loss = 0.47271928\n",
      "Iteration 3449, loss = 0.47274358\n",
      "Iteration 3450, loss = 0.47264690\n",
      "Iteration 3451, loss = 0.47262763\n",
      "Iteration 3452, loss = 0.47271180\n",
      "Iteration 3453, loss = 0.47263772\n",
      "Iteration 3454, loss = 0.47264687\n",
      "Iteration 3455, loss = 0.47268062\n",
      "Iteration 3456, loss = 0.47276700\n",
      "Iteration 3457, loss = 0.47259749\n",
      "Iteration 3458, loss = 0.47260120\n",
      "Iteration 3459, loss = 0.47260009\n",
      "Iteration 3460, loss = 0.47266023\n",
      "Iteration 3461, loss = 0.47266068\n",
      "Iteration 3462, loss = 0.47258984\n",
      "Iteration 3463, loss = 0.47259713\n",
      "Iteration 3464, loss = 0.47257618\n",
      "Iteration 3465, loss = 0.47283605\n",
      "Iteration 3466, loss = 0.47293689\n",
      "Iteration 3467, loss = 0.47268010\n",
      "Iteration 3468, loss = 0.47275477\n",
      "Iteration 3469, loss = 0.47266023\n",
      "Iteration 3470, loss = 0.47295135\n",
      "Iteration 3471, loss = 0.47287050\n",
      "Iteration 3472, loss = 0.47264810\n",
      "Iteration 3473, loss = 0.47254457\n",
      "Iteration 3474, loss = 0.47264279\n",
      "Iteration 3475, loss = 0.47287513\n",
      "Iteration 3476, loss = 0.47304602\n",
      "Iteration 3477, loss = 0.47318199\n",
      "Iteration 3478, loss = 0.47296127\n",
      "Iteration 3479, loss = 0.47308278\n",
      "Iteration 3480, loss = 0.47289898\n",
      "Iteration 3481, loss = 0.47295617\n",
      "Iteration 3482, loss = 0.47285099\n",
      "Iteration 3483, loss = 0.47268134\n",
      "Iteration 3484, loss = 0.47262388\n",
      "Iteration 3485, loss = 0.47262069\n",
      "Iteration 3486, loss = 0.47283511\n",
      "Iteration 3487, loss = 0.47253468\n",
      "Iteration 3488, loss = 0.47275907\n",
      "Iteration 3489, loss = 0.47295749\n",
      "Iteration 3490, loss = 0.47312737\n",
      "Iteration 3491, loss = 0.47291684\n",
      "Iteration 3492, loss = 0.47286128\n",
      "Iteration 3493, loss = 0.47288040\n",
      "Iteration 3494, loss = 0.47273178\n",
      "Iteration 3495, loss = 0.47264553\n",
      "Iteration 3496, loss = 0.47265973\n",
      "Iteration 3497, loss = 0.47275268\n",
      "Iteration 3498, loss = 0.47269379\n",
      "Iteration 3499, loss = 0.47267994\n",
      "Iteration 3500, loss = 0.47268230\n",
      "Iteration 3501, loss = 0.47292674\n",
      "Iteration 3502, loss = 0.47289400\n",
      "Iteration 3503, loss = 0.47267801\n",
      "Iteration 3504, loss = 0.47265163\n",
      "Iteration 3505, loss = 0.47263056\n",
      "Iteration 3506, loss = 0.47275302\n",
      "Iteration 3507, loss = 0.47287927\n",
      "Iteration 3508, loss = 0.47281777\n",
      "Iteration 3509, loss = 0.47275541\n",
      "Iteration 3510, loss = 0.47267141\n",
      "Iteration 3511, loss = 0.47263399\n",
      "Iteration 3512, loss = 0.47260146\n",
      "Iteration 3513, loss = 0.47269121\n",
      "Iteration 3514, loss = 0.47289994\n",
      "Iteration 3515, loss = 0.47289578\n",
      "Iteration 3516, loss = 0.47269157\n",
      "Iteration 3517, loss = 0.47251424\n",
      "Iteration 3518, loss = 0.47269542\n",
      "Iteration 3519, loss = 0.47284738\n",
      "Iteration 3520, loss = 0.47276476\n",
      "Iteration 3521, loss = 0.47281688\n",
      "Iteration 3522, loss = 0.47283639\n",
      "Iteration 3523, loss = 0.47289190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3524, loss = 0.47266308\n",
      "Iteration 3525, loss = 0.47259498\n",
      "Iteration 3526, loss = 0.47261179\n",
      "Iteration 3527, loss = 0.47271680\n",
      "Iteration 3528, loss = 0.47267379\n",
      "Iteration 3529, loss = 0.47266217\n",
      "Iteration 3530, loss = 0.47255916\n",
      "Iteration 3531, loss = 0.47287099\n",
      "Iteration 3532, loss = 0.47320344\n",
      "Iteration 3533, loss = 0.47319836\n",
      "Iteration 3534, loss = 0.47288175\n",
      "Iteration 3535, loss = 0.47260369\n",
      "Iteration 3536, loss = 0.47336541\n",
      "Iteration 3537, loss = 0.47263567\n",
      "Iteration 3538, loss = 0.47257010\n",
      "Iteration 3539, loss = 0.47272075\n",
      "Iteration 3540, loss = 0.47260494\n",
      "Iteration 3541, loss = 0.47257545\n",
      "Iteration 3542, loss = 0.47253838\n",
      "Iteration 3543, loss = 0.47261099\n",
      "Iteration 3544, loss = 0.47297339\n",
      "Iteration 3545, loss = 0.47264391\n",
      "Iteration 3546, loss = 0.47257463\n",
      "Iteration 3547, loss = 0.47309591\n",
      "Iteration 3548, loss = 0.47301094\n",
      "Iteration 3549, loss = 0.47326584\n",
      "Iteration 3550, loss = 0.47274775\n",
      "Iteration 3551, loss = 0.47261813\n",
      "Iteration 3552, loss = 0.47257242\n",
      "Iteration 3553, loss = 0.47252979\n",
      "Iteration 3554, loss = 0.47254919\n",
      "Iteration 3555, loss = 0.47299476\n",
      "Iteration 3556, loss = 0.47272333\n",
      "Iteration 3557, loss = 0.47278856\n",
      "Iteration 3558, loss = 0.47245490\n",
      "Iteration 3559, loss = 0.47320362\n",
      "Iteration 3560, loss = 0.47304923\n",
      "Iteration 3561, loss = 0.47278536\n",
      "Iteration 3562, loss = 0.47252748\n",
      "Iteration 3563, loss = 0.47285566\n",
      "Iteration 3564, loss = 0.47271936\n",
      "Iteration 3565, loss = 0.47283651\n",
      "Iteration 3566, loss = 0.47248065\n",
      "Iteration 3567, loss = 0.47247448\n",
      "Iteration 3568, loss = 0.47267279\n",
      "Iteration 3569, loss = 0.47261418\n",
      "Iteration 3570, loss = 0.47255023\n",
      "Iteration 3571, loss = 0.47255829\n",
      "Iteration 3572, loss = 0.47255940\n",
      "Iteration 3573, loss = 0.47272677\n",
      "Iteration 3574, loss = 0.47261402\n",
      "Iteration 3575, loss = 0.47236154\n",
      "Iteration 3576, loss = 0.47329297\n",
      "Iteration 3577, loss = 0.47339984\n",
      "Iteration 3578, loss = 0.47313704\n",
      "Iteration 3579, loss = 0.47302527\n",
      "Iteration 3580, loss = 0.47326788\n",
      "Iteration 3581, loss = 0.47270712\n",
      "Iteration 3582, loss = 0.47269308\n",
      "Iteration 3583, loss = 0.47322125\n",
      "Iteration 3584, loss = 0.47315410\n",
      "Iteration 3585, loss = 0.47260091\n",
      "Iteration 3586, loss = 0.47254253\n",
      "Iteration 3587, loss = 0.47255024\n",
      "Iteration 3588, loss = 0.47269019\n",
      "Iteration 3589, loss = 0.47293136\n",
      "Iteration 3590, loss = 0.47276741\n",
      "Iteration 3591, loss = 0.47329597\n",
      "Iteration 3592, loss = 0.47263642\n",
      "Iteration 3593, loss = 0.47259698\n",
      "Iteration 3594, loss = 0.47257644\n",
      "Iteration 3595, loss = 0.47237882\n",
      "Iteration 3596, loss = 0.47324051\n",
      "Iteration 3597, loss = 0.47276842\n",
      "Iteration 3598, loss = 0.47264480\n",
      "Iteration 3599, loss = 0.47314427\n",
      "Iteration 3600, loss = 0.47238980\n",
      "Iteration 3601, loss = 0.47272455\n",
      "Iteration 3602, loss = 0.47284992\n",
      "Iteration 3603, loss = 0.47277204\n",
      "Iteration 3604, loss = 0.47288448\n",
      "Iteration 3605, loss = 0.47243934\n",
      "Iteration 3606, loss = 0.47260043\n",
      "Iteration 3607, loss = 0.47288728\n",
      "Iteration 3608, loss = 0.47284866\n",
      "Iteration 3609, loss = 0.47271746\n",
      "Iteration 3610, loss = 0.47257348\n",
      "Iteration 3611, loss = 0.47243152\n",
      "Iteration 3612, loss = 0.47246252\n",
      "Iteration 3613, loss = 0.47273644\n",
      "Iteration 3614, loss = 0.47331035\n",
      "Iteration 3615, loss = 0.47275317\n",
      "Iteration 3616, loss = 0.47325956\n",
      "Iteration 3617, loss = 0.47274462\n",
      "Iteration 3618, loss = 0.47288652\n",
      "Iteration 3619, loss = 0.47272077\n",
      "Iteration 3620, loss = 0.47288890\n",
      "Iteration 3621, loss = 0.47310602\n",
      "Iteration 3622, loss = 0.47258702\n",
      "Iteration 3623, loss = 0.47373749\n",
      "Iteration 3624, loss = 0.47340313\n",
      "Iteration 3625, loss = 0.47307185\n",
      "Iteration 3626, loss = 0.47285269\n",
      "Iteration 3627, loss = 0.47292148\n",
      "Iteration 3628, loss = 0.47289709\n",
      "Iteration 3629, loss = 0.47263202\n",
      "Iteration 3630, loss = 0.47251890\n",
      "Iteration 3631, loss = 0.47242835\n",
      "Iteration 3632, loss = 0.47248227\n",
      "Iteration 3633, loss = 0.47266790\n",
      "Iteration 3634, loss = 0.47308105\n",
      "Iteration 3635, loss = 0.47306381\n",
      "Iteration 3636, loss = 0.47272255\n",
      "Iteration 3637, loss = 0.47254147\n",
      "Iteration 3638, loss = 0.47254544\n",
      "Iteration 3639, loss = 0.47251041\n",
      "Iteration 3640, loss = 0.47243081\n",
      "Iteration 3641, loss = 0.47283781\n",
      "Iteration 3642, loss = 0.47256519\n",
      "Iteration 3643, loss = 0.47243741\n",
      "Iteration 3644, loss = 0.47225856\n",
      "Iteration 3645, loss = 0.47272160\n",
      "Iteration 3646, loss = 0.47308186\n",
      "Iteration 3647, loss = 0.47324427\n",
      "Iteration 3648, loss = 0.47270232\n",
      "Iteration 3649, loss = 0.47242895\n",
      "Iteration 3650, loss = 0.47282986\n",
      "Iteration 3651, loss = 0.47281891\n",
      "Iteration 3652, loss = 0.47252546\n",
      "Iteration 3653, loss = 0.47239798\n",
      "Iteration 3654, loss = 0.47253458\n",
      "Iteration 3655, loss = 0.47271918\n",
      "Iteration 3656, loss = 0.47332741\n",
      "Iteration 3657, loss = 0.47251699\n",
      "Iteration 3658, loss = 0.47264404\n",
      "Iteration 3659, loss = 0.47277712\n",
      "Iteration 3660, loss = 0.47248473\n",
      "Iteration 3661, loss = 0.47260755\n",
      "Iteration 3662, loss = 0.47253248\n",
      "Iteration 3663, loss = 0.47251266\n",
      "Iteration 3664, loss = 0.47255737\n",
      "Iteration 3665, loss = 0.47270158\n",
      "Iteration 3666, loss = 0.47259872\n",
      "Iteration 3667, loss = 0.47247746\n",
      "Iteration 3668, loss = 0.47263576\n",
      "Iteration 3669, loss = 0.47262866\n",
      "Iteration 3670, loss = 0.47267596\n",
      "Iteration 3671, loss = 0.47242444\n",
      "Iteration 3672, loss = 0.47250641\n",
      "Iteration 3673, loss = 0.47272682\n",
      "Iteration 3674, loss = 0.47257449\n",
      "Iteration 3675, loss = 0.47247820\n",
      "Iteration 3676, loss = 0.47250746\n",
      "Iteration 3677, loss = 0.47238513\n",
      "Iteration 3678, loss = 0.47237529\n",
      "Iteration 3679, loss = 0.47240793\n",
      "Iteration 3680, loss = 0.47245192\n",
      "Iteration 3681, loss = 0.47260321\n",
      "Iteration 3682, loss = 0.47254762\n",
      "Iteration 3683, loss = 0.47257712\n",
      "Iteration 3684, loss = 0.47240178\n",
      "Iteration 3685, loss = 0.47277870\n",
      "Iteration 3686, loss = 0.47259008\n",
      "Iteration 3687, loss = 0.47246903\n",
      "Iteration 3688, loss = 0.47240333\n",
      "Iteration 3689, loss = 0.47249515\n",
      "Iteration 3690, loss = 0.47264185\n",
      "Iteration 3691, loss = 0.47272022\n",
      "Iteration 3692, loss = 0.47282625\n",
      "Iteration 3693, loss = 0.47251077\n",
      "Iteration 3694, loss = 0.47236998\n",
      "Iteration 3695, loss = 0.47232233\n",
      "Iteration 3696, loss = 0.47279535\n",
      "Iteration 3697, loss = 0.47291215\n",
      "Iteration 3698, loss = 0.47261666\n",
      "Iteration 3699, loss = 0.47291793\n",
      "Iteration 3700, loss = 0.47261925\n",
      "Iteration 3701, loss = 0.47276336\n",
      "Iteration 3702, loss = 0.47263838\n",
      "Iteration 3703, loss = 0.47250891\n",
      "Iteration 3704, loss = 0.47266673\n",
      "Iteration 3705, loss = 0.47284850\n",
      "Iteration 3706, loss = 0.47336875\n",
      "Iteration 3707, loss = 0.47349565\n",
      "Iteration 3708, loss = 0.47304671\n",
      "Iteration 3709, loss = 0.47254520\n",
      "Iteration 3710, loss = 0.47265421\n",
      "Iteration 3711, loss = 0.47313173\n",
      "Iteration 3712, loss = 0.47290200\n",
      "Iteration 3713, loss = 0.47299235\n",
      "Iteration 3714, loss = 0.47259169\n",
      "Iteration 3715, loss = 0.47276156\n",
      "Iteration 3716, loss = 0.47254040\n",
      "Iteration 3717, loss = 0.47256182\n",
      "Iteration 3718, loss = 0.47251572\n",
      "Iteration 3719, loss = 0.47250278\n",
      "Iteration 3720, loss = 0.47248964\n",
      "Iteration 3721, loss = 0.47284835\n",
      "Iteration 3722, loss = 0.47239691\n",
      "Iteration 3723, loss = 0.47261101\n",
      "Iteration 3724, loss = 0.47276686\n",
      "Iteration 3725, loss = 0.47287861\n",
      "Iteration 3726, loss = 0.47319521\n",
      "Iteration 3727, loss = 0.47310995\n",
      "Iteration 3728, loss = 0.47309301\n",
      "Iteration 3729, loss = 0.47260608\n",
      "Iteration 3730, loss = 0.47263682\n",
      "Iteration 3731, loss = 0.47323334\n",
      "Iteration 3732, loss = 0.47276056\n",
      "Iteration 3733, loss = 0.47245359\n",
      "Iteration 3734, loss = 0.47251843\n",
      "Iteration 3735, loss = 0.47257484\n",
      "Iteration 3736, loss = 0.47293461\n",
      "Iteration 3737, loss = 0.47262681\n",
      "Iteration 3738, loss = 0.47276676\n",
      "Iteration 3739, loss = 0.47363513\n",
      "Iteration 3740, loss = 0.47343420\n",
      "Iteration 3741, loss = 0.47260453\n",
      "Iteration 3742, loss = 0.47237727\n",
      "Iteration 3743, loss = 0.47302846\n",
      "Iteration 3744, loss = 0.47326962\n",
      "Iteration 3745, loss = 0.47301787\n",
      "Iteration 3746, loss = 0.47262462\n",
      "Iteration 3747, loss = 0.47252730\n",
      "Iteration 3748, loss = 0.47263156\n",
      "Iteration 3749, loss = 0.47273840\n",
      "Iteration 3750, loss = 0.47258704\n",
      "Iteration 3751, loss = 0.47247759\n",
      "Iteration 3752, loss = 0.47230771\n",
      "Iteration 3753, loss = 0.47242896\n",
      "Iteration 3754, loss = 0.47286097\n",
      "Iteration 3755, loss = 0.47332204\n",
      "Iteration 3756, loss = 0.47275433\n",
      "Iteration 3757, loss = 0.47299136\n",
      "Iteration 3758, loss = 0.47284856\n",
      "Iteration 3759, loss = 0.47283170\n",
      "Iteration 3760, loss = 0.47231872\n",
      "Iteration 3761, loss = 0.47216713\n",
      "Iteration 3762, loss = 0.47268515\n",
      "Iteration 3763, loss = 0.47361040\n",
      "Iteration 3764, loss = 0.47394503\n",
      "Iteration 3765, loss = 0.47375826\n",
      "Iteration 3766, loss = 0.47269402\n",
      "Iteration 3767, loss = 0.47278999\n",
      "Iteration 3768, loss = 0.47252224\n",
      "Iteration 3769, loss = 0.47340986\n",
      "Iteration 3770, loss = 0.47262811\n",
      "Iteration 3771, loss = 0.47249749\n",
      "Iteration 3772, loss = 0.47236157\n",
      "Iteration 3773, loss = 0.47276217\n",
      "Iteration 3774, loss = 0.47309588\n",
      "Iteration 3775, loss = 0.47304914\n",
      "Iteration 3776, loss = 0.47282185\n",
      "Iteration 3777, loss = 0.47236289\n",
      "Iteration 3778, loss = 0.47229146\n",
      "Iteration 3779, loss = 0.47246665\n",
      "Iteration 3780, loss = 0.47267427\n",
      "Iteration 3781, loss = 0.47280804\n",
      "Iteration 3782, loss = 0.47269079\n",
      "Iteration 3783, loss = 0.47239329\n",
      "Iteration 3784, loss = 0.47248069\n",
      "Iteration 3785, loss = 0.47262870\n",
      "Iteration 3786, loss = 0.47276919\n",
      "Iteration 3787, loss = 0.47252555\n",
      "Iteration 3788, loss = 0.47287896\n",
      "Iteration 3789, loss = 0.47248388\n",
      "Iteration 3790, loss = 0.47252750\n",
      "Iteration 3791, loss = 0.47277605\n",
      "Iteration 3792, loss = 0.47257473\n",
      "Iteration 3793, loss = 0.47224636\n",
      "Iteration 3794, loss = 0.47236865\n",
      "Iteration 3795, loss = 0.47275438\n",
      "Iteration 3796, loss = 0.47293584\n",
      "Iteration 3797, loss = 0.47307517\n",
      "Iteration 3798, loss = 0.47243988\n",
      "Iteration 3799, loss = 0.47230437\n",
      "Iteration 3800, loss = 0.47222636\n",
      "Iteration 3801, loss = 0.47270145\n",
      "Iteration 3802, loss = 0.47260512\n",
      "Iteration 3803, loss = 0.47257568\n",
      "Iteration 3804, loss = 0.47255137\n",
      "Iteration 3805, loss = 0.47250640\n",
      "Iteration 3806, loss = 0.47253951\n",
      "Iteration 3807, loss = 0.47253913\n",
      "Iteration 3808, loss = 0.47239645\n",
      "Iteration 3809, loss = 0.47240136\n",
      "Iteration 3810, loss = 0.47258915\n",
      "Iteration 3811, loss = 0.47253607\n",
      "Iteration 3812, loss = 0.47281418\n",
      "Iteration 3813, loss = 0.47234032\n",
      "Iteration 3814, loss = 0.47231070\n",
      "Iteration 3815, loss = 0.47235059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3816, loss = 0.47253123\n",
      "Iteration 3817, loss = 0.47277462\n",
      "Iteration 3818, loss = 0.47274105\n",
      "Iteration 3819, loss = 0.47230970\n",
      "Iteration 3820, loss = 0.47225279\n",
      "Iteration 3821, loss = 0.47249650\n",
      "Iteration 3822, loss = 0.47387816\n",
      "Iteration 3823, loss = 0.47329462\n",
      "Iteration 3824, loss = 0.47235038\n",
      "Iteration 3825, loss = 0.47251345\n",
      "Iteration 3826, loss = 0.47283625\n",
      "Iteration 3827, loss = 0.47267109\n",
      "Iteration 3828, loss = 0.47246853\n",
      "Iteration 3829, loss = 0.47240322\n",
      "Iteration 3830, loss = 0.47239056\n",
      "Iteration 3831, loss = 0.47257896\n",
      "Iteration 3832, loss = 0.47253744\n",
      "Iteration 3833, loss = 0.47246045\n",
      "Iteration 3834, loss = 0.47231319\n",
      "Iteration 3835, loss = 0.47240824\n",
      "Iteration 3836, loss = 0.47259262\n",
      "Iteration 3837, loss = 0.47246820\n",
      "Iteration 3838, loss = 0.47292875\n",
      "Iteration 3839, loss = 0.47231856\n",
      "Iteration 3840, loss = 0.47230694\n",
      "Iteration 3841, loss = 0.47250504\n",
      "Iteration 3842, loss = 0.47238156\n",
      "Iteration 3843, loss = 0.47227814\n",
      "Iteration 3844, loss = 0.47281394\n",
      "Iteration 3845, loss = 0.47266260\n",
      "Iteration 3846, loss = 0.47242506\n",
      "Iteration 3847, loss = 0.47230212\n",
      "Iteration 3848, loss = 0.47268357\n",
      "Iteration 3849, loss = 0.47241147\n",
      "Iteration 3850, loss = 0.47219587\n",
      "Iteration 3851, loss = 0.47278299\n",
      "Iteration 3852, loss = 0.47278896\n",
      "Iteration 3853, loss = 0.47268115\n",
      "Iteration 3854, loss = 0.47227994\n",
      "Iteration 3855, loss = 0.47225395\n",
      "Iteration 3856, loss = 0.47310205\n",
      "Iteration 3857, loss = 0.47299571\n",
      "Iteration 3858, loss = 0.47277347\n",
      "Iteration 3859, loss = 0.47240981\n",
      "Iteration 3860, loss = 0.47242037\n",
      "Iteration 3861, loss = 0.47274898\n",
      "Iteration 3862, loss = 0.47257611\n",
      "Iteration 3863, loss = 0.47233219\n",
      "Iteration 3864, loss = 0.47232212\n",
      "Iteration 3865, loss = 0.47222341\n",
      "Iteration 3866, loss = 0.47245886\n",
      "Iteration 3867, loss = 0.47273009\n",
      "Iteration 3868, loss = 0.47308996\n",
      "Iteration 3869, loss = 0.47305318\n",
      "Iteration 3870, loss = 0.47318689\n",
      "Iteration 3871, loss = 0.47281976\n",
      "Iteration 3872, loss = 0.47244767\n",
      "Iteration 3873, loss = 0.47269772\n",
      "Iteration 3874, loss = 0.47286569\n",
      "Iteration 3875, loss = 0.47305053\n",
      "Iteration 3876, loss = 0.47280321\n",
      "Iteration 3877, loss = 0.47261180\n",
      "Iteration 3878, loss = 0.47278235\n",
      "Iteration 3879, loss = 0.47280286\n",
      "Iteration 3880, loss = 0.47241672\n",
      "Iteration 3881, loss = 0.47253461\n",
      "Iteration 3882, loss = 0.47255216\n",
      "Iteration 3883, loss = 0.47272076\n",
      "Iteration 3884, loss = 0.47260833\n",
      "Iteration 3885, loss = 0.47244129\n",
      "Iteration 3886, loss = 0.47277801\n",
      "Iteration 3887, loss = 0.47230270\n",
      "Iteration 3888, loss = 0.47201948\n",
      "Iteration 3889, loss = 0.47300603\n",
      "Iteration 3890, loss = 0.47299774\n",
      "Iteration 3891, loss = 0.47273668\n",
      "Iteration 3892, loss = 0.47209367\n",
      "Iteration 3893, loss = 0.47238656\n",
      "Iteration 3894, loss = 0.47289130\n",
      "Iteration 3895, loss = 0.47349807\n",
      "Iteration 3896, loss = 0.47367210\n",
      "Iteration 3897, loss = 0.47296158\n",
      "Iteration 3898, loss = 0.47303558\n",
      "Iteration 3899, loss = 0.47229232\n",
      "Iteration 3900, loss = 0.47248447\n",
      "Iteration 3901, loss = 0.47287622\n",
      "Iteration 3902, loss = 0.47260091\n",
      "Iteration 3903, loss = 0.47247452\n",
      "Iteration 3904, loss = 0.47240007\n",
      "Iteration 3905, loss = 0.47238508\n",
      "Iteration 3906, loss = 0.47261494\n",
      "Iteration 3907, loss = 0.47237339\n",
      "Iteration 3908, loss = 0.47233184\n",
      "Iteration 3909, loss = 0.47316239\n",
      "Iteration 3910, loss = 0.47236545\n",
      "Iteration 3911, loss = 0.47226212\n",
      "Iteration 3912, loss = 0.47288624\n",
      "Iteration 3913, loss = 0.47268739\n",
      "Iteration 3914, loss = 0.47232632\n",
      "Iteration 3915, loss = 0.47241423\n",
      "Iteration 3916, loss = 0.47249774\n",
      "Iteration 3917, loss = 0.47240177\n",
      "Iteration 3918, loss = 0.47227122\n",
      "Iteration 3919, loss = 0.47222852\n",
      "Iteration 3920, loss = 0.47231709\n",
      "Iteration 3921, loss = 0.47291889\n",
      "Iteration 3922, loss = 0.47256841\n",
      "Iteration 3923, loss = 0.47241653\n",
      "Iteration 3924, loss = 0.47246526\n",
      "Iteration 3925, loss = 0.47233543\n",
      "Iteration 3926, loss = 0.47243536\n",
      "Iteration 3927, loss = 0.47243984\n",
      "Iteration 3928, loss = 0.47256119\n",
      "Iteration 3929, loss = 0.47229155\n",
      "Iteration 3930, loss = 0.47225452\n",
      "Iteration 3931, loss = 0.47224005\n",
      "Iteration 3932, loss = 0.47233478\n",
      "Iteration 3933, loss = 0.47242721\n",
      "Iteration 3934, loss = 0.47237156\n",
      "Iteration 3935, loss = 0.47234453\n",
      "Iteration 3936, loss = 0.47231002\n",
      "Iteration 3937, loss = 0.47225662\n",
      "Iteration 3938, loss = 0.47239166\n",
      "Iteration 3939, loss = 0.47218903\n",
      "Iteration 3940, loss = 0.47238995\n",
      "Iteration 3941, loss = 0.47259111\n",
      "Iteration 3942, loss = 0.47235179\n",
      "Iteration 3943, loss = 0.47227930\n",
      "Iteration 3944, loss = 0.47231574\n",
      "Iteration 3945, loss = 0.47237494\n",
      "Iteration 3946, loss = 0.47253233\n",
      "Iteration 3947, loss = 0.47248716\n",
      "Iteration 3948, loss = 0.47215403\n",
      "Iteration 3949, loss = 0.47288183\n",
      "Iteration 3950, loss = 0.47239856\n",
      "Iteration 3951, loss = 0.47263236\n",
      "Iteration 3952, loss = 0.47253902\n",
      "Iteration 3953, loss = 0.47233569\n",
      "Iteration 3954, loss = 0.47218428\n",
      "Iteration 3955, loss = 0.47225519\n",
      "Iteration 3956, loss = 0.47235350\n",
      "Iteration 3957, loss = 0.47228103\n",
      "Iteration 3958, loss = 0.47221426\n",
      "Iteration 3959, loss = 0.47221390\n",
      "Iteration 3960, loss = 0.47215861\n",
      "Iteration 3961, loss = 0.47232927\n",
      "Iteration 3962, loss = 0.47256647\n",
      "Iteration 3963, loss = 0.47296678\n",
      "Iteration 3964, loss = 0.47308705\n",
      "Iteration 3965, loss = 0.47257369\n",
      "Iteration 3966, loss = 0.47237944\n",
      "Iteration 3967, loss = 0.47244136\n",
      "Iteration 3968, loss = 0.47246097\n",
      "Iteration 3969, loss = 0.47245186\n",
      "Iteration 3970, loss = 0.47281946\n",
      "Iteration 3971, loss = 0.47271231\n",
      "Iteration 3972, loss = 0.47259966\n",
      "Iteration 3973, loss = 0.47246654\n",
      "Iteration 3974, loss = 0.47246093\n",
      "Iteration 3975, loss = 0.47245103\n",
      "Iteration 3976, loss = 0.47235094\n",
      "Iteration 3977, loss = 0.47227889\n",
      "Iteration 3978, loss = 0.47248865\n",
      "Iteration 3979, loss = 0.47256347\n",
      "Iteration 3980, loss = 0.47248421\n",
      "Iteration 3981, loss = 0.47223838\n",
      "Iteration 3982, loss = 0.47227396\n",
      "Iteration 3983, loss = 0.47241631\n",
      "Iteration 3984, loss = 0.47239719\n",
      "Iteration 3985, loss = 0.47273349\n",
      "Iteration 3986, loss = 0.47281831\n",
      "Iteration 3987, loss = 0.47224392\n",
      "Iteration 3988, loss = 0.47322092\n",
      "Iteration 3989, loss = 0.47297045\n",
      "Iteration 3990, loss = 0.47252341\n",
      "Iteration 3991, loss = 0.47245117\n",
      "Iteration 3992, loss = 0.47228753\n",
      "Iteration 3993, loss = 0.47251027\n",
      "Iteration 3994, loss = 0.47252727\n",
      "Iteration 3995, loss = 0.47219831\n",
      "Iteration 3996, loss = 0.47214261\n",
      "Iteration 3997, loss = 0.47229141\n",
      "Iteration 3998, loss = 0.47285679\n",
      "Iteration 3999, loss = 0.47278462\n",
      "Iteration 4000, loss = 0.47224496\n",
      "Iteration 4001, loss = 0.47210026\n",
      "Iteration 4002, loss = 0.47217117\n",
      "Iteration 4003, loss = 0.47243097\n",
      "Iteration 4004, loss = 0.47254137\n",
      "Iteration 4005, loss = 0.47236739\n",
      "Iteration 4006, loss = 0.47229735\n",
      "Iteration 4007, loss = 0.47224601\n",
      "Iteration 4008, loss = 0.47214105\n",
      "Iteration 4009, loss = 0.47245127\n",
      "Iteration 4010, loss = 0.47218451\n",
      "Iteration 4011, loss = 0.47220611\n",
      "Iteration 4012, loss = 0.47218730\n",
      "Iteration 4013, loss = 0.47219804\n",
      "Iteration 4014, loss = 0.47211793\n",
      "Iteration 4015, loss = 0.47226592\n",
      "Iteration 4016, loss = 0.47262278\n",
      "Iteration 4017, loss = 0.47306648\n",
      "Iteration 4018, loss = 0.47236810\n",
      "Iteration 4019, loss = 0.47255640\n",
      "Iteration 4020, loss = 0.47263299\n",
      "Iteration 4021, loss = 0.47220611\n",
      "Iteration 4022, loss = 0.47262058\n",
      "Iteration 4023, loss = 0.47292648\n",
      "Iteration 4024, loss = 0.47267639\n",
      "Iteration 4025, loss = 0.47248469\n",
      "Iteration 4026, loss = 0.47227254\n",
      "Iteration 4027, loss = 0.47210483\n",
      "Iteration 4028, loss = 0.47220450\n",
      "Iteration 4029, loss = 0.47234686\n",
      "Iteration 4030, loss = 0.47301921\n",
      "Iteration 4031, loss = 0.47218344\n",
      "Iteration 4032, loss = 0.47237801\n",
      "Iteration 4033, loss = 0.47245516\n",
      "Iteration 4034, loss = 0.47219772\n",
      "Iteration 4035, loss = 0.47228268\n",
      "Iteration 4036, loss = 0.47218145\n",
      "Iteration 4037, loss = 0.47229740\n",
      "Iteration 4038, loss = 0.47243014\n",
      "Iteration 4039, loss = 0.47237263\n",
      "Iteration 4040, loss = 0.47236278\n",
      "Iteration 4041, loss = 0.47221783\n",
      "Iteration 4042, loss = 0.47211763\n",
      "Iteration 4043, loss = 0.47217869\n",
      "Iteration 4044, loss = 0.47240487\n",
      "Iteration 4045, loss = 0.47266055\n",
      "Iteration 4046, loss = 0.47267472\n",
      "Iteration 4047, loss = 0.47238638\n",
      "Iteration 4048, loss = 0.47212535\n",
      "Iteration 4049, loss = 0.47238790\n",
      "Iteration 4050, loss = 0.47268837\n",
      "Iteration 4051, loss = 0.47297896\n",
      "Iteration 4052, loss = 0.47265289\n",
      "Iteration 4053, loss = 0.47247696\n",
      "Iteration 4054, loss = 0.47212862\n",
      "Iteration 4055, loss = 0.47224040\n",
      "Iteration 4056, loss = 0.47236983\n",
      "Iteration 4057, loss = 0.47218410\n",
      "Iteration 4058, loss = 0.47183676\n",
      "Iteration 4059, loss = 0.47303462\n",
      "Iteration 4060, loss = 0.47320279\n",
      "Iteration 4061, loss = 0.47284741\n",
      "Iteration 4062, loss = 0.47250400\n",
      "Iteration 4063, loss = 0.47233900\n",
      "Iteration 4064, loss = 0.47221376\n",
      "Iteration 4065, loss = 0.47224216\n",
      "Iteration 4066, loss = 0.47238917\n",
      "Iteration 4067, loss = 0.47275304\n",
      "Iteration 4068, loss = 0.47231455\n",
      "Iteration 4069, loss = 0.47235620\n",
      "Iteration 4070, loss = 0.47220040\n",
      "Iteration 4071, loss = 0.47232781\n",
      "Iteration 4072, loss = 0.47343135\n",
      "Iteration 4073, loss = 0.47285019\n",
      "Iteration 4074, loss = 0.47271904\n",
      "Iteration 4075, loss = 0.47261678\n",
      "Iteration 4076, loss = 0.47219251\n",
      "Iteration 4077, loss = 0.47213571\n",
      "Iteration 4078, loss = 0.47223207\n",
      "Iteration 4079, loss = 0.47237566\n",
      "Iteration 4080, loss = 0.47244847\n",
      "Iteration 4081, loss = 0.47231472\n",
      "Iteration 4082, loss = 0.47290638\n",
      "Iteration 4083, loss = 0.47321875\n",
      "Iteration 4084, loss = 0.47239496\n",
      "Iteration 4085, loss = 0.47231936\n",
      "Iteration 4086, loss = 0.47310305\n",
      "Iteration 4087, loss = 0.47233260\n",
      "Iteration 4088, loss = 0.47254149\n",
      "Iteration 4089, loss = 0.47216915\n",
      "Iteration 4090, loss = 0.47236980\n",
      "Iteration 4091, loss = 0.47222683\n",
      "Iteration 4092, loss = 0.47268300\n",
      "Iteration 4093, loss = 0.47251559\n",
      "Iteration 4094, loss = 0.47213990\n",
      "Iteration 4095, loss = 0.47208980\n",
      "Iteration 4096, loss = 0.47253032\n",
      "Iteration 4097, loss = 0.47255545\n",
      "Iteration 4098, loss = 0.47248434\n",
      "Iteration 4099, loss = 0.47273430\n",
      "Iteration 4100, loss = 0.47253086\n",
      "Iteration 4101, loss = 0.47228483\n",
      "Iteration 4102, loss = 0.47209562\n",
      "Iteration 4103, loss = 0.47222956\n",
      "Iteration 4104, loss = 0.47230164\n",
      "Iteration 4105, loss = 0.47253691\n",
      "Iteration 4106, loss = 0.47271860\n",
      "Iteration 4107, loss = 0.47258444\n",
      "Iteration 4108, loss = 0.47273143\n",
      "Iteration 4109, loss = 0.47234538\n",
      "Iteration 4110, loss = 0.47225835\n",
      "Iteration 4111, loss = 0.47241848\n",
      "Iteration 4112, loss = 0.47224939\n",
      "Iteration 4113, loss = 0.47230044\n",
      "Iteration 4114, loss = 0.47212179\n",
      "Iteration 4115, loss = 0.47265110\n",
      "Iteration 4116, loss = 0.47259853\n",
      "Iteration 4117, loss = 0.47232476\n",
      "Iteration 4118, loss = 0.47279123\n",
      "Iteration 4119, loss = 0.47223413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4120, loss = 0.47221960\n",
      "Iteration 4121, loss = 0.47230293\n",
      "Iteration 4122, loss = 0.47235817\n",
      "Iteration 4123, loss = 0.47239256\n",
      "Iteration 4124, loss = 0.47213134\n",
      "Iteration 4125, loss = 0.47163294\n",
      "Iteration 4126, loss = 0.47276134\n",
      "Iteration 4127, loss = 0.47378226\n",
      "Iteration 4128, loss = 0.47390991\n",
      "Iteration 4129, loss = 0.47326332\n",
      "Iteration 4130, loss = 0.47247649\n",
      "Iteration 4131, loss = 0.47299051\n",
      "Iteration 4132, loss = 0.47241500\n",
      "Iteration 4133, loss = 0.47241461\n",
      "Iteration 4134, loss = 0.47206570\n",
      "Iteration 4135, loss = 0.47225308\n",
      "Iteration 4136, loss = 0.47249558\n",
      "Iteration 4137, loss = 0.47213652\n",
      "Iteration 4138, loss = 0.47211790\n",
      "Iteration 4139, loss = 0.47224678\n",
      "Iteration 4140, loss = 0.47229635\n",
      "Iteration 4141, loss = 0.47224230\n",
      "Iteration 4142, loss = 0.47228310\n",
      "Iteration 4143, loss = 0.47212486\n",
      "Iteration 4144, loss = 0.47230653\n",
      "Iteration 4145, loss = 0.47235045\n",
      "Iteration 4146, loss = 0.47281046\n",
      "Iteration 4147, loss = 0.47301027\n",
      "Iteration 4148, loss = 0.47250427\n",
      "Iteration 4149, loss = 0.47207591\n",
      "Iteration 4150, loss = 0.47217028\n",
      "Iteration 4151, loss = 0.47279547\n",
      "Iteration 4152, loss = 0.47270846\n",
      "Iteration 4153, loss = 0.47225543\n",
      "Iteration 4154, loss = 0.47219999\n",
      "Iteration 4155, loss = 0.47208782\n",
      "Iteration 4156, loss = 0.47218421\n",
      "Iteration 4157, loss = 0.47215090\n",
      "Iteration 4158, loss = 0.47213429\n",
      "Iteration 4159, loss = 0.47223880\n",
      "Iteration 4160, loss = 0.47221599\n",
      "Iteration 4161, loss = 0.47241685\n",
      "Iteration 4162, loss = 0.47216432\n",
      "Iteration 4163, loss = 0.47223245\n",
      "Iteration 4164, loss = 0.47220961\n",
      "Iteration 4165, loss = 0.47252802\n",
      "Iteration 4166, loss = 0.47242157\n",
      "Iteration 4167, loss = 0.47282084\n",
      "Iteration 4168, loss = 0.47316169\n",
      "Iteration 4169, loss = 0.47236422\n",
      "Iteration 4170, loss = 0.47224687\n",
      "Iteration 4171, loss = 0.47250027\n",
      "Iteration 4172, loss = 0.47228379\n",
      "Iteration 4173, loss = 0.47217314\n",
      "Iteration 4174, loss = 0.47206576\n",
      "Iteration 4175, loss = 0.47211802\n",
      "Iteration 4176, loss = 0.47231793\n",
      "Iteration 4177, loss = 0.47219067\n",
      "Iteration 4178, loss = 0.47229062\n",
      "Iteration 4179, loss = 0.47209657\n",
      "Iteration 4180, loss = 0.47214083\n",
      "Iteration 4181, loss = 0.47235002\n",
      "Iteration 4182, loss = 0.47210976\n",
      "Iteration 4183, loss = 0.47205247\n",
      "Iteration 4184, loss = 0.47230524\n",
      "Iteration 4185, loss = 0.47239091\n",
      "Iteration 4186, loss = 0.47229387\n",
      "Iteration 4187, loss = 0.47223759\n",
      "Iteration 4188, loss = 0.47238992\n",
      "Iteration 4189, loss = 0.47226925\n",
      "Iteration 4190, loss = 0.47231178\n",
      "Iteration 4191, loss = 0.47225429\n",
      "Iteration 4192, loss = 0.47223808\n",
      "Iteration 4193, loss = 0.47189728\n",
      "Iteration 4194, loss = 0.47254411\n",
      "Iteration 4195, loss = 0.47289681\n",
      "Iteration 4196, loss = 0.47283272\n",
      "Iteration 4197, loss = 0.47212868\n",
      "Iteration 4198, loss = 0.47212439\n",
      "Iteration 4199, loss = 0.47207394\n",
      "Iteration 4200, loss = 0.47214445\n",
      "Iteration 4201, loss = 0.47228702\n",
      "Iteration 4202, loss = 0.47222934\n",
      "Iteration 4203, loss = 0.47238199\n",
      "Iteration 4204, loss = 0.47248749\n",
      "Iteration 4205, loss = 0.47236569\n",
      "Iteration 4206, loss = 0.47239457\n",
      "Iteration 4207, loss = 0.47214235\n",
      "Iteration 4208, loss = 0.47208606\n",
      "Iteration 4209, loss = 0.47222594\n",
      "Iteration 4210, loss = 0.47213993\n",
      "Iteration 4211, loss = 0.47218928\n",
      "Iteration 4212, loss = 0.47229593\n",
      "Iteration 4213, loss = 0.47209257\n",
      "Iteration 4214, loss = 0.47198563\n",
      "Iteration 4215, loss = 0.47245157\n",
      "Iteration 4216, loss = 0.47225217\n",
      "Iteration 4217, loss = 0.47217180\n",
      "Iteration 4218, loss = 0.47211291\n",
      "Iteration 4219, loss = 0.47224259\n",
      "Iteration 4220, loss = 0.47235710\n",
      "Iteration 4221, loss = 0.47204474\n",
      "Iteration 4222, loss = 0.47276643\n",
      "Iteration 4223, loss = 0.47203629\n",
      "Iteration 4224, loss = 0.47198736\n",
      "Iteration 4225, loss = 0.47241237\n",
      "Iteration 4226, loss = 0.47242281\n",
      "Iteration 4227, loss = 0.47223577\n",
      "Iteration 4228, loss = 0.47247176\n",
      "Iteration 4229, loss = 0.47245841\n",
      "Iteration 4230, loss = 0.47199974\n",
      "Iteration 4231, loss = 0.47227519\n",
      "Iteration 4232, loss = 0.47276654\n",
      "Iteration 4233, loss = 0.47235420\n",
      "Iteration 4234, loss = 0.47235655\n",
      "Iteration 4235, loss = 0.47261889\n",
      "Iteration 4236, loss = 0.47197785\n",
      "Iteration 4237, loss = 0.47253662\n",
      "Iteration 4238, loss = 0.47240764\n",
      "Iteration 4239, loss = 0.47249652\n",
      "Iteration 4240, loss = 0.47220331\n",
      "Iteration 4241, loss = 0.47194758\n",
      "Iteration 4242, loss = 0.47194232\n",
      "Iteration 4243, loss = 0.47224035\n",
      "Iteration 4244, loss = 0.47258431\n",
      "Iteration 4245, loss = 0.47236755\n",
      "Iteration 4246, loss = 0.47226142\n",
      "Iteration 4247, loss = 0.47251659\n",
      "Iteration 4248, loss = 0.47223135\n",
      "Iteration 4249, loss = 0.47210160\n",
      "Iteration 4250, loss = 0.47187978\n",
      "Iteration 4251, loss = 0.47235166\n",
      "Iteration 4252, loss = 0.47252967\n",
      "Iteration 4253, loss = 0.47237973\n",
      "Iteration 4254, loss = 0.47202113\n",
      "Iteration 4255, loss = 0.47211468\n",
      "Iteration 4256, loss = 0.47221407\n",
      "Iteration 4257, loss = 0.47242267\n",
      "Iteration 4258, loss = 0.47219760\n",
      "Iteration 4259, loss = 0.47207058\n",
      "Iteration 4260, loss = 0.47252324\n",
      "Iteration 4261, loss = 0.47225714\n",
      "Iteration 4262, loss = 0.47214776\n",
      "Iteration 4263, loss = 0.47213026\n",
      "Iteration 4264, loss = 0.47230625\n",
      "Iteration 4265, loss = 0.47222359\n",
      "Iteration 4266, loss = 0.47201495\n",
      "Iteration 4267, loss = 0.47328728\n",
      "Iteration 4268, loss = 0.47195191\n",
      "Iteration 4269, loss = 0.47247767\n",
      "Iteration 4270, loss = 0.47324349\n",
      "Iteration 4271, loss = 0.47279134\n",
      "Iteration 4272, loss = 0.47264591\n",
      "Iteration 4273, loss = 0.47229653\n",
      "Iteration 4274, loss = 0.47221652\n",
      "Iteration 4275, loss = 0.47265771\n",
      "Iteration 4276, loss = 0.47266763\n",
      "Iteration 4277, loss = 0.47227337\n",
      "Iteration 4278, loss = 0.47269511\n",
      "Iteration 4279, loss = 0.47216435\n",
      "Iteration 4280, loss = 0.47205784\n",
      "Iteration 4281, loss = 0.47206340\n",
      "Iteration 4282, loss = 0.47197748\n",
      "Iteration 4283, loss = 0.47228262\n",
      "Iteration 4284, loss = 0.47215850\n",
      "Iteration 4285, loss = 0.47189432\n",
      "Iteration 4286, loss = 0.47197957\n",
      "Iteration 4287, loss = 0.47227351\n",
      "Iteration 4288, loss = 0.47242710\n",
      "Iteration 4289, loss = 0.47297036\n",
      "Iteration 4290, loss = 0.47297929\n",
      "Iteration 4291, loss = 0.47272920\n",
      "Iteration 4292, loss = 0.47252205\n",
      "Iteration 4293, loss = 0.47191353\n",
      "Iteration 4294, loss = 0.47269713\n",
      "Iteration 4295, loss = 0.47309622\n",
      "Iteration 4296, loss = 0.47292766\n",
      "Iteration 4297, loss = 0.47239323\n",
      "Iteration 4298, loss = 0.47194454\n",
      "Iteration 4299, loss = 0.47260719\n",
      "Iteration 4300, loss = 0.47261019\n",
      "Iteration 4301, loss = 0.47249566\n",
      "Iteration 4302, loss = 0.47228303\n",
      "Iteration 4303, loss = 0.47196940\n",
      "Iteration 4304, loss = 0.47199482\n",
      "Iteration 4305, loss = 0.47233213\n",
      "Iteration 4306, loss = 0.47227255\n",
      "Iteration 4307, loss = 0.47211955\n",
      "Iteration 4308, loss = 0.47206992\n",
      "Iteration 4309, loss = 0.47208433\n",
      "Iteration 4310, loss = 0.47226511\n",
      "Iteration 4311, loss = 0.47225881\n",
      "Iteration 4312, loss = 0.47248162\n",
      "Iteration 4313, loss = 0.47206625\n",
      "Iteration 4314, loss = 0.47212445\n",
      "Iteration 4315, loss = 0.47202670\n",
      "Iteration 4316, loss = 0.47207744\n",
      "Iteration 4317, loss = 0.47219395\n",
      "Iteration 4318, loss = 0.47202951\n",
      "Iteration 4319, loss = 0.47214637\n",
      "Iteration 4320, loss = 0.47220482\n",
      "Iteration 4321, loss = 0.47221745\n",
      "Iteration 4322, loss = 0.47213180\n",
      "Iteration 4323, loss = 0.47252443\n",
      "Iteration 4324, loss = 0.47198160\n",
      "Iteration 4325, loss = 0.47194548\n",
      "Iteration 4326, loss = 0.47190284\n",
      "Iteration 4327, loss = 0.47219797\n",
      "Iteration 4328, loss = 0.47291491\n",
      "Iteration 4329, loss = 0.47234722\n",
      "Iteration 4330, loss = 0.47257793\n",
      "Iteration 4331, loss = 0.47247105\n",
      "Iteration 4332, loss = 0.47245157\n",
      "Iteration 4333, loss = 0.47200842\n",
      "Iteration 4334, loss = 0.47237152\n",
      "Iteration 4335, loss = 0.47263956\n",
      "Iteration 4336, loss = 0.47267369\n",
      "Iteration 4337, loss = 0.47220953\n",
      "Iteration 4338, loss = 0.47230483\n",
      "Iteration 4339, loss = 0.47223078\n",
      "Iteration 4340, loss = 0.47238811\n",
      "Iteration 4341, loss = 0.47250242\n",
      "Iteration 4342, loss = 0.47276685\n",
      "Iteration 4343, loss = 0.47254417\n",
      "Iteration 4344, loss = 0.47254526\n",
      "Iteration 4345, loss = 0.47234797\n",
      "Iteration 4346, loss = 0.47238173\n",
      "Iteration 4347, loss = 0.47221278\n",
      "Iteration 4348, loss = 0.47347705\n",
      "Iteration 4349, loss = 0.47336789\n",
      "Iteration 4350, loss = 0.47257405\n",
      "Iteration 4351, loss = 0.47217429\n",
      "Iteration 4352, loss = 0.47214441\n",
      "Iteration 4353, loss = 0.47216128\n",
      "Iteration 4354, loss = 0.47217194\n",
      "Iteration 4355, loss = 0.47224757\n",
      "Iteration 4356, loss = 0.47217221\n",
      "Iteration 4357, loss = 0.47206238\n",
      "Iteration 4358, loss = 0.47214450\n",
      "Iteration 4359, loss = 0.47213377\n",
      "Iteration 4360, loss = 0.47192824\n",
      "Iteration 4361, loss = 0.47193183\n",
      "Iteration 4362, loss = 0.47231281\n",
      "Iteration 4363, loss = 0.47206879\n",
      "Iteration 4364, loss = 0.47252789\n",
      "Iteration 4365, loss = 0.47225788\n",
      "Iteration 4366, loss = 0.47236820\n",
      "Iteration 4367, loss = 0.47224266\n",
      "Iteration 4368, loss = 0.47205630\n",
      "Iteration 4369, loss = 0.47192278\n",
      "Iteration 4370, loss = 0.47199519\n",
      "Iteration 4371, loss = 0.47223516\n",
      "Iteration 4372, loss = 0.47227906\n",
      "Iteration 4373, loss = 0.47207192\n",
      "Iteration 4374, loss = 0.47217918\n",
      "Iteration 4375, loss = 0.47210720\n",
      "Iteration 4376, loss = 0.47202847\n",
      "Iteration 4377, loss = 0.47203539\n",
      "Iteration 4378, loss = 0.47205508\n",
      "Iteration 4379, loss = 0.47273703\n",
      "Iteration 4380, loss = 0.47226200\n",
      "Iteration 4381, loss = 0.47205553\n",
      "Iteration 4382, loss = 0.47219014\n",
      "Iteration 4383, loss = 0.47206544\n",
      "Iteration 4384, loss = 0.47202612\n",
      "Iteration 4385, loss = 0.47200585\n",
      "Iteration 4386, loss = 0.47210939\n",
      "Iteration 4387, loss = 0.47195241\n",
      "Iteration 4388, loss = 0.47246607\n",
      "Iteration 4389, loss = 0.47292777\n",
      "Iteration 4390, loss = 0.47217957\n",
      "Iteration 4391, loss = 0.47215314\n",
      "Iteration 4392, loss = 0.47177003\n",
      "Iteration 4393, loss = 0.47234672\n",
      "Iteration 4394, loss = 0.47253056\n",
      "Iteration 4395, loss = 0.47251079\n",
      "Iteration 4396, loss = 0.47201676\n",
      "Iteration 4397, loss = 0.47220063\n",
      "Iteration 4398, loss = 0.47209326\n",
      "Iteration 4399, loss = 0.47200337\n",
      "Iteration 4400, loss = 0.47203477\n",
      "Iteration 4401, loss = 0.47209413\n",
      "Iteration 4402, loss = 0.47218796\n",
      "Iteration 4403, loss = 0.47244088\n",
      "Iteration 4404, loss = 0.47185503\n",
      "Iteration 4405, loss = 0.47240018\n",
      "Iteration 4406, loss = 0.47225118\n",
      "Iteration 4407, loss = 0.47207000\n",
      "Iteration 4408, loss = 0.47186405\n",
      "Iteration 4409, loss = 0.47189195\n",
      "Iteration 4410, loss = 0.47215818\n",
      "Iteration 4411, loss = 0.47266148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4412, loss = 0.47253967\n",
      "Iteration 4413, loss = 0.47216218\n",
      "Iteration 4414, loss = 0.47181305\n",
      "Iteration 4415, loss = 0.47188805\n",
      "Iteration 4416, loss = 0.47313309\n",
      "Iteration 4417, loss = 0.47300471\n",
      "Iteration 4418, loss = 0.47241799\n",
      "Iteration 4419, loss = 0.47224048\n",
      "Iteration 4420, loss = 0.47238376\n",
      "Iteration 4421, loss = 0.47239904\n",
      "Iteration 4422, loss = 0.47234678\n",
      "Iteration 4423, loss = 0.47210767\n",
      "Iteration 4424, loss = 0.47188806\n",
      "Iteration 4425, loss = 0.47212513\n",
      "Iteration 4426, loss = 0.47241082\n",
      "Iteration 4427, loss = 0.47248200\n",
      "Iteration 4428, loss = 0.47210573\n",
      "Iteration 4429, loss = 0.47182661\n",
      "Iteration 4430, loss = 0.47184806\n",
      "Iteration 4431, loss = 0.47249873\n",
      "Iteration 4432, loss = 0.47288545\n",
      "Iteration 4433, loss = 0.47265384\n",
      "Iteration 4434, loss = 0.47216772\n",
      "Iteration 4435, loss = 0.47173304\n",
      "Iteration 4436, loss = 0.47168018\n",
      "Iteration 4437, loss = 0.47372872\n",
      "Iteration 4438, loss = 0.47425212\n",
      "Iteration 4439, loss = 0.47348236\n",
      "Iteration 4440, loss = 0.47275590\n",
      "Iteration 4441, loss = 0.47248733\n",
      "Iteration 4442, loss = 0.47187830\n",
      "Iteration 4443, loss = 0.47259932\n",
      "Iteration 4444, loss = 0.47247328\n",
      "Iteration 4445, loss = 0.47193435\n",
      "Iteration 4446, loss = 0.47172135\n",
      "Iteration 4447, loss = 0.47260510\n",
      "Iteration 4448, loss = 0.47269503\n",
      "Iteration 4449, loss = 0.47248046\n",
      "Iteration 4450, loss = 0.47219100\n",
      "Iteration 4451, loss = 0.47186787\n",
      "Iteration 4452, loss = 0.47202767\n",
      "Iteration 4453, loss = 0.47250459\n",
      "Iteration 4454, loss = 0.47260650\n",
      "Iteration 4455, loss = 0.47222243\n",
      "Iteration 4456, loss = 0.47198956\n",
      "Iteration 4457, loss = 0.47198541\n",
      "Iteration 4458, loss = 0.47226949\n",
      "Iteration 4459, loss = 0.47197880\n",
      "Iteration 4460, loss = 0.47225448\n",
      "Iteration 4461, loss = 0.47199588\n",
      "Iteration 4462, loss = 0.47204832\n",
      "Iteration 4463, loss = 0.47195180\n",
      "Iteration 4464, loss = 0.47193993\n",
      "Iteration 4465, loss = 0.47190248\n",
      "Iteration 4466, loss = 0.47191188\n",
      "Iteration 4467, loss = 0.47190616\n",
      "Iteration 4468, loss = 0.47196909\n",
      "Iteration 4469, loss = 0.47206914\n",
      "Iteration 4470, loss = 0.47210632\n",
      "Iteration 4471, loss = 0.47201076\n",
      "Iteration 4472, loss = 0.47215458\n",
      "Iteration 4473, loss = 0.47234556\n",
      "Iteration 4474, loss = 0.47196426\n",
      "Iteration 4475, loss = 0.47171918\n",
      "Iteration 4476, loss = 0.47196151\n",
      "Iteration 4477, loss = 0.47253994\n",
      "Iteration 4478, loss = 0.47277391\n",
      "Iteration 4479, loss = 0.47268754\n",
      "Iteration 4480, loss = 0.47233941\n",
      "Iteration 4481, loss = 0.47214559\n",
      "Iteration 4482, loss = 0.47195494\n",
      "Iteration 4483, loss = 0.47195857\n",
      "Iteration 4484, loss = 0.47253371\n",
      "Iteration 4485, loss = 0.47262755\n",
      "Iteration 4486, loss = 0.47281105\n",
      "Iteration 4487, loss = 0.47199159\n",
      "Iteration 4488, loss = 0.47189510\n",
      "Iteration 4489, loss = 0.47196625\n",
      "Iteration 4490, loss = 0.47220382\n",
      "Iteration 4491, loss = 0.47220650\n",
      "Iteration 4492, loss = 0.47204356\n",
      "Iteration 4493, loss = 0.47191230\n",
      "Iteration 4494, loss = 0.47198605\n",
      "Iteration 4495, loss = 0.47199474\n",
      "Iteration 4496, loss = 0.47213821\n",
      "Iteration 4497, loss = 0.47208520\n",
      "Iteration 4498, loss = 0.47186317\n",
      "Iteration 4499, loss = 0.47164610\n",
      "Iteration 4500, loss = 0.47259553\n",
      "Iteration 4501, loss = 0.47313622\n",
      "Iteration 4502, loss = 0.47306988\n",
      "Iteration 4503, loss = 0.47236789\n",
      "Iteration 4504, loss = 0.47170873\n",
      "Iteration 4505, loss = 0.47175223\n",
      "Iteration 4506, loss = 0.47247376\n",
      "Iteration 4507, loss = 0.47368271\n",
      "Iteration 4508, loss = 0.47350478\n",
      "Iteration 4509, loss = 0.47286385\n",
      "Iteration 4510, loss = 0.47196523\n",
      "Iteration 4511, loss = 0.47182014\n",
      "Iteration 4512, loss = 0.47244738\n",
      "Iteration 4513, loss = 0.47282060\n",
      "Iteration 4514, loss = 0.47301484\n",
      "Iteration 4515, loss = 0.47276535\n",
      "Iteration 4516, loss = 0.47236822\n",
      "Iteration 4517, loss = 0.47181718\n",
      "Iteration 4518, loss = 0.47287291\n",
      "Iteration 4519, loss = 0.47279336\n",
      "Iteration 4520, loss = 0.47247778\n",
      "Iteration 4521, loss = 0.47228533\n",
      "Iteration 4522, loss = 0.47219828\n",
      "Iteration 4523, loss = 0.47209719\n",
      "Iteration 4524, loss = 0.47205386\n",
      "Iteration 4525, loss = 0.47206302\n",
      "Iteration 4526, loss = 0.47192128\n",
      "Iteration 4527, loss = 0.47206681\n",
      "Iteration 4528, loss = 0.47220136\n",
      "Iteration 4529, loss = 0.47225077\n",
      "Iteration 4530, loss = 0.47243809\n",
      "Iteration 4531, loss = 0.47193737\n",
      "Iteration 4532, loss = 0.47199353\n",
      "Iteration 4533, loss = 0.47199073\n",
      "Iteration 4534, loss = 0.47186478\n",
      "Iteration 4535, loss = 0.47290242\n",
      "Iteration 4536, loss = 0.47220560\n",
      "Iteration 4537, loss = 0.47203395\n",
      "Iteration 4538, loss = 0.47201248\n",
      "Iteration 4539, loss = 0.47190021\n",
      "Iteration 4540, loss = 0.47186967\n",
      "Iteration 4541, loss = 0.47208768\n",
      "Iteration 4542, loss = 0.47217745\n",
      "Iteration 4543, loss = 0.47194640\n",
      "Iteration 4544, loss = 0.47219023\n",
      "Iteration 4545, loss = 0.47204235\n",
      "Iteration 4546, loss = 0.47222187\n",
      "Iteration 4547, loss = 0.47196454\n",
      "Iteration 4548, loss = 0.47205920\n",
      "Iteration 4549, loss = 0.47187161\n",
      "Iteration 4550, loss = 0.47194063\n",
      "Iteration 4551, loss = 0.47219142\n",
      "Iteration 4552, loss = 0.47234055\n",
      "Iteration 4553, loss = 0.47239735\n",
      "Iteration 4554, loss = 0.47237156\n",
      "Iteration 4555, loss = 0.47187875\n",
      "Iteration 4556, loss = 0.47174746\n",
      "Iteration 4557, loss = 0.47238994\n",
      "Iteration 4558, loss = 0.47316362\n",
      "Iteration 4559, loss = 0.47313534\n",
      "Iteration 4560, loss = 0.47247044\n",
      "Iteration 4561, loss = 0.47185904\n",
      "Iteration 4562, loss = 0.47200192\n",
      "Iteration 4563, loss = 0.47230336\n",
      "Iteration 4564, loss = 0.47273444\n",
      "Iteration 4565, loss = 0.47258167\n",
      "Iteration 4566, loss = 0.47270161\n",
      "Iteration 4567, loss = 0.47218248\n",
      "Iteration 4568, loss = 0.47197066\n",
      "Iteration 4569, loss = 0.47180016\n",
      "Iteration 4570, loss = 0.47197119\n",
      "Iteration 4571, loss = 0.47210298\n",
      "Iteration 4572, loss = 0.47205033\n",
      "Iteration 4573, loss = 0.47252842\n",
      "Iteration 4574, loss = 0.47203374\n",
      "Iteration 4575, loss = 0.47244118\n",
      "Iteration 4576, loss = 0.47209715\n",
      "Iteration 4577, loss = 0.47195049\n",
      "Iteration 4578, loss = 0.47189721\n",
      "Iteration 4579, loss = 0.47197967\n",
      "Iteration 4580, loss = 0.47190922\n",
      "Iteration 4581, loss = 0.47187387\n",
      "Iteration 4582, loss = 0.47212230\n",
      "Iteration 4583, loss = 0.47221625\n",
      "Iteration 4584, loss = 0.47212540\n",
      "Iteration 4585, loss = 0.47195181\n",
      "Iteration 4586, loss = 0.47220434\n",
      "Iteration 4587, loss = 0.47222659\n",
      "Iteration 4588, loss = 0.47184846\n",
      "Iteration 4589, loss = 0.47209563\n",
      "Iteration 4590, loss = 0.47208067\n",
      "Iteration 4591, loss = 0.47196930\n",
      "Iteration 4592, loss = 0.47195155\n",
      "Iteration 4593, loss = 0.47195102\n",
      "Iteration 4594, loss = 0.47190513\n",
      "Iteration 4595, loss = 0.47193476\n",
      "Iteration 4596, loss = 0.47193569\n",
      "Iteration 4597, loss = 0.47195488\n",
      "Iteration 4598, loss = 0.47195135\n",
      "Iteration 4599, loss = 0.47205272\n",
      "Iteration 4600, loss = 0.47204173\n",
      "Iteration 4601, loss = 0.47253584\n",
      "Iteration 4602, loss = 0.47207201\n",
      "Iteration 4603, loss = 0.47291162\n",
      "Iteration 4604, loss = 0.47232041\n",
      "Iteration 4605, loss = 0.47216946\n",
      "Iteration 4606, loss = 0.47216494\n",
      "Iteration 4607, loss = 0.47217232\n",
      "Iteration 4608, loss = 0.47209332\n",
      "Iteration 4609, loss = 0.47202493\n",
      "Iteration 4610, loss = 0.47213168\n",
      "Iteration 4611, loss = 0.47196297\n",
      "Iteration 4612, loss = 0.47195295\n",
      "Iteration 4613, loss = 0.47191139\n",
      "Iteration 4614, loss = 0.47192752\n",
      "Iteration 4615, loss = 0.47273401\n",
      "Iteration 4616, loss = 0.47226571\n",
      "Iteration 4617, loss = 0.47194604\n",
      "Iteration 4618, loss = 0.47185114\n",
      "Iteration 4619, loss = 0.47181385\n",
      "Iteration 4620, loss = 0.47202343\n",
      "Iteration 4621, loss = 0.47187529\n",
      "Iteration 4622, loss = 0.47197111\n",
      "Iteration 4623, loss = 0.47195176\n",
      "Iteration 4624, loss = 0.47183556\n",
      "Iteration 4625, loss = 0.47188108\n",
      "Iteration 4626, loss = 0.47190132\n",
      "Iteration 4627, loss = 0.47240395\n",
      "Iteration 4628, loss = 0.47184752\n",
      "Iteration 4629, loss = 0.47190715\n",
      "Iteration 4630, loss = 0.47207355\n",
      "Iteration 4631, loss = 0.47217312\n",
      "Iteration 4632, loss = 0.47227669\n",
      "Iteration 4633, loss = 0.47208616\n",
      "Iteration 4634, loss = 0.47236242\n",
      "Iteration 4635, loss = 0.47200557\n",
      "Iteration 4636, loss = 0.47238335\n",
      "Iteration 4637, loss = 0.47266155\n",
      "Iteration 4638, loss = 0.47221566\n",
      "Iteration 4639, loss = 0.47203915\n",
      "Iteration 4640, loss = 0.47306064\n",
      "Iteration 4641, loss = 0.47196085\n",
      "Iteration 4642, loss = 0.47184372\n",
      "Iteration 4643, loss = 0.47198928\n",
      "Iteration 4644, loss = 0.47206524\n",
      "Iteration 4645, loss = 0.47206274\n",
      "Iteration 4646, loss = 0.47239650\n",
      "Iteration 4647, loss = 0.47194842\n",
      "Iteration 4648, loss = 0.47208209\n",
      "Iteration 4649, loss = 0.47192337\n",
      "Iteration 4650, loss = 0.47181758\n",
      "Iteration 4651, loss = 0.47174785\n",
      "Iteration 4652, loss = 0.47196563\n",
      "Iteration 4653, loss = 0.47228630\n",
      "Iteration 4654, loss = 0.47242832\n",
      "Iteration 4655, loss = 0.47249981\n",
      "Iteration 4656, loss = 0.47226117\n",
      "Iteration 4657, loss = 0.47171266\n",
      "Iteration 4658, loss = 0.47230404\n",
      "Iteration 4659, loss = 0.47253726\n",
      "Iteration 4660, loss = 0.47238781\n",
      "Iteration 4661, loss = 0.47257387\n",
      "Iteration 4662, loss = 0.47200391\n",
      "Iteration 4663, loss = 0.47224772\n",
      "Iteration 4664, loss = 0.47313396\n",
      "Iteration 4665, loss = 0.47186986\n",
      "Iteration 4666, loss = 0.47216461\n",
      "Iteration 4667, loss = 0.47204958\n",
      "Iteration 4668, loss = 0.47191784\n",
      "Iteration 4669, loss = 0.47193261\n",
      "Iteration 4670, loss = 0.47195869\n",
      "Iteration 4671, loss = 0.47188171\n",
      "Iteration 4672, loss = 0.47186920\n",
      "Iteration 4673, loss = 0.47192701\n",
      "Iteration 4674, loss = 0.47195891\n",
      "Iteration 4675, loss = 0.47214791\n",
      "Iteration 4676, loss = 0.47194194\n",
      "Iteration 4677, loss = 0.47217061\n",
      "Iteration 4678, loss = 0.47214607\n",
      "Iteration 4679, loss = 0.47204472\n",
      "Iteration 4680, loss = 0.47189326\n",
      "Iteration 4681, loss = 0.47169480\n",
      "Iteration 4682, loss = 0.47214466\n",
      "Iteration 4683, loss = 0.47246870\n",
      "Iteration 4684, loss = 0.47265575\n",
      "Iteration 4685, loss = 0.47242085\n",
      "Iteration 4686, loss = 0.47211355\n",
      "Iteration 4687, loss = 0.47175109\n",
      "Iteration 4688, loss = 0.47345039\n",
      "Iteration 4689, loss = 0.47250464\n",
      "Iteration 4690, loss = 0.47213806\n",
      "Iteration 4691, loss = 0.47219301\n",
      "Iteration 4692, loss = 0.47188163\n",
      "Iteration 4693, loss = 0.47194131\n",
      "Iteration 4694, loss = 0.47218206\n",
      "Iteration 4695, loss = 0.47241390\n",
      "Iteration 4696, loss = 0.47236071\n",
      "Iteration 4697, loss = 0.47190491\n",
      "Iteration 4698, loss = 0.47160052\n",
      "Iteration 4699, loss = 0.47193393\n",
      "Iteration 4700, loss = 0.47274909\n",
      "Iteration 4701, loss = 0.47335251\n",
      "Iteration 4702, loss = 0.47268475\n",
      "Iteration 4703, loss = 0.47209147\n",
      "Iteration 4704, loss = 0.47199733\n",
      "Iteration 4705, loss = 0.47227391\n",
      "Iteration 4706, loss = 0.47234199\n",
      "Iteration 4707, loss = 0.47209388\n",
      "Iteration 4708, loss = 0.47177979\n",
      "Iteration 4709, loss = 0.47175263\n",
      "Iteration 4710, loss = 0.47265581\n",
      "Iteration 4711, loss = 0.47266003\n",
      "Iteration 4712, loss = 0.47199019\n",
      "Iteration 4713, loss = 0.47184824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4714, loss = 0.47193093\n",
      "Iteration 4715, loss = 0.47167370\n",
      "Iteration 4716, loss = 0.47196702\n",
      "Iteration 4717, loss = 0.47262073\n",
      "Iteration 4718, loss = 0.47287327\n",
      "Iteration 4719, loss = 0.47222672\n",
      "Iteration 4720, loss = 0.47198926\n",
      "Iteration 4721, loss = 0.47225028\n",
      "Iteration 4722, loss = 0.47194357\n",
      "Iteration 4723, loss = 0.47222520\n",
      "Iteration 4724, loss = 0.47201650\n",
      "Iteration 4725, loss = 0.47204218\n",
      "Iteration 4726, loss = 0.47205032\n",
      "Iteration 4727, loss = 0.47182038\n",
      "Iteration 4728, loss = 0.47209271\n",
      "Iteration 4729, loss = 0.47193966\n",
      "Iteration 4730, loss = 0.47209659\n",
      "Iteration 4731, loss = 0.47188422\n",
      "Iteration 4732, loss = 0.47196338\n",
      "Iteration 4733, loss = 0.47203869\n",
      "Iteration 4734, loss = 0.47225050\n",
      "Iteration 4735, loss = 0.47236937\n",
      "Iteration 4736, loss = 0.47172890\n",
      "Iteration 4737, loss = 0.47164837\n",
      "Iteration 4738, loss = 0.47291115\n",
      "Iteration 4739, loss = 0.47350331\n",
      "Iteration 4740, loss = 0.47326544\n",
      "Iteration 4741, loss = 0.47285217\n",
      "Iteration 4742, loss = 0.47184065\n",
      "Iteration 4743, loss = 0.47204505\n",
      "Iteration 4744, loss = 0.47230979\n",
      "Iteration 4745, loss = 0.47243245\n",
      "Iteration 4746, loss = 0.47257074\n",
      "Iteration 4747, loss = 0.47218714\n",
      "Iteration 4748, loss = 0.47216391\n",
      "Iteration 4749, loss = 0.47222877\n",
      "Iteration 4750, loss = 0.47205017\n",
      "Iteration 4751, loss = 0.47213809\n",
      "Iteration 4752, loss = 0.47186126\n",
      "Iteration 4753, loss = 0.47195805\n",
      "Iteration 4754, loss = 0.47185472\n",
      "Iteration 4755, loss = 0.47186675\n",
      "Iteration 4756, loss = 0.47211307\n",
      "Iteration 4757, loss = 0.47182429\n",
      "Iteration 4758, loss = 0.47244870\n",
      "Iteration 4759, loss = 0.47213189\n",
      "Iteration 4760, loss = 0.47187784\n",
      "Iteration 4761, loss = 0.47305005\n",
      "Iteration 4762, loss = 0.47192632\n",
      "Iteration 4763, loss = 0.47210382\n",
      "Iteration 4764, loss = 0.47180656\n",
      "Iteration 4765, loss = 0.47180834\n",
      "Iteration 4766, loss = 0.47183638\n",
      "Iteration 4767, loss = 0.47194784\n",
      "Iteration 4768, loss = 0.47189864\n",
      "Iteration 4769, loss = 0.47189059\n",
      "Iteration 4770, loss = 0.47191858\n",
      "Iteration 4771, loss = 0.47183578\n",
      "Iteration 4772, loss = 0.47240582\n",
      "Iteration 4773, loss = 0.47240217\n",
      "Iteration 4774, loss = 0.47185575\n",
      "Iteration 4775, loss = 0.47182355\n",
      "Iteration 4776, loss = 0.47182469\n",
      "Iteration 4777, loss = 0.47185207\n",
      "Iteration 4778, loss = 0.47178450\n",
      "Iteration 4779, loss = 0.47179909\n",
      "Iteration 4780, loss = 0.47204548\n",
      "Iteration 4781, loss = 0.47202328\n",
      "Iteration 4782, loss = 0.47227186\n",
      "Iteration 4783, loss = 0.47184570\n",
      "Iteration 4784, loss = 0.47175388\n",
      "Iteration 4785, loss = 0.47191111\n",
      "Iteration 4786, loss = 0.47201688\n",
      "Iteration 4787, loss = 0.47184756\n",
      "Iteration 4788, loss = 0.47188727\n",
      "Iteration 4789, loss = 0.47186636\n",
      "Iteration 4790, loss = 0.47200519\n",
      "Iteration 4791, loss = 0.47208266\n",
      "Iteration 4792, loss = 0.47195459\n",
      "Iteration 4793, loss = 0.47183409\n",
      "Iteration 4794, loss = 0.47199917\n",
      "Iteration 4795, loss = 0.47218968\n",
      "Iteration 4796, loss = 0.47218158\n",
      "Iteration 4797, loss = 0.47223695\n",
      "Iteration 4798, loss = 0.47221402\n",
      "Iteration 4799, loss = 0.47198986\n",
      "Iteration 4800, loss = 0.47186812\n",
      "Iteration 4801, loss = 0.47195332\n",
      "Iteration 4802, loss = 0.47180409\n",
      "Iteration 4803, loss = 0.47190762\n",
      "Iteration 4804, loss = 0.47201881\n",
      "Iteration 4805, loss = 0.47224869\n",
      "Iteration 4806, loss = 0.47209238\n",
      "Iteration 4807, loss = 0.47275361\n",
      "Iteration 4808, loss = 0.47201549\n",
      "Iteration 4809, loss = 0.47214509\n",
      "Iteration 4810, loss = 0.47216883\n",
      "Iteration 4811, loss = 0.47214505\n",
      "Iteration 4812, loss = 0.47209609\n",
      "Iteration 4813, loss = 0.47214212\n",
      "Iteration 4814, loss = 0.47212315\n",
      "Iteration 4815, loss = 0.47224668\n",
      "Iteration 4816, loss = 0.47292784\n",
      "Iteration 4817, loss = 0.47233092\n",
      "Iteration 4818, loss = 0.47217362\n",
      "Iteration 4819, loss = 0.47188085\n",
      "Iteration 4820, loss = 0.47175962\n",
      "Iteration 4821, loss = 0.47194884\n",
      "Iteration 4822, loss = 0.47207792\n",
      "Iteration 4823, loss = 0.47210042\n",
      "Iteration 4824, loss = 0.47189947\n",
      "Iteration 4825, loss = 0.47187407\n",
      "Iteration 4826, loss = 0.47174563\n",
      "Iteration 4827, loss = 0.47202055\n",
      "Iteration 4828, loss = 0.47206143\n",
      "Iteration 4829, loss = 0.47197902\n",
      "Iteration 4830, loss = 0.47193283\n",
      "Iteration 4831, loss = 0.47174246\n",
      "Iteration 4832, loss = 0.47179044\n",
      "Iteration 4833, loss = 0.47194428\n",
      "Iteration 4834, loss = 0.47213242\n",
      "Iteration 4835, loss = 0.47214001\n",
      "Iteration 4836, loss = 0.47187233\n",
      "Iteration 4837, loss = 0.47179457\n",
      "Iteration 4838, loss = 0.47188082\n",
      "Iteration 4839, loss = 0.47190127\n",
      "Iteration 4840, loss = 0.47179653\n",
      "Iteration 4841, loss = 0.47202020\n",
      "Iteration 4842, loss = 0.47183160\n",
      "Iteration 4843, loss = 0.47214498\n",
      "Iteration 4844, loss = 0.47195202\n",
      "Iteration 4845, loss = 0.47188040\n",
      "Iteration 4846, loss = 0.47213396\n",
      "Iteration 4847, loss = 0.47187469\n",
      "Iteration 4848, loss = 0.47187615\n",
      "Iteration 4849, loss = 0.47217241\n",
      "Iteration 4850, loss = 0.47199080\n",
      "Iteration 4851, loss = 0.47173797\n",
      "Iteration 4852, loss = 0.47180273\n",
      "Iteration 4853, loss = 0.47201299\n",
      "Iteration 4854, loss = 0.47207427\n",
      "Iteration 4855, loss = 0.47213183\n",
      "Iteration 4856, loss = 0.47204261\n",
      "Iteration 4857, loss = 0.47190622\n",
      "Iteration 4858, loss = 0.47173602\n",
      "Iteration 4859, loss = 0.47193514\n",
      "Iteration 4860, loss = 0.47214890\n",
      "Iteration 4861, loss = 0.47223948\n",
      "Iteration 4862, loss = 0.47197985\n",
      "Iteration 4863, loss = 0.47171919\n",
      "Iteration 4864, loss = 0.47178141\n",
      "Iteration 4865, loss = 0.47190917\n",
      "Iteration 4866, loss = 0.47215851\n",
      "Iteration 4867, loss = 0.47188287\n",
      "Iteration 4868, loss = 0.47166924\n",
      "Iteration 4869, loss = 0.47434492\n",
      "Iteration 4870, loss = 0.47219233\n",
      "Iteration 4871, loss = 0.47185652\n",
      "Iteration 4872, loss = 0.47174959\n",
      "Iteration 4873, loss = 0.47189786\n",
      "Iteration 4874, loss = 0.47238330\n",
      "Iteration 4875, loss = 0.47257918\n",
      "Iteration 4876, loss = 0.47179765\n",
      "Iteration 4877, loss = 0.47221207\n",
      "Iteration 4878, loss = 0.47181452\n",
      "Iteration 4879, loss = 0.47179114\n",
      "Iteration 4880, loss = 0.47174341\n",
      "Iteration 4881, loss = 0.47186774\n",
      "Iteration 4882, loss = 0.47204516\n",
      "Iteration 4883, loss = 0.47236497\n",
      "Iteration 4884, loss = 0.47235832\n",
      "Iteration 4885, loss = 0.47205271\n",
      "Iteration 4886, loss = 0.47270088\n",
      "Iteration 4887, loss = 0.47178196\n",
      "Iteration 4888, loss = 0.47180096\n",
      "Iteration 4889, loss = 0.47197696\n",
      "Iteration 4890, loss = 0.47174395\n",
      "Iteration 4891, loss = 0.47181010\n",
      "Iteration 4892, loss = 0.47260885\n",
      "Iteration 4893, loss = 0.47192964\n",
      "Iteration 4894, loss = 0.47195206\n",
      "Iteration 4895, loss = 0.47193870\n",
      "Iteration 4896, loss = 0.47253042\n",
      "Iteration 4897, loss = 0.47326335\n",
      "Iteration 4898, loss = 0.47320128\n",
      "Iteration 4899, loss = 0.47298781\n",
      "Iteration 4900, loss = 0.47168674\n",
      "Iteration 4901, loss = 0.47181442\n",
      "Iteration 4902, loss = 0.47207358\n",
      "Iteration 4903, loss = 0.47255711\n",
      "Iteration 4904, loss = 0.47267510\n",
      "Iteration 4905, loss = 0.47256487\n",
      "Iteration 4906, loss = 0.47246382\n",
      "Iteration 4907, loss = 0.47189475\n",
      "Iteration 4908, loss = 0.47170746\n",
      "Iteration 4909, loss = 0.47160539\n",
      "Iteration 4910, loss = 0.47310854\n",
      "Iteration 4911, loss = 0.47274819\n",
      "Iteration 4912, loss = 0.47207190\n",
      "Iteration 4913, loss = 0.47244503\n",
      "Iteration 4914, loss = 0.47202937\n",
      "Iteration 4915, loss = 0.47212791\n",
      "Iteration 4916, loss = 0.47195382\n",
      "Iteration 4917, loss = 0.47290160\n",
      "Iteration 4918, loss = 0.47183427\n",
      "Iteration 4919, loss = 0.47188931\n",
      "Iteration 4920, loss = 0.47202862\n",
      "Iteration 4921, loss = 0.47187318\n",
      "Iteration 4922, loss = 0.47185752\n",
      "Iteration 4923, loss = 0.47178145\n",
      "Iteration 4924, loss = 0.47181308\n",
      "Iteration 4925, loss = 0.47177788\n",
      "Iteration 4926, loss = 0.47179777\n",
      "Iteration 4927, loss = 0.47179007\n",
      "Iteration 4928, loss = 0.47187530\n",
      "Iteration 4929, loss = 0.47198177\n",
      "Iteration 4930, loss = 0.47176803\n",
      "Iteration 4931, loss = 0.47182146\n",
      "Iteration 4932, loss = 0.47192760\n",
      "Iteration 4933, loss = 0.47231306\n",
      "Iteration 4934, loss = 0.47205952\n",
      "Iteration 4935, loss = 0.47177882\n",
      "Iteration 4936, loss = 0.47238857\n",
      "Iteration 4937, loss = 0.47186380\n",
      "Iteration 4938, loss = 0.47195936\n",
      "Iteration 4939, loss = 0.47180592\n",
      "Iteration 4940, loss = 0.47180406\n",
      "Iteration 4941, loss = 0.47183528\n",
      "Iteration 4942, loss = 0.47193346\n",
      "Iteration 4943, loss = 0.47177662\n",
      "Iteration 4944, loss = 0.47180390\n",
      "Iteration 4945, loss = 0.47177128\n",
      "Iteration 4946, loss = 0.47189364\n",
      "Iteration 4947, loss = 0.47190837\n",
      "Iteration 4948, loss = 0.47220997\n",
      "Iteration 4949, loss = 0.47186335\n",
      "Iteration 4950, loss = 0.47181553\n",
      "Iteration 4951, loss = 0.47178109\n",
      "Iteration 4952, loss = 0.47211434\n",
      "Iteration 4953, loss = 0.47193058\n",
      "Iteration 4954, loss = 0.47212707\n",
      "Iteration 4955, loss = 0.47188576\n",
      "Iteration 4956, loss = 0.47167997\n",
      "Iteration 4957, loss = 0.47282496\n",
      "Iteration 4958, loss = 0.47200622\n",
      "Iteration 4959, loss = 0.47170161\n",
      "Iteration 4960, loss = 0.47197853\n",
      "Iteration 4961, loss = 0.47190092\n",
      "Iteration 4962, loss = 0.47199620\n",
      "Iteration 4963, loss = 0.47197700\n",
      "Iteration 4964, loss = 0.47205397\n",
      "Iteration 4965, loss = 0.47190654\n",
      "Iteration 4966, loss = 0.47181577\n",
      "Iteration 4967, loss = 0.47171434\n",
      "Iteration 4968, loss = 0.47172404\n",
      "Iteration 4969, loss = 0.47217602\n",
      "Iteration 4970, loss = 0.47231731\n",
      "Iteration 4971, loss = 0.47211098\n",
      "Iteration 4972, loss = 0.47203809\n",
      "Iteration 4973, loss = 0.47188763\n",
      "Iteration 4974, loss = 0.47200068\n",
      "Iteration 4975, loss = 0.47177468\n",
      "Iteration 4976, loss = 0.47254917\n",
      "Iteration 4977, loss = 0.47205342\n",
      "Iteration 4978, loss = 0.47173352\n",
      "Iteration 4979, loss = 0.47183553\n",
      "Iteration 4980, loss = 0.47188535\n",
      "Iteration 4981, loss = 0.47201056\n",
      "Iteration 4982, loss = 0.47199016\n",
      "Iteration 4983, loss = 0.47197998\n",
      "Iteration 4984, loss = 0.47179419\n",
      "Iteration 4985, loss = 0.47183270\n",
      "Iteration 4986, loss = 0.47176853\n",
      "Iteration 4987, loss = 0.47181154\n",
      "Iteration 4988, loss = 0.47189324\n",
      "Iteration 4989, loss = 0.47184566\n",
      "Iteration 4990, loss = 0.47177275\n",
      "Iteration 4991, loss = 0.47179696\n",
      "Iteration 4992, loss = 0.47173298\n",
      "Iteration 4993, loss = 0.47176300\n",
      "Iteration 4994, loss = 0.47176683\n",
      "Iteration 4995, loss = 0.47175325\n",
      "Iteration 4996, loss = 0.47167010\n",
      "Iteration 4997, loss = 0.47164539\n",
      "Iteration 4998, loss = 0.47190237\n",
      "Iteration 4999, loss = 0.47207783\n",
      "Iteration 5000, loss = 0.47278936\n",
      "Iteration 5001, loss = 0.47217353\n",
      "Iteration 5002, loss = 0.47213776\n",
      "Iteration 5003, loss = 0.47187880\n",
      "Iteration 5004, loss = 0.47209180\n",
      "Iteration 5005, loss = 0.47226575\n",
      "Iteration 5006, loss = 0.47231330\n",
      "Iteration 5007, loss = 0.47197794\n",
      "Iteration 5008, loss = 0.47176945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5009, loss = 0.47172758\n",
      "Iteration 5010, loss = 0.47241503\n",
      "Iteration 5011, loss = 0.47316081\n",
      "Iteration 5012, loss = 0.47190917\n",
      "Iteration 5013, loss = 0.47213971\n",
      "Iteration 5014, loss = 0.47210958\n",
      "Iteration 5015, loss = 0.47187253\n",
      "Iteration 5016, loss = 0.47254245\n",
      "Iteration 5017, loss = 0.47176490\n",
      "Iteration 5018, loss = 0.47185079\n",
      "Iteration 5019, loss = 0.47177958\n",
      "Iteration 5020, loss = 0.47179394\n",
      "Iteration 5021, loss = 0.47165288\n",
      "Iteration 5022, loss = 0.47179270\n",
      "Iteration 5023, loss = 0.47195850\n",
      "Iteration 5024, loss = 0.47200943\n",
      "Iteration 5025, loss = 0.47183309\n",
      "Iteration 5026, loss = 0.47182149\n",
      "Iteration 5027, loss = 0.47200364\n",
      "Iteration 5028, loss = 0.47190580\n",
      "Iteration 5029, loss = 0.47195915\n",
      "Iteration 5030, loss = 0.47167055\n",
      "Iteration 5031, loss = 0.47167145\n",
      "Iteration 5032, loss = 0.47174123\n",
      "Iteration 5033, loss = 0.47181294\n",
      "Iteration 5034, loss = 0.47174091\n",
      "Iteration 5035, loss = 0.47171142\n",
      "Iteration 5036, loss = 0.47180775\n",
      "Iteration 5037, loss = 0.47176505\n",
      "Iteration 5038, loss = 0.47173351\n",
      "Iteration 5039, loss = 0.47177265\n",
      "Iteration 5040, loss = 0.47225786\n",
      "Iteration 5041, loss = 0.47186781\n",
      "Iteration 5042, loss = 0.47170316\n",
      "Iteration 5043, loss = 0.47190371\n",
      "Iteration 5044, loss = 0.47208087\n",
      "Iteration 5045, loss = 0.47201396\n",
      "Iteration 5046, loss = 0.47181350\n",
      "Iteration 5047, loss = 0.47183375\n",
      "Iteration 5048, loss = 0.47184326\n",
      "Iteration 5049, loss = 0.47181644\n",
      "Iteration 5050, loss = 0.47203281\n",
      "Iteration 5051, loss = 0.47194000\n",
      "Iteration 5052, loss = 0.47179769\n",
      "Iteration 5053, loss = 0.47198414\n",
      "Iteration 5054, loss = 0.47178706\n",
      "Iteration 5055, loss = 0.47165451\n",
      "Iteration 5056, loss = 0.47167604\n",
      "Iteration 5057, loss = 0.47218183\n",
      "Iteration 5058, loss = 0.47202063\n",
      "Iteration 5059, loss = 0.47255510\n",
      "Iteration 5060, loss = 0.47176432\n",
      "Iteration 5061, loss = 0.47167622\n",
      "Iteration 5062, loss = 0.47175485\n",
      "Iteration 5063, loss = 0.47184709\n",
      "Iteration 5064, loss = 0.47191968\n",
      "Iteration 5065, loss = 0.47218798\n",
      "Iteration 5066, loss = 0.47177655\n",
      "Iteration 5067, loss = 0.47184669\n",
      "Iteration 5068, loss = 0.47173933\n",
      "Iteration 5069, loss = 0.47175050\n",
      "Iteration 5070, loss = 0.47169146\n",
      "Iteration 5071, loss = 0.47188333\n",
      "Iteration 5072, loss = 0.47179984\n",
      "Iteration 5073, loss = 0.47163132\n",
      "Iteration 5074, loss = 0.47205505\n",
      "Iteration 5075, loss = 0.47190639\n",
      "Iteration 5076, loss = 0.47167807\n",
      "Iteration 5077, loss = 0.47150045\n",
      "Iteration 5078, loss = 0.47204852\n",
      "Iteration 5079, loss = 0.47248485\n",
      "Iteration 5080, loss = 0.47241878\n",
      "Iteration 5081, loss = 0.47187393\n",
      "Iteration 5082, loss = 0.47206564\n",
      "Iteration 5083, loss = 0.47174721\n",
      "Iteration 5084, loss = 0.47171962\n",
      "Iteration 5085, loss = 0.47169935\n",
      "Iteration 5086, loss = 0.47207545\n",
      "Iteration 5087, loss = 0.47166897\n",
      "Iteration 5088, loss = 0.47198792\n",
      "Iteration 5089, loss = 0.47193079\n",
      "Iteration 5090, loss = 0.47189646\n",
      "Iteration 5091, loss = 0.47200083\n",
      "Iteration 5092, loss = 0.47170438\n",
      "Iteration 5093, loss = 0.47168371\n",
      "Iteration 5094, loss = 0.47186819\n",
      "Iteration 5095, loss = 0.47184503\n",
      "Iteration 5096, loss = 0.47180045\n",
      "Iteration 5097, loss = 0.47176394\n",
      "Iteration 5098, loss = 0.47184964\n",
      "Iteration 5099, loss = 0.47174849\n",
      "Iteration 5100, loss = 0.47178789\n",
      "Iteration 5101, loss = 0.47166546\n",
      "Iteration 5102, loss = 0.47163329\n",
      "Iteration 5103, loss = 0.47167708\n",
      "Iteration 5104, loss = 0.47177428\n",
      "Iteration 5105, loss = 0.47200929\n",
      "Iteration 5106, loss = 0.47174010\n",
      "Iteration 5107, loss = 0.47196377\n",
      "Iteration 5108, loss = 0.47165456\n",
      "Iteration 5109, loss = 0.47168019\n",
      "Iteration 5110, loss = 0.47172831\n",
      "Iteration 5111, loss = 0.47163097\n",
      "Iteration 5112, loss = 0.47159267\n",
      "Iteration 5113, loss = 0.47186860\n",
      "Iteration 5114, loss = 0.47182808\n",
      "Iteration 5115, loss = 0.47212497\n",
      "Iteration 5116, loss = 0.47227518\n",
      "Iteration 5117, loss = 0.47237477\n",
      "Iteration 5118, loss = 0.47158621\n",
      "Iteration 5119, loss = 0.47221411\n",
      "Iteration 5120, loss = 0.47239918\n",
      "Iteration 5121, loss = 0.47216814\n",
      "Iteration 5122, loss = 0.47160417\n",
      "Iteration 5123, loss = 0.47269218\n",
      "Iteration 5124, loss = 0.47192035\n",
      "Iteration 5125, loss = 0.47187531\n",
      "Iteration 5126, loss = 0.47160259\n",
      "Iteration 5127, loss = 0.47162176\n",
      "Iteration 5128, loss = 0.47178383\n",
      "Iteration 5129, loss = 0.47174488\n",
      "Iteration 5130, loss = 0.47171526\n",
      "Iteration 5131, loss = 0.47168022\n",
      "Iteration 5132, loss = 0.47158121\n",
      "Iteration 5133, loss = 0.47180551\n",
      "Iteration 5134, loss = 0.47189188\n",
      "Iteration 5135, loss = 0.47197880\n",
      "Iteration 5136, loss = 0.47173984\n",
      "Iteration 5137, loss = 0.47194985\n",
      "Iteration 5138, loss = 0.47192518\n",
      "Iteration 5139, loss = 0.47212105\n",
      "Iteration 5140, loss = 0.47179693\n",
      "Iteration 5141, loss = 0.47197135\n",
      "Iteration 5142, loss = 0.47162709\n",
      "Iteration 5143, loss = 0.47186985\n",
      "Iteration 5144, loss = 0.47172455\n",
      "Iteration 5145, loss = 0.47178809\n",
      "Iteration 5146, loss = 0.47173421\n",
      "Iteration 5147, loss = 0.47165723\n",
      "Iteration 5148, loss = 0.47254061\n",
      "Iteration 5149, loss = 0.47181358\n",
      "Iteration 5150, loss = 0.47172680\n",
      "Iteration 5151, loss = 0.47148568\n",
      "Iteration 5152, loss = 0.47206659\n",
      "Iteration 5153, loss = 0.47299703\n",
      "Iteration 5154, loss = 0.47298900\n",
      "Iteration 5155, loss = 0.47220065\n",
      "Iteration 5156, loss = 0.47214350\n",
      "Iteration 5157, loss = 0.47159362\n",
      "Iteration 5158, loss = 0.47165347\n",
      "Iteration 5159, loss = 0.47207720\n",
      "Iteration 5160, loss = 0.47245928\n",
      "Iteration 5161, loss = 0.47168711\n",
      "Iteration 5162, loss = 0.47168026\n",
      "Iteration 5163, loss = 0.47170578\n",
      "Iteration 5164, loss = 0.47168538\n",
      "Iteration 5165, loss = 0.47191089\n",
      "Iteration 5166, loss = 0.47218054\n",
      "Iteration 5167, loss = 0.47225179\n",
      "Iteration 5168, loss = 0.47256619\n",
      "Iteration 5169, loss = 0.47196363\n",
      "Iteration 5170, loss = 0.47176891\n",
      "Iteration 5171, loss = 0.47191339\n",
      "Iteration 5172, loss = 0.47165042\n",
      "Iteration 5173, loss = 0.47183312\n",
      "Iteration 5174, loss = 0.47178505\n",
      "Iteration 5175, loss = 0.47165011\n",
      "Iteration 5176, loss = 0.47207093\n",
      "Iteration 5177, loss = 0.47205138\n",
      "Iteration 5178, loss = 0.47170960\n",
      "Iteration 5179, loss = 0.47207405\n",
      "Iteration 5180, loss = 0.47177532\n",
      "Iteration 5181, loss = 0.47145331\n",
      "Iteration 5182, loss = 0.47221690\n",
      "Iteration 5183, loss = 0.47220413\n",
      "Iteration 5184, loss = 0.47200071\n",
      "Iteration 5185, loss = 0.47200651\n",
      "Iteration 5186, loss = 0.47172056\n",
      "Iteration 5187, loss = 0.47196341\n",
      "Iteration 5188, loss = 0.47239759\n",
      "Iteration 5189, loss = 0.47169600\n",
      "Iteration 5190, loss = 0.47197229\n",
      "Iteration 5191, loss = 0.47155537\n",
      "Iteration 5192, loss = 0.47163958\n",
      "Iteration 5193, loss = 0.47214442\n",
      "Iteration 5194, loss = 0.47251474\n",
      "Iteration 5195, loss = 0.47238540\n",
      "Iteration 5196, loss = 0.47185497\n",
      "Iteration 5197, loss = 0.47221102\n",
      "Iteration 5198, loss = 0.47178438\n",
      "Iteration 5199, loss = 0.47194388\n",
      "Iteration 5200, loss = 0.47181096\n",
      "Iteration 5201, loss = 0.47184660\n",
      "Iteration 5202, loss = 0.47211133\n",
      "Iteration 5203, loss = 0.47228375\n",
      "Iteration 5204, loss = 0.47178901\n",
      "Iteration 5205, loss = 0.47171805\n",
      "Iteration 5206, loss = 0.47191484\n",
      "Iteration 5207, loss = 0.47194350\n",
      "Iteration 5208, loss = 0.47198836\n",
      "Iteration 5209, loss = 0.47211725\n",
      "Iteration 5210, loss = 0.47189778\n",
      "Iteration 5211, loss = 0.47173255\n",
      "Iteration 5212, loss = 0.47167395\n",
      "Iteration 5213, loss = 0.47161887\n",
      "Iteration 5214, loss = 0.47241286\n",
      "Iteration 5215, loss = 0.47160242\n",
      "Iteration 5216, loss = 0.47157165\n",
      "Iteration 5217, loss = 0.47182649\n",
      "Iteration 5218, loss = 0.47206936\n",
      "Iteration 5219, loss = 0.47232865\n",
      "Iteration 5220, loss = 0.47183431\n",
      "Iteration 5221, loss = 0.47172950\n",
      "Iteration 5222, loss = 0.47180007\n",
      "Iteration 5223, loss = 0.47212036\n",
      "Iteration 5224, loss = 0.47175610\n",
      "Iteration 5225, loss = 0.47162753\n",
      "Iteration 5226, loss = 0.47187641\n",
      "Iteration 5227, loss = 0.47185629\n",
      "Iteration 5228, loss = 0.47219268\n",
      "Iteration 5229, loss = 0.47192136\n",
      "Iteration 5230, loss = 0.47162442\n",
      "Iteration 5231, loss = 0.47179116\n",
      "Iteration 5232, loss = 0.47201901\n",
      "Iteration 5233, loss = 0.47207121\n",
      "Iteration 5234, loss = 0.47216487\n",
      "Iteration 5235, loss = 0.47179626\n",
      "Iteration 5236, loss = 0.47192079\n",
      "Iteration 5237, loss = 0.47181686\n",
      "Iteration 5238, loss = 0.47182977\n",
      "Iteration 5239, loss = 0.47173865\n",
      "Iteration 5240, loss = 0.47155955\n",
      "Iteration 5241, loss = 0.47193663\n",
      "Iteration 5242, loss = 0.47177177\n",
      "Iteration 5243, loss = 0.47193679\n",
      "Iteration 5244, loss = 0.47174353\n",
      "Iteration 5245, loss = 0.47166292\n",
      "Iteration 5246, loss = 0.47173434\n",
      "Iteration 5247, loss = 0.47158202\n",
      "Iteration 5248, loss = 0.47197602\n",
      "Iteration 5249, loss = 0.47171814\n",
      "Iteration 5250, loss = 0.47175582\n",
      "Iteration 5251, loss = 0.47164986\n",
      "Iteration 5252, loss = 0.47192699\n",
      "Iteration 5253, loss = 0.47176966\n",
      "Iteration 5254, loss = 0.47166368\n",
      "Iteration 5255, loss = 0.47170170\n",
      "Iteration 5256, loss = 0.47178165\n",
      "Iteration 5257, loss = 0.47169022\n",
      "Iteration 5258, loss = 0.47175199\n",
      "Iteration 5259, loss = 0.47170598\n",
      "Iteration 5260, loss = 0.47172173\n",
      "Iteration 5261, loss = 0.47200147\n",
      "Iteration 5262, loss = 0.47194712\n",
      "Iteration 5263, loss = 0.47199672\n",
      "Iteration 5264, loss = 0.47216245\n",
      "Iteration 5265, loss = 0.47163166\n",
      "Iteration 5266, loss = 0.47150562\n",
      "Iteration 5267, loss = 0.47199033\n",
      "Iteration 5268, loss = 0.47187585\n",
      "Iteration 5269, loss = 0.47169016\n",
      "Iteration 5270, loss = 0.47147886\n",
      "Iteration 5271, loss = 0.47191694\n",
      "Iteration 5272, loss = 0.47197402\n",
      "Iteration 5273, loss = 0.47205479\n",
      "Iteration 5274, loss = 0.47182833\n",
      "Iteration 5275, loss = 0.47195894\n",
      "Iteration 5276, loss = 0.47180996\n",
      "Iteration 5277, loss = 0.47176315\n",
      "Iteration 5278, loss = 0.47169883\n",
      "Iteration 5279, loss = 0.47164286\n",
      "Iteration 5280, loss = 0.47197853\n",
      "Iteration 5281, loss = 0.47196543\n",
      "Iteration 5282, loss = 0.47169042\n",
      "Iteration 5283, loss = 0.47155655\n",
      "Iteration 5284, loss = 0.47158031\n",
      "Iteration 5285, loss = 0.47192725\n",
      "Iteration 5286, loss = 0.47188921\n",
      "Iteration 5287, loss = 0.47174559\n",
      "Iteration 5288, loss = 0.47136499\n",
      "Iteration 5289, loss = 0.47240525\n",
      "Iteration 5290, loss = 0.47251552\n",
      "Iteration 5291, loss = 0.47255889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5292, loss = 0.47194367\n",
      "Iteration 5293, loss = 0.47165219\n",
      "Iteration 5294, loss = 0.47162614\n",
      "Iteration 5295, loss = 0.47172969\n",
      "Iteration 5296, loss = 0.47169681\n",
      "Iteration 5297, loss = 0.47162940\n",
      "Iteration 5298, loss = 0.47163758\n",
      "Iteration 5299, loss = 0.47165699\n",
      "Iteration 5300, loss = 0.47174205\n",
      "Iteration 5301, loss = 0.47166331\n",
      "Iteration 5302, loss = 0.47160547\n",
      "Iteration 5303, loss = 0.47227502\n",
      "Iteration 5304, loss = 0.47192758\n",
      "Iteration 5305, loss = 0.47152119\n",
      "Iteration 5306, loss = 0.47179584\n",
      "Iteration 5307, loss = 0.47212644\n",
      "Iteration 5308, loss = 0.47210958\n",
      "Iteration 5309, loss = 0.47264654\n",
      "Iteration 5310, loss = 0.47175285\n",
      "Iteration 5311, loss = 0.47181607\n",
      "Iteration 5312, loss = 0.47179927\n",
      "Iteration 5313, loss = 0.47164030\n",
      "Iteration 5314, loss = 0.47156117\n",
      "Iteration 5315, loss = 0.47163122\n",
      "Iteration 5316, loss = 0.47206020\n",
      "Iteration 5317, loss = 0.47220505\n",
      "Iteration 5318, loss = 0.47227058\n",
      "Iteration 5319, loss = 0.47170009\n",
      "Iteration 5320, loss = 0.47134459\n",
      "Iteration 5321, loss = 0.47260531\n",
      "Iteration 5322, loss = 0.47248088\n",
      "Iteration 5323, loss = 0.47221117\n",
      "Iteration 5324, loss = 0.47182510\n",
      "Iteration 5325, loss = 0.47180561\n",
      "Iteration 5326, loss = 0.47171028\n",
      "Iteration 5327, loss = 0.47183958\n",
      "Iteration 5328, loss = 0.47172634\n",
      "Iteration 5329, loss = 0.47160622\n",
      "Iteration 5330, loss = 0.47167680\n",
      "Iteration 5331, loss = 0.47163202\n",
      "Iteration 5332, loss = 0.47168613\n",
      "Iteration 5333, loss = 0.47172834\n",
      "Iteration 5334, loss = 0.47157902\n",
      "Iteration 5335, loss = 0.47160920\n",
      "Iteration 5336, loss = 0.47173576\n",
      "Iteration 5337, loss = 0.47172423\n",
      "Iteration 5338, loss = 0.47161353\n",
      "Iteration 5339, loss = 0.47166478\n",
      "Iteration 5340, loss = 0.47168997\n",
      "Iteration 5341, loss = 0.47164628\n",
      "Iteration 5342, loss = 0.47180582\n",
      "Iteration 5343, loss = 0.47217243\n",
      "Iteration 5344, loss = 0.47200002\n",
      "Iteration 5345, loss = 0.47163887\n",
      "Iteration 5346, loss = 0.47173273\n",
      "Iteration 5347, loss = 0.47171826\n",
      "Iteration 5348, loss = 0.47169133\n",
      "Iteration 5349, loss = 0.47173210\n",
      "Iteration 5350, loss = 0.47183808\n",
      "Iteration 5351, loss = 0.47165833\n",
      "Iteration 5352, loss = 0.47164729\n",
      "Iteration 5353, loss = 0.47163168\n",
      "Iteration 5354, loss = 0.47164913\n",
      "Iteration 5355, loss = 0.47173527\n",
      "Iteration 5356, loss = 0.47204402\n",
      "Iteration 5357, loss = 0.47173968\n",
      "Iteration 5358, loss = 0.47147215\n",
      "Iteration 5359, loss = 0.47160183\n",
      "Iteration 5360, loss = 0.47196618\n",
      "Iteration 5361, loss = 0.47228451\n",
      "Iteration 5362, loss = 0.47191823\n",
      "Iteration 5363, loss = 0.47192639\n",
      "Iteration 5364, loss = 0.47179389\n",
      "Iteration 5365, loss = 0.47187661\n",
      "Iteration 5366, loss = 0.47179056\n",
      "Iteration 5367, loss = 0.47174204\n",
      "Iteration 5368, loss = 0.47157290\n",
      "Iteration 5369, loss = 0.47168555\n",
      "Iteration 5370, loss = 0.47168884\n",
      "Iteration 5371, loss = 0.47178736\n",
      "Iteration 5372, loss = 0.47215883\n",
      "Iteration 5373, loss = 0.47198456\n",
      "Iteration 5374, loss = 0.47201563\n",
      "Iteration 5375, loss = 0.47178599\n",
      "Iteration 5376, loss = 0.47169925\n",
      "Iteration 5377, loss = 0.47167600\n",
      "Iteration 5378, loss = 0.47176671\n",
      "Iteration 5379, loss = 0.47185251\n",
      "Iteration 5380, loss = 0.47176102\n",
      "Iteration 5381, loss = 0.47177389\n",
      "Iteration 5382, loss = 0.47179794\n",
      "Iteration 5383, loss = 0.47176783\n",
      "Iteration 5384, loss = 0.47163563\n",
      "Iteration 5385, loss = 0.47155758\n",
      "Iteration 5386, loss = 0.47156709\n",
      "Iteration 5387, loss = 0.47197522\n",
      "Iteration 5388, loss = 0.47172750\n",
      "Iteration 5389, loss = 0.47159728\n",
      "Iteration 5390, loss = 0.47162879\n",
      "Iteration 5391, loss = 0.47175018\n",
      "Iteration 5392, loss = 0.47165088\n",
      "Iteration 5393, loss = 0.47187512\n",
      "Iteration 5394, loss = 0.47155367\n",
      "Iteration 5395, loss = 0.47202652\n",
      "Iteration 5396, loss = 0.47163460\n",
      "Iteration 5397, loss = 0.47234442\n",
      "Iteration 5398, loss = 0.47184210\n",
      "Iteration 5399, loss = 0.47189267\n",
      "Iteration 5400, loss = 0.47188667\n",
      "Iteration 5401, loss = 0.47176341\n",
      "Iteration 5402, loss = 0.47176433\n",
      "Iteration 5403, loss = 0.47170677\n",
      "Iteration 5404, loss = 0.47180419\n",
      "Iteration 5405, loss = 0.47164873\n",
      "Iteration 5406, loss = 0.47186751\n",
      "Iteration 5407, loss = 0.47186672\n",
      "Iteration 5408, loss = 0.47159763\n",
      "Iteration 5409, loss = 0.47164091\n",
      "Iteration 5410, loss = 0.47159011\n",
      "Iteration 5411, loss = 0.47153764\n",
      "Iteration 5412, loss = 0.47178850\n",
      "Iteration 5413, loss = 0.47205266\n",
      "Iteration 5414, loss = 0.47180050\n",
      "Iteration 5415, loss = 0.47206177\n",
      "Iteration 5416, loss = 0.47170995\n",
      "Iteration 5417, loss = 0.47179019\n",
      "Iteration 5418, loss = 0.47168018\n",
      "Iteration 5419, loss = 0.47181584\n",
      "Iteration 5420, loss = 0.47163161\n",
      "Iteration 5421, loss = 0.47174932\n",
      "Iteration 5422, loss = 0.47166036\n",
      "Iteration 5423, loss = 0.47168770\n",
      "Iteration 5424, loss = 0.47202628\n",
      "Iteration 5425, loss = 0.47135626\n",
      "Iteration 5426, loss = 0.47154500\n",
      "Iteration 5427, loss = 0.47250900\n",
      "Iteration 5428, loss = 0.47215398\n",
      "Iteration 5429, loss = 0.47166405\n",
      "Iteration 5430, loss = 0.47130354\n",
      "Iteration 5431, loss = 0.47258066\n",
      "Iteration 5432, loss = 0.47268970\n",
      "Iteration 5433, loss = 0.47253800\n",
      "Iteration 5434, loss = 0.47175490\n",
      "Iteration 5435, loss = 0.47172595\n",
      "Iteration 5436, loss = 0.47176875\n",
      "Iteration 5437, loss = 0.47332821\n",
      "Iteration 5438, loss = 0.47240188\n",
      "Iteration 5439, loss = 0.47189667\n",
      "Iteration 5440, loss = 0.47129294\n",
      "Iteration 5441, loss = 0.47186235\n",
      "Iteration 5442, loss = 0.47270724\n",
      "Iteration 5443, loss = 0.47342137\n",
      "Iteration 5444, loss = 0.47312677\n",
      "Iteration 5445, loss = 0.47268271\n",
      "Iteration 5446, loss = 0.47227762\n",
      "Iteration 5447, loss = 0.47159399\n",
      "Iteration 5448, loss = 0.47150851\n",
      "Iteration 5449, loss = 0.47179481\n",
      "Iteration 5450, loss = 0.47215342\n",
      "Iteration 5451, loss = 0.47233588\n",
      "Iteration 5452, loss = 0.47207283\n",
      "Iteration 5453, loss = 0.47194344\n",
      "Iteration 5454, loss = 0.47152469\n",
      "Iteration 5455, loss = 0.47156041\n",
      "Iteration 5456, loss = 0.47214512\n",
      "Iteration 5457, loss = 0.47272067\n",
      "Iteration 5458, loss = 0.47249435\n",
      "Iteration 5459, loss = 0.47205046\n",
      "Iteration 5460, loss = 0.47210845\n",
      "Iteration 5461, loss = 0.47182541\n",
      "Iteration 5462, loss = 0.47194185\n",
      "Iteration 5463, loss = 0.47181304\n",
      "Iteration 5464, loss = 0.47178571\n",
      "Iteration 5465, loss = 0.47213399\n",
      "Iteration 5466, loss = 0.47213276\n",
      "Iteration 5467, loss = 0.47191788\n",
      "Iteration 5468, loss = 0.47171383\n",
      "Iteration 5469, loss = 0.47167872\n",
      "Iteration 5470, loss = 0.47208277\n",
      "Iteration 5471, loss = 0.47214478\n",
      "Iteration 5472, loss = 0.47198694\n",
      "Iteration 5473, loss = 0.47259747\n",
      "Iteration 5474, loss = 0.47163306\n",
      "Iteration 5475, loss = 0.47185110\n",
      "Iteration 5476, loss = 0.47192369\n",
      "Iteration 5477, loss = 0.47172386\n",
      "Iteration 5478, loss = 0.47175374\n",
      "Iteration 5479, loss = 0.47170901\n",
      "Iteration 5480, loss = 0.47161036\n",
      "Iteration 5481, loss = 0.47153799\n",
      "Iteration 5482, loss = 0.47174361\n",
      "Iteration 5483, loss = 0.47185212\n",
      "Iteration 5484, loss = 0.47193563\n",
      "Iteration 5485, loss = 0.47207083\n",
      "Iteration 5486, loss = 0.47156071\n",
      "Iteration 5487, loss = 0.47171597\n",
      "Iteration 5488, loss = 0.47187014\n",
      "Iteration 5489, loss = 0.47237792\n",
      "Iteration 5490, loss = 0.47189622\n",
      "Iteration 5491, loss = 0.47216337\n",
      "Iteration 5492, loss = 0.47204023\n",
      "Iteration 5493, loss = 0.47150457\n",
      "Iteration 5494, loss = 0.47205671\n",
      "Iteration 5495, loss = 0.47221596\n",
      "Iteration 5496, loss = 0.47171585\n",
      "Iteration 5497, loss = 0.47152368\n",
      "Iteration 5498, loss = 0.47187741\n",
      "Iteration 5499, loss = 0.47181226\n",
      "Iteration 5500, loss = 0.47186724\n",
      "Iteration 5501, loss = 0.47175332\n",
      "Iteration 5502, loss = 0.47157156\n",
      "Iteration 5503, loss = 0.47211255\n",
      "Iteration 5504, loss = 0.47161651\n",
      "Iteration 5505, loss = 0.47160334\n",
      "Iteration 5506, loss = 0.47156445\n",
      "Iteration 5507, loss = 0.47157248\n",
      "Iteration 5508, loss = 0.47158260\n",
      "Iteration 5509, loss = 0.47158351\n",
      "Iteration 5510, loss = 0.47162429\n",
      "Iteration 5511, loss = 0.47160829\n",
      "Iteration 5512, loss = 0.47158900\n",
      "Iteration 5513, loss = 0.47147217\n",
      "Iteration 5514, loss = 0.47206164\n",
      "Iteration 5515, loss = 0.47169711\n",
      "Iteration 5516, loss = 0.47167695\n",
      "Iteration 5517, loss = 0.47160587\n",
      "Iteration 5518, loss = 0.47181559\n",
      "Iteration 5519, loss = 0.47181115\n",
      "Iteration 5520, loss = 0.47156212\n",
      "Iteration 5521, loss = 0.47179626\n",
      "Iteration 5522, loss = 0.47176591\n",
      "Iteration 5523, loss = 0.47188231\n",
      "Iteration 5524, loss = 0.47185476\n",
      "Iteration 5525, loss = 0.47180725\n",
      "Iteration 5526, loss = 0.47224887\n",
      "Iteration 5527, loss = 0.47179678\n",
      "Iteration 5528, loss = 0.47179544\n",
      "Iteration 5529, loss = 0.47186871\n",
      "Iteration 5530, loss = 0.47185919\n",
      "Iteration 5531, loss = 0.47189487\n",
      "Iteration 5532, loss = 0.47184334\n",
      "Iteration 5533, loss = 0.47164142\n",
      "Iteration 5534, loss = 0.47162334\n",
      "Iteration 5535, loss = 0.47155893\n",
      "Iteration 5536, loss = 0.47169952\n",
      "Iteration 5537, loss = 0.47164804\n",
      "Iteration 5538, loss = 0.47165802\n",
      "Iteration 5539, loss = 0.47175273\n",
      "Iteration 5540, loss = 0.47244366\n",
      "Iteration 5541, loss = 0.47170853\n",
      "Iteration 5542, loss = 0.47169446\n",
      "Iteration 5543, loss = 0.47172428\n",
      "Iteration 5544, loss = 0.47171075\n",
      "Iteration 5545, loss = 0.47159825\n",
      "Iteration 5546, loss = 0.47176940\n",
      "Iteration 5547, loss = 0.47184405\n",
      "Iteration 5548, loss = 0.47180863\n",
      "Iteration 5549, loss = 0.47158448\n",
      "Iteration 5550, loss = 0.47153854\n",
      "Iteration 5551, loss = 0.47183875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5552, loss = 0.47156426\n",
      "Iteration 5553, loss = 0.47165472\n",
      "Iteration 5554, loss = 0.47189568\n",
      "Iteration 5555, loss = 0.47187721\n",
      "Iteration 5556, loss = 0.47166653\n",
      "Iteration 5557, loss = 0.47151276\n",
      "Iteration 5558, loss = 0.47157063\n",
      "Iteration 5559, loss = 0.47202753\n",
      "Iteration 5560, loss = 0.47244925\n",
      "Iteration 5561, loss = 0.47238238\n",
      "Iteration 5562, loss = 0.47187030\n",
      "Iteration 5563, loss = 0.47194282\n",
      "Iteration 5564, loss = 0.47201325\n",
      "Iteration 5565, loss = 0.47172172\n",
      "Iteration 5566, loss = 0.47159277\n",
      "Iteration 5567, loss = 0.47165905\n",
      "Iteration 5568, loss = 0.47167485\n",
      "Iteration 5569, loss = 0.47193107\n",
      "Iteration 5570, loss = 0.47182319\n",
      "Iteration 5571, loss = 0.47187366\n",
      "Iteration 5572, loss = 0.47202684\n",
      "Iteration 5573, loss = 0.47199762\n",
      "Iteration 5574, loss = 0.47222828\n",
      "Iteration 5575, loss = 0.47192293\n",
      "Iteration 5576, loss = 0.47177131\n",
      "Iteration 5577, loss = 0.47157384\n",
      "Iteration 5578, loss = 0.47186894\n",
      "Iteration 5579, loss = 0.47175873\n",
      "Iteration 5580, loss = 0.47173706\n",
      "Iteration 5581, loss = 0.47169659\n",
      "Iteration 5582, loss = 0.47172794\n",
      "Iteration 5583, loss = 0.47211501\n",
      "Iteration 5584, loss = 0.47176767\n",
      "Iteration 5585, loss = 0.47172221\n",
      "Iteration 5586, loss = 0.47185811\n",
      "Iteration 5587, loss = 0.47155419\n",
      "Iteration 5588, loss = 0.47155058\n",
      "Iteration 5589, loss = 0.47169757\n",
      "Iteration 5590, loss = 0.47148517\n",
      "Iteration 5591, loss = 0.47166158\n",
      "Iteration 5592, loss = 0.47159274\n",
      "Iteration 5593, loss = 0.47170254\n",
      "Iteration 5594, loss = 0.47179314\n",
      "Iteration 5595, loss = 0.47150663\n",
      "Iteration 5596, loss = 0.47141812\n",
      "Iteration 5597, loss = 0.47172370\n",
      "Iteration 5598, loss = 0.47194583\n",
      "Iteration 5599, loss = 0.47203819\n",
      "Iteration 5600, loss = 0.47199393\n",
      "Iteration 5601, loss = 0.47196238\n",
      "Iteration 5602, loss = 0.47169925\n",
      "Iteration 5603, loss = 0.47168196\n",
      "Iteration 5604, loss = 0.47181101\n",
      "Iteration 5605, loss = 0.47198603\n",
      "Iteration 5606, loss = 0.47176942\n",
      "Iteration 5607, loss = 0.47149229\n",
      "Iteration 5608, loss = 0.47145286\n",
      "Iteration 5609, loss = 0.47209204\n",
      "Iteration 5610, loss = 0.47229091\n",
      "Iteration 5611, loss = 0.47194655\n",
      "Iteration 5612, loss = 0.47183901\n",
      "Iteration 5613, loss = 0.47163569\n",
      "Iteration 5614, loss = 0.47180500\n",
      "Iteration 5615, loss = 0.47203788\n",
      "Iteration 5616, loss = 0.47179164\n",
      "Iteration 5617, loss = 0.47132877\n",
      "Iteration 5618, loss = 0.47205135\n",
      "Iteration 5619, loss = 0.47217691\n",
      "Iteration 5620, loss = 0.47221409\n",
      "Iteration 5621, loss = 0.47191677\n",
      "Iteration 5622, loss = 0.47180932\n",
      "Iteration 5623, loss = 0.47168047\n",
      "Iteration 5624, loss = 0.47155141\n",
      "Iteration 5625, loss = 0.47169550\n",
      "Iteration 5626, loss = 0.47151911\n",
      "Iteration 5627, loss = 0.47168766\n",
      "Iteration 5628, loss = 0.47183497\n",
      "Iteration 5629, loss = 0.47157805\n",
      "Iteration 5630, loss = 0.47150628\n",
      "Iteration 5631, loss = 0.47159326\n",
      "Iteration 5632, loss = 0.47171232\n",
      "Iteration 5633, loss = 0.47237907\n",
      "Iteration 5634, loss = 0.47187092\n",
      "Iteration 5635, loss = 0.47231791\n",
      "Iteration 5636, loss = 0.47216792\n",
      "Iteration 5637, loss = 0.47159606\n",
      "Iteration 5638, loss = 0.47239120\n",
      "Iteration 5639, loss = 0.47190590\n",
      "Iteration 5640, loss = 0.47265351\n",
      "Iteration 5641, loss = 0.47204870\n",
      "Iteration 5642, loss = 0.47153960\n",
      "Iteration 5643, loss = 0.47129718\n",
      "Iteration 5644, loss = 0.47200792\n",
      "Iteration 5645, loss = 0.47243055\n",
      "Iteration 5646, loss = 0.47210210\n",
      "Iteration 5647, loss = 0.47236756\n",
      "Iteration 5648, loss = 0.47199985\n",
      "Iteration 5649, loss = 0.47156874\n",
      "Iteration 5650, loss = 0.47166244\n",
      "Iteration 5651, loss = 0.47215525\n",
      "Iteration 5652, loss = 0.47235432\n",
      "Iteration 5653, loss = 0.47225639\n",
      "Iteration 5654, loss = 0.47207624\n",
      "Iteration 5655, loss = 0.47162511\n",
      "Iteration 5656, loss = 0.47154119\n",
      "Iteration 5657, loss = 0.47177838\n",
      "Iteration 5658, loss = 0.47169772\n",
      "Iteration 5659, loss = 0.47226899\n",
      "Iteration 5660, loss = 0.47217020\n",
      "Iteration 5661, loss = 0.47183340\n",
      "Iteration 5662, loss = 0.47128821\n",
      "Iteration 5663, loss = 0.47162243\n",
      "Iteration 5664, loss = 0.47220054\n",
      "Iteration 5665, loss = 0.47250056\n",
      "Iteration 5666, loss = 0.47223344\n",
      "Iteration 5667, loss = 0.47180134\n",
      "Iteration 5668, loss = 0.47135487\n",
      "Iteration 5669, loss = 0.47178438\n",
      "Iteration 5670, loss = 0.47211540\n",
      "Iteration 5671, loss = 0.47186949\n",
      "Iteration 5672, loss = 0.47181891\n",
      "Iteration 5673, loss = 0.47140695\n",
      "Iteration 5674, loss = 0.47270392\n",
      "Iteration 5675, loss = 0.47165809\n",
      "Iteration 5676, loss = 0.47257689\n",
      "Iteration 5677, loss = 0.47165551\n",
      "Iteration 5678, loss = 0.47148166\n",
      "Iteration 5679, loss = 0.47156849\n",
      "Iteration 5680, loss = 0.47166458\n",
      "Iteration 5681, loss = 0.47155738\n",
      "Iteration 5682, loss = 0.47158796\n",
      "Iteration 5683, loss = 0.47154822\n",
      "Iteration 5684, loss = 0.47151511\n",
      "Iteration 5685, loss = 0.47158086\n",
      "Iteration 5686, loss = 0.47160995\n",
      "Iteration 5687, loss = 0.47160997\n",
      "Iteration 5688, loss = 0.47149791\n",
      "Iteration 5689, loss = 0.47156284\n",
      "Iteration 5690, loss = 0.47159710\n",
      "Iteration 5691, loss = 0.47179562\n",
      "Iteration 5692, loss = 0.47171668\n",
      "Iteration 5693, loss = 0.47202181\n",
      "Iteration 5694, loss = 0.47182957\n",
      "Iteration 5695, loss = 0.47169613\n",
      "Iteration 5696, loss = 0.47167417\n",
      "Iteration 5697, loss = 0.47167449\n",
      "Iteration 5698, loss = 0.47152585\n",
      "Iteration 5699, loss = 0.47148592\n",
      "Iteration 5700, loss = 0.47140265\n",
      "Iteration 5701, loss = 0.47172000\n",
      "Iteration 5702, loss = 0.47156383\n",
      "Iteration 5703, loss = 0.47160339\n",
      "Iteration 5704, loss = 0.47170525\n",
      "Iteration 5705, loss = 0.47157752\n",
      "Iteration 5706, loss = 0.47199505\n",
      "Iteration 5707, loss = 0.47185075\n",
      "Iteration 5708, loss = 0.47147228\n",
      "Iteration 5709, loss = 0.47155450\n",
      "Iteration 5710, loss = 0.47155106\n",
      "Iteration 5711, loss = 0.47201197\n",
      "Iteration 5712, loss = 0.47185725\n",
      "Iteration 5713, loss = 0.47156582\n",
      "Iteration 5714, loss = 0.47144182\n",
      "Iteration 5715, loss = 0.47228085\n",
      "Iteration 5716, loss = 0.47168763\n",
      "Iteration 5717, loss = 0.47161094\n",
      "Iteration 5718, loss = 0.47156487\n",
      "Iteration 5719, loss = 0.47167723\n",
      "Iteration 5720, loss = 0.47167414\n",
      "Iteration 5721, loss = 0.47176381\n",
      "Iteration 5722, loss = 0.47149658\n",
      "Iteration 5723, loss = 0.47137412\n",
      "Iteration 5724, loss = 0.47145416\n",
      "Iteration 5725, loss = 0.47158962\n",
      "Iteration 5726, loss = 0.47165356\n",
      "Iteration 5727, loss = 0.47155335\n",
      "Iteration 5728, loss = 0.47139743\n",
      "Iteration 5729, loss = 0.47134304\n",
      "Iteration 5730, loss = 0.47154998\n",
      "Iteration 5731, loss = 0.47197668\n",
      "Iteration 5732, loss = 0.47245455\n",
      "Iteration 5733, loss = 0.47181767\n",
      "Iteration 5734, loss = 0.47126938\n",
      "Iteration 5735, loss = 0.47185405\n",
      "Iteration 5736, loss = 0.47253137\n",
      "Iteration 5737, loss = 0.47228579\n",
      "Iteration 5738, loss = 0.47176145\n",
      "Iteration 5739, loss = 0.47121417\n",
      "Iteration 5740, loss = 0.47178642\n",
      "Iteration 5741, loss = 0.47226000\n",
      "Iteration 5742, loss = 0.47269072\n",
      "Iteration 5743, loss = 0.47234383\n",
      "Iteration 5744, loss = 0.47273497\n",
      "Iteration 5745, loss = 0.47178697\n",
      "Iteration 5746, loss = 0.47213618\n",
      "Iteration 5747, loss = 0.47326675\n",
      "Iteration 5748, loss = 0.47289804\n",
      "Iteration 5749, loss = 0.47180774\n",
      "Iteration 5750, loss = 0.47145219\n",
      "Iteration 5751, loss = 0.47140840\n",
      "Iteration 5752, loss = 0.47319161\n",
      "Iteration 5753, loss = 0.47309383\n",
      "Iteration 5754, loss = 0.47235816\n",
      "Iteration 5755, loss = 0.47166819\n",
      "Iteration 5756, loss = 0.47217948\n",
      "Iteration 5757, loss = 0.47171017\n",
      "Iteration 5758, loss = 0.47192349\n",
      "Iteration 5759, loss = 0.47203109\n",
      "Iteration 5760, loss = 0.47155795\n",
      "Iteration 5761, loss = 0.47162406\n",
      "Iteration 5762, loss = 0.47153520\n",
      "Iteration 5763, loss = 0.47158074\n",
      "Iteration 5764, loss = 0.47149715\n",
      "Iteration 5765, loss = 0.47144900\n",
      "Iteration 5766, loss = 0.47149322\n",
      "Iteration 5767, loss = 0.47166546\n",
      "Iteration 5768, loss = 0.47162068\n",
      "Iteration 5769, loss = 0.47156327\n",
      "Iteration 5770, loss = 0.47160631\n",
      "Iteration 5771, loss = 0.47151820\n",
      "Iteration 5772, loss = 0.47171438\n",
      "Iteration 5773, loss = 0.47147652\n",
      "Iteration 5774, loss = 0.47172652\n",
      "Iteration 5775, loss = 0.47151531\n",
      "Iteration 5776, loss = 0.47139632\n",
      "Iteration 5777, loss = 0.47141281\n",
      "Iteration 5778, loss = 0.47151493\n",
      "Iteration 5779, loss = 0.47165108\n",
      "Iteration 5780, loss = 0.47183367\n",
      "Iteration 5781, loss = 0.47190698\n",
      "Iteration 5782, loss = 0.47152929\n",
      "Iteration 5783, loss = 0.47164085\n",
      "Iteration 5784, loss = 0.47175733\n",
      "Iteration 5785, loss = 0.47145552\n",
      "Iteration 5786, loss = 0.47134655\n",
      "Iteration 5787, loss = 0.47168298\n",
      "Iteration 5788, loss = 0.47189418\n",
      "Iteration 5789, loss = 0.47191671\n",
      "Iteration 5790, loss = 0.47154328\n",
      "Iteration 5791, loss = 0.47156537\n",
      "Iteration 5792, loss = 0.47173760\n",
      "Iteration 5793, loss = 0.47165734\n",
      "Iteration 5794, loss = 0.47181390\n",
      "Iteration 5795, loss = 0.47181327\n",
      "Iteration 5796, loss = 0.47215133\n",
      "Iteration 5797, loss = 0.47154530\n",
      "Iteration 5798, loss = 0.47163247\n",
      "Iteration 5799, loss = 0.47154937\n",
      "Iteration 5800, loss = 0.47156989\n",
      "Iteration 5801, loss = 0.47171288\n",
      "Iteration 5802, loss = 0.47148241\n",
      "Iteration 5803, loss = 0.47148506\n",
      "Iteration 5804, loss = 0.47166350\n",
      "Iteration 5805, loss = 0.47180643\n",
      "Iteration 5806, loss = 0.47136635\n",
      "Iteration 5807, loss = 0.47182782\n",
      "Iteration 5808, loss = 0.47176020\n",
      "Iteration 5809, loss = 0.47144762\n",
      "Iteration 5810, loss = 0.47153909\n",
      "Iteration 5811, loss = 0.47159536\n",
      "Iteration 5812, loss = 0.47171747\n",
      "Iteration 5813, loss = 0.47151127\n",
      "Iteration 5814, loss = 0.47149164\n",
      "Iteration 5815, loss = 0.47168204\n",
      "Iteration 5816, loss = 0.47187518\n",
      "Iteration 5817, loss = 0.47190900\n",
      "Iteration 5818, loss = 0.47239768\n",
      "Iteration 5819, loss = 0.47180900\n",
      "Iteration 5820, loss = 0.47179633\n",
      "Iteration 5821, loss = 0.47205459\n",
      "Iteration 5822, loss = 0.47165775\n",
      "Iteration 5823, loss = 0.47166954\n",
      "Iteration 5824, loss = 0.47152016\n",
      "Iteration 5825, loss = 0.47189704\n",
      "Iteration 5826, loss = 0.47172044\n",
      "Iteration 5827, loss = 0.47166114\n",
      "Iteration 5828, loss = 0.47176608\n",
      "Iteration 5829, loss = 0.47164079\n",
      "Iteration 5830, loss = 0.47144875\n",
      "Iteration 5831, loss = 0.47158457\n",
      "Iteration 5832, loss = 0.47130214\n",
      "Iteration 5833, loss = 0.47193439\n",
      "Iteration 5834, loss = 0.47209454\n",
      "Iteration 5835, loss = 0.47170018\n",
      "Iteration 5836, loss = 0.47174405\n",
      "Iteration 5837, loss = 0.47137877\n",
      "Iteration 5838, loss = 0.47166101\n",
      "Iteration 5839, loss = 0.47146760\n",
      "Iteration 5840, loss = 0.47140401\n",
      "Iteration 5841, loss = 0.47146458\n",
      "Iteration 5842, loss = 0.47154581\n",
      "Iteration 5843, loss = 0.47145406\n",
      "Iteration 5844, loss = 0.47160821\n",
      "Iteration 5845, loss = 0.47162550\n",
      "Iteration 5846, loss = 0.47144946\n",
      "Iteration 5847, loss = 0.47134229\n",
      "Iteration 5848, loss = 0.47152970\n",
      "Iteration 5849, loss = 0.47168828\n",
      "Iteration 5850, loss = 0.47162244\n",
      "Iteration 5851, loss = 0.47139827\n",
      "Iteration 5852, loss = 0.47173032\n",
      "Iteration 5853, loss = 0.47154800\n",
      "Iteration 5854, loss = 0.47163765\n",
      "Iteration 5855, loss = 0.47159545\n",
      "Iteration 5856, loss = 0.47160285\n",
      "Iteration 5857, loss = 0.47155388\n",
      "Iteration 5858, loss = 0.47148611\n",
      "Iteration 5859, loss = 0.47139820\n",
      "Iteration 5860, loss = 0.47144024\n",
      "Iteration 5861, loss = 0.47146231\n",
      "Iteration 5862, loss = 0.47154982\n",
      "Iteration 5863, loss = 0.47157877\n",
      "Iteration 5864, loss = 0.47144141\n",
      "Iteration 5865, loss = 0.47139842\n",
      "Iteration 5866, loss = 0.47144725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5867, loss = 0.47148799\n",
      "Iteration 5868, loss = 0.47143398\n",
      "Iteration 5869, loss = 0.47138820\n",
      "Iteration 5870, loss = 0.47138211\n",
      "Iteration 5871, loss = 0.47172637\n",
      "Iteration 5872, loss = 0.47134414\n",
      "Iteration 5873, loss = 0.47146272\n",
      "Iteration 5874, loss = 0.47239126\n",
      "Iteration 5875, loss = 0.47245123\n",
      "Iteration 5876, loss = 0.47186138\n",
      "Iteration 5877, loss = 0.47190763\n",
      "Iteration 5878, loss = 0.47191018\n",
      "Iteration 5879, loss = 0.47189595\n",
      "Iteration 5880, loss = 0.47172342\n",
      "Iteration 5881, loss = 0.47168521\n",
      "Iteration 5882, loss = 0.47146923\n",
      "Iteration 5883, loss = 0.47153404\n",
      "Iteration 5884, loss = 0.47157647\n",
      "Iteration 5885, loss = 0.47182908\n",
      "Iteration 5886, loss = 0.47164840\n",
      "Iteration 5887, loss = 0.47158331\n",
      "Iteration 5888, loss = 0.47166104\n",
      "Iteration 5889, loss = 0.47184080\n",
      "Iteration 5890, loss = 0.47190544\n",
      "Iteration 5891, loss = 0.47164651\n",
      "Iteration 5892, loss = 0.47191225\n",
      "Iteration 5893, loss = 0.47151242\n",
      "Iteration 5894, loss = 0.47165046\n",
      "Iteration 5895, loss = 0.47181513\n",
      "Iteration 5896, loss = 0.47184815\n",
      "Iteration 5897, loss = 0.47171455\n",
      "Iteration 5898, loss = 0.47177408\n",
      "Iteration 5899, loss = 0.47159182\n",
      "Iteration 5900, loss = 0.47169907\n",
      "Iteration 5901, loss = 0.47157429\n",
      "Iteration 5902, loss = 0.47160105\n",
      "Iteration 5903, loss = 0.47156821\n",
      "Iteration 5904, loss = 0.47156355\n",
      "Iteration 5905, loss = 0.47151415\n",
      "Iteration 5906, loss = 0.47157223\n",
      "Iteration 5907, loss = 0.47151062\n",
      "Iteration 5908, loss = 0.47149852\n",
      "Iteration 5909, loss = 0.47163960\n",
      "Iteration 5910, loss = 0.47145523\n",
      "Iteration 5911, loss = 0.47135866\n",
      "Iteration 5912, loss = 0.47138890\n",
      "Iteration 5913, loss = 0.47166437\n",
      "Iteration 5914, loss = 0.47227716\n",
      "Iteration 5915, loss = 0.47238313\n",
      "Iteration 5916, loss = 0.47169685\n",
      "Iteration 5917, loss = 0.47122689\n",
      "Iteration 5918, loss = 0.47152216\n",
      "Iteration 5919, loss = 0.47231429\n",
      "Iteration 5920, loss = 0.47244727\n",
      "Iteration 5921, loss = 0.47195235\n",
      "Iteration 5922, loss = 0.47141341\n",
      "Iteration 5923, loss = 0.47129671\n",
      "Iteration 5924, loss = 0.47160775\n",
      "Iteration 5925, loss = 0.47242733\n",
      "Iteration 5926, loss = 0.47287378\n",
      "Iteration 5927, loss = 0.47179501\n",
      "Iteration 5928, loss = 0.47147655\n",
      "Iteration 5929, loss = 0.47219266\n",
      "Iteration 5930, loss = 0.47251755\n",
      "Iteration 5931, loss = 0.47247778\n",
      "Iteration 5932, loss = 0.47181731\n",
      "Iteration 5933, loss = 0.47149780\n",
      "Iteration 5934, loss = 0.47154676\n",
      "Iteration 5935, loss = 0.47173655\n",
      "Iteration 5936, loss = 0.47194217\n",
      "Iteration 5937, loss = 0.47202848\n",
      "Iteration 5938, loss = 0.47157168\n",
      "Iteration 5939, loss = 0.47141169\n",
      "Iteration 5940, loss = 0.47128358\n",
      "Iteration 5941, loss = 0.47171799\n",
      "Iteration 5942, loss = 0.47302583\n",
      "Iteration 5943, loss = 0.47321759\n",
      "Iteration 5944, loss = 0.47331777\n",
      "Iteration 5945, loss = 0.47192138\n",
      "Iteration 5946, loss = 0.47163824\n",
      "Iteration 5947, loss = 0.47146361\n",
      "Iteration 5948, loss = 0.47266937\n",
      "Iteration 5949, loss = 0.47162255\n",
      "Iteration 5950, loss = 0.47177262\n",
      "Iteration 5951, loss = 0.47193291\n",
      "Iteration 5952, loss = 0.47239794\n",
      "Iteration 5953, loss = 0.47268993\n",
      "Iteration 5954, loss = 0.47206220\n",
      "Iteration 5955, loss = 0.47186362\n",
      "Iteration 5956, loss = 0.47125579\n",
      "Iteration 5957, loss = 0.47145114\n",
      "Iteration 5958, loss = 0.47203372\n",
      "Iteration 5959, loss = 0.47223851\n",
      "Iteration 5960, loss = 0.47165046\n",
      "Iteration 5961, loss = 0.47196620\n",
      "Iteration 5962, loss = 0.47150441\n",
      "Iteration 5963, loss = 0.47165714\n",
      "Iteration 5964, loss = 0.47180919\n",
      "Iteration 5965, loss = 0.47188772\n",
      "Iteration 5966, loss = 0.47168222\n",
      "Iteration 5967, loss = 0.47151489\n",
      "Iteration 5968, loss = 0.47159366\n",
      "Iteration 5969, loss = 0.47192207\n",
      "Iteration 5970, loss = 0.47185691\n",
      "Iteration 5971, loss = 0.47213780\n",
      "Iteration 5972, loss = 0.47187704\n",
      "Iteration 5973, loss = 0.47155192\n",
      "Iteration 5974, loss = 0.47155030\n",
      "Iteration 5975, loss = 0.47182469\n",
      "Iteration 5976, loss = 0.47187745\n",
      "Iteration 5977, loss = 0.47158884\n",
      "Iteration 5978, loss = 0.47169722\n",
      "Iteration 5979, loss = 0.47157324\n",
      "Iteration 5980, loss = 0.47169137\n",
      "Iteration 5981, loss = 0.47186990\n",
      "Iteration 5982, loss = 0.47160272\n",
      "Iteration 5983, loss = 0.47163731\n",
      "Iteration 5984, loss = 0.47144177\n",
      "Iteration 5985, loss = 0.47139884\n",
      "Iteration 5986, loss = 0.47150141\n",
      "Iteration 5987, loss = 0.47149917\n",
      "Iteration 5988, loss = 0.47142505\n",
      "Iteration 5989, loss = 0.47137090\n",
      "Iteration 5990, loss = 0.47156916\n",
      "Iteration 5991, loss = 0.47143619\n",
      "Iteration 5992, loss = 0.47136716\n",
      "Iteration 5993, loss = 0.47128841\n",
      "Iteration 5994, loss = 0.47234956\n",
      "Iteration 5995, loss = 0.47166301\n",
      "Iteration 5996, loss = 0.47137234\n",
      "Iteration 5997, loss = 0.47155997\n",
      "Iteration 5998, loss = 0.47155423\n",
      "Iteration 5999, loss = 0.47170271\n",
      "Iteration 6000, loss = 0.47168579\n",
      "Iteration 6001, loss = 0.47206124\n",
      "Iteration 6002, loss = 0.47147668\n",
      "Iteration 6003, loss = 0.47177583\n",
      "Iteration 6004, loss = 0.47176141\n",
      "Iteration 6005, loss = 0.47219462\n",
      "Iteration 6006, loss = 0.47208375\n",
      "Iteration 6007, loss = 0.47163986\n",
      "Iteration 6008, loss = 0.47161448\n",
      "Iteration 6009, loss = 0.47154259\n",
      "Iteration 6010, loss = 0.47202024\n",
      "Iteration 6011, loss = 0.47222847\n",
      "Iteration 6012, loss = 0.47201089\n",
      "Iteration 6013, loss = 0.47228213\n",
      "Iteration 6014, loss = 0.47147751\n",
      "Iteration 6015, loss = 0.47150601\n",
      "Iteration 6016, loss = 0.47149031\n",
      "Iteration 6017, loss = 0.47143258\n",
      "Iteration 6018, loss = 0.47142449\n",
      "Iteration 6019, loss = 0.47139521\n",
      "Iteration 6020, loss = 0.47162523\n",
      "Iteration 6021, loss = 0.47132765\n",
      "Iteration 6022, loss = 0.47149070\n",
      "Iteration 6023, loss = 0.47170200\n",
      "Iteration 6024, loss = 0.47189243\n",
      "Iteration 6025, loss = 0.47155226\n",
      "Iteration 6026, loss = 0.47151400\n",
      "Iteration 6027, loss = 0.47141756\n",
      "Iteration 6028, loss = 0.47207399\n",
      "Iteration 6029, loss = 0.47194499\n",
      "Iteration 6030, loss = 0.47173556\n",
      "Iteration 6031, loss = 0.47143177\n",
      "Iteration 6032, loss = 0.47173032\n",
      "Iteration 6033, loss = 0.47144092\n",
      "Iteration 6034, loss = 0.47162872\n",
      "Iteration 6035, loss = 0.47156789\n",
      "Iteration 6036, loss = 0.47138208\n",
      "Iteration 6037, loss = 0.47178180\n",
      "Iteration 6038, loss = 0.47194545\n",
      "Iteration 6039, loss = 0.47183722\n",
      "Iteration 6040, loss = 0.47147620\n",
      "Iteration 6041, loss = 0.47154445\n",
      "Iteration 6042, loss = 0.47197071\n",
      "Iteration 6043, loss = 0.47138995\n",
      "Iteration 6044, loss = 0.47129087\n",
      "Iteration 6045, loss = 0.47152924\n",
      "Iteration 6046, loss = 0.47205902\n",
      "Iteration 6047, loss = 0.47159155\n",
      "Iteration 6048, loss = 0.47208125\n",
      "Iteration 6049, loss = 0.47171256\n",
      "Iteration 6050, loss = 0.47175019\n",
      "Iteration 6051, loss = 0.47195623\n",
      "Iteration 6052, loss = 0.47158346\n",
      "Iteration 6053, loss = 0.47220181\n",
      "Iteration 6054, loss = 0.47177882\n",
      "Iteration 6055, loss = 0.47166948\n",
      "Iteration 6056, loss = 0.47133215\n",
      "Iteration 6057, loss = 0.47180417\n",
      "Iteration 6058, loss = 0.47313504\n",
      "Iteration 6059, loss = 0.47250116\n",
      "Iteration 6060, loss = 0.47133313\n",
      "Iteration 6061, loss = 0.47062005\n",
      "Iteration 6062, loss = 0.47298216\n",
      "Iteration 6063, loss = 0.47490910\n",
      "Iteration 6064, loss = 0.47335650\n",
      "Iteration 6065, loss = 0.47175202\n",
      "Iteration 6066, loss = 0.47246378\n",
      "Iteration 6067, loss = 0.47227311\n",
      "Iteration 6068, loss = 0.47270733\n",
      "Iteration 6069, loss = 0.47268518\n",
      "Iteration 6070, loss = 0.47247559\n",
      "Iteration 6071, loss = 0.47205564\n",
      "Iteration 6072, loss = 0.47148241\n",
      "Iteration 6073, loss = 0.47160654\n",
      "Iteration 6074, loss = 0.47154564\n",
      "Iteration 6075, loss = 0.47194386\n",
      "Iteration 6076, loss = 0.47156421\n",
      "Iteration 6077, loss = 0.47157644\n",
      "Iteration 6078, loss = 0.47170271\n",
      "Iteration 6079, loss = 0.47170743\n",
      "Iteration 6080, loss = 0.47179263\n",
      "Iteration 6081, loss = 0.47182049\n",
      "Iteration 6082, loss = 0.47153414\n",
      "Iteration 6083, loss = 0.47148680\n",
      "Iteration 6084, loss = 0.47150419\n",
      "Iteration 6085, loss = 0.47224945\n",
      "Iteration 6086, loss = 0.47151718\n",
      "Iteration 6087, loss = 0.47138634\n",
      "Iteration 6088, loss = 0.47219523\n",
      "Iteration 6089, loss = 0.47189778\n",
      "Iteration 6090, loss = 0.47175042\n",
      "Iteration 6091, loss = 0.47138242\n",
      "Iteration 6092, loss = 0.47160360\n",
      "Iteration 6093, loss = 0.47215386\n",
      "Iteration 6094, loss = 0.47236573\n",
      "Iteration 6095, loss = 0.47227134\n",
      "Iteration 6096, loss = 0.47220869\n",
      "Iteration 6097, loss = 0.47161022\n",
      "Iteration 6098, loss = 0.47148692\n",
      "Iteration 6099, loss = 0.47159372\n",
      "Iteration 6100, loss = 0.47147635\n",
      "Iteration 6101, loss = 0.47142494\n",
      "Iteration 6102, loss = 0.47155483\n",
      "Iteration 6103, loss = 0.47199279\n",
      "Iteration 6104, loss = 0.47226249\n",
      "Iteration 6105, loss = 0.47175673\n",
      "Iteration 6106, loss = 0.47124406\n",
      "Iteration 6107, loss = 0.47143340\n",
      "Iteration 6108, loss = 0.47189591\n",
      "Iteration 6109, loss = 0.47275354\n",
      "Iteration 6110, loss = 0.47202393\n",
      "Iteration 6111, loss = 0.47216270\n",
      "Iteration 6112, loss = 0.47166634\n",
      "Iteration 6113, loss = 0.47173129\n",
      "Iteration 6114, loss = 0.47142903\n",
      "Iteration 6115, loss = 0.47148566\n",
      "Iteration 6116, loss = 0.47158330\n",
      "Iteration 6117, loss = 0.47156411\n",
      "Iteration 6118, loss = 0.47168190\n",
      "Iteration 6119, loss = 0.47170167\n",
      "Iteration 6120, loss = 0.47158501\n",
      "Iteration 6121, loss = 0.47153433\n",
      "Iteration 6122, loss = 0.47150723\n",
      "Iteration 6123, loss = 0.47143762\n",
      "Iteration 6124, loss = 0.47173425\n",
      "Iteration 6125, loss = 0.47144454\n",
      "Iteration 6126, loss = 0.47152529\n",
      "Iteration 6127, loss = 0.47152187\n",
      "Iteration 6128, loss = 0.47183777\n",
      "Iteration 6129, loss = 0.47155213\n",
      "Iteration 6130, loss = 0.47151674\n",
      "Iteration 6131, loss = 0.47193630\n",
      "Iteration 6132, loss = 0.47132138\n",
      "Iteration 6133, loss = 0.47162296\n",
      "Iteration 6134, loss = 0.47169180\n",
      "Iteration 6135, loss = 0.47162782\n",
      "Iteration 6136, loss = 0.47192283\n",
      "Iteration 6137, loss = 0.47160072\n",
      "Iteration 6138, loss = 0.47161489\n",
      "Iteration 6139, loss = 0.47208937\n",
      "Iteration 6140, loss = 0.47220116\n",
      "Iteration 6141, loss = 0.47140983\n",
      "Iteration 6142, loss = 0.47159372\n",
      "Iteration 6143, loss = 0.47153411\n",
      "Iteration 6144, loss = 0.47138506\n",
      "Iteration 6145, loss = 0.47140717\n",
      "Iteration 6146, loss = 0.47142997\n",
      "Iteration 6147, loss = 0.47141960\n",
      "Iteration 6148, loss = 0.47142474\n",
      "Iteration 6149, loss = 0.47145800\n",
      "Iteration 6150, loss = 0.47155071\n",
      "Iteration 6151, loss = 0.47151903\n",
      "Iteration 6152, loss = 0.47151009\n",
      "Iteration 6153, loss = 0.47137853\n",
      "Iteration 6154, loss = 0.47150737\n",
      "Iteration 6155, loss = 0.47153153\n",
      "Iteration 6156, loss = 0.47142210\n",
      "Iteration 6157, loss = 0.47130979\n",
      "Iteration 6158, loss = 0.47134745\n",
      "Iteration 6159, loss = 0.47147255\n",
      "Iteration 6160, loss = 0.47148878\n",
      "Iteration 6161, loss = 0.47150111\n",
      "Iteration 6162, loss = 0.47126136\n",
      "Iteration 6163, loss = 0.47122464\n",
      "Iteration 6164, loss = 0.47172612\n",
      "Iteration 6165, loss = 0.47195437\n",
      "Iteration 6166, loss = 0.47187281\n",
      "Iteration 6167, loss = 0.47153357\n",
      "Iteration 6168, loss = 0.47113763\n",
      "Iteration 6169, loss = 0.47133473\n",
      "Iteration 6170, loss = 0.47282772\n",
      "Iteration 6171, loss = 0.47254252\n",
      "Iteration 6172, loss = 0.47183988\n",
      "Iteration 6173, loss = 0.47137555\n",
      "Iteration 6174, loss = 0.47116312\n",
      "Iteration 6175, loss = 0.47159867\n",
      "Iteration 6176, loss = 0.47216198\n",
      "Iteration 6177, loss = 0.47242208\n",
      "Iteration 6178, loss = 0.47222470\n",
      "Iteration 6179, loss = 0.47246412\n",
      "Iteration 6180, loss = 0.47189694\n",
      "Iteration 6181, loss = 0.47175794\n",
      "Iteration 6182, loss = 0.47161489\n",
      "Iteration 6183, loss = 0.47155050\n",
      "Iteration 6184, loss = 0.47151069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6185, loss = 0.47155566\n",
      "Iteration 6186, loss = 0.47153671\n",
      "Iteration 6187, loss = 0.47140511\n",
      "Iteration 6188, loss = 0.47156921\n",
      "Iteration 6189, loss = 0.47137511\n",
      "Iteration 6190, loss = 0.47146279\n",
      "Iteration 6191, loss = 0.47158391\n",
      "Iteration 6192, loss = 0.47164758\n",
      "Iteration 6193, loss = 0.47162517\n",
      "Iteration 6194, loss = 0.47167533\n",
      "Iteration 6195, loss = 0.47165105\n",
      "Iteration 6196, loss = 0.47151328\n",
      "Iteration 6197, loss = 0.47144630\n",
      "Iteration 6198, loss = 0.47153031\n",
      "Iteration 6199, loss = 0.47189836\n",
      "Iteration 6200, loss = 0.47135326\n",
      "Iteration 6201, loss = 0.47146080\n",
      "Iteration 6202, loss = 0.47158652\n",
      "Iteration 6203, loss = 0.47139073\n",
      "Iteration 6204, loss = 0.47131145\n",
      "Iteration 6205, loss = 0.47167592\n",
      "Iteration 6206, loss = 0.47161520\n",
      "Iteration 6207, loss = 0.47149867\n",
      "Iteration 6208, loss = 0.47159703\n",
      "Iteration 6209, loss = 0.47139713\n",
      "Iteration 6210, loss = 0.47150034\n",
      "Iteration 6211, loss = 0.47136846\n",
      "Iteration 6212, loss = 0.47194269\n",
      "Iteration 6213, loss = 0.47132074\n",
      "Iteration 6214, loss = 0.47132049\n",
      "Iteration 6215, loss = 0.47142437\n",
      "Iteration 6216, loss = 0.47154232\n",
      "Iteration 6217, loss = 0.47135427\n",
      "Iteration 6218, loss = 0.47139775\n",
      "Iteration 6219, loss = 0.47139007\n",
      "Iteration 6220, loss = 0.47150088\n",
      "Iteration 6221, loss = 0.47148065\n",
      "Iteration 6222, loss = 0.47131847\n",
      "Iteration 6223, loss = 0.47135635\n",
      "Iteration 6224, loss = 0.47138231\n",
      "Iteration 6225, loss = 0.47136743\n",
      "Iteration 6226, loss = 0.47141478\n",
      "Iteration 6227, loss = 0.47160983\n",
      "Iteration 6228, loss = 0.47234394\n",
      "Iteration 6229, loss = 0.47210592\n",
      "Iteration 6230, loss = 0.47146116\n",
      "Iteration 6231, loss = 0.47154945\n",
      "Iteration 6232, loss = 0.47133587\n",
      "Iteration 6233, loss = 0.47134988\n",
      "Iteration 6234, loss = 0.47154781\n",
      "Iteration 6235, loss = 0.47162910\n",
      "Iteration 6236, loss = 0.47181292\n",
      "Iteration 6237, loss = 0.47149353\n",
      "Iteration 6238, loss = 0.47145062\n",
      "Iteration 6239, loss = 0.47143029\n",
      "Iteration 6240, loss = 0.47153007\n",
      "Iteration 6241, loss = 0.47171223\n",
      "Iteration 6242, loss = 0.47142257\n",
      "Iteration 6243, loss = 0.47136574\n",
      "Iteration 6244, loss = 0.47158489\n",
      "Iteration 6245, loss = 0.47169276\n",
      "Iteration 6246, loss = 0.47163211\n",
      "Iteration 6247, loss = 0.47151921\n",
      "Iteration 6248, loss = 0.47128818\n",
      "Iteration 6249, loss = 0.47127143\n",
      "Iteration 6250, loss = 0.47189848\n",
      "Iteration 6251, loss = 0.47219715\n",
      "Iteration 6252, loss = 0.47182397\n",
      "Iteration 6253, loss = 0.47137043\n",
      "Iteration 6254, loss = 0.47207179\n",
      "Iteration 6255, loss = 0.47238812\n",
      "Iteration 6256, loss = 0.47217210\n",
      "Iteration 6257, loss = 0.47164216\n",
      "Iteration 6258, loss = 0.47223228\n",
      "Iteration 6259, loss = 0.47142293\n",
      "Iteration 6260, loss = 0.47133121\n",
      "Iteration 6261, loss = 0.47156925\n",
      "Iteration 6262, loss = 0.47136621\n",
      "Iteration 6263, loss = 0.47139071\n",
      "Iteration 6264, loss = 0.47144240\n",
      "Iteration 6265, loss = 0.47143887\n",
      "Iteration 6266, loss = 0.47135619\n",
      "Iteration 6267, loss = 0.47130154\n",
      "Iteration 6268, loss = 0.47129616\n",
      "Iteration 6269, loss = 0.47141387\n",
      "Iteration 6270, loss = 0.47159479\n",
      "Iteration 6271, loss = 0.47188851\n",
      "Iteration 6272, loss = 0.47134065\n",
      "Iteration 6273, loss = 0.47132998\n",
      "Iteration 6274, loss = 0.47138598\n",
      "Iteration 6275, loss = 0.47139476\n",
      "Iteration 6276, loss = 0.47162362\n",
      "Iteration 6277, loss = 0.47154630\n",
      "Iteration 6278, loss = 0.47156062\n",
      "Iteration 6279, loss = 0.47117872\n",
      "Iteration 6280, loss = 0.47144152\n",
      "Iteration 6281, loss = 0.47183204\n",
      "Iteration 6282, loss = 0.47196382\n",
      "Iteration 6283, loss = 0.47175243\n",
      "Iteration 6284, loss = 0.47181912\n",
      "Iteration 6285, loss = 0.47135690\n",
      "Iteration 6286, loss = 0.47184497\n",
      "Iteration 6287, loss = 0.47166291\n",
      "Iteration 6288, loss = 0.47152096\n",
      "Iteration 6289, loss = 0.47157668\n",
      "Iteration 6290, loss = 0.47144825\n",
      "Iteration 6291, loss = 0.47153493\n",
      "Iteration 6292, loss = 0.47153373\n",
      "Iteration 6293, loss = 0.47154384\n",
      "Iteration 6294, loss = 0.47173002\n",
      "Iteration 6295, loss = 0.47151137\n",
      "Iteration 6296, loss = 0.47132577\n",
      "Iteration 6297, loss = 0.47147430\n",
      "Iteration 6298, loss = 0.47139481\n",
      "Iteration 6299, loss = 0.47132359\n",
      "Iteration 6300, loss = 0.47147622\n",
      "Iteration 6301, loss = 0.47169707\n",
      "Iteration 6302, loss = 0.47138893\n",
      "Iteration 6303, loss = 0.47192825\n",
      "Iteration 6304, loss = 0.47166325\n",
      "Iteration 6305, loss = 0.47177102\n",
      "Iteration 6306, loss = 0.47131242\n",
      "Iteration 6307, loss = 0.47131249\n",
      "Iteration 6308, loss = 0.47130036\n",
      "Iteration 6309, loss = 0.47146129\n",
      "Iteration 6310, loss = 0.47172794\n",
      "Iteration 6311, loss = 0.47137932\n",
      "Iteration 6312, loss = 0.47130160\n",
      "Iteration 6313, loss = 0.47134265\n",
      "Iteration 6314, loss = 0.47143421\n",
      "Iteration 6315, loss = 0.47146080\n",
      "Iteration 6316, loss = 0.47149259\n",
      "Iteration 6317, loss = 0.47137039\n",
      "Iteration 6318, loss = 0.47128403\n",
      "Iteration 6319, loss = 0.47135426\n",
      "Iteration 6320, loss = 0.47139397\n",
      "Iteration 6321, loss = 0.47138359\n",
      "Iteration 6322, loss = 0.47142011\n",
      "Iteration 6323, loss = 0.47137074\n",
      "Iteration 6324, loss = 0.47130745\n",
      "Iteration 6325, loss = 0.47178420\n",
      "Iteration 6326, loss = 0.47217558\n",
      "Iteration 6327, loss = 0.47136917\n",
      "Iteration 6328, loss = 0.47164685\n",
      "Iteration 6329, loss = 0.47158265\n",
      "Iteration 6330, loss = 0.47161868\n",
      "Iteration 6331, loss = 0.47138413\n",
      "Iteration 6332, loss = 0.47141081\n",
      "Iteration 6333, loss = 0.47157286\n",
      "Iteration 6334, loss = 0.47178443\n",
      "Iteration 6335, loss = 0.47186260\n",
      "Iteration 6336, loss = 0.47177389\n",
      "Iteration 6337, loss = 0.47212623\n",
      "Iteration 6338, loss = 0.47170884\n",
      "Iteration 6339, loss = 0.47136955\n",
      "Iteration 6340, loss = 0.47201834\n",
      "Iteration 6341, loss = 0.47212739\n",
      "Iteration 6342, loss = 0.47159364\n",
      "Iteration 6343, loss = 0.47156267\n",
      "Iteration 6344, loss = 0.47135933\n",
      "Iteration 6345, loss = 0.47137210\n",
      "Iteration 6346, loss = 0.47154868\n",
      "Iteration 6347, loss = 0.47158970\n",
      "Iteration 6348, loss = 0.47195956\n",
      "Iteration 6349, loss = 0.47220038\n",
      "Iteration 6350, loss = 0.47166932\n",
      "Iteration 6351, loss = 0.47189466\n",
      "Iteration 6352, loss = 0.47201910\n",
      "Iteration 6353, loss = 0.47149856\n",
      "Iteration 6354, loss = 0.47114835\n",
      "Iteration 6355, loss = 0.47153444\n",
      "Iteration 6356, loss = 0.47195966\n",
      "Iteration 6357, loss = 0.47228662\n",
      "Iteration 6358, loss = 0.47267592\n",
      "Iteration 6359, loss = 0.47235224\n",
      "Iteration 6360, loss = 0.47192721\n",
      "Iteration 6361, loss = 0.47174069\n",
      "Iteration 6362, loss = 0.47141819\n",
      "Iteration 6363, loss = 0.47215364\n",
      "Iteration 6364, loss = 0.47276230\n",
      "Iteration 6365, loss = 0.47227434\n",
      "Iteration 6366, loss = 0.47135341\n",
      "Iteration 6367, loss = 0.47139050\n",
      "Iteration 6368, loss = 0.47170854\n",
      "Iteration 6369, loss = 0.47191964\n",
      "Iteration 6370, loss = 0.47164364\n",
      "Iteration 6371, loss = 0.47134638\n",
      "Iteration 6372, loss = 0.47156123\n",
      "Iteration 6373, loss = 0.47140342\n",
      "Iteration 6374, loss = 0.47154187\n",
      "Iteration 6375, loss = 0.47135502\n",
      "Iteration 6376, loss = 0.47135511\n",
      "Iteration 6377, loss = 0.47155564\n",
      "Iteration 6378, loss = 0.47162460\n",
      "Iteration 6379, loss = 0.47195478\n",
      "Iteration 6380, loss = 0.47200285\n",
      "Iteration 6381, loss = 0.47184482\n",
      "Iteration 6382, loss = 0.47234107\n",
      "Iteration 6383, loss = 0.47155392\n",
      "Iteration 6384, loss = 0.47143298\n",
      "Iteration 6385, loss = 0.47172448\n",
      "Iteration 6386, loss = 0.47200152\n",
      "Iteration 6387, loss = 0.47165906\n",
      "Iteration 6388, loss = 0.47114309\n",
      "Iteration 6389, loss = 0.47182878\n",
      "Iteration 6390, loss = 0.47181322\n",
      "Iteration 6391, loss = 0.47193404\n",
      "Iteration 6392, loss = 0.47139201\n",
      "Iteration 6393, loss = 0.47146929\n",
      "Iteration 6394, loss = 0.47209632\n",
      "Iteration 6395, loss = 0.47154916\n",
      "Iteration 6396, loss = 0.47124938\n",
      "Iteration 6397, loss = 0.47129376\n",
      "Iteration 6398, loss = 0.47163515\n",
      "Iteration 6399, loss = 0.47209842\n",
      "Iteration 6400, loss = 0.47199019\n",
      "Iteration 6401, loss = 0.47196799\n",
      "Iteration 6402, loss = 0.47161150\n",
      "Iteration 6403, loss = 0.47121575\n",
      "Iteration 6404, loss = 0.47132446\n",
      "Iteration 6405, loss = 0.47206514\n",
      "Iteration 6406, loss = 0.47239196\n",
      "Iteration 6407, loss = 0.47218046\n",
      "Iteration 6408, loss = 0.47221958\n",
      "Iteration 6409, loss = 0.47134627\n",
      "Iteration 6410, loss = 0.47212555\n",
      "Iteration 6411, loss = 0.47147969\n",
      "Iteration 6412, loss = 0.47128374\n",
      "Iteration 6413, loss = 0.47133891\n",
      "Iteration 6414, loss = 0.47197493\n",
      "Iteration 6415, loss = 0.47219753\n",
      "Iteration 6416, loss = 0.47187083\n",
      "Iteration 6417, loss = 0.47202721\n",
      "Iteration 6418, loss = 0.47193727\n",
      "Iteration 6419, loss = 0.47141785\n",
      "Iteration 6420, loss = 0.47164142\n",
      "Iteration 6421, loss = 0.47143880\n",
      "Iteration 6422, loss = 0.47143688\n",
      "Iteration 6423, loss = 0.47161503\n",
      "Iteration 6424, loss = 0.47147999\n",
      "Iteration 6425, loss = 0.47147596\n",
      "Iteration 6426, loss = 0.47133729\n",
      "Iteration 6427, loss = 0.47160517\n",
      "Iteration 6428, loss = 0.47187066\n",
      "Iteration 6429, loss = 0.47172217\n",
      "Iteration 6430, loss = 0.47142308\n",
      "Iteration 6431, loss = 0.47145559\n",
      "Iteration 6432, loss = 0.47142857\n",
      "Iteration 6433, loss = 0.47157753\n",
      "Iteration 6434, loss = 0.47233087\n",
      "Iteration 6435, loss = 0.47156884\n",
      "Iteration 6436, loss = 0.47125535\n",
      "Iteration 6437, loss = 0.47185030\n",
      "Iteration 6438, loss = 0.47170660\n",
      "Iteration 6439, loss = 0.47169694\n",
      "Iteration 6440, loss = 0.47196381\n",
      "Iteration 6441, loss = 0.47132783\n",
      "Iteration 6442, loss = 0.47137204\n",
      "Iteration 6443, loss = 0.47154207\n",
      "Iteration 6444, loss = 0.47160404\n",
      "Iteration 6445, loss = 0.47205142\n",
      "Iteration 6446, loss = 0.47192633\n",
      "Iteration 6447, loss = 0.47122511\n",
      "Iteration 6448, loss = 0.47140686\n",
      "Iteration 6449, loss = 0.47153216\n",
      "Iteration 6450, loss = 0.47149256\n",
      "Iteration 6451, loss = 0.47154677\n",
      "Iteration 6452, loss = 0.47147459\n",
      "Iteration 6453, loss = 0.47147808\n",
      "Iteration 6454, loss = 0.47138719\n",
      "Iteration 6455, loss = 0.47134183\n",
      "Iteration 6456, loss = 0.47135758\n",
      "Iteration 6457, loss = 0.47130059\n",
      "Iteration 6458, loss = 0.47198930\n",
      "Iteration 6459, loss = 0.47239412\n",
      "Iteration 6460, loss = 0.47149876\n",
      "Iteration 6461, loss = 0.47170668\n",
      "Iteration 6462, loss = 0.47159881\n",
      "Iteration 6463, loss = 0.47153895\n",
      "Iteration 6464, loss = 0.47129710\n",
      "Iteration 6465, loss = 0.47130993\n",
      "Iteration 6466, loss = 0.47142835\n",
      "Iteration 6467, loss = 0.47136384\n",
      "Iteration 6468, loss = 0.47140267\n",
      "Iteration 6469, loss = 0.47147822\n",
      "Iteration 6470, loss = 0.47138977\n",
      "Iteration 6471, loss = 0.47137040\n",
      "Iteration 6472, loss = 0.47134592\n",
      "Iteration 6473, loss = 0.47145472\n",
      "Iteration 6474, loss = 0.47146791\n",
      "Iteration 6475, loss = 0.47159063\n",
      "Iteration 6476, loss = 0.47179340\n",
      "Iteration 6477, loss = 0.47187435\n",
      "Iteration 6478, loss = 0.47146699\n",
      "Iteration 6479, loss = 0.47173559\n",
      "Iteration 6480, loss = 0.47136923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6481, loss = 0.47163497\n",
      "Iteration 6482, loss = 0.47183909\n",
      "Iteration 6483, loss = 0.47152621\n",
      "Iteration 6484, loss = 0.47160752\n",
      "Iteration 6485, loss = 0.47159442\n",
      "Iteration 6486, loss = 0.47148203\n",
      "Iteration 6487, loss = 0.47156457\n",
      "Iteration 6488, loss = 0.47143278\n",
      "Iteration 6489, loss = 0.47139944\n",
      "Iteration 6490, loss = 0.47140667\n",
      "Iteration 6491, loss = 0.47145064\n",
      "Iteration 6492, loss = 0.47144847\n",
      "Iteration 6493, loss = 0.47160092\n",
      "Iteration 6494, loss = 0.47140421\n",
      "Iteration 6495, loss = 0.47138896\n",
      "Iteration 6496, loss = 0.47126814\n",
      "Iteration 6497, loss = 0.47138609\n",
      "Iteration 6498, loss = 0.47164643\n",
      "Iteration 6499, loss = 0.47175626\n",
      "Iteration 6500, loss = 0.47245846\n",
      "Iteration 6501, loss = 0.47130816\n",
      "Iteration 6502, loss = 0.47164590\n",
      "Iteration 6503, loss = 0.47132161\n",
      "Iteration 6504, loss = 0.47136689\n",
      "Iteration 6505, loss = 0.47144205\n",
      "Iteration 6506, loss = 0.47131145\n",
      "Iteration 6507, loss = 0.47132385\n",
      "Iteration 6508, loss = 0.47135084\n",
      "Iteration 6509, loss = 0.47147201\n",
      "Iteration 6510, loss = 0.47202042\n",
      "Iteration 6511, loss = 0.47179484\n",
      "Iteration 6512, loss = 0.47148823\n",
      "Iteration 6513, loss = 0.47122378\n",
      "Iteration 6514, loss = 0.47118565\n",
      "Iteration 6515, loss = 0.47150630\n",
      "Iteration 6516, loss = 0.47245782\n",
      "Iteration 6517, loss = 0.47344594\n",
      "Iteration 6518, loss = 0.47301917\n",
      "Iteration 6519, loss = 0.47179276\n",
      "Iteration 6520, loss = 0.47115288\n",
      "Iteration 6521, loss = 0.47291389\n",
      "Iteration 6522, loss = 0.47240196\n",
      "Iteration 6523, loss = 0.47234500\n",
      "Iteration 6524, loss = 0.47157040\n",
      "Iteration 6525, loss = 0.47139550\n",
      "Iteration 6526, loss = 0.47193518\n",
      "Iteration 6527, loss = 0.47191875\n",
      "Iteration 6528, loss = 0.47165835\n",
      "Iteration 6529, loss = 0.47168491\n",
      "Iteration 6530, loss = 0.47161573\n",
      "Iteration 6531, loss = 0.47126427\n",
      "Iteration 6532, loss = 0.47119580\n",
      "Iteration 6533, loss = 0.47156474\n",
      "Iteration 6534, loss = 0.47210918\n",
      "Iteration 6535, loss = 0.47172976\n",
      "Iteration 6536, loss = 0.47111134\n",
      "Iteration 6537, loss = 0.47176966\n",
      "Iteration 6538, loss = 0.47197843\n",
      "Iteration 6539, loss = 0.47236986\n",
      "Iteration 6540, loss = 0.47174609\n",
      "Iteration 6541, loss = 0.47179558\n",
      "Iteration 6542, loss = 0.47164894\n",
      "Iteration 6543, loss = 0.47183295\n",
      "Iteration 6544, loss = 0.47134428\n",
      "Iteration 6545, loss = 0.47129176\n",
      "Iteration 6546, loss = 0.47139320\n",
      "Iteration 6547, loss = 0.47138522\n",
      "Iteration 6548, loss = 0.47184377\n",
      "Iteration 6549, loss = 0.47149173\n",
      "Iteration 6550, loss = 0.47153886\n",
      "Iteration 6551, loss = 0.47130557\n",
      "Iteration 6552, loss = 0.47137220\n",
      "Iteration 6553, loss = 0.47142477\n",
      "Iteration 6554, loss = 0.47136632\n",
      "Iteration 6555, loss = 0.47191011\n",
      "Iteration 6556, loss = 0.47138392\n",
      "Iteration 6557, loss = 0.47128747\n",
      "Iteration 6558, loss = 0.47133893\n",
      "Iteration 6559, loss = 0.47127769\n",
      "Iteration 6560, loss = 0.47150164\n",
      "Iteration 6561, loss = 0.47133667\n",
      "Iteration 6562, loss = 0.47132060\n",
      "Iteration 6563, loss = 0.47166120\n",
      "Iteration 6564, loss = 0.47179568\n",
      "Iteration 6565, loss = 0.47142115\n",
      "Iteration 6566, loss = 0.47140635\n",
      "Iteration 6567, loss = 0.47129306\n",
      "Iteration 6568, loss = 0.47163231\n",
      "Iteration 6569, loss = 0.47151457\n",
      "Iteration 6570, loss = 0.47155707\n",
      "Iteration 6571, loss = 0.47179315\n",
      "Iteration 6572, loss = 0.47203969\n",
      "Iteration 6573, loss = 0.47197201\n",
      "Iteration 6574, loss = 0.47170003\n",
      "Iteration 6575, loss = 0.47152181\n",
      "Iteration 6576, loss = 0.47135920\n",
      "Iteration 6577, loss = 0.47140787\n",
      "Iteration 6578, loss = 0.47133849\n",
      "Iteration 6579, loss = 0.47150650\n",
      "Iteration 6580, loss = 0.47173988\n",
      "Iteration 6581, loss = 0.47194367\n",
      "Iteration 6582, loss = 0.47153043\n",
      "Iteration 6583, loss = 0.47164867\n",
      "Iteration 6584, loss = 0.47117734\n",
      "Iteration 6585, loss = 0.47127052\n",
      "Iteration 6586, loss = 0.47172545\n",
      "Iteration 6587, loss = 0.47205665\n",
      "Iteration 6588, loss = 0.47194673\n",
      "Iteration 6589, loss = 0.47163279\n",
      "Iteration 6590, loss = 0.47168106\n",
      "Iteration 6591, loss = 0.47131538\n",
      "Iteration 6592, loss = 0.47132931\n",
      "Iteration 6593, loss = 0.47154983\n",
      "Iteration 6594, loss = 0.47153596\n",
      "Iteration 6595, loss = 0.47136501\n",
      "Iteration 6596, loss = 0.47215470\n",
      "Iteration 6597, loss = 0.47153330\n",
      "Iteration 6598, loss = 0.47156526\n",
      "Iteration 6599, loss = 0.47127305\n",
      "Iteration 6600, loss = 0.47137150\n",
      "Iteration 6601, loss = 0.47138563\n",
      "Iteration 6602, loss = 0.47145373\n",
      "Iteration 6603, loss = 0.47144675\n",
      "Iteration 6604, loss = 0.47168893\n",
      "Iteration 6605, loss = 0.47141901\n",
      "Iteration 6606, loss = 0.47139222\n",
      "Iteration 6607, loss = 0.47103490\n",
      "Iteration 6608, loss = 0.47134892\n",
      "Iteration 6609, loss = 0.47226402\n",
      "Iteration 6610, loss = 0.47262615\n",
      "Iteration 6611, loss = 0.47240705\n",
      "Iteration 6612, loss = 0.47198185\n",
      "Iteration 6613, loss = 0.47136648\n",
      "Iteration 6614, loss = 0.47166136\n",
      "Iteration 6615, loss = 0.47140555\n",
      "Iteration 6616, loss = 0.47158432\n",
      "Iteration 6617, loss = 0.47210304\n",
      "Iteration 6618, loss = 0.47138244\n",
      "Iteration 6619, loss = 0.47121961\n",
      "Iteration 6620, loss = 0.47121399\n",
      "Iteration 6621, loss = 0.47218107\n",
      "Iteration 6622, loss = 0.47140638\n",
      "Iteration 6623, loss = 0.47158186\n",
      "Iteration 6624, loss = 0.47136105\n",
      "Iteration 6625, loss = 0.47120704\n",
      "Iteration 6626, loss = 0.47122301\n",
      "Iteration 6627, loss = 0.47155098\n",
      "Iteration 6628, loss = 0.47157402\n",
      "Iteration 6629, loss = 0.47150161\n",
      "Iteration 6630, loss = 0.47117436\n",
      "Iteration 6631, loss = 0.47178676\n",
      "Iteration 6632, loss = 0.47178419\n",
      "Iteration 6633, loss = 0.47149556\n",
      "Iteration 6634, loss = 0.47127299\n",
      "Iteration 6635, loss = 0.47126850\n",
      "Iteration 6636, loss = 0.47132591\n",
      "Iteration 6637, loss = 0.47141088\n",
      "Iteration 6638, loss = 0.47179538\n",
      "Iteration 6639, loss = 0.47171284\n",
      "Iteration 6640, loss = 0.47127566\n",
      "Iteration 6641, loss = 0.47176316\n",
      "Iteration 6642, loss = 0.47171868\n",
      "Iteration 6643, loss = 0.47176707\n",
      "Iteration 6644, loss = 0.47150741\n",
      "Iteration 6645, loss = 0.47119966\n",
      "Iteration 6646, loss = 0.47125194\n",
      "Iteration 6647, loss = 0.47142578\n",
      "Iteration 6648, loss = 0.47193494\n",
      "Iteration 6649, loss = 0.47188736\n",
      "Iteration 6650, loss = 0.47180295\n",
      "Iteration 6651, loss = 0.47137090\n",
      "Iteration 6652, loss = 0.47131019\n",
      "Iteration 6653, loss = 0.47143836\n",
      "Iteration 6654, loss = 0.47150247\n",
      "Iteration 6655, loss = 0.47123620\n",
      "Iteration 6656, loss = 0.47131927\n",
      "Iteration 6657, loss = 0.47132549\n",
      "Iteration 6658, loss = 0.47124695\n",
      "Iteration 6659, loss = 0.47127138\n",
      "Iteration 6660, loss = 0.47125685\n",
      "Iteration 6661, loss = 0.47125012\n",
      "Iteration 6662, loss = 0.47139449\n",
      "Iteration 6663, loss = 0.47126986\n",
      "Iteration 6664, loss = 0.47133372\n",
      "Iteration 6665, loss = 0.47129168\n",
      "Iteration 6666, loss = 0.47133583\n",
      "Iteration 6667, loss = 0.47138046\n",
      "Iteration 6668, loss = 0.47138358\n",
      "Iteration 6669, loss = 0.47142468\n",
      "Iteration 6670, loss = 0.47132601\n",
      "Iteration 6671, loss = 0.47127219\n",
      "Iteration 6672, loss = 0.47118894\n",
      "Iteration 6673, loss = 0.47120480\n",
      "Iteration 6674, loss = 0.47134510\n",
      "Iteration 6675, loss = 0.47133127\n",
      "Iteration 6676, loss = 0.47146043\n",
      "Iteration 6677, loss = 0.47131086\n",
      "Iteration 6678, loss = 0.47142480\n",
      "Iteration 6679, loss = 0.47137346\n",
      "Iteration 6680, loss = 0.47173859\n",
      "Iteration 6681, loss = 0.47141859\n",
      "Iteration 6682, loss = 0.47145775\n",
      "Iteration 6683, loss = 0.47176326\n",
      "Iteration 6684, loss = 0.47167873\n",
      "Iteration 6685, loss = 0.47167883\n",
      "Iteration 6686, loss = 0.47181878\n",
      "Iteration 6687, loss = 0.47157763\n",
      "Iteration 6688, loss = 0.47155401\n",
      "Iteration 6689, loss = 0.47151758\n",
      "Iteration 6690, loss = 0.47120565\n",
      "Iteration 6691, loss = 0.47125333\n",
      "Iteration 6692, loss = 0.47170354\n",
      "Iteration 6693, loss = 0.47162339\n",
      "Iteration 6694, loss = 0.47131636\n",
      "Iteration 6695, loss = 0.47139856\n",
      "Iteration 6696, loss = 0.47129001\n",
      "Iteration 6697, loss = 0.47138539\n",
      "Iteration 6698, loss = 0.47136537\n",
      "Iteration 6699, loss = 0.47139185\n",
      "Iteration 6700, loss = 0.47128569\n",
      "Iteration 6701, loss = 0.47131019\n",
      "Iteration 6702, loss = 0.47128018\n",
      "Iteration 6703, loss = 0.47125491\n",
      "Iteration 6704, loss = 0.47148033\n",
      "Iteration 6705, loss = 0.47141913\n",
      "Iteration 6706, loss = 0.47132122\n",
      "Iteration 6707, loss = 0.47130612\n",
      "Iteration 6708, loss = 0.47142687\n",
      "Iteration 6709, loss = 0.47139908\n",
      "Iteration 6710, loss = 0.47131833\n",
      "Iteration 6711, loss = 0.47133409\n",
      "Iteration 6712, loss = 0.47136208\n",
      "Iteration 6713, loss = 0.47164175\n",
      "Iteration 6714, loss = 0.47118421\n",
      "Iteration 6715, loss = 0.47115732\n",
      "Iteration 6716, loss = 0.47179991\n",
      "Iteration 6717, loss = 0.47238341\n",
      "Iteration 6718, loss = 0.47183871\n",
      "Iteration 6719, loss = 0.47163352\n",
      "Iteration 6720, loss = 0.47142922\n",
      "Iteration 6721, loss = 0.47153016\n",
      "Iteration 6722, loss = 0.47180648\n",
      "Iteration 6723, loss = 0.47154183\n",
      "Iteration 6724, loss = 0.47153863\n",
      "Iteration 6725, loss = 0.47137029\n",
      "Iteration 6726, loss = 0.47132724\n",
      "Iteration 6727, loss = 0.47169045\n",
      "Iteration 6728, loss = 0.47132823\n",
      "Iteration 6729, loss = 0.47125184\n",
      "Iteration 6730, loss = 0.47115040\n",
      "Iteration 6731, loss = 0.47118574\n",
      "Iteration 6732, loss = 0.47150171\n",
      "Iteration 6733, loss = 0.47159535\n",
      "Iteration 6734, loss = 0.47156293\n",
      "Iteration 6735, loss = 0.47136925\n",
      "Iteration 6736, loss = 0.47121703\n",
      "Iteration 6737, loss = 0.47122158\n",
      "Iteration 6738, loss = 0.47130573\n",
      "Iteration 6739, loss = 0.47128673\n",
      "Iteration 6740, loss = 0.47139728\n",
      "Iteration 6741, loss = 0.47144690\n",
      "Iteration 6742, loss = 0.47142820\n",
      "Iteration 6743, loss = 0.47135452\n",
      "Iteration 6744, loss = 0.47142489\n",
      "Iteration 6745, loss = 0.47149891\n",
      "Iteration 6746, loss = 0.47158016\n",
      "Iteration 6747, loss = 0.47149222\n",
      "Iteration 6748, loss = 0.47198816\n",
      "Iteration 6749, loss = 0.47212254\n",
      "Iteration 6750, loss = 0.47187871\n",
      "Iteration 6751, loss = 0.47211418\n",
      "Iteration 6752, loss = 0.47153482\n",
      "Iteration 6753, loss = 0.47160596\n",
      "Iteration 6754, loss = 0.47121200\n",
      "Iteration 6755, loss = 0.47127358\n",
      "Iteration 6756, loss = 0.47155301\n",
      "Iteration 6757, loss = 0.47125164\n",
      "Iteration 6758, loss = 0.47119820\n",
      "Iteration 6759, loss = 0.47130745\n",
      "Iteration 6760, loss = 0.47144439\n",
      "Iteration 6761, loss = 0.47130893\n",
      "Iteration 6762, loss = 0.47104431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6763, loss = 0.47165227\n",
      "Iteration 6764, loss = 0.47168285\n",
      "Iteration 6765, loss = 0.47156310\n",
      "Iteration 6766, loss = 0.47142063\n",
      "Iteration 6767, loss = 0.47108081\n",
      "Iteration 6768, loss = 0.47216925\n",
      "Iteration 6769, loss = 0.47186599\n",
      "Iteration 6770, loss = 0.47128867\n",
      "Iteration 6771, loss = 0.47127801\n",
      "Iteration 6772, loss = 0.47147731\n",
      "Iteration 6773, loss = 0.47162059\n",
      "Iteration 6774, loss = 0.47159485\n",
      "Iteration 6775, loss = 0.47227278\n",
      "Iteration 6776, loss = 0.47132750\n",
      "Iteration 6777, loss = 0.47141209\n",
      "Iteration 6778, loss = 0.47203638\n",
      "Iteration 6779, loss = 0.47118918\n",
      "Iteration 6780, loss = 0.47125436\n",
      "Iteration 6781, loss = 0.47150989\n",
      "Iteration 6782, loss = 0.47129357\n",
      "Iteration 6783, loss = 0.47163831\n",
      "Iteration 6784, loss = 0.47146104\n",
      "Iteration 6785, loss = 0.47197801\n",
      "Iteration 6786, loss = 0.47133275\n",
      "Iteration 6787, loss = 0.47115618\n",
      "Iteration 6788, loss = 0.47105485\n",
      "Iteration 6789, loss = 0.47126841\n",
      "Iteration 6790, loss = 0.47176375\n",
      "Iteration 6791, loss = 0.47207275\n",
      "Iteration 6792, loss = 0.47183703\n",
      "Iteration 6793, loss = 0.47204756\n",
      "Iteration 6794, loss = 0.47143756\n",
      "Iteration 6795, loss = 0.47191858\n",
      "Iteration 6796, loss = 0.47149460\n",
      "Iteration 6797, loss = 0.47146928\n",
      "Iteration 6798, loss = 0.47150938\n",
      "Iteration 6799, loss = 0.47175160\n",
      "Iteration 6800, loss = 0.47231505\n",
      "Iteration 6801, loss = 0.47186312\n",
      "Iteration 6802, loss = 0.47169552\n",
      "Iteration 6803, loss = 0.47170730\n",
      "Iteration 6804, loss = 0.47134645\n",
      "Iteration 6805, loss = 0.47130755\n",
      "Iteration 6806, loss = 0.47134044\n",
      "Iteration 6807, loss = 0.47154648\n",
      "Iteration 6808, loss = 0.47156858\n",
      "Iteration 6809, loss = 0.47155996\n",
      "Iteration 6810, loss = 0.47162064\n",
      "Iteration 6811, loss = 0.47148611\n",
      "Iteration 6812, loss = 0.47128858\n",
      "Iteration 6813, loss = 0.47159522\n",
      "Iteration 6814, loss = 0.47125291\n",
      "Iteration 6815, loss = 0.47117331\n",
      "Iteration 6816, loss = 0.47183321\n",
      "Iteration 6817, loss = 0.47135154\n",
      "Iteration 6818, loss = 0.47127519\n",
      "Iteration 6819, loss = 0.47112003\n",
      "Iteration 6820, loss = 0.47109220\n",
      "Iteration 6821, loss = 0.47169156\n",
      "Iteration 6822, loss = 0.47229147\n",
      "Iteration 6823, loss = 0.47200863\n",
      "Iteration 6824, loss = 0.47173034\n",
      "Iteration 6825, loss = 0.47139277\n",
      "Iteration 6826, loss = 0.47137212\n",
      "Iteration 6827, loss = 0.47162792\n",
      "Iteration 6828, loss = 0.47181330\n",
      "Iteration 6829, loss = 0.47190283\n",
      "Iteration 6830, loss = 0.47179295\n",
      "Iteration 6831, loss = 0.47153969\n",
      "Iteration 6832, loss = 0.47163798\n",
      "Iteration 6833, loss = 0.47161293\n",
      "Iteration 6834, loss = 0.47132432\n",
      "Iteration 6835, loss = 0.47150075\n",
      "Iteration 6836, loss = 0.47155712\n",
      "Iteration 6837, loss = 0.47137513\n",
      "Iteration 6838, loss = 0.47153722\n",
      "Iteration 6839, loss = 0.47125360\n",
      "Iteration 6840, loss = 0.47123162\n",
      "Iteration 6841, loss = 0.47133318\n",
      "Iteration 6842, loss = 0.47116521\n",
      "Iteration 6843, loss = 0.47121561\n",
      "Iteration 6844, loss = 0.47141062\n",
      "Iteration 6845, loss = 0.47163491\n",
      "Iteration 6846, loss = 0.47136417\n",
      "Iteration 6847, loss = 0.47178330\n",
      "Iteration 6848, loss = 0.47154004\n",
      "Iteration 6849, loss = 0.47194830\n",
      "Iteration 6850, loss = 0.47149784\n",
      "Iteration 6851, loss = 0.47115080\n",
      "Iteration 6852, loss = 0.47131249\n",
      "Iteration 6853, loss = 0.47142459\n",
      "Iteration 6854, loss = 0.47149363\n",
      "Iteration 6855, loss = 0.47126796\n",
      "Iteration 6856, loss = 0.47157294\n",
      "Iteration 6857, loss = 0.47160149\n",
      "Iteration 6858, loss = 0.47158864\n",
      "Iteration 6859, loss = 0.47145356\n",
      "Iteration 6860, loss = 0.47149010\n",
      "Iteration 6861, loss = 0.47153704\n",
      "Iteration 6862, loss = 0.47144553\n",
      "Iteration 6863, loss = 0.47138209\n",
      "Iteration 6864, loss = 0.47147642\n",
      "Iteration 6865, loss = 0.47153527\n",
      "Iteration 6866, loss = 0.47142657\n",
      "Iteration 6867, loss = 0.47136173\n",
      "Iteration 6868, loss = 0.47138674\n",
      "Iteration 6869, loss = 0.47135444\n",
      "Iteration 6870, loss = 0.47137720\n",
      "Iteration 6871, loss = 0.47132341\n",
      "Iteration 6872, loss = 0.47138071\n",
      "Iteration 6873, loss = 0.47131871\n",
      "Iteration 6874, loss = 0.47134100\n",
      "Iteration 6875, loss = 0.47155804\n",
      "Iteration 6876, loss = 0.47129844\n",
      "Iteration 6877, loss = 0.47126474\n",
      "Iteration 6878, loss = 0.47129304\n",
      "Iteration 6879, loss = 0.47192634\n",
      "Iteration 6880, loss = 0.47208410\n",
      "Iteration 6881, loss = 0.47180999\n",
      "Iteration 6882, loss = 0.47137967\n",
      "Iteration 6883, loss = 0.47111770\n",
      "Iteration 6884, loss = 0.47124854\n",
      "Iteration 6885, loss = 0.47197913\n",
      "Iteration 6886, loss = 0.47237617\n",
      "Iteration 6887, loss = 0.47235320\n",
      "Iteration 6888, loss = 0.47186005\n",
      "Iteration 6889, loss = 0.47202357\n",
      "Iteration 6890, loss = 0.47133530\n",
      "Iteration 6891, loss = 0.47142058\n",
      "Iteration 6892, loss = 0.47123695\n",
      "Iteration 6893, loss = 0.47130362\n",
      "Iteration 6894, loss = 0.47129641\n",
      "Iteration 6895, loss = 0.47120774\n",
      "Iteration 6896, loss = 0.47118996\n",
      "Iteration 6897, loss = 0.47122437\n",
      "Iteration 6898, loss = 0.47151182\n",
      "Iteration 6899, loss = 0.47162398\n",
      "Iteration 6900, loss = 0.47127841\n",
      "Iteration 6901, loss = 0.47123705\n",
      "Iteration 6902, loss = 0.47125480\n",
      "Iteration 6903, loss = 0.47140148\n",
      "Iteration 6904, loss = 0.47119581\n",
      "Iteration 6905, loss = 0.47170820\n",
      "Iteration 6906, loss = 0.47150356\n",
      "Iteration 6907, loss = 0.47137559\n",
      "Iteration 6908, loss = 0.47136658\n",
      "Iteration 6909, loss = 0.47109920\n",
      "Iteration 6910, loss = 0.47153887\n",
      "Iteration 6911, loss = 0.47194232\n",
      "Iteration 6912, loss = 0.47173827\n",
      "Iteration 6913, loss = 0.47150062\n",
      "Iteration 6914, loss = 0.47124247\n",
      "Iteration 6915, loss = 0.47135067\n",
      "Iteration 6916, loss = 0.47167801\n",
      "Iteration 6917, loss = 0.47211610\n",
      "Iteration 6918, loss = 0.47235850\n",
      "Iteration 6919, loss = 0.47182036\n",
      "Iteration 6920, loss = 0.47145397\n",
      "Iteration 6921, loss = 0.47170095\n",
      "Iteration 6922, loss = 0.47131865\n",
      "Iteration 6923, loss = 0.47138324\n",
      "Iteration 6924, loss = 0.47129158\n",
      "Iteration 6925, loss = 0.47135281\n",
      "Iteration 6926, loss = 0.47126902\n",
      "Iteration 6927, loss = 0.47133526\n",
      "Iteration 6928, loss = 0.47128345\n",
      "Iteration 6929, loss = 0.47152561\n",
      "Iteration 6930, loss = 0.47109598\n",
      "Iteration 6931, loss = 0.47177591\n",
      "Iteration 6932, loss = 0.47145292\n",
      "Iteration 6933, loss = 0.47122453\n",
      "Iteration 6934, loss = 0.47155453\n",
      "Iteration 6935, loss = 0.47140591\n",
      "Iteration 6936, loss = 0.47161110\n",
      "Iteration 6937, loss = 0.47189661\n",
      "Iteration 6938, loss = 0.47171547\n",
      "Iteration 6939, loss = 0.47141522\n",
      "Iteration 6940, loss = 0.47117459\n",
      "Iteration 6941, loss = 0.47143670\n",
      "Iteration 6942, loss = 0.47162813\n",
      "Iteration 6943, loss = 0.47145121\n",
      "Iteration 6944, loss = 0.47187885\n",
      "Iteration 6945, loss = 0.47123436\n",
      "Iteration 6946, loss = 0.47135805\n",
      "Iteration 6947, loss = 0.47122842\n",
      "Iteration 6948, loss = 0.47126052\n",
      "Iteration 6949, loss = 0.47134103\n",
      "Iteration 6950, loss = 0.47141948\n",
      "Iteration 6951, loss = 0.47156704\n",
      "Iteration 6952, loss = 0.47172106\n",
      "Iteration 6953, loss = 0.47154874\n",
      "Iteration 6954, loss = 0.47153416\n",
      "Iteration 6955, loss = 0.47132887\n",
      "Iteration 6956, loss = 0.47155639\n",
      "Iteration 6957, loss = 0.47124100\n",
      "Iteration 6958, loss = 0.47139370\n",
      "Iteration 6959, loss = 0.47151576\n",
      "Iteration 6960, loss = 0.47175479\n",
      "Iteration 6961, loss = 0.47170180\n",
      "Iteration 6962, loss = 0.47216439\n",
      "Iteration 6963, loss = 0.47188645\n",
      "Iteration 6964, loss = 0.47151856\n",
      "Iteration 6965, loss = 0.47270877\n",
      "Iteration 6966, loss = 0.47140293\n",
      "Iteration 6967, loss = 0.47130731\n",
      "Iteration 6968, loss = 0.47149003\n",
      "Iteration 6969, loss = 0.47135337\n",
      "Iteration 6970, loss = 0.47116597\n",
      "Iteration 6971, loss = 0.47117861\n",
      "Iteration 6972, loss = 0.47123794\n",
      "Iteration 6973, loss = 0.47113990\n",
      "Iteration 6974, loss = 0.47117509\n",
      "Iteration 6975, loss = 0.47119898\n",
      "Iteration 6976, loss = 0.47122383\n",
      "Iteration 6977, loss = 0.47113579\n",
      "Iteration 6978, loss = 0.47108658\n",
      "Iteration 6979, loss = 0.47126021\n",
      "Iteration 6980, loss = 0.47163709\n",
      "Iteration 6981, loss = 0.47149865\n",
      "Iteration 6982, loss = 0.47125415\n",
      "Iteration 6983, loss = 0.47115755\n",
      "Iteration 6984, loss = 0.47119541\n",
      "Iteration 6985, loss = 0.47123900\n",
      "Iteration 6986, loss = 0.47112693\n",
      "Iteration 6987, loss = 0.47155199\n",
      "Iteration 6988, loss = 0.47121955\n",
      "Iteration 6989, loss = 0.47203104\n",
      "Iteration 6990, loss = 0.47128254\n",
      "Iteration 6991, loss = 0.47115472\n",
      "Iteration 6992, loss = 0.47109151\n",
      "Iteration 6993, loss = 0.47135342\n",
      "Iteration 6994, loss = 0.47132652\n",
      "Iteration 6995, loss = 0.47195432\n",
      "Iteration 6996, loss = 0.47278630\n",
      "Iteration 6997, loss = 0.47181111\n",
      "Iteration 6998, loss = 0.47182283\n",
      "Iteration 6999, loss = 0.47139144\n",
      "Iteration 7000, loss = 0.47164640\n",
      "Iteration 7001, loss = 0.47157047\n",
      "Iteration 7002, loss = 0.47171762\n",
      "Iteration 7003, loss = 0.47131739\n",
      "Iteration 7004, loss = 0.47129585\n",
      "Iteration 7005, loss = 0.47143590\n",
      "Iteration 7006, loss = 0.47154481\n",
      "Iteration 7007, loss = 0.47125821\n",
      "Iteration 7008, loss = 0.47120909\n",
      "Iteration 7009, loss = 0.47174178\n",
      "Iteration 7010, loss = 0.47138762\n",
      "Iteration 7011, loss = 0.47142845\n",
      "Iteration 7012, loss = 0.47115006\n",
      "Iteration 7013, loss = 0.47136828\n",
      "Iteration 7014, loss = 0.47140904\n",
      "Iteration 7015, loss = 0.47175169\n",
      "Iteration 7016, loss = 0.47139735\n",
      "Iteration 7017, loss = 0.47138363\n",
      "Iteration 7018, loss = 0.47132783\n",
      "Iteration 7019, loss = 0.47124854\n",
      "Iteration 7020, loss = 0.47126256\n",
      "Iteration 7021, loss = 0.47120329\n",
      "Iteration 7022, loss = 0.47116480\n",
      "Iteration 7023, loss = 0.47123233\n",
      "Iteration 7024, loss = 0.47120109\n",
      "Iteration 7025, loss = 0.47140959\n",
      "Iteration 7026, loss = 0.47123901\n",
      "Iteration 7027, loss = 0.47133191\n",
      "Iteration 7028, loss = 0.47128996\n",
      "Iteration 7029, loss = 0.47146378\n",
      "Iteration 7030, loss = 0.47102012\n",
      "Iteration 7031, loss = 0.47195861\n",
      "Iteration 7032, loss = 0.47161524\n",
      "Iteration 7033, loss = 0.47137110\n",
      "Iteration 7034, loss = 0.47119818\n",
      "Iteration 7035, loss = 0.47110725\n",
      "Iteration 7036, loss = 0.47134823\n",
      "Iteration 7037, loss = 0.47179382\n",
      "Iteration 7038, loss = 0.47125108\n",
      "Iteration 7039, loss = 0.47164274\n",
      "Iteration 7040, loss = 0.47174043\n",
      "Iteration 7041, loss = 0.47165597\n",
      "Iteration 7042, loss = 0.47153619\n",
      "Iteration 7043, loss = 0.47171172\n",
      "Iteration 7044, loss = 0.47165960\n",
      "Iteration 7045, loss = 0.47166074\n",
      "Iteration 7046, loss = 0.47156477\n",
      "Iteration 7047, loss = 0.47156548\n",
      "Iteration 7048, loss = 0.47193575\n",
      "Iteration 7049, loss = 0.47139482\n",
      "Iteration 7050, loss = 0.47125452\n",
      "Iteration 7051, loss = 0.47145253\n",
      "Iteration 7052, loss = 0.47188597\n",
      "Iteration 7053, loss = 0.47190644\n",
      "Iteration 7054, loss = 0.47180089\n",
      "Iteration 7055, loss = 0.47223937\n",
      "Iteration 7056, loss = 0.47120584\n",
      "Iteration 7057, loss = 0.47135943\n",
      "Iteration 7058, loss = 0.47150206\n",
      "Iteration 7059, loss = 0.47171396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7060, loss = 0.47163243\n",
      "Iteration 7061, loss = 0.47151677\n",
      "Iteration 7062, loss = 0.47145897\n",
      "Iteration 7063, loss = 0.47120473\n",
      "Iteration 7064, loss = 0.47130338\n",
      "Iteration 7065, loss = 0.47122938\n",
      "Iteration 7066, loss = 0.47113523\n",
      "Iteration 7067, loss = 0.47118363\n",
      "Iteration 7068, loss = 0.47123105\n",
      "Iteration 7069, loss = 0.47137200\n",
      "Iteration 7070, loss = 0.47143951\n",
      "Iteration 7071, loss = 0.47132938\n",
      "Iteration 7072, loss = 0.47120997\n",
      "Iteration 7073, loss = 0.47097380\n",
      "Iteration 7074, loss = 0.47147076\n",
      "Iteration 7075, loss = 0.47161534\n",
      "Iteration 7076, loss = 0.47166022\n",
      "Iteration 7077, loss = 0.47151691\n",
      "Iteration 7078, loss = 0.47152105\n",
      "Iteration 7079, loss = 0.47126304\n",
      "Iteration 7080, loss = 0.47124663\n",
      "Iteration 7081, loss = 0.47128755\n",
      "Iteration 7082, loss = 0.47140070\n",
      "Iteration 7083, loss = 0.47154725\n",
      "Iteration 7084, loss = 0.47151888\n",
      "Iteration 7085, loss = 0.47181948\n",
      "Iteration 7086, loss = 0.47134543\n",
      "Iteration 7087, loss = 0.47138634\n",
      "Iteration 7088, loss = 0.47124646\n",
      "Iteration 7089, loss = 0.47149588\n",
      "Iteration 7090, loss = 0.47125980\n",
      "Iteration 7091, loss = 0.47119781\n",
      "Iteration 7092, loss = 0.47115540\n",
      "Iteration 7093, loss = 0.47118810\n",
      "Iteration 7094, loss = 0.47123383\n",
      "Iteration 7095, loss = 0.47187077\n",
      "Iteration 7096, loss = 0.47156501\n",
      "Iteration 7097, loss = 0.47110304\n",
      "Iteration 7098, loss = 0.47092229\n",
      "Iteration 7099, loss = 0.47133125\n",
      "Iteration 7100, loss = 0.47198383\n",
      "Iteration 7101, loss = 0.47233014\n",
      "Iteration 7102, loss = 0.47231651\n",
      "Iteration 7103, loss = 0.47151435\n",
      "Iteration 7104, loss = 0.47133193\n",
      "Iteration 7105, loss = 0.47143068\n",
      "Iteration 7106, loss = 0.47115328\n",
      "Iteration 7107, loss = 0.47122775\n",
      "Iteration 7108, loss = 0.47149145\n",
      "Iteration 7109, loss = 0.47144151\n",
      "Iteration 7110, loss = 0.47156724\n",
      "Iteration 7111, loss = 0.47159630\n",
      "Iteration 7112, loss = 0.47138724\n",
      "Iteration 7113, loss = 0.47149928\n",
      "Iteration 7114, loss = 0.47124361\n",
      "Iteration 7115, loss = 0.47129989\n",
      "Iteration 7116, loss = 0.47136446\n",
      "Iteration 7117, loss = 0.47127062\n",
      "Iteration 7118, loss = 0.47121994\n",
      "Iteration 7119, loss = 0.47117597\n",
      "Iteration 7120, loss = 0.47125982\n",
      "Iteration 7121, loss = 0.47107298\n",
      "Iteration 7122, loss = 0.47133671\n",
      "Iteration 7123, loss = 0.47137501\n",
      "Iteration 7124, loss = 0.47144493\n",
      "Iteration 7125, loss = 0.47157506\n",
      "Iteration 7126, loss = 0.47128314\n",
      "Iteration 7127, loss = 0.47151456\n",
      "Iteration 7128, loss = 0.47128449\n",
      "Iteration 7129, loss = 0.47116568\n",
      "Iteration 7130, loss = 0.47155012\n",
      "Iteration 7131, loss = 0.47121244\n",
      "Iteration 7132, loss = 0.47118936\n",
      "Iteration 7133, loss = 0.47121800\n",
      "Iteration 7134, loss = 0.47119565\n",
      "Iteration 7135, loss = 0.47123139\n",
      "Iteration 7136, loss = 0.47118453\n",
      "Iteration 7137, loss = 0.47120319\n",
      "Iteration 7138, loss = 0.47130456\n",
      "Iteration 7139, loss = 0.47119886\n",
      "Iteration 7140, loss = 0.47115824\n",
      "Iteration 7141, loss = 0.47131750\n",
      "Iteration 7142, loss = 0.47116424\n",
      "Iteration 7143, loss = 0.47110670\n",
      "Iteration 7144, loss = 0.47121492\n",
      "Iteration 7145, loss = 0.47127310\n",
      "Iteration 7146, loss = 0.47121852\n",
      "Iteration 7147, loss = 0.47189736\n",
      "Iteration 7148, loss = 0.47128883\n",
      "Iteration 7149, loss = 0.47132604\n",
      "Iteration 7150, loss = 0.47133326\n",
      "Iteration 7151, loss = 0.47142475\n",
      "Iteration 7152, loss = 0.47133059\n",
      "Iteration 7153, loss = 0.47134408\n",
      "Iteration 7154, loss = 0.47127625\n",
      "Iteration 7155, loss = 0.47131521\n",
      "Iteration 7156, loss = 0.47137244\n",
      "Iteration 7157, loss = 0.47145600\n",
      "Iteration 7158, loss = 0.47183051\n",
      "Iteration 7159, loss = 0.47139118\n",
      "Iteration 7160, loss = 0.47154440\n",
      "Iteration 7161, loss = 0.47141803\n",
      "Iteration 7162, loss = 0.47138152\n",
      "Iteration 7163, loss = 0.47141593\n",
      "Iteration 7164, loss = 0.47131250\n",
      "Iteration 7165, loss = 0.47114982\n",
      "Iteration 7166, loss = 0.47119296\n",
      "Iteration 7167, loss = 0.47151707\n",
      "Iteration 7168, loss = 0.47186821\n",
      "Iteration 7169, loss = 0.47187632\n",
      "Iteration 7170, loss = 0.47177678\n",
      "Iteration 7171, loss = 0.47155180\n",
      "Iteration 7172, loss = 0.47135054\n",
      "Iteration 7173, loss = 0.47124194\n",
      "Iteration 7174, loss = 0.47130174\n",
      "Iteration 7175, loss = 0.47141317\n",
      "Iteration 7176, loss = 0.47147404\n",
      "Iteration 7177, loss = 0.47165252\n",
      "Iteration 7178, loss = 0.47156827\n",
      "Iteration 7179, loss = 0.47110513\n",
      "Iteration 7180, loss = 0.47122472\n",
      "Iteration 7181, loss = 0.47158617\n",
      "Iteration 7182, loss = 0.47194871\n",
      "Iteration 7183, loss = 0.47180343\n",
      "Iteration 7184, loss = 0.47210534\n",
      "Iteration 7185, loss = 0.47135709\n",
      "Iteration 7186, loss = 0.47145380\n",
      "Iteration 7187, loss = 0.47119906\n",
      "Iteration 7188, loss = 0.47175900\n",
      "Iteration 7189, loss = 0.47121606\n",
      "Iteration 7190, loss = 0.47121107\n",
      "Iteration 7191, loss = 0.47106443\n",
      "Iteration 7192, loss = 0.47128003\n",
      "Iteration 7193, loss = 0.47125030\n",
      "Iteration 7194, loss = 0.47117060\n",
      "Iteration 7195, loss = 0.47158573\n",
      "Iteration 7196, loss = 0.47109416\n",
      "Iteration 7197, loss = 0.47122494\n",
      "Iteration 7198, loss = 0.47118882\n",
      "Iteration 7199, loss = 0.47136453\n",
      "Iteration 7200, loss = 0.47141999\n",
      "Iteration 7201, loss = 0.47138675\n",
      "Iteration 7202, loss = 0.47120855\n",
      "Iteration 7203, loss = 0.47132348\n",
      "Iteration 7204, loss = 0.47120249\n",
      "Iteration 7205, loss = 0.47116259\n",
      "Iteration 7206, loss = 0.47103723\n",
      "Iteration 7207, loss = 0.47107011\n",
      "Iteration 7208, loss = 0.47150191\n",
      "Iteration 7209, loss = 0.47159773\n",
      "Iteration 7210, loss = 0.47109798\n",
      "Iteration 7211, loss = 0.47090669\n",
      "Iteration 7212, loss = 0.47136023\n",
      "Iteration 7213, loss = 0.47213139\n",
      "Iteration 7214, loss = 0.47235072\n",
      "Iteration 7215, loss = 0.47157738\n",
      "Iteration 7216, loss = 0.47116777\n",
      "Iteration 7217, loss = 0.47116597\n",
      "Iteration 7218, loss = 0.47229516\n",
      "Iteration 7219, loss = 0.47285419\n",
      "Iteration 7220, loss = 0.47282145\n",
      "Iteration 7221, loss = 0.47225022\n",
      "Iteration 7222, loss = 0.47166725\n",
      "Iteration 7223, loss = 0.47126989\n",
      "Iteration 7224, loss = 0.47131437\n",
      "Iteration 7225, loss = 0.47116564\n",
      "Iteration 7226, loss = 0.47154578\n",
      "Iteration 7227, loss = 0.47191198\n",
      "Iteration 7228, loss = 0.47122742\n",
      "Iteration 7229, loss = 0.47123921\n",
      "Iteration 7230, loss = 0.47139263\n",
      "Iteration 7231, loss = 0.47118928\n",
      "Iteration 7232, loss = 0.47116300\n",
      "Iteration 7233, loss = 0.47128380\n",
      "Iteration 7234, loss = 0.47137291\n",
      "Iteration 7235, loss = 0.47141346\n",
      "Iteration 7236, loss = 0.47136144\n",
      "Iteration 7237, loss = 0.47111802\n",
      "Iteration 7238, loss = 0.47146418\n",
      "Iteration 7239, loss = 0.47144102\n",
      "Iteration 7240, loss = 0.47211873\n",
      "Iteration 7241, loss = 0.47134944\n",
      "Iteration 7242, loss = 0.47116847\n",
      "Iteration 7243, loss = 0.47175967\n",
      "Iteration 7244, loss = 0.47187468\n",
      "Iteration 7245, loss = 0.47144488\n",
      "Iteration 7246, loss = 0.47182741\n",
      "Iteration 7247, loss = 0.47117582\n",
      "Iteration 7248, loss = 0.47120863\n",
      "Iteration 7249, loss = 0.47112873\n",
      "Iteration 7250, loss = 0.47148894\n",
      "Iteration 7251, loss = 0.47130302\n",
      "Iteration 7252, loss = 0.47117749\n",
      "Iteration 7253, loss = 0.47124416\n",
      "Iteration 7254, loss = 0.47143387\n",
      "Iteration 7255, loss = 0.47115812\n",
      "Iteration 7256, loss = 0.47123051\n",
      "Iteration 7257, loss = 0.47118453\n",
      "Iteration 7258, loss = 0.47115635\n",
      "Iteration 7259, loss = 0.47114165\n",
      "Iteration 7260, loss = 0.47150311\n",
      "Iteration 7261, loss = 0.47112023\n",
      "Iteration 7262, loss = 0.47134747\n",
      "Iteration 7263, loss = 0.47142448\n",
      "Iteration 7264, loss = 0.47133585\n",
      "Iteration 7265, loss = 0.47118754\n",
      "Iteration 7266, loss = 0.47118535\n",
      "Iteration 7267, loss = 0.47160667\n",
      "Iteration 7268, loss = 0.47145740\n",
      "Iteration 7269, loss = 0.47128348\n",
      "Iteration 7270, loss = 0.47096104\n",
      "Iteration 7271, loss = 0.47136186\n",
      "Iteration 7272, loss = 0.47208860\n",
      "Iteration 7273, loss = 0.47175420\n",
      "Iteration 7274, loss = 0.47100747\n",
      "Iteration 7275, loss = 0.47122453\n",
      "Iteration 7276, loss = 0.47152821\n",
      "Iteration 7277, loss = 0.47163682\n",
      "Iteration 7278, loss = 0.47166952\n",
      "Iteration 7279, loss = 0.47152455\n",
      "Iteration 7280, loss = 0.47158213\n",
      "Iteration 7281, loss = 0.47150283\n",
      "Iteration 7282, loss = 0.47136963\n",
      "Iteration 7283, loss = 0.47134461\n",
      "Iteration 7284, loss = 0.47110581\n",
      "Iteration 7285, loss = 0.47115497\n",
      "Iteration 7286, loss = 0.47105028\n",
      "Iteration 7287, loss = 0.47132207\n",
      "Iteration 7288, loss = 0.47139793\n",
      "Iteration 7289, loss = 0.47112123\n",
      "Iteration 7290, loss = 0.47202276\n",
      "Iteration 7291, loss = 0.47121349\n",
      "Iteration 7292, loss = 0.47110927\n",
      "Iteration 7293, loss = 0.47112661\n",
      "Iteration 7294, loss = 0.47127162\n",
      "Iteration 7295, loss = 0.47114966\n",
      "Iteration 7296, loss = 0.47156276\n",
      "Iteration 7297, loss = 0.47144863\n",
      "Iteration 7298, loss = 0.47105725\n",
      "Iteration 7299, loss = 0.47122720\n",
      "Iteration 7300, loss = 0.47134800\n",
      "Iteration 7301, loss = 0.47157284\n",
      "Iteration 7302, loss = 0.47127524\n",
      "Iteration 7303, loss = 0.47103716\n",
      "Iteration 7304, loss = 0.47115584\n",
      "Iteration 7305, loss = 0.47148399\n",
      "Iteration 7306, loss = 0.47197784\n",
      "Iteration 7307, loss = 0.47136657\n",
      "Iteration 7308, loss = 0.47140870\n",
      "Iteration 7309, loss = 0.47151640\n",
      "Iteration 7310, loss = 0.47179525\n",
      "Iteration 7311, loss = 0.47174621\n",
      "Iteration 7312, loss = 0.47139040\n",
      "Iteration 7313, loss = 0.47151868\n",
      "Iteration 7314, loss = 0.47118445\n",
      "Iteration 7315, loss = 0.47175445\n",
      "Iteration 7316, loss = 0.47159781\n",
      "Iteration 7317, loss = 0.47131163\n",
      "Iteration 7318, loss = 0.47200186\n",
      "Iteration 7319, loss = 0.47121363\n",
      "Iteration 7320, loss = 0.47098313\n",
      "Iteration 7321, loss = 0.47145668\n",
      "Iteration 7322, loss = 0.47168414\n",
      "Iteration 7323, loss = 0.47118061\n",
      "Iteration 7324, loss = 0.47144323\n",
      "Iteration 7325, loss = 0.47151033\n",
      "Iteration 7326, loss = 0.47142593\n",
      "Iteration 7327, loss = 0.47116691\n",
      "Iteration 7328, loss = 0.47141201\n",
      "Iteration 7329, loss = 0.47128989\n",
      "Iteration 7330, loss = 0.47109707\n",
      "Iteration 7331, loss = 0.47122437\n",
      "Iteration 7332, loss = 0.47115822\n",
      "Iteration 7333, loss = 0.47116020\n",
      "Iteration 7334, loss = 0.47128631\n",
      "Iteration 7335, loss = 0.47125305\n",
      "Iteration 7336, loss = 0.47172773\n",
      "Iteration 7337, loss = 0.47196949\n",
      "Iteration 7338, loss = 0.47112168\n",
      "Iteration 7339, loss = 0.47106152\n",
      "Iteration 7340, loss = 0.47117128\n",
      "Iteration 7341, loss = 0.47110222\n",
      "Iteration 7342, loss = 0.47107124\n",
      "Iteration 7343, loss = 0.47125961\n",
      "Iteration 7344, loss = 0.47168615\n",
      "Iteration 7345, loss = 0.47168956\n",
      "Iteration 7346, loss = 0.47107193\n",
      "Iteration 7347, loss = 0.47130126\n",
      "Iteration 7348, loss = 0.47180207\n",
      "Iteration 7349, loss = 0.47182155\n",
      "Iteration 7350, loss = 0.47145905\n",
      "Iteration 7351, loss = 0.47093637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7352, loss = 0.47179850\n",
      "Iteration 7353, loss = 0.47181414\n",
      "Iteration 7354, loss = 0.47154117\n",
      "Iteration 7355, loss = 0.47209127\n",
      "Iteration 7356, loss = 0.47137061\n",
      "Iteration 7357, loss = 0.47141521\n",
      "Iteration 7358, loss = 0.47133283\n",
      "Iteration 7359, loss = 0.47122088\n",
      "Iteration 7360, loss = 0.47134895\n",
      "Iteration 7361, loss = 0.47152487\n",
      "Iteration 7362, loss = 0.47147881\n",
      "Iteration 7363, loss = 0.47154988\n",
      "Iteration 7364, loss = 0.47126568\n",
      "Iteration 7365, loss = 0.47117298\n",
      "Iteration 7366, loss = 0.47127685\n",
      "Iteration 7367, loss = 0.47151619\n",
      "Iteration 7368, loss = 0.47156397\n",
      "Iteration 7369, loss = 0.47159184\n",
      "Iteration 7370, loss = 0.47111971\n",
      "Iteration 7371, loss = 0.47098289\n",
      "Iteration 7372, loss = 0.47140537\n",
      "Iteration 7373, loss = 0.47187044\n",
      "Iteration 7374, loss = 0.47165610\n",
      "Iteration 7375, loss = 0.47117126\n",
      "Iteration 7376, loss = 0.47093785\n",
      "Iteration 7377, loss = 0.47134320\n",
      "Iteration 7378, loss = 0.47264853\n",
      "Iteration 7379, loss = 0.47256971\n",
      "Iteration 7380, loss = 0.47261890\n",
      "Iteration 7381, loss = 0.47131478\n",
      "Iteration 7382, loss = 0.47099174\n",
      "Iteration 7383, loss = 0.47139927\n",
      "Iteration 7384, loss = 0.47163263\n",
      "Iteration 7385, loss = 0.47162264\n",
      "Iteration 7386, loss = 0.47139616\n",
      "Iteration 7387, loss = 0.47101059\n",
      "Iteration 7388, loss = 0.47149807\n",
      "Iteration 7389, loss = 0.47137659\n",
      "Iteration 7390, loss = 0.47127684\n",
      "Iteration 7391, loss = 0.47157705\n",
      "Iteration 7392, loss = 0.47124283\n",
      "Iteration 7393, loss = 0.47113825\n",
      "Iteration 7394, loss = 0.47133103\n",
      "Iteration 7395, loss = 0.47114168\n",
      "Iteration 7396, loss = 0.47118102\n",
      "Iteration 7397, loss = 0.47105080\n",
      "Iteration 7398, loss = 0.47135435\n",
      "Iteration 7399, loss = 0.47130638\n",
      "Iteration 7400, loss = 0.47126568\n",
      "Iteration 7401, loss = 0.47106259\n",
      "Iteration 7402, loss = 0.47112099\n",
      "Iteration 7403, loss = 0.47124799\n",
      "Iteration 7404, loss = 0.47137889\n",
      "Iteration 7405, loss = 0.47153985\n",
      "Iteration 7406, loss = 0.47129542\n",
      "Iteration 7407, loss = 0.47151470\n",
      "Iteration 7408, loss = 0.47127095\n",
      "Iteration 7409, loss = 0.47118900\n",
      "Iteration 7410, loss = 0.47105652\n",
      "Iteration 7411, loss = 0.47115032\n",
      "Iteration 7412, loss = 0.47120675\n",
      "Iteration 7413, loss = 0.47150062\n",
      "Iteration 7414, loss = 0.47115013\n",
      "Iteration 7415, loss = 0.47121840\n",
      "Iteration 7416, loss = 0.47142043\n",
      "Iteration 7417, loss = 0.47127175\n",
      "Iteration 7418, loss = 0.47109747\n",
      "Iteration 7419, loss = 0.47111691\n",
      "Iteration 7420, loss = 0.47108970\n",
      "Iteration 7421, loss = 0.47132730\n",
      "Iteration 7422, loss = 0.47148475\n",
      "Iteration 7423, loss = 0.47150782\n",
      "Iteration 7424, loss = 0.47146192\n",
      "Iteration 7425, loss = 0.47139682\n",
      "Iteration 7426, loss = 0.47164099\n",
      "Iteration 7427, loss = 0.47163801\n",
      "Iteration 7428, loss = 0.47118351\n",
      "Iteration 7429, loss = 0.47125979\n",
      "Iteration 7430, loss = 0.47113841\n",
      "Iteration 7431, loss = 0.47140640\n",
      "Iteration 7432, loss = 0.47140466\n",
      "Iteration 7433, loss = 0.47123997\n",
      "Iteration 7434, loss = 0.47127148\n",
      "Iteration 7435, loss = 0.47181812\n",
      "Iteration 7436, loss = 0.47120962\n",
      "Iteration 7437, loss = 0.47118183\n",
      "Iteration 7438, loss = 0.47115335\n",
      "Iteration 7439, loss = 0.47107956\n",
      "Iteration 7440, loss = 0.47118576\n",
      "Iteration 7441, loss = 0.47117959\n",
      "Iteration 7442, loss = 0.47140155\n",
      "Iteration 7443, loss = 0.47117163\n",
      "Iteration 7444, loss = 0.47097088\n",
      "Iteration 7445, loss = 0.47111924\n",
      "Iteration 7446, loss = 0.47146198\n",
      "Iteration 7447, loss = 0.47150382\n",
      "Iteration 7448, loss = 0.47093491\n",
      "Iteration 7449, loss = 0.47146653\n",
      "Iteration 7450, loss = 0.47161645\n",
      "Iteration 7451, loss = 0.47190425\n",
      "Iteration 7452, loss = 0.47216676\n",
      "Iteration 7453, loss = 0.47149726\n",
      "Iteration 7454, loss = 0.47125576\n",
      "Iteration 7455, loss = 0.47199436\n",
      "Iteration 7456, loss = 0.47130063\n",
      "Iteration 7457, loss = 0.47169861\n",
      "Iteration 7458, loss = 0.47104004\n",
      "Iteration 7459, loss = 0.47121091\n",
      "Iteration 7460, loss = 0.47106995\n",
      "Iteration 7461, loss = 0.47104291\n",
      "Iteration 7462, loss = 0.47173609\n",
      "Iteration 7463, loss = 0.47126627\n",
      "Iteration 7464, loss = 0.47135475\n",
      "Iteration 7465, loss = 0.47110713\n",
      "Iteration 7466, loss = 0.47145775\n",
      "Iteration 7467, loss = 0.47132159\n",
      "Iteration 7468, loss = 0.47110675\n",
      "Iteration 7469, loss = 0.47113651\n",
      "Iteration 7470, loss = 0.47119968\n",
      "Iteration 7471, loss = 0.47136383\n",
      "Iteration 7472, loss = 0.47157463\n",
      "Iteration 7473, loss = 0.47152664\n",
      "Iteration 7474, loss = 0.47128346\n",
      "Iteration 7475, loss = 0.47124796\n",
      "Iteration 7476, loss = 0.47124242\n",
      "Iteration 7477, loss = 0.47142971\n",
      "Iteration 7478, loss = 0.47183302\n",
      "Iteration 7479, loss = 0.47194646\n",
      "Iteration 7480, loss = 0.47161394\n",
      "Iteration 7481, loss = 0.47161619\n",
      "Iteration 7482, loss = 0.47159609\n",
      "Iteration 7483, loss = 0.47120916\n",
      "Iteration 7484, loss = 0.47115562\n",
      "Iteration 7485, loss = 0.47122943\n",
      "Iteration 7486, loss = 0.47144970\n",
      "Iteration 7487, loss = 0.47114372\n",
      "Iteration 7488, loss = 0.47141770\n",
      "Iteration 7489, loss = 0.47189911\n",
      "Iteration 7490, loss = 0.47144973\n",
      "Iteration 7491, loss = 0.47131536\n",
      "Iteration 7492, loss = 0.47126910\n",
      "Iteration 7493, loss = 0.47140170\n",
      "Iteration 7494, loss = 0.47136791\n",
      "Iteration 7495, loss = 0.47103810\n",
      "Iteration 7496, loss = 0.47127628\n",
      "Iteration 7497, loss = 0.47123939\n",
      "Iteration 7498, loss = 0.47131788\n",
      "Iteration 7499, loss = 0.47120294\n",
      "Iteration 7500, loss = 0.47107746\n",
      "Iteration 7501, loss = 0.47115601\n",
      "Iteration 7502, loss = 0.47148735\n",
      "Iteration 7503, loss = 0.47192354\n",
      "Iteration 7504, loss = 0.47152051\n",
      "Iteration 7505, loss = 0.47165813\n",
      "Iteration 7506, loss = 0.47176245\n",
      "Iteration 7507, loss = 0.47162536\n",
      "Iteration 7508, loss = 0.47104096\n",
      "Iteration 7509, loss = 0.47135306\n",
      "Iteration 7510, loss = 0.47134575\n",
      "Iteration 7511, loss = 0.47131566\n",
      "Iteration 7512, loss = 0.47166779\n",
      "Iteration 7513, loss = 0.47123959\n",
      "Iteration 7514, loss = 0.47130159\n",
      "Iteration 7515, loss = 0.47105511\n",
      "Iteration 7516, loss = 0.47111707\n",
      "Iteration 7517, loss = 0.47104106\n",
      "Iteration 7518, loss = 0.47112686\n",
      "Iteration 7519, loss = 0.47116020\n",
      "Iteration 7520, loss = 0.47113008\n",
      "Iteration 7521, loss = 0.47112876\n",
      "Iteration 7522, loss = 0.47124653\n",
      "Iteration 7523, loss = 0.47120813\n",
      "Iteration 7524, loss = 0.47120467\n",
      "Iteration 7525, loss = 0.47107994\n",
      "Iteration 7526, loss = 0.47120052\n",
      "Iteration 7527, loss = 0.47153993\n",
      "Iteration 7528, loss = 0.47090058\n",
      "Iteration 7529, loss = 0.47100437\n",
      "Iteration 7530, loss = 0.47219240\n",
      "Iteration 7531, loss = 0.47182071\n",
      "Iteration 7532, loss = 0.47145177\n",
      "Iteration 7533, loss = 0.47104456\n",
      "Iteration 7534, loss = 0.47165202\n",
      "Iteration 7535, loss = 0.47134654\n",
      "Iteration 7536, loss = 0.47113482\n",
      "Iteration 7537, loss = 0.47110587\n",
      "Iteration 7538, loss = 0.47100899\n",
      "Iteration 7539, loss = 0.47121348\n",
      "Iteration 7540, loss = 0.47140246\n",
      "Iteration 7541, loss = 0.47135001\n",
      "Iteration 7542, loss = 0.47144077\n",
      "Iteration 7543, loss = 0.47126810\n",
      "Iteration 7544, loss = 0.47168199\n",
      "Iteration 7545, loss = 0.47116370\n",
      "Iteration 7546, loss = 0.47104208\n",
      "Iteration 7547, loss = 0.47145156\n",
      "Iteration 7548, loss = 0.47178555\n",
      "Iteration 7549, loss = 0.47159055\n",
      "Iteration 7550, loss = 0.47178017\n",
      "Iteration 7551, loss = 0.47153495\n",
      "Iteration 7552, loss = 0.47132070\n",
      "Iteration 7553, loss = 0.47122032\n",
      "Iteration 7554, loss = 0.47123714\n",
      "Iteration 7555, loss = 0.47112574\n",
      "Iteration 7556, loss = 0.47108196\n",
      "Iteration 7557, loss = 0.47110222\n",
      "Iteration 7558, loss = 0.47116508\n",
      "Iteration 7559, loss = 0.47103928\n",
      "Iteration 7560, loss = 0.47157642\n",
      "Iteration 7561, loss = 0.47097772\n",
      "Iteration 7562, loss = 0.47138112\n",
      "Iteration 7563, loss = 0.47158744\n",
      "Iteration 7564, loss = 0.47131846\n",
      "Iteration 7565, loss = 0.47108042\n",
      "Iteration 7566, loss = 0.47127558\n",
      "Iteration 7567, loss = 0.47121946\n",
      "Iteration 7568, loss = 0.47128144\n",
      "Iteration 7569, loss = 0.47110883\n",
      "Iteration 7570, loss = 0.47095466\n",
      "Iteration 7571, loss = 0.47112658\n",
      "Iteration 7572, loss = 0.47129595\n",
      "Iteration 7573, loss = 0.47134080\n",
      "Iteration 7574, loss = 0.47131864\n",
      "Iteration 7575, loss = 0.47103746\n",
      "Iteration 7576, loss = 0.47125333\n",
      "Iteration 7577, loss = 0.47098591\n",
      "Iteration 7578, loss = 0.47105009\n",
      "Iteration 7579, loss = 0.47131973\n",
      "Iteration 7580, loss = 0.47113183\n",
      "Iteration 7581, loss = 0.47174714\n",
      "Iteration 7582, loss = 0.47218753\n",
      "Iteration 7583, loss = 0.47225286\n",
      "Iteration 7584, loss = 0.47187562\n",
      "Iteration 7585, loss = 0.47138853\n",
      "Iteration 7586, loss = 0.47140794\n",
      "Iteration 7587, loss = 0.47130093\n",
      "Iteration 7588, loss = 0.47128320\n",
      "Iteration 7589, loss = 0.47138338\n",
      "Iteration 7590, loss = 0.47132047\n",
      "Iteration 7591, loss = 0.47121732\n",
      "Iteration 7592, loss = 0.47140733\n",
      "Iteration 7593, loss = 0.47169273\n",
      "Iteration 7594, loss = 0.47185172\n",
      "Iteration 7595, loss = 0.47186906\n",
      "Iteration 7596, loss = 0.47131373\n",
      "Iteration 7597, loss = 0.47125824\n",
      "Iteration 7598, loss = 0.47120722\n",
      "Iteration 7599, loss = 0.47104030\n",
      "Iteration 7600, loss = 0.47100196\n",
      "Iteration 7601, loss = 0.47101541\n",
      "Iteration 7602, loss = 0.47111638\n",
      "Iteration 7603, loss = 0.47144314\n",
      "Iteration 7604, loss = 0.47159030\n",
      "Iteration 7605, loss = 0.47103818\n",
      "Iteration 7606, loss = 0.47112210\n",
      "Iteration 7607, loss = 0.47106676\n",
      "Iteration 7608, loss = 0.47104191\n",
      "Iteration 7609, loss = 0.47098812\n",
      "Iteration 7610, loss = 0.47134022\n",
      "Iteration 7611, loss = 0.47119565\n",
      "Iteration 7612, loss = 0.47108945\n",
      "Iteration 7613, loss = 0.47202584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7614, loss = 0.47146683\n",
      "Iteration 7615, loss = 0.47166204\n",
      "Iteration 7616, loss = 0.47118004\n",
      "Iteration 7617, loss = 0.47153787\n",
      "Iteration 7618, loss = 0.47120719\n",
      "Iteration 7619, loss = 0.47116712\n",
      "Iteration 7620, loss = 0.47103526\n",
      "Iteration 7621, loss = 0.47150103\n",
      "Iteration 7622, loss = 0.47136891\n",
      "Iteration 7623, loss = 0.47109459\n",
      "Iteration 7624, loss = 0.47115773\n",
      "Iteration 7625, loss = 0.47114216\n",
      "Iteration 7626, loss = 0.47112577\n",
      "Iteration 7627, loss = 0.47107101\n",
      "Iteration 7628, loss = 0.47138447\n",
      "Iteration 7629, loss = 0.47116157\n",
      "Iteration 7630, loss = 0.47120243\n",
      "Iteration 7631, loss = 0.47113908\n",
      "Iteration 7632, loss = 0.47130213\n",
      "Iteration 7633, loss = 0.47120561\n",
      "Iteration 7634, loss = 0.47117037\n",
      "Iteration 7635, loss = 0.47105110\n",
      "Iteration 7636, loss = 0.47103200\n",
      "Iteration 7637, loss = 0.47119135\n",
      "Iteration 7638, loss = 0.47119519\n",
      "Iteration 7639, loss = 0.47127978\n",
      "Iteration 7640, loss = 0.47111044\n",
      "Iteration 7641, loss = 0.47106246\n",
      "Iteration 7642, loss = 0.47139483\n",
      "Iteration 7643, loss = 0.47108392\n",
      "Iteration 7644, loss = 0.47103901\n",
      "Iteration 7645, loss = 0.47104610\n",
      "Iteration 7646, loss = 0.47105668\n",
      "Iteration 7647, loss = 0.47119736\n",
      "Iteration 7648, loss = 0.47113961\n",
      "Iteration 7649, loss = 0.47108252\n",
      "Iteration 7650, loss = 0.47123401\n",
      "Iteration 7651, loss = 0.47119228\n",
      "Iteration 7652, loss = 0.47109383\n",
      "Iteration 7653, loss = 0.47107976\n",
      "Iteration 7654, loss = 0.47136248\n",
      "Iteration 7655, loss = 0.47177356\n",
      "Iteration 7656, loss = 0.47164649\n",
      "Iteration 7657, loss = 0.47178076\n",
      "Iteration 7658, loss = 0.47131052\n",
      "Iteration 7659, loss = 0.47108669\n",
      "Iteration 7660, loss = 0.47102117\n",
      "Iteration 7661, loss = 0.47111277\n",
      "Iteration 7662, loss = 0.47113929\n",
      "Iteration 7663, loss = 0.47121587\n",
      "Iteration 7664, loss = 0.47101491\n",
      "Iteration 7665, loss = 0.47099670\n",
      "Iteration 7666, loss = 0.47105852\n",
      "Iteration 7667, loss = 0.47126104\n",
      "Iteration 7668, loss = 0.47136231\n",
      "Iteration 7669, loss = 0.47145136\n",
      "Iteration 7670, loss = 0.47159910\n",
      "Iteration 7671, loss = 0.47120661\n",
      "Iteration 7672, loss = 0.47136263\n",
      "Iteration 7673, loss = 0.47125118\n",
      "Iteration 7674, loss = 0.47106075\n",
      "Iteration 7675, loss = 0.47121594\n",
      "Iteration 7676, loss = 0.47120305\n",
      "Iteration 7677, loss = 0.47136408\n",
      "Iteration 7678, loss = 0.47135502\n",
      "Iteration 7679, loss = 0.47104893\n",
      "Iteration 7680, loss = 0.47095003\n",
      "Iteration 7681, loss = 0.47120004\n",
      "Iteration 7682, loss = 0.47117670\n",
      "Iteration 7683, loss = 0.47115488\n",
      "Iteration 7684, loss = 0.47111063\n",
      "Iteration 7685, loss = 0.47104756\n",
      "Iteration 7686, loss = 0.47113998\n",
      "Iteration 7687, loss = 0.47112115\n",
      "Iteration 7688, loss = 0.47133664\n",
      "Iteration 7689, loss = 0.47110586\n",
      "Iteration 7690, loss = 0.47088754\n",
      "Iteration 7691, loss = 0.47124945\n",
      "Iteration 7692, loss = 0.47139334\n",
      "Iteration 7693, loss = 0.47140579\n",
      "Iteration 7694, loss = 0.47105664\n",
      "Iteration 7695, loss = 0.47104369\n",
      "Iteration 7696, loss = 0.47144278\n",
      "Iteration 7697, loss = 0.47256221\n",
      "Iteration 7698, loss = 0.47284002\n",
      "Iteration 7699, loss = 0.47217891\n",
      "Iteration 7700, loss = 0.47160533\n",
      "Iteration 7701, loss = 0.47159041\n",
      "Iteration 7702, loss = 0.47155474\n",
      "Iteration 7703, loss = 0.47154055\n",
      "Iteration 7704, loss = 0.47120513\n",
      "Iteration 7705, loss = 0.47099037\n",
      "Iteration 7706, loss = 0.47140496\n",
      "Iteration 7707, loss = 0.47144703\n",
      "Iteration 7708, loss = 0.47114858\n",
      "Iteration 7709, loss = 0.47126695\n",
      "Iteration 7710, loss = 0.47116454\n",
      "Iteration 7711, loss = 0.47125960\n",
      "Iteration 7712, loss = 0.47116548\n",
      "Iteration 7713, loss = 0.47137287\n",
      "Iteration 7714, loss = 0.47146104\n",
      "Iteration 7715, loss = 0.47136443\n",
      "Iteration 7716, loss = 0.47125094\n",
      "Iteration 7717, loss = 0.47096253\n",
      "Iteration 7718, loss = 0.47102055\n",
      "Iteration 7719, loss = 0.47115834\n",
      "Iteration 7720, loss = 0.47113181\n",
      "Iteration 7721, loss = 0.47125600\n",
      "Iteration 7722, loss = 0.47130902\n",
      "Iteration 7723, loss = 0.47121487\n",
      "Iteration 7724, loss = 0.47130458\n",
      "Iteration 7725, loss = 0.47132285\n",
      "Iteration 7726, loss = 0.47142521\n",
      "Iteration 7727, loss = 0.47155554\n",
      "Iteration 7728, loss = 0.47140765\n",
      "Iteration 7729, loss = 0.47161950\n",
      "Iteration 7730, loss = 0.47175280\n",
      "Iteration 7731, loss = 0.47137992\n",
      "Iteration 7732, loss = 0.47103736\n",
      "Iteration 7733, loss = 0.47126998\n",
      "Iteration 7734, loss = 0.47133352\n",
      "Iteration 7735, loss = 0.47141743\n",
      "Iteration 7736, loss = 0.47166159\n",
      "Iteration 7737, loss = 0.47142665\n",
      "Iteration 7738, loss = 0.47101760\n",
      "Iteration 7739, loss = 0.47131367\n",
      "Iteration 7740, loss = 0.47146675\n",
      "Iteration 7741, loss = 0.47140384\n",
      "Iteration 7742, loss = 0.47119872\n",
      "Iteration 7743, loss = 0.47128706\n",
      "Iteration 7744, loss = 0.47126156\n",
      "Iteration 7745, loss = 0.47134438\n",
      "Iteration 7746, loss = 0.47134171\n",
      "Iteration 7747, loss = 0.47135445\n",
      "Iteration 7748, loss = 0.47137273\n",
      "Iteration 7749, loss = 0.47133679\n",
      "Iteration 7750, loss = 0.47147859\n",
      "Iteration 7751, loss = 0.47115846\n",
      "Iteration 7752, loss = 0.47119314\n",
      "Iteration 7753, loss = 0.47145645\n",
      "Iteration 7754, loss = 0.47127838\n",
      "Iteration 7755, loss = 0.47127715\n",
      "Iteration 7756, loss = 0.47116756\n",
      "Iteration 7757, loss = 0.47117268\n",
      "Iteration 7758, loss = 0.47132670\n",
      "Iteration 7759, loss = 0.47132362\n",
      "Iteration 7760, loss = 0.47142497\n",
      "Iteration 7761, loss = 0.47127119\n",
      "Iteration 7762, loss = 0.47126194\n",
      "Iteration 7763, loss = 0.47112954\n",
      "Iteration 7764, loss = 0.47114674\n",
      "Iteration 7765, loss = 0.47125828\n",
      "Iteration 7766, loss = 0.47136647\n",
      "Iteration 7767, loss = 0.47124051\n",
      "Iteration 7768, loss = 0.47134640\n",
      "Iteration 7769, loss = 0.47191675\n",
      "Iteration 7770, loss = 0.47117346\n",
      "Iteration 7771, loss = 0.47122309\n",
      "Iteration 7772, loss = 0.47114182\n",
      "Iteration 7773, loss = 0.47107421\n",
      "Iteration 7774, loss = 0.47113353\n",
      "Iteration 7775, loss = 0.47141001\n",
      "Iteration 7776, loss = 0.47124862\n",
      "Iteration 7777, loss = 0.47128416\n",
      "Iteration 7778, loss = 0.47113739\n",
      "Iteration 7779, loss = 0.47120139\n",
      "Iteration 7780, loss = 0.47115795\n",
      "Iteration 7781, loss = 0.47124599\n",
      "Iteration 7782, loss = 0.47110516\n",
      "Iteration 7783, loss = 0.47102929\n",
      "Iteration 7784, loss = 0.47110080\n",
      "Iteration 7785, loss = 0.47134395\n",
      "Iteration 7786, loss = 0.47155645\n",
      "Iteration 7787, loss = 0.47177028\n",
      "Iteration 7788, loss = 0.47191345\n",
      "Iteration 7789, loss = 0.47114875\n",
      "Iteration 7790, loss = 0.47122729\n",
      "Iteration 7791, loss = 0.47112512\n",
      "Iteration 7792, loss = 0.47130128\n",
      "Iteration 7793, loss = 0.47152252\n",
      "Iteration 7794, loss = 0.47112663\n",
      "Iteration 7795, loss = 0.47122248\n",
      "Iteration 7796, loss = 0.47131175\n",
      "Iteration 7797, loss = 0.47209857\n",
      "Iteration 7798, loss = 0.47193693\n",
      "Iteration 7799, loss = 0.47141416\n",
      "Iteration 7800, loss = 0.47113754\n",
      "Iteration 7801, loss = 0.47106084\n",
      "Iteration 7802, loss = 0.47112236\n",
      "Iteration 7803, loss = 0.47142454\n",
      "Iteration 7804, loss = 0.47163151\n",
      "Iteration 7805, loss = 0.47158758\n",
      "Iteration 7806, loss = 0.47128376\n",
      "Iteration 7807, loss = 0.47139679\n",
      "Iteration 7808, loss = 0.47117816\n",
      "Iteration 7809, loss = 0.47132661\n",
      "Iteration 7810, loss = 0.47107032\n",
      "Iteration 7811, loss = 0.47137174\n",
      "Iteration 7812, loss = 0.47143175\n",
      "Iteration 7813, loss = 0.47139659\n",
      "Iteration 7814, loss = 0.47127903\n",
      "Iteration 7815, loss = 0.47113493\n",
      "Iteration 7816, loss = 0.47112163\n",
      "Iteration 7817, loss = 0.47110013\n",
      "Iteration 7818, loss = 0.47119012\n",
      "Iteration 7819, loss = 0.47141594\n",
      "Iteration 7820, loss = 0.47186880\n",
      "Iteration 7821, loss = 0.47134902\n",
      "Iteration 7822, loss = 0.47096999\n",
      "Iteration 7823, loss = 0.47135092\n",
      "Iteration 7824, loss = 0.47159586\n",
      "Iteration 7825, loss = 0.47169933\n",
      "Iteration 7826, loss = 0.47137282\n",
      "Iteration 7827, loss = 0.47122214\n",
      "Iteration 7828, loss = 0.47107855\n",
      "Iteration 7829, loss = 0.47125502\n",
      "Iteration 7830, loss = 0.47144079\n",
      "Iteration 7831, loss = 0.47158313\n",
      "Iteration 7832, loss = 0.47132755\n",
      "Iteration 7833, loss = 0.47123340\n",
      "Iteration 7834, loss = 0.47146888\n",
      "Iteration 7835, loss = 0.47112259\n",
      "Iteration 7836, loss = 0.47120123\n",
      "Iteration 7837, loss = 0.47133912\n",
      "Iteration 7838, loss = 0.47140971\n",
      "Iteration 7839, loss = 0.47140919\n",
      "Iteration 7840, loss = 0.47119678\n",
      "Iteration 7841, loss = 0.47099822\n",
      "Iteration 7842, loss = 0.47177709\n",
      "Iteration 7843, loss = 0.47127947\n",
      "Iteration 7844, loss = 0.47104788\n",
      "Iteration 7845, loss = 0.47104389\n",
      "Iteration 7846, loss = 0.47130376\n",
      "Iteration 7847, loss = 0.47126675\n",
      "Iteration 7848, loss = 0.47119324\n",
      "Iteration 7849, loss = 0.47116532\n",
      "Iteration 7850, loss = 0.47113854\n",
      "Iteration 7851, loss = 0.47121441\n",
      "Iteration 7852, loss = 0.47124084\n",
      "Iteration 7853, loss = 0.47104567\n",
      "Iteration 7854, loss = 0.47116880\n",
      "Iteration 7855, loss = 0.47116154\n",
      "Iteration 7856, loss = 0.47102156\n",
      "Iteration 7857, loss = 0.47120873\n",
      "Iteration 7858, loss = 0.47139629\n",
      "Iteration 7859, loss = 0.47115242\n",
      "Iteration 7860, loss = 0.47122539\n",
      "Iteration 7861, loss = 0.47138835\n",
      "Iteration 7862, loss = 0.47140550\n",
      "Iteration 7863, loss = 0.47150472\n",
      "Iteration 7864, loss = 0.47135862\n",
      "Iteration 7865, loss = 0.47110289\n",
      "Iteration 7866, loss = 0.47088351\n",
      "Iteration 7867, loss = 0.47170745\n",
      "Iteration 7868, loss = 0.47143558\n",
      "Iteration 7869, loss = 0.47137288\n",
      "Iteration 7870, loss = 0.47100954\n",
      "Iteration 7871, loss = 0.47102296\n",
      "Iteration 7872, loss = 0.47107980\n",
      "Iteration 7873, loss = 0.47119262\n",
      "Iteration 7874, loss = 0.47118455\n",
      "Iteration 7875, loss = 0.47100298\n",
      "Iteration 7876, loss = 0.47097829\n",
      "Iteration 7877, loss = 0.47108670\n",
      "Iteration 7878, loss = 0.47141455\n",
      "Iteration 7879, loss = 0.47168121\n",
      "Iteration 7880, loss = 0.47124210\n",
      "Iteration 7881, loss = 0.47119935\n",
      "Iteration 7882, loss = 0.47134272\n",
      "Iteration 7883, loss = 0.47109358\n",
      "Iteration 7884, loss = 0.47120343\n",
      "Iteration 7885, loss = 0.47108783\n",
      "Iteration 7886, loss = 0.47127132\n",
      "Iteration 7887, loss = 0.47137709\n",
      "Iteration 7888, loss = 0.47187722\n",
      "Iteration 7889, loss = 0.47159164\n",
      "Iteration 7890, loss = 0.47149663\n",
      "Iteration 7891, loss = 0.47121097\n",
      "Iteration 7892, loss = 0.47114555\n",
      "Iteration 7893, loss = 0.47108497\n",
      "Iteration 7894, loss = 0.47100123\n",
      "Iteration 7895, loss = 0.47143562\n",
      "Iteration 7896, loss = 0.47152276\n",
      "Iteration 7897, loss = 0.47130524\n",
      "Iteration 7898, loss = 0.47206136\n",
      "Iteration 7899, loss = 0.47097922\n",
      "Iteration 7900, loss = 0.47117636\n",
      "Iteration 7901, loss = 0.47107085\n",
      "Iteration 7902, loss = 0.47098139\n",
      "Iteration 7903, loss = 0.47139725\n",
      "Iteration 7904, loss = 0.47108903\n",
      "Iteration 7905, loss = 0.47104758\n",
      "Iteration 7906, loss = 0.47112994\n",
      "Iteration 7907, loss = 0.47132036\n",
      "Iteration 7908, loss = 0.47118913\n",
      "Iteration 7909, loss = 0.47110066\n",
      "Iteration 7910, loss = 0.47106353\n",
      "Iteration 7911, loss = 0.47107435\n",
      "Iteration 7912, loss = 0.47113038\n",
      "Iteration 7913, loss = 0.47103669\n",
      "Iteration 7914, loss = 0.47126988\n",
      "Iteration 7915, loss = 0.47096889\n",
      "Iteration 7916, loss = 0.47157900\n",
      "Iteration 7917, loss = 0.47126403\n",
      "Iteration 7918, loss = 0.47105810\n",
      "Iteration 7919, loss = 0.47092504\n",
      "Iteration 7920, loss = 0.47099483\n",
      "Iteration 7921, loss = 0.47139266\n",
      "Iteration 7922, loss = 0.47118984\n",
      "Iteration 7923, loss = 0.47088036\n",
      "Iteration 7924, loss = 0.47160052\n",
      "Iteration 7925, loss = 0.47114302\n",
      "Iteration 7926, loss = 0.47114779\n",
      "Iteration 7927, loss = 0.47111840\n",
      "Iteration 7928, loss = 0.47138598\n",
      "Iteration 7929, loss = 0.47202125\n",
      "Iteration 7930, loss = 0.47154015\n",
      "Iteration 7931, loss = 0.47118640\n",
      "Iteration 7932, loss = 0.47094039\n",
      "Iteration 7933, loss = 0.47139530\n",
      "Iteration 7934, loss = 0.47171399\n",
      "Iteration 7935, loss = 0.47165206\n",
      "Iteration 7936, loss = 0.47140469\n",
      "Iteration 7937, loss = 0.47107744\n",
      "Iteration 7938, loss = 0.47224989\n",
      "Iteration 7939, loss = 0.47137393\n",
      "Iteration 7940, loss = 0.47084996\n",
      "Iteration 7941, loss = 0.47108755\n",
      "Iteration 7942, loss = 0.47146341\n",
      "Iteration 7943, loss = 0.47167713\n",
      "Iteration 7944, loss = 0.47160162\n",
      "Iteration 7945, loss = 0.47166219\n",
      "Iteration 7946, loss = 0.47128120\n",
      "Iteration 7947, loss = 0.47120114\n",
      "Iteration 7948, loss = 0.47103552\n",
      "Iteration 7949, loss = 0.47115346\n",
      "Iteration 7950, loss = 0.47113391\n",
      "Iteration 7951, loss = 0.47106693\n",
      "Iteration 7952, loss = 0.47094546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7953, loss = 0.47082509\n",
      "Iteration 7954, loss = 0.47159874\n",
      "Iteration 7955, loss = 0.47213488\n",
      "Iteration 7956, loss = 0.47182154\n",
      "Iteration 7957, loss = 0.47137406\n",
      "Iteration 7958, loss = 0.47104145\n",
      "Iteration 7959, loss = 0.47111048\n",
      "Iteration 7960, loss = 0.47144528\n",
      "Iteration 7961, loss = 0.47183115\n",
      "Iteration 7962, loss = 0.47172603\n",
      "Iteration 7963, loss = 0.47134114\n",
      "Iteration 7964, loss = 0.47114520\n",
      "Iteration 7965, loss = 0.47099054\n",
      "Iteration 7966, loss = 0.47118717\n",
      "Iteration 7967, loss = 0.47112039\n",
      "Iteration 7968, loss = 0.47120264\n",
      "Iteration 7969, loss = 0.47110004\n",
      "Iteration 7970, loss = 0.47093365\n",
      "Iteration 7971, loss = 0.47128420\n",
      "Iteration 7972, loss = 0.47126024\n",
      "Iteration 7973, loss = 0.47126772\n",
      "Iteration 7974, loss = 0.47125486\n",
      "Iteration 7975, loss = 0.47103977\n",
      "Iteration 7976, loss = 0.47115221\n",
      "Iteration 7977, loss = 0.47110535\n",
      "Iteration 7978, loss = 0.47095498\n",
      "Iteration 7979, loss = 0.47095023\n",
      "Iteration 7980, loss = 0.47200883\n",
      "Iteration 7981, loss = 0.47207848\n",
      "Iteration 7982, loss = 0.47242405\n",
      "Iteration 7983, loss = 0.47135453\n",
      "Iteration 7984, loss = 0.47126057\n",
      "Iteration 7985, loss = 0.47138646\n",
      "Iteration 7986, loss = 0.47135880\n",
      "Iteration 7987, loss = 0.47153136\n",
      "Iteration 7988, loss = 0.47145928\n",
      "Iteration 7989, loss = 0.47142697\n",
      "Iteration 7990, loss = 0.47118719\n",
      "Iteration 7991, loss = 0.47124429\n",
      "Iteration 7992, loss = 0.47105002\n",
      "Iteration 7993, loss = 0.47113406\n",
      "Iteration 7994, loss = 0.47135743\n",
      "Iteration 7995, loss = 0.47168855\n",
      "Iteration 7996, loss = 0.47105059\n",
      "Iteration 7997, loss = 0.47094610\n",
      "Iteration 7998, loss = 0.47095100\n",
      "Iteration 7999, loss = 0.47137474\n",
      "Iteration 8000, loss = 0.47139975\n",
      "Iteration 8001, loss = 0.47117798\n",
      "Iteration 8002, loss = 0.47077885\n",
      "Iteration 8003, loss = 0.47099540\n",
      "Iteration 8004, loss = 0.47156068\n",
      "Iteration 8005, loss = 0.47176031\n",
      "Iteration 8006, loss = 0.47156742\n",
      "Iteration 8007, loss = 0.47110056\n",
      "Iteration 8008, loss = 0.47090815\n",
      "Iteration 8009, loss = 0.47126810\n",
      "Iteration 8010, loss = 0.47119489\n",
      "Iteration 8011, loss = 0.47113191\n",
      "Iteration 8012, loss = 0.47103560\n",
      "Iteration 8013, loss = 0.47100787\n",
      "Iteration 8014, loss = 0.47096687\n",
      "Iteration 8015, loss = 0.47098982\n",
      "Iteration 8016, loss = 0.47106538\n",
      "Iteration 8017, loss = 0.47112160\n",
      "Iteration 8018, loss = 0.47098477\n",
      "Iteration 8019, loss = 0.47107304\n",
      "Iteration 8020, loss = 0.47151313\n",
      "Iteration 8021, loss = 0.47124270\n",
      "Iteration 8022, loss = 0.47110789\n",
      "Iteration 8023, loss = 0.47100985\n",
      "Iteration 8024, loss = 0.47111835\n",
      "Iteration 8025, loss = 0.47124524\n",
      "Iteration 8026, loss = 0.47125616\n",
      "Iteration 8027, loss = 0.47114836\n",
      "Iteration 8028, loss = 0.47115802\n",
      "Iteration 8029, loss = 0.47180732\n",
      "Iteration 8030, loss = 0.47120552\n",
      "Iteration 8031, loss = 0.47124145\n",
      "Iteration 8032, loss = 0.47116479\n",
      "Iteration 8033, loss = 0.47130630\n",
      "Iteration 8034, loss = 0.47127495\n",
      "Iteration 8035, loss = 0.47109308\n",
      "Iteration 8036, loss = 0.47106347\n",
      "Iteration 8037, loss = 0.47108702\n",
      "Iteration 8038, loss = 0.47113362\n",
      "Iteration 8039, loss = 0.47105450\n",
      "Iteration 8040, loss = 0.47139191\n",
      "Iteration 8041, loss = 0.47101520\n",
      "Iteration 8042, loss = 0.47094412\n",
      "Iteration 8043, loss = 0.47181458\n",
      "Iteration 8044, loss = 0.47116797\n",
      "Iteration 8045, loss = 0.47124478\n",
      "Iteration 8046, loss = 0.47105242\n",
      "Iteration 8047, loss = 0.47099183\n",
      "Iteration 8048, loss = 0.47119189\n",
      "Iteration 8049, loss = 0.47151561\n",
      "Iteration 8050, loss = 0.47102439\n",
      "Iteration 8051, loss = 0.47102974\n",
      "Iteration 8052, loss = 0.47104481\n",
      "Iteration 8053, loss = 0.47093581\n",
      "Iteration 8054, loss = 0.47097278\n",
      "Iteration 8055, loss = 0.47105253\n",
      "Iteration 8056, loss = 0.47122550\n",
      "Iteration 8057, loss = 0.47130103\n",
      "Iteration 8058, loss = 0.47129270\n",
      "Iteration 8059, loss = 0.47098026\n",
      "Iteration 8060, loss = 0.47093989\n",
      "Iteration 8061, loss = 0.47120388\n",
      "Iteration 8062, loss = 0.47147566\n",
      "Training loss did not improve more than tol=0.000100 for 2000 consecutive epochs. Stopping.\n",
      "Training: accuracy 0.8154\n",
      "Testing: accuracy: 0.7333\n",
      "\n",
      "relu\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "NNC = MLPClassifier(solver='adam', random_state=0, max_iter=100000,n_iter_no_change=2000, verbose=1)\n",
    "param_grid = {'hidden_layer_sizes': [(16,32,8,4)],\n",
    "              'learning_rate_init':[1e-3],\n",
    "              'alpha':[0.1],\n",
    "              'activation':['relu']}\n",
    "nn = GridSearchCV(estimator=NNC,\n",
    "                  param_grid=param_grid,\n",
    "                  cv=5)\n",
    "nn.fit(new_data, labels)\n",
    "\n",
    "nn_model = nn.best_estimator_\n",
    "print(nn_model)\n",
    "nn_model.fit(new_data, labels)\n",
    "\n",
    "score = nn_model.score(new_data, labels)\n",
    "print(\"Training: accuracy %0.4f\" % (score))\n",
    "score = nn_model.score(new_test_data, test_labels)\n",
    "print(\"Testing: accuracy: %0.4f\\n\" % (score))\n",
    "\n",
    "print(nn_model.activation)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\n",
    "ax[0].plot(nn_model.loss_curve_)\n",
    "ax[0].set_xlabel('Number of iterations')\n",
    "ax[0].set_ylabel('Loss')\n",
    "plt.ylim(0,1)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edf90682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time:  6.578357934951782\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model = MLPClassifier(solver='adam', hidden_layer_sizes=(16,32,8,4), alpha=4, n_iter_no_change=2000, early_stopping=True, learning_rate_init=0.0001,random_state=0, max_iter=200000)\n",
    "start = time.time()\n",
    "nn_model.fit(data, labels)\n",
    "stop = time.time()\n",
    "print(\"Training Time: \", (stop-start))\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\n",
    "ax[0].plot(nn_model.loss_curve_)\n",
    "ax[0].set_xlabel('Number of iterations')\n",
    "ax[0].set_ylabel('Loss')\n",
    "plt.ylim(0,1)\n",
    "\n",
    "\n",
    "ax[1].plot(nn_model.validation_scores_)\n",
    "ax[1].set_xlabel('Number of iterations')\n",
    "ax[1].set_ylabel('Accuracy')\n",
    "\n",
    "plt.ylim(0,1)\n",
    "plt.show()\n",
    "nn_model.validation_scores_[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38af336e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
